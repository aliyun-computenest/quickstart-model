{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"Flux/1Krea/doc/","text":"\ud83c\udfa8 Flux.1 Krea Dev \u56fe\u50cf\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u72ec\u7279\u7f8e\u5b66\u98ce\u683c\u4e0e\u81ea\u7136\u7ec6\u8282\u7684\u5b8c\u7f8e\u7ed3\u5408 \ud83c\udfaf \u72ec\u7279\u7f8e\u5b66 \u2728 \u81ea\u7136\u7ec6\u8282 \ud83c\udfc6 \u5353\u8d8a\u771f\u5b9e\u611f \ud83d\udccb Flux.1 Krea Dev \u6a21\u578b\u6982\u89c8 [Flux.1 Krea Dev](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev) \u662f\u7531 Black Forest Labs (BFL) \u4e0e Krea \u5408\u4f5c\u5f00\u53d1\u7684\u5148\u8fdb\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u3002\u8fd9\u662f\u76ee\u524d\u6700\u597d\u7684\u5f00\u6e90\u6743\u91cd FLUX \u6a21\u578b\uff0c\u4e13\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u800c\u8bbe\u8ba1\u3002 \ud83c\udfaf \u72ec\u7279\u7f8e\u5b66\u98ce\u683c \u4e13\u6ce8\u4e8e\u751f\u6210\u5177\u6709\u72ec\u7279\u7f8e\u5b66\u7684\u56fe\u50cf\uff0c\u907f\u514d\u5e38\u89c1\u7684\"AI\u611f\"\u5916\u89c2 \u2728 \u81ea\u7136\u7ec6\u8282 \u4e0d\u4f1a\u4ea7\u751f\u8fc7\u66dd\u7684\u9ad8\u5149\uff0c\u4fdd\u6301\u81ea\u7136\u7684\u7ec6\u8282\u8868\u73b0 \ud83c\udfc6 \u5353\u8d8a\u7684\u771f\u5b9e\u611f \u63d0\u4f9b\u51fa\u8272\u7684\u771f\u5b9e\u611f\u548c\u56fe\u50cf\u8d28\u91cf \ud83d\udd27 \u5b8c\u5168\u517c\u5bb9\u67b6\u6784 \u4e0e FLUX.1 [dev] \u5b8c\u5168\u517c\u5bb9\u7684\u67b6\u6784\u8bbe\u8ba1 \ud83d\udcc4 \u6a21\u578b\u8bb8\u53ef \u8be5\u6a21\u578b\u91c7\u7528 flux-1-dev-non-commercial-license \u8bb8\u53ef\u53d1\u5e03\uff0c\u4ec5\u9650\u975e\u5546\u4e1a\u7528\u9014\u3002 \ud83d\ude80 Flux.1 Krea Dev ComfyUI \u5de5\u4f5c\u6d41 \u26a0\ufe0f \u73af\u5883\u8981\u6c42 \ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c \u2022 \u63a8\u8350\u4f7f\u7528\u6700\u65b0\u5f00\u53d1\u7248\uff08nightly\uff09\u83b7\u5f97\u5b8c\u6574\u529f\u80fd \u2022 \u672c\u6307\u5357\u7684\u5de5\u4f5c\u6d41\u53ef\u5728 ComfyUI \u7684\u5de5\u4f5c\u6d41\u6a21\u677f\u4e2d\u627e\u5230 \u2022 \u5982\u679c\u52a0\u8f7d\u5de5\u4f5c\u6d41\u65f6\u6709\u8282\u70b9\u7f3a\u5931\uff0c\u8bf7\u68c0\u67e5 ComfyUI \u7248\u672c\u6216\u8282\u70b9\u5bfc\u5165\u72b6\u6001 \ud83d\udce5 \u4e0b\u8f7d\u94fe\u63a5 ComfyUI \u4e0b\u8f7d ComfyUI \u66f4\u65b0\u6559\u7a0b \u5de5\u4f5c\u6d41\u6a21\u677f \ud83d\udd27 \u5e38\u89c1\u95ee\u9898 \u8282\u70b9\u7f3a\u5931\uff1a\u7248\u672c\u8fc7\u65e7\u6216\u5bfc\u5165\u5931\u8d25 \u529f\u80fd\u4e0d\u5168\uff1a\u4f7f\u7528\u7a33\u5b9a\u7248\u800c\u975e\u5f00\u53d1\u7248 \u52a0\u8f7d\u5931\u8d25\uff1a\u542f\u52a8\u65f6\u8282\u70b9\u5bfc\u5165\u5f02\u5e38 \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d \u4ece\u6a21\u7248\u6253\u5f00\uff1a ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u56fe\u7247\u6216 JSON \u6587\u4ef6\uff0c\u5e76\u62d6\u5165 ComfyUI \u4ee5\u52a0\u8f7d\u5bf9\u5e94\u5de5\u4f5c\u6d41\u3002 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 #### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 flux1-krea-dev_fp8_scaled.safetensors \u6216 flux1-krea-dev.safetensors \u2502 \u251c\u2500\u2500 text_encoders/ \u2502 \u2502 \u251c\u2500\u2500 clip_l.safetensors \u2502 \u2502 \u2514\u2500\u2500 t5xxl_fp16.safetensors \u6216 t5xxl_fp8_e4m3fn.safetensors \u2502 \u2514\u2500\u2500 vae/ \u2502 \u2514\u2500\u2500 ae.safetensors #### \ud83d\udca1 \u6a21\u578b\u7248\u672c\u9009\u62e9\u5efa\u8bae \ud83d\udc9a FP8 \u91cf\u5316\u7248\u672c \u663e\u5b58\u9700\u6c42\uff1a12-16GB \u9002\u5408\uff1aRTX 4060 Ti 16GB\u3001RTX 4070 \u7b49 \u8d28\u91cf\uff1a\u63a5\u8fd1\u539f\u7248\uff0c\u663e\u5b58\u53cb\u597d \ud83d\udd25 \u539f\u59cb\u6743\u91cd\u7248\u672c \u663e\u5b58\u9700\u6c42\uff1a24GB+ \u9002\u5408\uff1aRTX 4090\u3001A100 \u7b49 \u8d28\u91cf\uff1a\u6700\u9ad8\u8d28\u91cf\u8f93\u51fa \u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u2022 flux1-krea-dev.safetensors \u6587\u4ef6\u9700\u8981\u540c\u610f black-forest-labs/FLUX.1-Krea-dev \u7684\u534f\u8bae\u540e\u624d\u80fd\u4e0b\u8f7d \u2022 \u5982\u679c\u4f60\u4f7f\u7528\u8fc7\u5176\u4ed6 Flux \u5de5\u4f5c\u6d41\uff0cText Encoders \u548c VAE \u6587\u4ef6\u53ef\u4ee5\u590d\u7528\uff0c\u65e0\u9700\u91cd\u590d\u4e0b\u8f7d \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c \u26a0\ufe0f \u4f4e\u663e\u5b58\u7528\u6237\u63d0\u9192 \u5bf9\u4e8e\u4f4e\u663e\u5b58\u7528\u6237\uff0c\u8fd9\u4e2a\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u5728\u4f60\u7684\u8bbe\u5907\u4e0a\u987a\u5229\u8fd0\u884c\uff0c\u5efa\u8bae\u7b49\u5f85\u793e\u533a\u63d0\u4f9b\u66f4\u591a\u4f18\u5316\u7248\u672c\u6216\u4f7f\u7528 FP8 \u91cf\u5316\u7248\u672c\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 Diffusion Model \u52a0\u8f7d \u5728 Load Diffusion Model \u8282\u70b9\u4e2d\u9009\u62e9\uff1a flux1-krea-dev_fp8_scaled.safetensors \uff08\u63a8\u8350\uff09 \u6216 flux1-krea-dev.safetensors \uff08\u9ad8\u663e\u5b58\uff09 \ud83d\udcdd Text Encoders \u914d\u7f6e \u5728 DualCLIPLoader \u8282\u70b9\u4e2d\u786e\u4fdd\uff1a clip_name1 : t5xxl_fp16.safetensors \u6216 t5xxl_fp8_e4m3fn.safetensors clip_name2 : clip_l.safetensors \ud83c\udfa8 VAE \u52a0\u8f7d \u5728 Load VAE \u8282\u70b9\u4e2d\u52a0\u8f7d ae.safetensors \ud83d\udcdd \u63d0\u793a\u8bcd\u8bbe\u7f6e \u5728 CLIP Text Encode \u8282\u70b9\u4e2d\u8f93\u5165\u4f60\u7684\u521b\u610f\u63d0\u793a\u8bcd #### \ud83c\udf9b\ufe0f Text Encoder \u9009\u62e9\u6307\u5357 \u663e\u5b58\u5bb9\u91cf \u63a8\u8350 T5 \u7248\u672c \u8bf4\u660e < 16GB t5xxl_fp8_e4m3fn.safetensors \u4f4e\u663e\u5b58\u4f18\u5316\u7248\u672c 16-24GB t5xxl_fp8_e4m3fn.safetensors \u5e73\u8861\u6027\u80fd\u4e0e\u8d28\u91cf \u2265 32GB t5xxl_fp16.safetensors \u6700\u4f73\u8d28\u91cf\u4f53\u9a8c #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Flux Krea Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local UNET_MODEL = \"flux1-krea-dev_fp8_scaled.safetensors\" VAE_MODEL = \"flux-ae.safetensors\" CLIP_L_MODEL = \"clip_l_bf16.safetensors\" CLIP_T5_MODEL = \"t5xxl_fp16.safetensors\" # Default Parameters DEFAULT_PROMPT = \"Highly realistic portrait of a Nordic woman with blonde hair and blue eyes, very few freckles on her face, gaze sharp and intellectual. The lighting should reflect the unique coolness of Northern Europe. Outfit is minimalist and modern, background is blurred in cool tones. Needs to perfectly capture the characteristics of a Scandinavian woman. solo, Centered composition\" class ComfyUIFluxKreaClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_flux_krea_image(self, prompt, steps=20, cfg=1, width=1024, height=1024, seed=None): \"\"\"Generate Flux Krea text-to-image based on original JSON workflow\"\"\" print(\"Starting Flux Krea text-to-image generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Workflow based on your provided JSON workflow = { \"8\": { \"inputs\": { \"samples\": [\"31\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"9\": { \"inputs\": { \"filename_prefix\": \"flux_krea/flux_krea\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"27\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": {\"title\": \"Empty SD3 Latent Image\"} }, \"31\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"38\", 0], \"positive\": [\"45\", 0], \"negative\": [\"42\", 0], \"latent_image\": [\"27\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"38\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"40\": { \"inputs\": { \"clip_name1\": CLIP_L_MODEL, \"clip_name2\": CLIP_T5_MODEL, \"type\": \"flux\", \"device\": \"default\" }, \"class_type\": \"DualCLIPLoader\", \"_meta\": {\"title\": \"Dual CLIP Loader\"} }, \"42\": { \"inputs\": { \"conditioning\": [\"45\", 0] }, \"class_type\": \"ConditioningZeroOut\", \"_meta\": {\"title\": \"Conditioning Zero Out\"} }, \"45\": { \"inputs\": { \"text\": prompt, \"clip\": [\"40\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode\"} } } print(\"Submitting Flux Krea text-to-image workflow...\") print(f\"Prompt: {prompt}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompts_list, **kwargs): \"\"\"Batch generate images\"\"\" results = [] for i, prompt in enumerate(prompts_list): print(f\"\\nStarting task {i+1}/{len(prompts_list)}...\") try: task_id, seed = self.generate_flux_krea_image(prompt, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'prompt': prompt }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_variations(self, base_prompt, variations, **kwargs): \"\"\"Generate images with prompt variations\"\"\" prompts = [f\"{base_prompt}, {variation}\" for variation in variations] return self.generate_batch(prompts, **kwargs) def main(): \"\"\"Main function - Execute Flux Krea text-to-image generation\"\"\" client = ComfyUIFluxKreaClient() try: print(\"Flux Krea text-to-image client started...\") # Single image generation example print(\"\\n=== Single Image Generation ===\") task_id, seed = client.generate_flux_krea_image( prompt=DEFAULT_PROMPT, steps=20, cfg=1, width=1024, height=1024 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(5) # Download images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_prompts = [ \"Portrait of a young Asian woman with long black hair, wearing traditional kimono, cherry blossoms in background, soft lighting\", \"Professional headshot of a businessman in a navy suit, confident expression, office background, studio lighting\", \"Artistic portrait of an elderly man with a white beard, weathered face, dramatic side lighting, black and white\" ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_prompts, steps=20, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} images\") # Variation generation example print(\"\\n=== Variation Generation Example ===\") base_prompt = \"Portrait of a woman\" variations = [ \"smiling, warm lighting\", \"serious expression, dramatic lighting\", \"laughing, natural outdoor lighting\", \"contemplative, soft studio lighting\" ] # Uncomment to run variation generation # variation_results = client.generate_with_variations(base_prompt, variations, steps=15) # print(f\"Variation generation completed, generated {len(variation_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Flux.1 Krea Dev \u7279\u8272\u4f18\u52bf \ud83c\udfa8 \u827a\u672f\u7f8e\u5b66 \u72ec\u7279\u7684\u7f8e\u5b66\u98ce\u683c\uff0c\u907f\u514d\u5e38\u89c1\u7684 AI \u751f\u6210\u75d5\u8ff9 \ud83d\udcf8 \u6444\u5f71\u7ea7\u8d28\u611f \u81ea\u7136\u7684\u5149\u5f71\u5904\u7406\uff0c\u4e0d\u4f1a\u4ea7\u751f\u8fc7\u66dd\u7684\u9ad8\u5149\u6548\u679c \ud83c\udfc6 \u5f00\u6e90\u6700\u4f73 \u76ee\u524d\u6700\u597d\u7684\u5f00\u6e90 FLUX \u6a21\u578b\u6743\u91cd \ud83d\udd04 \u5b8c\u5168\u517c\u5bb9 \u4e0e\u73b0\u6709 FLUX.1 [dev] \u5de5\u4f5c\u6d41\u5b8c\u5168\u517c\u5bb9 \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u4f7f\u7528\u8be6\u7ec6\u3001\u5177\u4f53\u7684\u63d0\u793a\u8bcd\u63cf\u8ff0 \u5145\u5206\u5229\u7528\u6a21\u578b\u7684\u7f8e\u5b66\u98ce\u683c\u7279\u70b9 \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u7248\u672c \u5c1d\u8bd5\u4e0d\u540c\u7684\u91c7\u6837\u5668\u548c\u6b65\u6570\u7ec4\u5408 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd\u6709\u8db3\u591f\u7684\u663e\u5b58\u8fd0\u884c\u6a21\u578b \u539f\u59cb\u6743\u91cd\u7248\u672c\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 \u6ce8\u610f\u6a21\u578b\u7684\u975e\u5546\u4e1a\u8bb8\u53ef\u9650\u5236 \u9996\u6b21\u8fd0\u884c\u53ef\u80fd\u9700\u8981\u8f83\u957f\u52a0\u8f7d\u65f6\u95f4 \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 FP8 \u7248\u672c \u539f\u59cb\u6743\u91cd\u7248\u672c GPU \u663e\u5b58 12-16GB 24GB+ \u7cfb\u7edf\u5185\u5b58 16GB+ 32GB+ \u5b58\u50a8\u7a7a\u95f4 15GB 25GB \u63a8\u8350 GPU RTX 4060 Ti 16GB / RTX 4070 RTX 4090 / A100 ### \ud83c\udfa8 \u751f\u6210\u80fd\u529b \u4eba\u7269\u8096\u50cf \u98ce\u666f\u6444\u5f71 \u827a\u672f\u521b\u4f5c \u4ea7\u54c1\u8bbe\u8ba1 \u5efa\u7b51\u6e32\u67d3 \u6982\u5ff5\u827a\u672f \ud83c\udfaf \u5e94\u7528\u573a\u666f \ud83c\udfa8 \u827a\u672f\u521b\u4f5c \u6982\u5ff5\u827a\u672f\u3001\u63d2\u753b\u8bbe\u8ba1\u3001\u89c6\u89c9\u827a\u672f\u521b\u4f5c \ud83d\udcf1 \u5185\u5bb9\u521b\u4f5c \u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u3001\u535a\u5ba2\u914d\u56fe\u3001\u8425\u9500\u7d20\u6750 \ud83c\udfac \u5f71\u89c6\u5236\u4f5c \u5206\u955c\u5934\u8bbe\u8ba1\u3001\u6982\u5ff5\u9884\u89c8\u3001\u89c6\u89c9\u5f00\u53d1 \ud83d\udd2c \u7814\u7a76\u5b9e\u9a8c AI \u7814\u7a76\u3001\u6a21\u578b\u5bf9\u6bd4\u3001\u5b66\u672f\u7814\u7a76 \ud83c\udfa8 Flux.1 Krea Dev \u56fe\u50cf\u751f\u6210 | \u72ec\u7279\u7f8e\u5b66\u98ce\u683c\u4e0e\u81ea\u7136\u7ec6\u8282\u7684\u5b8c\u7f8e\u7ed3\u5408 \u00a9 2025 Black Forest Labs & Krea | \u975e\u5546\u4e1a\u8bb8\u53ef | \u8ba9 AI \u56fe\u50cf\u751f\u6210\u66f4\u5177\u827a\u672f\u7f8e\u611f","title":"Index"},{"location":"Flux/1Krea/doc/#flux1-krea-dev","text":"[Flux.1 Krea Dev](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev) \u662f\u7531 Black Forest Labs (BFL) \u4e0e Krea \u5408\u4f5c\u5f00\u53d1\u7684\u5148\u8fdb\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u3002\u8fd9\u662f\u76ee\u524d\u6700\u597d\u7684\u5f00\u6e90\u6743\u91cd FLUX \u6a21\u578b\uff0c\u4e13\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u800c\u8bbe\u8ba1\u3002 \ud83c\udfaf","title":"\ud83d\udccb Flux.1 Krea Dev \u6a21\u578b\u6982\u89c8"},{"location":"Flux/1Krea/doc/#flux1-krea-dev-comfyui","text":"","title":"\ud83d\ude80 Flux.1 Krea Dev ComfyUI \u5de5\u4f5c\u6d41"},{"location":"Flux/1Krea/doc/#_1","text":"\ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c \u2022 \u63a8\u8350\u4f7f\u7528\u6700\u65b0\u5f00\u53d1\u7248\uff08nightly\uff09\u83b7\u5f97\u5b8c\u6574\u529f\u80fd \u2022 \u672c\u6307\u5357\u7684\u5de5\u4f5c\u6d41\u53ef\u5728 ComfyUI \u7684\u5de5\u4f5c\u6d41\u6a21\u677f\u4e2d\u627e\u5230 \u2022 \u5982\u679c\u52a0\u8f7d\u5de5\u4f5c\u6d41\u65f6\u6709\u8282\u70b9\u7f3a\u5931\uff0c\u8bf7\u68c0\u67e5 ComfyUI \u7248\u672c\u6216\u8282\u70b9\u5bfc\u5165\u72b6\u6001","title":"\u26a0\ufe0f \u73af\u5883\u8981\u6c42"},{"location":"Flux/1Krea/doc/#_2","text":"\u4ece\u6a21\u7248\u6253\u5f00\uff1a ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u56fe\u7247\u6216 JSON \u6587\u4ef6\uff0c\u5e76\u62d6\u5165 ComfyUI \u4ee5\u52a0\u8f7d\u5bf9\u5e94\u5de5\u4f5c\u6d41\u3002 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d"},{"location":"Flux/1Krea/doc/#_3","text":"#### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 flux1-krea-dev_fp8_scaled.safetensors \u6216 flux1-krea-dev.safetensors \u2502 \u251c\u2500\u2500 text_encoders/ \u2502 \u2502 \u251c\u2500\u2500 clip_l.safetensors \u2502 \u2502 \u2514\u2500\u2500 t5xxl_fp16.safetensors \u6216 t5xxl_fp8_e4m3fn.safetensors \u2502 \u2514\u2500\u2500 vae/ \u2502 \u2514\u2500\u2500 ae.safetensors #### \ud83d\udca1 \u6a21\u578b\u7248\u672c\u9009\u62e9\u5efa\u8bae","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"Flux/1Krea/doc/#_4","text":"\u26a0\ufe0f \u4f4e\u663e\u5b58\u7528\u6237\u63d0\u9192 \u5bf9\u4e8e\u4f4e\u663e\u5b58\u7528\u6237\uff0c\u8fd9\u4e2a\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u5728\u4f60\u7684\u8bbe\u5907\u4e0a\u987a\u5229\u8fd0\u884c\uff0c\u5efa\u8bae\u7b49\u5f85\u793e\u533a\u63d0\u4f9b\u66f4\u591a\u4f18\u5316\u7248\u672c\u6216\u4f7f\u7528 FP8 \u91cf\u5316\u7248\u672c\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c"},{"location":"Flux/1Krea/doc/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Flux Krea Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local UNET_MODEL = \"flux1-krea-dev_fp8_scaled.safetensors\" VAE_MODEL = \"flux-ae.safetensors\" CLIP_L_MODEL = \"clip_l_bf16.safetensors\" CLIP_T5_MODEL = \"t5xxl_fp16.safetensors\" # Default Parameters DEFAULT_PROMPT = \"Highly realistic portrait of a Nordic woman with blonde hair and blue eyes, very few freckles on her face, gaze sharp and intellectual. The lighting should reflect the unique coolness of Northern Europe. Outfit is minimalist and modern, background is blurred in cool tones. Needs to perfectly capture the characteristics of a Scandinavian woman. solo, Centered composition\" class ComfyUIFluxKreaClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_flux_krea_image(self, prompt, steps=20, cfg=1, width=1024, height=1024, seed=None): \"\"\"Generate Flux Krea text-to-image based on original JSON workflow\"\"\" print(\"Starting Flux Krea text-to-image generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Workflow based on your provided JSON workflow = { \"8\": { \"inputs\": { \"samples\": [\"31\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"9\": { \"inputs\": { \"filename_prefix\": \"flux_krea/flux_krea\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"27\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": {\"title\": \"Empty SD3 Latent Image\"} }, \"31\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"38\", 0], \"positive\": [\"45\", 0], \"negative\": [\"42\", 0], \"latent_image\": [\"27\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"38\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"40\": { \"inputs\": { \"clip_name1\": CLIP_L_MODEL, \"clip_name2\": CLIP_T5_MODEL, \"type\": \"flux\", \"device\": \"default\" }, \"class_type\": \"DualCLIPLoader\", \"_meta\": {\"title\": \"Dual CLIP Loader\"} }, \"42\": { \"inputs\": { \"conditioning\": [\"45\", 0] }, \"class_type\": \"ConditioningZeroOut\", \"_meta\": {\"title\": \"Conditioning Zero Out\"} }, \"45\": { \"inputs\": { \"text\": prompt, \"clip\": [\"40\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode\"} } } print(\"Submitting Flux Krea text-to-image workflow...\") print(f\"Prompt: {prompt}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompts_list, **kwargs): \"\"\"Batch generate images\"\"\" results = [] for i, prompt in enumerate(prompts_list): print(f\"\\nStarting task {i+1}/{len(prompts_list)}...\") try: task_id, seed = self.generate_flux_krea_image(prompt, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'prompt': prompt }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_variations(self, base_prompt, variations, **kwargs): \"\"\"Generate images with prompt variations\"\"\" prompts = [f\"{base_prompt}, {variation}\" for variation in variations] return self.generate_batch(prompts, **kwargs) def main(): \"\"\"Main function - Execute Flux Krea text-to-image generation\"\"\" client = ComfyUIFluxKreaClient() try: print(\"Flux Krea text-to-image client started...\") # Single image generation example print(\"\\n=== Single Image Generation ===\") task_id, seed = client.generate_flux_krea_image( prompt=DEFAULT_PROMPT, steps=20, cfg=1, width=1024, height=1024 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(5) # Download images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_prompts = [ \"Portrait of a young Asian woman with long black hair, wearing traditional kimono, cherry blossoms in background, soft lighting\", \"Professional headshot of a businessman in a navy suit, confident expression, office background, studio lighting\", \"Artistic portrait of an elderly man with a white beard, weathered face, dramatic side lighting, black and white\" ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_prompts, steps=20, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} images\") # Variation generation example print(\"\\n=== Variation Generation Example ===\") base_prompt = \"Portrait of a woman\" variations = [ \"smiling, warm lighting\", \"serious expression, dramatic lighting\", \"laughing, natural outdoor lighting\", \"contemplative, soft studio lighting\" ] # Uncomment to run variation generation # variation_results = client.generate_with_variations(base_prompt, variations, steps=15) # print(f\"Variation generation completed, generated {len(variation_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"Flux/1Krea/doc/#flux1-krea-dev_1","text":"\ud83c\udfa8","title":"\ud83c\udfaf Flux.1 Krea Dev \u7279\u8272\u4f18\u52bf"},{"location":"Flux/1Krea/doc/#_5","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"Flux/1Krea/doc/#_6","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 FP8 \u7248\u672c \u539f\u59cb\u6743\u91cd\u7248\u672c GPU \u663e\u5b58 12-16GB 24GB+ \u7cfb\u7edf\u5185\u5b58 16GB+ 32GB+ \u5b58\u50a8\u7a7a\u95f4 15GB 25GB \u63a8\u8350 GPU RTX 4060 Ti 16GB / RTX 4070 RTX 4090 / A100 ### \ud83c\udfa8 \u751f\u6210\u80fd\u529b \u4eba\u7269\u8096\u50cf \u98ce\u666f\u6444\u5f71 \u827a\u672f\u521b\u4f5c \u4ea7\u54c1\u8bbe\u8ba1 \u5efa\u7b51\u6e32\u67d3 \u6982\u5ff5\u827a\u672f","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Flux/1Krea/doc/#_7","text":"\ud83c\udfa8","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f"},{"location":"Flux/1Krea/doc/index-en/","text":"\ud83c\udfa8 Flux.1 Krea Dev Image Generation ComfyUI Native Workflow - Perfect Combination of Unique Aesthetic Style and Natural Details \ud83c\udfaf Unique Aesthetics \u2728 Natural Details \ud83c\udfc6 Exceptional Realism \ud83d\udccb Flux.1 Krea Dev Model Overview [Flux.1 Krea Dev](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev) is an advanced text-to-image model developed through collaboration between Black Forest Labs (BFL) and Krea. This is currently the best open-source FLUX model, specifically designed for text-to-image generation. \ud83c\udfaf Unique Aesthetic Style Focuses on generating images with unique aesthetics, avoiding common \"AI-generated\" appearance \u2728 Natural Details Avoids overexposed highlights, maintaining natural detail representation \ud83c\udfc6 Exceptional Realism Provides outstanding realism and image quality \ud83d\udd27 Fully Compatible Architecture Completely compatible architecture design with FLUX.1 [dev] \ud83d\udcc4 Model License This model is released under the flux-1-dev-non-commercial-license , for non-commercial use only. \ud83d\ude80 Flux.1 Krea Dev ComfyUI Workflow \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Workflow Templates \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83d\udce5 Step 1: Download Workflow Files Download the following image or JSON file and drag it into ComfyUI to load the corresponding workflow. Click image to download, drag into ComfyUI to load workflow \ud83d\udcc4 Download JSON Workflow File \ud83d\udd17 Step 2: Model Files #### \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 flux1-krea-dev_fp8_scaled.safetensors or flux1-krea-dev.safetensors \u2502 \u251c\u2500\u2500 text_encoders/ \u2502 \u2502 \u251c\u2500\u2500 clip_l.safetensors \u2502 \u2502 \u2514\u2500\u2500 t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn.safetensors \u2502 \u2514\u2500\u2500 vae/ \u2502 \u2514\u2500\u2500 ae.safetensors #### \ud83d\udca1 Model Version Selection Guide \ud83d\udc9a FP8 Quantized Version VRAM requirement: 12-16GB Suitable for: RTX 4060 Ti 16GB, RTX 4070, etc. Quality: Close to original, VRAM friendly \ud83d\udd25 Original Weights Version VRAM requirement: 24GB+ Suitable for: RTX 4090, A100, etc. Quality: Highest quality output \u26a0\ufe0f Important Notes \u2022 The flux1-krea-dev.safetensors file requires agreeing to the black-forest-labs/FLUX.1-Krea-dev license before download \u2022 If you've used other Flux workflows, Text Encoders and VAE files can be reused without re-downloading \ud83d\udd27 Step 3: Workflow Configuration Operations open the comfyui template: ![img_1.png](img_1.png) \u26a0\ufe0f Low VRAM User Notice For low VRAM users, this model may not run smoothly on your device. Consider waiting for community-optimized versions or use the FP8 quantized version. #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 Diffusion Model Loading In the Load Diffusion Model node, select: flux1-krea-dev_fp8_scaled.safetensors (recommended) or flux1-krea-dev.safetensors (high VRAM) \ud83d\udcdd Text Encoders Configuration In the DualCLIPLoader node, ensure: clip_name1 : t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn.safetensors clip_name2 : clip_l.safetensors \ud83c\udfa8 VAE Loading In the Load VAE node, load ae.safetensors \ud83d\udcdd Prompt Settings Enter your creative prompts in the CLIP Text Encode node #### \ud83c\udf9b\ufe0f Text Encoder Selection Guide VRAM Capacity Recommended T5 Version Description < 16GB t5xxl_fp8_e4m3fn.safetensors Low VRAM optimized version 16-24GB t5xxl_fp8_e4m3fn.safetensors Balanced performance and quality \u2265 32GB t5xxl_fp16.safetensors Best quality experience #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click Queue button or use shortcut Ctrl(Cmd) + Enter to run workflow API Execute \ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Flux Krea Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local UNET_MODEL = \"flux1-krea-dev_fp8_scaled.safetensors\" VAE_MODEL = \"flux-ae.safetensors\" CLIP_L_MODEL = \"clip_l_bf16.safetensors\" CLIP_T5_MODEL = \"t5xxl_fp16.safetensors\" # Default Parameters DEFAULT_PROMPT = \"Highly realistic portrait of a Nordic woman with blonde hair and blue eyes, very few freckles on her face, gaze sharp and intellectual. The lighting should reflect the unique coolness of Northern Europe. Outfit is minimalist and modern, background is blurred in cool tones. Needs to perfectly capture the characteristics of a Scandinavian woman. solo, Centered composition\" class ComfyUIFluxKreaClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_flux_krea_image(self, prompt, steps=20, cfg=1, width=1024, height=1024, seed=None): \"\"\"Generate Flux Krea text-to-image based on original JSON workflow\"\"\" print(\"Starting Flux Krea text-to-image generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Workflow based on your provided JSON workflow = { \"8\": { \"inputs\": { \"samples\": [\"31\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"9\": { \"inputs\": { \"filename_prefix\": \"flux_krea/flux_krea\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"27\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": {\"title\": \"Empty SD3 Latent Image\"} }, \"31\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"38\", 0], \"positive\": [\"45\", 0], \"negative\": [\"42\", 0], \"latent_image\": [\"27\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"38\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"40\": { \"inputs\": { \"clip_name1\": CLIP_L_MODEL, \"clip_name2\": CLIP_T5_MODEL, \"type\": \"flux\", \"device\": \"default\" }, \"class_type\": \"DualCLIPLoader\", \"_meta\": {\"title\": \"Dual CLIP Loader\"} }, \"42\": { \"inputs\": { \"conditioning\": [\"45\", 0] }, \"class_type\": \"ConditioningZeroOut\", \"_meta\": {\"title\": \"Conditioning Zero Out\"} }, \"45\": { \"inputs\": { \"text\": prompt, \"clip\": [\"40\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode\"} } } print(\"Submitting Flux Krea text-to-image workflow...\") print(f\"Prompt: {prompt}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompts_list, **kwargs): \"\"\"Batch generate images\"\"\" results = [] for i, prompt in enumerate(prompts_list): print(f\"\\nStarting task {i+1}/{len(prompts_list)}...\") try: task_id, seed = self.generate_flux_krea_image(prompt, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'prompt': prompt }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_variations(self, base_prompt, variations, **kwargs): \"\"\"Generate images with prompt variations\"\"\" prompts = [f\"{base_prompt}, {variation}\" for variation in variations] return self.generate_batch(prompts, **kwargs) def main(): \"\"\"Main function - Execute Flux Krea text-to-image generation\"\"\" client = ComfyUIFluxKreaClient() try: print(\"Flux Krea text-to-image client started...\") # Single image generation example print(\"\\n=== Single Image Generation ===\") task_id, seed = client.generate_flux_krea_image( prompt=DEFAULT_PROMPT, steps=20, cfg=1, width=1024, height=1024 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(5) # Download images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_prompts = [ \"Portrait of a young Asian woman with long black hair, wearing traditional kimono, cherry blossoms in background, soft lighting\", \"Professional headshot of a businessman in a navy suit, confident expression, office background, studio lighting\", \"Artistic portrait of an elderly man with a white beard, weathered face, dramatic side lighting, black and white\" ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_prompts, steps=20, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} images\") # Variation generation example print(\"\\n=== Variation Generation Example ===\") base_prompt = \"Portrait of a woman\" variations = [ \"smiling, warm lighting\", \"serious expression, dramatic lighting\", \"laughing, natural outdoor lighting\", \"contemplative, soft studio lighting\" ] # Uncomment to run variation generation # variation_results = client.generate_with_variations(base_prompt, variations, steps=15) # print(f\"Variation generation completed, generated {len(variation_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Flux.1 Krea Dev Key Advantages \ud83c\udfa8 Artistic Aesthetics Unique aesthetic style that avoids common AI-generated artifacts \ud83d\udcf8 Photographic Quality Natural lighting treatment without overexposed highlights \ud83c\udfc6 Best Open Source Currently the best open-source FLUX model weights \ud83d\udd04 Fully Compatible Completely compatible with existing FLUX.1 [dev] workflows \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Use detailed, specific prompt descriptions Take full advantage of the model's aesthetic style features Choose appropriate model version based on VRAM capacity Experiment with different sampler and step combinations \u26a0\ufe0f Important Notes Ensure sufficient VRAM to run the model Original weights version requires more computational resources Note the model's non-commercial license restrictions First run may require longer loading time \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component FP8 Version Original Weights Version GPU VRAM 12-16GB 24GB+ System RAM 16GB+ 32GB+ Storage Space 15GB 25GB Recommended GPU RTX 4060 Ti 16GB / RTX 4070 RTX 4090 / A100 ### \ud83c\udfa8 Generation Capabilities Portrait Photography Landscape Photography Artistic Creation Product Design Architectural Rendering Concept Art \ud83c\udfaf Application Scenarios \ud83c\udfa8 Artistic Creation Concept art, illustration design, visual art creation \ud83d\udcf1 Content Creation Social media content, blog illustrations, marketing materials \ud83c\udfac Film Production Storyboard design, concept previews, visual development \ud83d\udd2c Research & Experimentation AI research, model comparison, academic studies \ud83c\udf1f Performance Comparison ### \ud83d\udcca Quality Metrics \ud83c\udfaf Aesthetic Quality Superior aesthetic appeal compared to standard FLUX models, with reduced AI artifacts \ud83d\udcf8 Photorealism Enhanced natural lighting and detail preservation without overexposure issues \u26a1 Generation Speed Comparable generation speed to FLUX.1 [dev] with improved output quality ### \ud83d\udd04 Compatibility \ud83d\udd27 Workflow Compatibility Flux.1 Krea Dev is fully compatible with existing FLUX.1 [dev] workflows, making it a drop-in replacement for enhanced quality output. \ud83c\udfa8 Flux.1 Krea Dev Image Generation | Perfect Combination of Unique Aesthetic Style and Natural Details \u00a9 2025 Black Forest Labs & Krea | Non-Commercial License | Making AI Image Generation More Aesthetically Beautiful","title":"Index en"},{"location":"Flux/1Krea/doc/index-en/#flux1-krea-dev-model-overview","text":"[Flux.1 Krea Dev](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev) is an advanced text-to-image model developed through collaboration between Black Forest Labs (BFL) and Krea. This is currently the best open-source FLUX model, specifically designed for text-to-image generation. \ud83c\udfaf","title":"\ud83d\udccb Flux.1 Krea Dev Model Overview"},{"location":"Flux/1Krea/doc/index-en/#flux1-krea-dev-comfyui-workflow","text":"","title":"\ud83d\ude80 Flux.1 Krea Dev ComfyUI Workflow"},{"location":"Flux/1Krea/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status","title":"\u26a0\ufe0f Environment Requirements"},{"location":"Flux/1Krea/doc/index-en/#step-1-download-workflow-files","text":"Download the following image or JSON file and drag it into ComfyUI to load the corresponding workflow. Click image to download, drag into ComfyUI to load workflow \ud83d\udcc4 Download JSON Workflow File","title":"\ud83d\udce5 Step 1: Download Workflow Files"},{"location":"Flux/1Krea/doc/index-en/#step-2-model-files","text":"#### \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 flux1-krea-dev_fp8_scaled.safetensors or flux1-krea-dev.safetensors \u2502 \u251c\u2500\u2500 text_encoders/ \u2502 \u2502 \u251c\u2500\u2500 clip_l.safetensors \u2502 \u2502 \u2514\u2500\u2500 t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn.safetensors \u2502 \u2514\u2500\u2500 vae/ \u2502 \u2514\u2500\u2500 ae.safetensors #### \ud83d\udca1 Model Version Selection Guide","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"Flux/1Krea/doc/index-en/#step-3-workflow-configuration-operations","text":"open the comfyui template: ![img_1.png](img_1.png) \u26a0\ufe0f Low VRAM User Notice For low VRAM users, this model may not run smoothly on your device. Consider waiting for community-optimized versions or use the FP8 quantized version. #### \ud83d\udccb Detailed Configuration Steps","title":"\ud83d\udd27 Step 3: Workflow Configuration Operations"},{"location":"Flux/1Krea/doc/index-en/#api-execute","text":"\ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Flux Krea Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local UNET_MODEL = \"flux1-krea-dev_fp8_scaled.safetensors\" VAE_MODEL = \"flux-ae.safetensors\" CLIP_L_MODEL = \"clip_l_bf16.safetensors\" CLIP_T5_MODEL = \"t5xxl_fp16.safetensors\" # Default Parameters DEFAULT_PROMPT = \"Highly realistic portrait of a Nordic woman with blonde hair and blue eyes, very few freckles on her face, gaze sharp and intellectual. The lighting should reflect the unique coolness of Northern Europe. Outfit is minimalist and modern, background is blurred in cool tones. Needs to perfectly capture the characteristics of a Scandinavian woman. solo, Centered composition\" class ComfyUIFluxKreaClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_flux_krea_image(self, prompt, steps=20, cfg=1, width=1024, height=1024, seed=None): \"\"\"Generate Flux Krea text-to-image based on original JSON workflow\"\"\" print(\"Starting Flux Krea text-to-image generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Workflow based on your provided JSON workflow = { \"8\": { \"inputs\": { \"samples\": [\"31\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"9\": { \"inputs\": { \"filename_prefix\": \"flux_krea/flux_krea\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"27\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": {\"title\": \"Empty SD3 Latent Image\"} }, \"31\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"38\", 0], \"positive\": [\"45\", 0], \"negative\": [\"42\", 0], \"latent_image\": [\"27\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"38\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"40\": { \"inputs\": { \"clip_name1\": CLIP_L_MODEL, \"clip_name2\": CLIP_T5_MODEL, \"type\": \"flux\", \"device\": \"default\" }, \"class_type\": \"DualCLIPLoader\", \"_meta\": {\"title\": \"Dual CLIP Loader\"} }, \"42\": { \"inputs\": { \"conditioning\": [\"45\", 0] }, \"class_type\": \"ConditioningZeroOut\", \"_meta\": {\"title\": \"Conditioning Zero Out\"} }, \"45\": { \"inputs\": { \"text\": prompt, \"clip\": [\"40\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode\"} } } print(\"Submitting Flux Krea text-to-image workflow...\") print(f\"Prompt: {prompt}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompts_list, **kwargs): \"\"\"Batch generate images\"\"\" results = [] for i, prompt in enumerate(prompts_list): print(f\"\\nStarting task {i+1}/{len(prompts_list)}...\") try: task_id, seed = self.generate_flux_krea_image(prompt, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'prompt': prompt }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_variations(self, base_prompt, variations, **kwargs): \"\"\"Generate images with prompt variations\"\"\" prompts = [f\"{base_prompt}, {variation}\" for variation in variations] return self.generate_batch(prompts, **kwargs) def main(): \"\"\"Main function - Execute Flux Krea text-to-image generation\"\"\" client = ComfyUIFluxKreaClient() try: print(\"Flux Krea text-to-image client started...\") # Single image generation example print(\"\\n=== Single Image Generation ===\") task_id, seed = client.generate_flux_krea_image( prompt=DEFAULT_PROMPT, steps=20, cfg=1, width=1024, height=1024 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(5) # Download images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_prompts = [ \"Portrait of a young Asian woman with long black hair, wearing traditional kimono, cherry blossoms in background, soft lighting\", \"Professional headshot of a businessman in a navy suit, confident expression, office background, studio lighting\", \"Artistic portrait of an elderly man with a white beard, weathered face, dramatic side lighting, black and white\" ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_prompts, steps=20, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} images\") # Variation generation example print(\"\\n=== Variation Generation Example ===\") base_prompt = \"Portrait of a woman\" variations = [ \"smiling, warm lighting\", \"serious expression, dramatic lighting\", \"laughing, natural outdoor lighting\", \"contemplative, soft studio lighting\" ] # Uncomment to run variation generation # variation_results = client.generate_with_variations(base_prompt, variations, steps=15) # print(f\"Variation generation completed, generated {len(variation_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API Execute"},{"location":"Flux/1Krea/doc/index-en/#flux1-krea-dev-key-advantages","text":"\ud83c\udfa8","title":"\ud83c\udfaf Flux.1 Krea Dev Key Advantages"},{"location":"Flux/1Krea/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"Flux/1Krea/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component FP8 Version Original Weights Version GPU VRAM 12-16GB 24GB+ System RAM 16GB+ 32GB+ Storage Space 15GB 25GB Recommended GPU RTX 4060 Ti 16GB / RTX 4070 RTX 4090 / A100 ### \ud83c\udfa8 Generation Capabilities Portrait Photography Landscape Photography Artistic Creation Product Design Architectural Rendering Concept Art","title":"\ud83d\udd27 Technical Specifications"},{"location":"Flux/1Krea/doc/index-en/#application-scenarios","text":"\ud83c\udfa8","title":"\ud83c\udfaf Application Scenarios"},{"location":"Flux/1Krea/doc/index-en/#performance-comparison","text":"### \ud83d\udcca Quality Metrics \ud83c\udfaf Aesthetic Quality Superior aesthetic appeal compared to standard FLUX models, with reduced AI artifacts \ud83d\udcf8 Photorealism Enhanced natural lighting and detail preservation without overexposure issues \u26a1 Generation Speed Comparable generation speed to FLUX.1 [dev] with improved output quality ### \ud83d\udd04 Compatibility \ud83d\udd27 Workflow Compatibility Flux.1 Krea Dev is fully compatible with existing FLUX.1 [dev] workflows, making it a drop-in replacement for enhanced quality output. \ud83c\udfa8 Flux.1 Krea Dev Image Generation | Perfect Combination of Unique Aesthetic Style and Natural Details \u00a9 2025 Black Forest Labs & Krea | Non-Commercial License | Making AI Image Generation More Aesthetically Beautiful","title":"\ud83c\udf1f Performance Comparison"},{"location":"Flux/1dev/doc/","text":"\ud83c\udfa8 Flux1-Dev \u6a21\u578b\u4f7f\u7528\u6307\u5357 \u2728 Flux1-Dev \u6a21\u578b \u7531 Black Forest Labs \u5f00\u53d1\u7684\u5148\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b **Flux1-Dev** \u662f\u5f53\u524d\u5f00\u6e90\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u6700\u9ad8\u6c34\u51c6\u4ee3\u8868\uff0c\u57fa\u4e8e\u6d41\u5339\u914d\uff08Flow Matching\uff09\u6280\u672f\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u6587\u672c\u7406\u89e3\u80fd\u529b\u548c\u751f\u6210\u901f\u5ea6\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002 \ud83d\ude80 \u6838\u5fc3\u7279\u6027 \ud83c\udfd7\ufe0f \u5148\u8fdb\u67b6\u6784 \u57fa\u4e8e\u6d41\u5339\u914d\u6280\u672f\u7684\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784 \ud83c\udfaf \u5353\u8d8a\u8d28\u91cf \u751f\u6210\u56fe\u50cf\u8d28\u91cf\u63a5\u8fd1\u5546\u4e1a\u7ea7\u6a21\u578b\u6c34\u51c6 \ud83e\udde0 \u5f3a\u5927\u6587\u672c\u7406\u89e3 \u96c6\u6210\u5b8c\u6574FP16\u7248CLIP-L\u4e0eT5\u6587\u672c\u7f16\u7801\u5668 \ud83d\udcd0 \u9ad8\u5206\u8fa8\u7387\u652f\u6301 \u539f\u751f\u652f\u63011024\u00d71024\u53ca\u66f4\u9ad8\u5206\u8fa8\u7387 \u26a1 \u5feb\u901f\u751f\u6210 \u4f18\u5316\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u652f\u6301\u5c11\u6b65\u751f\u6210 \ud83c\udfa8 \u591a\u6837\u5316\u98ce\u683c \u652f\u6301\u5199\u5b9e\u3001\u827a\u672f\u3001\u6982\u5ff5\u8bbe\u8ba1\u7b49\u591a\u79cd\u98ce\u683c \ud83d\udcca \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08Text-to-Image\uff09 \u6587\u672c\u7f16\u7801\u5668 T5-XXL + CLIP-L VAE \u4e13\u7528\u7684flux-ae\u53d8\u5206\u81ea\u7f16\u7801\u5668 \u539f\u751f\u5206\u8fa8\u7387 1024\u00d71024 \u652f\u6301\u5206\u8fa8\u7387 512\u00d7512 \u5230 2048\u00d72048 \u63a8\u8350\u6b65\u6570 4-50\u6b65\uff088\u6b65\u4e3a\u6700\u4f73\u5e73\u8861\u70b9\uff09 \ud83c\udfc6 \u6a21\u578b\u4f18\u52bf **\u6838\u5fc3\u4f18\u52bf\u7279\u70b9** - **\ud83d\uddbc\ufe0f \u56fe\u50cf\u8d28\u91cf**\uff1a\u7ec6\u8282\u4e30\u5bcc\uff0c\u8272\u5f69\u81ea\u7136\uff0c\u6784\u56fe\u5408\u7406 - **\ud83d\udcdd \u6587\u672c\u9075\u5faa**\uff1a\u7cbe\u786e\u7406\u89e3\u590d\u6742\u6587\u672c\u63cf\u8ff0 - **\ud83c\udfad \u98ce\u683c\u591a\u6837**\uff1a\u4ece\u7167\u7247\u7ea7\u5199\u5b9e\u5230\u62bd\u8c61\u827a\u672f - **\ud83c\udfaf \u4e00\u81f4\u6027**\uff1a\u751f\u6210\u7ed3\u679c\u7a33\u5b9a\u53ef\u63a7 - **\u26a1 \u6548\u7387**\uff1a\u76f8\u6bd4\u540c\u7ea7\u522b\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u66f4\u5feb \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \ud83d\udcc1 \u6a21\u578b\u6587\u4ef6 \ud83c\udf10 WebUI \u73af\u5883 **\u4e3b\u6a21\u578b** - `flux.1_dev_8x8_e4m3fn.safetensors` **VAE** - `flux-ae.safetensors` **\u6587\u672c\u7f16\u7801\u5668** - `t5xxl_fp16.safetensors` - `clip_l.safetensors` - `clip_g.safetensors` \ud83c\udf9b\ufe0f ComfyUI \u73af\u5883 **\u4e3b\u6a21\u578b** - `flux1-dev.safetensors` **VAE** - `flux-ae.safetensors` **\u6587\u672c\u7f16\u7801\u5668** - `t5xxl_fp16.safetensors` - `clip_l.safetensors` \ud83d\udcd6 \u4f7f\u7528\u6307\u5357 \ud83c\udf9b\ufe0f ComfyUI \u4f7f\u7528 \ud83d\uddb1\ufe0f \u754c\u9762\u64cd\u4f5c **\u6b65\u9aa4 1\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41** - \u5728\u5de5\u4f5c\u6d41\u6846\u5904\u9009\u62e9\u8be5\u5de5\u4f5c\u6d41 ![img.png](text2img.png) **\u6b65\u9aa4 2\uff1a\u8f93\u5165\u63d0\u793a\u8bcd** - \u8f93\u5165\u4f60\u60f3\u8981\u7684\u5185\u5bb9\u63cf\u8ff0 ![img.png](text2img2.png) **\u6b65\u9aa4 3\uff1a\u521b\u610f\u793a\u4f8b** - \u53ef\u4ee5\u8f93\u5165\u4e00\u4e9b\u6709\u8da3\u7684\u5185\u5bb9\uff0c\u6bd4\u5982\"\u5173\u7fbd\u5927\u6218\u767d\u96ea\u516c\u4e3b\" **\u6b65\u9aa4 4\uff1a\u53c2\u6570\u8bbe\u7f6e** - \u8bbe\u7f6e\u56fe\u7247\u7684\u5206\u8fa8\u7387\u548c\u6570\u91cf - \u5982\u9700\u52a0\u5feb\u751f\u6210\u901f\u5ea6\uff0c\u53ef\u5c06 batch_size \u8bbe\u7f6e\u4e3a 1 ![img.png](text2img3.png) **\u6b65\u9aa4 5\uff1a\u7b49\u5f85\u751f\u6210** - \u8010\u5fc3\u7b49\u5f85\u56fe\u7247\u751f\u6210\u5b8c\u6210 \ud83d\udd0c ComfyUI API\u8c03\u7528 **\u83b7\u53d6Token** - \u70b9\u51fb\u53f3\u4e0a\u65b9\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\u83b7\u53d6token ![img_1.png](img_3.png) **\u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740** - COMFYUI_SERVER\u7684\u83b7\u53d6\u53c2\u8003 ![img_2.png](img_2.png) \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests, json, uuid, time, random, os COMFYUI_SERVER, COMFYUI_TOKEN = \"#\u5728\u8fd9\u91cc\u586b\u5165\u4f60\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u5728\u8fd9\u91cc\u586b\u5165\u4f60\u7684token\" UNET_MODEL, VAE_MODEL, CLIP1_MODEL, CLIP2_MODEL = \"flux1-dev.safetensors\", \"ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\" PROMPT = \"A beautiful anime girl with long flowing hair, wearing elegant dress, standing in a magical garden with glowing flowers, soft lighting, high quality, detailed\" class FluxClient: def __init__(self): self.base_url, self.client_id = f\"http://{COMFYUI_SERVER}\", str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {COMFYUI_TOKEN}\"} if COMFYUI_TOKEN else {})} def generate(self, prompt, aspect=\"1:1 square 1024x1024\", steps=35, guidance=3.5, batch=1): workflow = {\"6\": {\"inputs\": {\"text\": prompt, \"clip\": [\"11\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"8\": {\"inputs\": {\"samples\": [\"13\", 0], \"vae\": [\"10\", 0]}, \"class_type\": \"VAEDecode\"}, \"9\": {\"inputs\": {\"filename_prefix\": \"Flux\", \"images\": [\"8\", 0]}, \"class_type\": \"SaveImage\"}, \"10\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"11\": {\"inputs\": {\"clip_name1\": CLIP1_MODEL, \"clip_name2\": CLIP2_MODEL, \"type\": \"flux\", \"device\": \"default\"}, \"class_type\": \"DualCLIPLoader\"}, \"12\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"noise\": [\"25\", 0], \"guider\": [\"22\", 0], \"sampler\": [\"16\", 0], \"sigmas\": [\"17\", 0], \"latent_image\": [\"85\", 4]}, \"class_type\": \"SamplerCustomAdvanced\"}, \"16\": {\"inputs\": {\"sampler_name\": \"dpmpp_2m\"}, \"class_type\": \"KSamplerSelect\"}, \"17\": {\"inputs\": {\"scheduler\": \"sgm_uniform\", \"steps\": steps, \"denoise\": 1, \"model\": [\"61\", 0]}, \"class_type\": \"BasicScheduler\"}, \"22\": {\"inputs\": {\"model\": [\"61\", 0], \"conditioning\": [\"60\", 0]}, \"class_type\": \"BasicGuider\"}, \"25\": {\"inputs\": {\"noise_seed\": random.randint(1, 1000000000000000)}, \"class_type\": \"RandomNoise\"}, \"60\": {\"inputs\": {\"guidance\": guidance, \"conditioning\": [\"6\", 0]}, \"class_type\": \"FluxGuidance\"}, \"61\": {\"inputs\": {\"max_shift\": 1.15, \"base_shift\": 0.5, \"width\": [\"85\", 0], \"height\": [\"85\", 1], \"model\": [\"12\", 0]}, \"class_type\": \"ModelSamplingFlux\"}, \"85\": {\"inputs\": {\"width\": 1024, \"height\": 1024, \"aspect_ratio\": aspect, \"swap_dimensions\": \"Off\", \"upscale_factor\": 1, \"batch_size\": batch}, \"class_type\": \"CR SDXL Aspect Ratio\"}} return requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}).json()[\"prompt_id\"] def status(self, task_id): queue = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() return \"processing\" if any(item[1] == task_id for item in queue.get(\"queue_running\", [])) else \"pending\" if any(item[1] == task_id for item in queue.get(\"queue_pending\", [])) else \"completed\" if task_id in requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() else \"processing\" def download(self, task_id, output_dir=\"./flux_output/\"): history = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() files = [] if task_id in history: for output in history[task_id]['outputs'].values(): if 'images' in output: os.makedirs(output_dir, exist_ok=True) for img in output['images']: path = os.path.join(output_dir, img['filename']) with open(path, \"wb\") as f: f.write(requests.get(f\"{self.base_url}/view?filename={img['filename']}\", headers=self.headers).content) files.append(path) return files def main(): client = FluxClient() print(f\"\ud83c\udfa8 \u751f\u6210: {PROMPT}\") task_id = client.generate(PROMPT) print(f\"\ud83c\udd94 ID: {task_id}\") while True: status = client.status(task_id) print(f\"\ud83d\udcca {status}\") if status == \"completed\": break time.sleep(5) files = client.download(task_id) print(f\"\ud83c\udf89 \u5b8c\u6210! \u751f\u6210 {len(files)} \u5f20\u56fe\u7247: {files}\") if __name__ == \"__main__\": main() \ud83c\udf10 Web UI \u4f7f\u7528 \ud83d\uddb1\ufe0f \u754c\u9762\u64cd\u4f5c **1. \u6a21\u578b\u5207\u6362** - \u5728Checkpoint\u6a21\u578b\u9009\u62e9\u5668\u4e2d\u9009\u62e9Flux1-Dev\uff08HyFY-8-Step-Hybrid-v1.0.safetensors\uff09\u6a21\u578b **2. VAE\u548cCLIP\u6a21\u578b\u9009\u62e9** - \u9009\u62e9\uff1a`Clip_l.safetensors`, `t5xxl_fp16.safetensors`, `flux-ae.safetensors` ![img_1.png](img_1.png) **3. \u63d0\u793a\u8bcd\u8f93\u5165** - **\u6b63\u5411\u63d0\u793a\u8bcd**\uff1a\u8be6\u7ec6\u63cf\u8ff0\u60f3\u8981\u751f\u6210\u7684\u56fe\u50cf - **\u8d1f\u5411\u63d0\u793a\u8bcd**\uff1a\u63cf\u8ff0\u4e0d\u60f3\u8981\u7684\u5143\u7d20\uff08Flux\u6a21\u578b\u5bf9\u8d1f\u5411\u63d0\u793a\u8bcd\u4e0d\u654f\u611f\uff09 **4. \u53c2\u6570\u8bbe\u7f6e** - **\u6b65\u6570**\uff1a\u63a8\u8350 8-20 \u6b65 - **CFG**\uff1a\u63a8\u8350 1.0-3.5\uff08\u8f83\u4f4e\u503c\u6548\u679c\u66f4\u597d\uff09 - **\u91c7\u6837\u5668**\uff1a\u63a8\u8350 Euler \u6216 DPM++ 2M - **\u5206\u8fa8\u7387**\uff1a1024\u00d71024 \u6216\u5176\u4ed6\u652f\u6301\u7684\u5c3a\u5bf8 **5. \u751f\u6210\u56fe\u50cf** - \u70b9\u51fb\"Generate\"\u6309\u94ae\u5f00\u59cb\u751f\u6210 **6. \u7ed3\u679c\u5904\u7406** - \u67e5\u770b\u3001\u4fdd\u5b58\u6216\u8fdb\u4e00\u6b65\u7f16\u8f91\u751f\u6210\u7684\u56fe\u50cf \ud83c\udfa8 \u63d0\u793a\u8bcd\u793a\u4f8b \ud83d\udcf8 \u5199\u5b9e\u98ce\u683c \"a professional portrait of a young woman, natural lighting, high resolution, detailed skin texture, photorealistic\" \ud83c\udfa8 \u827a\u672f\u98ce\u683c \"an impressionist painting of a garden in spring, soft brushstrokes, vibrant colors, artistic masterpiece\" \ud83e\udd16 \u6982\u5ff5\u8bbe\u8ba1 \"futuristic robot design, sleek metallic surface, glowing blue accents, concept art, highly detailed\" \ud83c\udfd4\ufe0f \u98ce\u666f\u6444\u5f71 \"mountain landscape at golden hour, dramatic clouds, professional photography, ultra-wide angle, HDR\" \ud83d\uddbc\ufe0f UI\u754c\u9762\u4f7f\u7528\u793a\u4f8b \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00WebUI API\u8c03\u7528Python\u4ee3\u7801 import base64 import time import uuid # \u914d\u7f6e base_url = \"http://127.0.0.1:7680\" auth = (\"admin\", \"${APIKEY}\") session_hash = str(uuid.uuid4())[:12] # \u8bbe\u7f6eVAE/Text Encoder print(\"\u6b63\u5728\u8bbe\u7f6eVAE/Text Encoder...\") requests.post(f\"{base_url}/run/predict\", json={ \"data\": [[\"flux-ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\", \"clip_g.safetensors\"]], \"event_data\": None, \"fn_index\": 9, \"trigger_id\": 1001, \"session_hash\": session_hash }, auth=auth) time.sleep(3) # \u5207\u6362FLUX\u6a21\u578b print(\"\u6b63\u5728\u5207\u6362FLUX\u6a21\u578b...\") requests.post(f\"{base_url}/queue/join\", json={ \"data\": [\"HyFY-8-Step-Hybrid-v1.0.safetensors\"], \"event_data\": None, \"fn_index\": 8, \"trigger_id\": 1002, \"session_hash\": session_hash }, auth=auth) time.sleep(15) # \u751f\u6210\u56fe\u7247 print(\"\u6b63\u5728\u751f\u6210\u56fe\u7247...\") result = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json={ \"prompt\": \"a beautiful cat\", \"steps\": 8, \"width\": 1024, \"height\": 1024, \"cfg_scale\": 1.0, \"sampler_name\": \"Euler\" }, auth=auth).json() # \u4fdd\u5b58\u56fe\u7247 if \"images\" in result: with open(\"output.png\", \"wb\") as f: f.write(base64.b64decode(result[\"images\"][0])) print(\"\u56fe\u7247\u5df2\u4fdd\u5b58\u4e3a output.png\") else: print(\"\u9519\u8bef:\", result)</code></pre> --- ## \ud83d\udd04 \u5176\u4ed6\u5185\u7f6e\u6a21\u578b **\u6a21\u578b\u652f\u6301\u8bf4\u660e** \u5f53\u524d\u670d\u52a1\u4e2d\uff0c**Flux\u6a21\u578b**\u4f1a\u90e8\u7f72\u5230ECS\u5b9e\u4f8b\u4e2d\u3002\u9664\u4e86\u5f53\u524d\u7684**Flux-dev\u6a21\u578b**\uff0c\u8fd8\u652f\u6301\u4e86**SD1.5**\u548c**SD3**\u6a21\u578b\uff0c\u53ef\u5728**WebUI Forge\u754c\u9762**\u8fdb\u884c\u52a8\u6001\u5207\u6362\u3002 --- \ud83c\udfa8 \u5f00\u59cb\u4f60\u7684AI\u827a\u672f\u521b\u4f5c\u4e4b\u65c5\uff01 | \u4f7f\u7528Flux1-Dev\u6a21\u578b\uff0c\u8ba9\u60f3\u8c61\u529b\u53d8\u4e3a\u73b0\u5b9e","title":"\ud83c\udfa8 Flux1-Dev \u6a21\u578b\u4f7f\u7528\u6307\u5357"},{"location":"Flux/1dev/doc/#flux1-dev","text":"","title":"\ud83c\udfa8 Flux1-Dev \u6a21\u578b\u4f7f\u7528\u6307\u5357"},{"location":"Flux/1dev/doc/#_1","text":"","title":"\ud83d\ude80 \u6838\u5fc3\u7279\u6027"},{"location":"Flux/1dev/doc/#_2","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08Text-to-Image\uff09 \u6587\u672c\u7f16\u7801\u5668 T5-XXL + CLIP-L VAE \u4e13\u7528\u7684flux-ae\u53d8\u5206\u81ea\u7f16\u7801\u5668 \u539f\u751f\u5206\u8fa8\u7387 1024\u00d71024 \u652f\u6301\u5206\u8fa8\u7387 512\u00d7512 \u5230 2048\u00d72048 \u63a8\u8350\u6b65\u6570 4-50\u6b65\uff088\u6b65\u4e3a\u6700\u4f73\u5e73\u8861\u70b9\uff09","title":"\ud83d\udcca \u6280\u672f\u89c4\u683c"},{"location":"Flux/1dev/doc/#_3","text":"**\u6838\u5fc3\u4f18\u52bf\u7279\u70b9** - **\ud83d\uddbc\ufe0f \u56fe\u50cf\u8d28\u91cf**\uff1a\u7ec6\u8282\u4e30\u5bcc\uff0c\u8272\u5f69\u81ea\u7136\uff0c\u6784\u56fe\u5408\u7406 - **\ud83d\udcdd \u6587\u672c\u9075\u5faa**\uff1a\u7cbe\u786e\u7406\u89e3\u590d\u6742\u6587\u672c\u63cf\u8ff0 - **\ud83c\udfad \u98ce\u683c\u591a\u6837**\uff1a\u4ece\u7167\u7247\u7ea7\u5199\u5b9e\u5230\u62bd\u8c61\u827a\u672f - **\ud83c\udfaf \u4e00\u81f4\u6027**\uff1a\u751f\u6210\u7ed3\u679c\u7a33\u5b9a\u53ef\u63a7 - **\u26a1 \u6548\u7387**\uff1a\u76f8\u6bd4\u540c\u7ea7\u522b\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u66f4\u5feb","title":"\ud83c\udfc6 \u6a21\u578b\u4f18\u52bf"},{"location":"Flux/1dev/doc/#_4","text":"","title":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e"},{"location":"Flux/1dev/doc/#_5","text":"","title":"\ud83d\udcc1 \u6a21\u578b\u6587\u4ef6"},{"location":"Flux/1dev/doc/#_6","text":"","title":"\ud83d\udcd6 \u4f7f\u7528\u6307\u5357"},{"location":"Flux/1dev/doc/#comfyui","text":"","title":"\ud83c\udf9b\ufe0f ComfyUI \u4f7f\u7528"},{"location":"Flux/1dev/doc/#_7","text":"**\u6b65\u9aa4 1\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41** - \u5728\u5de5\u4f5c\u6d41\u6846\u5904\u9009\u62e9\u8be5\u5de5\u4f5c\u6d41 ![img.png](text2img.png) **\u6b65\u9aa4 2\uff1a\u8f93\u5165\u63d0\u793a\u8bcd** - \u8f93\u5165\u4f60\u60f3\u8981\u7684\u5185\u5bb9\u63cf\u8ff0 ![img.png](text2img2.png) **\u6b65\u9aa4 3\uff1a\u521b\u610f\u793a\u4f8b** - \u53ef\u4ee5\u8f93\u5165\u4e00\u4e9b\u6709\u8da3\u7684\u5185\u5bb9\uff0c\u6bd4\u5982\"\u5173\u7fbd\u5927\u6218\u767d\u96ea\u516c\u4e3b\" **\u6b65\u9aa4 4\uff1a\u53c2\u6570\u8bbe\u7f6e** - \u8bbe\u7f6e\u56fe\u7247\u7684\u5206\u8fa8\u7387\u548c\u6570\u91cf - \u5982\u9700\u52a0\u5feb\u751f\u6210\u901f\u5ea6\uff0c\u53ef\u5c06 batch_size \u8bbe\u7f6e\u4e3a 1 ![img.png](text2img3.png) **\u6b65\u9aa4 5\uff1a\u7b49\u5f85\u751f\u6210** - \u8010\u5fc3\u7b49\u5f85\u56fe\u7247\u751f\u6210\u5b8c\u6210","title":"\ud83d\uddb1\ufe0f \u754c\u9762\u64cd\u4f5c"},{"location":"Flux/1dev/doc/#comfyui-api","text":"**\u83b7\u53d6Token** - \u70b9\u51fb\u53f3\u4e0a\u65b9\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\u83b7\u53d6token ![img_1.png](img_3.png) **\u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740** - COMFYUI_SERVER\u7684\u83b7\u53d6\u53c2\u8003 ![img_2.png](img_2.png) \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests, json, uuid, time, random, os COMFYUI_SERVER, COMFYUI_TOKEN = \"#\u5728\u8fd9\u91cc\u586b\u5165\u4f60\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u5728\u8fd9\u91cc\u586b\u5165\u4f60\u7684token\" UNET_MODEL, VAE_MODEL, CLIP1_MODEL, CLIP2_MODEL = \"flux1-dev.safetensors\", \"ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\" PROMPT = \"A beautiful anime girl with long flowing hair, wearing elegant dress, standing in a magical garden with glowing flowers, soft lighting, high quality, detailed\" class FluxClient: def __init__(self): self.base_url, self.client_id = f\"http://{COMFYUI_SERVER}\", str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {COMFYUI_TOKEN}\"} if COMFYUI_TOKEN else {})} def generate(self, prompt, aspect=\"1:1 square 1024x1024\", steps=35, guidance=3.5, batch=1): workflow = {\"6\": {\"inputs\": {\"text\": prompt, \"clip\": [\"11\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"8\": {\"inputs\": {\"samples\": [\"13\", 0], \"vae\": [\"10\", 0]}, \"class_type\": \"VAEDecode\"}, \"9\": {\"inputs\": {\"filename_prefix\": \"Flux\", \"images\": [\"8\", 0]}, \"class_type\": \"SaveImage\"}, \"10\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"11\": {\"inputs\": {\"clip_name1\": CLIP1_MODEL, \"clip_name2\": CLIP2_MODEL, \"type\": \"flux\", \"device\": \"default\"}, \"class_type\": \"DualCLIPLoader\"}, \"12\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"noise\": [\"25\", 0], \"guider\": [\"22\", 0], \"sampler\": [\"16\", 0], \"sigmas\": [\"17\", 0], \"latent_image\": [\"85\", 4]}, \"class_type\": \"SamplerCustomAdvanced\"}, \"16\": {\"inputs\": {\"sampler_name\": \"dpmpp_2m\"}, \"class_type\": \"KSamplerSelect\"}, \"17\": {\"inputs\": {\"scheduler\": \"sgm_uniform\", \"steps\": steps, \"denoise\": 1, \"model\": [\"61\", 0]}, \"class_type\": \"BasicScheduler\"}, \"22\": {\"inputs\": {\"model\": [\"61\", 0], \"conditioning\": [\"60\", 0]}, \"class_type\": \"BasicGuider\"}, \"25\": {\"inputs\": {\"noise_seed\": random.randint(1, 1000000000000000)}, \"class_type\": \"RandomNoise\"}, \"60\": {\"inputs\": {\"guidance\": guidance, \"conditioning\": [\"6\", 0]}, \"class_type\": \"FluxGuidance\"}, \"61\": {\"inputs\": {\"max_shift\": 1.15, \"base_shift\": 0.5, \"width\": [\"85\", 0], \"height\": [\"85\", 1], \"model\": [\"12\", 0]}, \"class_type\": \"ModelSamplingFlux\"}, \"85\": {\"inputs\": {\"width\": 1024, \"height\": 1024, \"aspect_ratio\": aspect, \"swap_dimensions\": \"Off\", \"upscale_factor\": 1, \"batch_size\": batch}, \"class_type\": \"CR SDXL Aspect Ratio\"}} return requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}).json()[\"prompt_id\"] def status(self, task_id): queue = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() return \"processing\" if any(item[1] == task_id for item in queue.get(\"queue_running\", [])) else \"pending\" if any(item[1] == task_id for item in queue.get(\"queue_pending\", [])) else \"completed\" if task_id in requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() else \"processing\" def download(self, task_id, output_dir=\"./flux_output/\"): history = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() files = [] if task_id in history: for output in history[task_id]['outputs'].values(): if 'images' in output: os.makedirs(output_dir, exist_ok=True) for img in output['images']: path = os.path.join(output_dir, img['filename']) with open(path, \"wb\") as f: f.write(requests.get(f\"{self.base_url}/view?filename={img['filename']}\", headers=self.headers).content) files.append(path) return files def main(): client = FluxClient() print(f\"\ud83c\udfa8 \u751f\u6210: {PROMPT}\") task_id = client.generate(PROMPT) print(f\"\ud83c\udd94 ID: {task_id}\") while True: status = client.status(task_id) print(f\"\ud83d\udcca {status}\") if status == \"completed\": break time.sleep(5) files = client.download(task_id) print(f\"\ud83c\udf89 \u5b8c\u6210! \u751f\u6210 {len(files)} \u5f20\u56fe\u7247: {files}\") if __name__ == \"__main__\": main()","title":"\ud83d\udd0c ComfyUI API\u8c03\u7528"},{"location":"Flux/1dev/doc/#web-ui","text":"","title":"\ud83c\udf10 Web UI \u4f7f\u7528"},{"location":"Flux/1dev/doc/#_8","text":"**1. \u6a21\u578b\u5207\u6362** - \u5728Checkpoint\u6a21\u578b\u9009\u62e9\u5668\u4e2d\u9009\u62e9Flux1-Dev\uff08HyFY-8-Step-Hybrid-v1.0.safetensors\uff09\u6a21\u578b **2. VAE\u548cCLIP\u6a21\u578b\u9009\u62e9** - \u9009\u62e9\uff1a`Clip_l.safetensors`, `t5xxl_fp16.safetensors`, `flux-ae.safetensors` ![img_1.png](img_1.png) **3. \u63d0\u793a\u8bcd\u8f93\u5165** - **\u6b63\u5411\u63d0\u793a\u8bcd**\uff1a\u8be6\u7ec6\u63cf\u8ff0\u60f3\u8981\u751f\u6210\u7684\u56fe\u50cf - **\u8d1f\u5411\u63d0\u793a\u8bcd**\uff1a\u63cf\u8ff0\u4e0d\u60f3\u8981\u7684\u5143\u7d20\uff08Flux\u6a21\u578b\u5bf9\u8d1f\u5411\u63d0\u793a\u8bcd\u4e0d\u654f\u611f\uff09 **4. \u53c2\u6570\u8bbe\u7f6e** - **\u6b65\u6570**\uff1a\u63a8\u8350 8-20 \u6b65 - **CFG**\uff1a\u63a8\u8350 1.0-3.5\uff08\u8f83\u4f4e\u503c\u6548\u679c\u66f4\u597d\uff09 - **\u91c7\u6837\u5668**\uff1a\u63a8\u8350 Euler \u6216 DPM++ 2M - **\u5206\u8fa8\u7387**\uff1a1024\u00d71024 \u6216\u5176\u4ed6\u652f\u6301\u7684\u5c3a\u5bf8 **5. \u751f\u6210\u56fe\u50cf** - \u70b9\u51fb\"Generate\"\u6309\u94ae\u5f00\u59cb\u751f\u6210 **6. \u7ed3\u679c\u5904\u7406** - \u67e5\u770b\u3001\u4fdd\u5b58\u6216\u8fdb\u4e00\u6b65\u7f16\u8f91\u751f\u6210\u7684\u56fe\u50cf","title":"\ud83d\uddb1\ufe0f \u754c\u9762\u64cd\u4f5c"},{"location":"Flux/1dev/doc/#_9","text":"","title":"\ud83c\udfa8 \u63d0\u793a\u8bcd\u793a\u4f8b"},{"location":"Flux/1dev/doc/#ui","text":"\ud83d\udc0d \u70b9\u51fb\u5c55\u5f00WebUI API\u8c03\u7528Python\u4ee3\u7801 import base64 import time import uuid # \u914d\u7f6e base_url = \"http://127.0.0.1:7680\" auth = (\"admin\", \"${APIKEY}\") session_hash = str(uuid.uuid4())[:12] # \u8bbe\u7f6eVAE/Text Encoder print(\"\u6b63\u5728\u8bbe\u7f6eVAE/Text Encoder...\") requests.post(f\"{base_url}/run/predict\", json={ \"data\": [[\"flux-ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\", \"clip_g.safetensors\"]], \"event_data\": None, \"fn_index\": 9, \"trigger_id\": 1001, \"session_hash\": session_hash }, auth=auth) time.sleep(3) # \u5207\u6362FLUX\u6a21\u578b print(\"\u6b63\u5728\u5207\u6362FLUX\u6a21\u578b...\") requests.post(f\"{base_url}/queue/join\", json={ \"data\": [\"HyFY-8-Step-Hybrid-v1.0.safetensors\"], \"event_data\": None, \"fn_index\": 8, \"trigger_id\": 1002, \"session_hash\": session_hash }, auth=auth) time.sleep(15) # \u751f\u6210\u56fe\u7247 print(\"\u6b63\u5728\u751f\u6210\u56fe\u7247...\") result = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json={ \"prompt\": \"a beautiful cat\", \"steps\": 8, \"width\": 1024, \"height\": 1024, \"cfg_scale\": 1.0, \"sampler_name\": \"Euler\" }, auth=auth).json() # \u4fdd\u5b58\u56fe\u7247 if \"images\" in result: with open(\"output.png\", \"wb\") as f: f.write(base64.b64decode(result[\"images\"][0])) print(\"\u56fe\u7247\u5df2\u4fdd\u5b58\u4e3a output.png\") else: print(\"\u9519\u8bef:\", result)</code></pre> --- ## \ud83d\udd04 \u5176\u4ed6\u5185\u7f6e\u6a21\u578b **\u6a21\u578b\u652f\u6301\u8bf4\u660e** \u5f53\u524d\u670d\u52a1\u4e2d\uff0c**Flux\u6a21\u578b**\u4f1a\u90e8\u7f72\u5230ECS\u5b9e\u4f8b\u4e2d\u3002\u9664\u4e86\u5f53\u524d\u7684**Flux-dev\u6a21\u578b**\uff0c\u8fd8\u652f\u6301\u4e86**SD1.5**\u548c**SD3**\u6a21\u578b\uff0c\u53ef\u5728**WebUI Forge\u754c\u9762**\u8fdb\u884c\u52a8\u6001\u5207\u6362\u3002 --- \ud83c\udfa8 \u5f00\u59cb\u4f60\u7684AI\u827a\u672f\u521b\u4f5c\u4e4b\u65c5\uff01 | \u4f7f\u7528Flux1-Dev\u6a21\u578b\uff0c\u8ba9\u60f3\u8c61\u529b\u53d8\u4e3a\u73b0\u5b9e","title":"\ud83d\uddbc\ufe0f UI\u754c\u9762\u4f7f\u7528\u793a\u4f8b"},{"location":"Flux/1dev/doc/index-en/","text":"\ud83c\udfa8 Flux1-Dev Model Usage Guide Advanced Text-to-Image Generation Model by Black Forest Labs **Flux1-Dev** represents the pinnacle of open-source image generation technology, built on Flow Matching techniques with significant improvements in image quality, text understanding capabilities, and generation speed. \ud83d\ude80 Core Features \ud83c\udfd7\ufe0f Advanced Architecture Diffusion Transformer architecture based on Flow Matching technology \ud83c\udfaf Exceptional Quality Generated image quality approaches commercial-grade model standards \ud83e\udde0 Powerful Text Understanding Integrated full FP16 CLIP-L and T5 text encoders \ud83d\udcd0 High Resolution Support Native support for 1024\u00d71024 and higher resolutions \u26a1 Fast Generation Optimized inference speed with few-step generation support \ud83c\udfa8 Diverse Styles Supports photorealistic, artistic, concept design, and various styles \ud83d\udcca Technical Specifications Specification Details Model Type Text-to-Image Generation Text Encoder T5-XXL + CLIP-L VAE Dedicated flux-ae Variational Autoencoder Native Resolution 1024\u00d71024 Supported Resolutions 512\u00d7512 to 2048\u00d72048 Recommended Steps 4-50 steps (8 steps for optimal balance) \ud83c\udfc6 Model Advantages **Core Advantages** - **\ud83d\uddbc\ufe0f Image Quality**: Rich details, natural colors, reasonable composition - **\ud83d\udcdd Text Following**: Precise understanding of complex text descriptions - **\ud83c\udfad Style Diversity**: From photorealistic to abstract art - **\ud83c\udfaf Consistency**: Stable and controllable generation results - **\u26a1 Efficiency**: Faster inference speed compared to similar-level models \u2699\ufe0f Configuration Guide \ud83d\udcc1 Model Files \ud83c\udf10 WebUI Environment **Main Model** - `flux.1_dev_8x8_e4m3fn.safetensors` **VAE** - `flux-ae.safetensors` **Text Encoders** - `t5xxl_fp16.safetensors` - `clip_l.safetensors` - `clip_g.safetensors` \ud83c\udf9b\ufe0f ComfyUI Environment **Main Model** - `Flux1-dev.safetensors` **VAE** - `flux-ae.safetensors` **Text Encoders** - `t5xxl_fp16.safetensors` - `clip_l.safetensors` \ud83d\udcd6 Usage Guide \ud83c\udf9b\ufe0f ComfyUI Usage \ud83d\uddb1\ufe0f Interface Operations **Step 1: Select Workflow** - Choose the workflow in the workflow selector ![img.png](text2img.png) **Step 2: Input Prompts** - Enter your desired content description ![img.png](text2img2.png) **Step 3: Creative Examples** - You can input interesting content, such as \"Guan Yu fighting Snow White\" **Step 4: Parameter Settings** - Set image resolution and quantity - To speed up generation, set batch_size to 1 ![img.png](text2img3.png) **Step 5: Wait for Generation** - Patiently wait for image generation to complete \ud83d\udd0c ComfyUI API Integration **Get Token** - Click the top-right button to open the bottom panel and get the token ![img_1.png](img_3.png) **Get Server Address** - Refer to COMFYUI_SERVER acquisition guide ![img_2.png](img_2.png) \ud83d\udccb Click to Expand ComfyUI API Python Code import requests, json, uuid, time, random, os COMFYUI_SERVER, COMFYUI_TOKEN = \"#Fill in your server address here\", \"Fill in your token here\" UNET_MODEL, VAE_MODEL, CLIP1_MODEL, CLIP2_MODEL = \"flux1-dev.safetensors\", \"ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\" PROMPT = \"A beautiful anime girl with long flowing hair, wearing elegant dress, standing in a magical garden with glowing flowers, soft lighting, high quality, detailed\" class FluxClient: def __init__(self): self.base_url, self.client_id = f\"http://{COMFYUI_SERVER}\", str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {COMFYUI_TOKEN}\"} if COMFYUI_TOKEN else {})} def generate(self, prompt, aspect=\"1:1 square 1024x1024\", steps=35, guidance=3.5, batch=1): workflow = {\"6\": {\"inputs\": {\"text\": prompt, \"clip\": [\"11\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"8\": {\"inputs\": {\"samples\": [\"13\", 0], \"vae\": [\"10\", 0]}, \"class_type\": \"VAEDecode\"}, \"9\": {\"inputs\": {\"filename_prefix\": \"Flux\", \"images\": [\"8\", 0]}, \"class_type\": \"SaveImage\"}, \"10\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"11\": {\"inputs\": {\"clip_name1\": CLIP1_MODEL, \"clip_name2\": CLIP2_MODEL, \"type\": \"flux\", \"device\": \"default\"}, \"class_type\": \"DualCLIPLoader\"}, \"12\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"noise\": [\"25\", 0], \"guider\": [\"22\", 0], \"sampler\": [\"16\", 0], \"sigmas\": [\"17\", 0], \"latent_image\": [\"85\", 4]}, \"class_type\": \"SamplerCustomAdvanced\"}, \"16\": {\"inputs\": {\"sampler_name\": \"dpmpp_2m\"}, \"class_type\": \"KSamplerSelect\"}, \"17\": {\"inputs\": {\"scheduler\": \"sgm_uniform\", \"steps\": steps, \"denoise\": 1, \"model\": [\"61\", 0]}, \"class_type\": \"BasicScheduler\"}, \"22\": {\"inputs\": {\"model\": [\"61\", 0], \"conditioning\": [\"60\", 0]}, \"class_type\": \"BasicGuider\"}, \"25\": {\"inputs\": {\"noise_seed\": random.randint(1, 1000000000000000)}, \"class_type\": \"RandomNoise\"}, \"60\": {\"inputs\": {\"guidance\": guidance, \"conditioning\": [\"6\", 0]}, \"class_type\": \"FluxGuidance\"}, \"61\": {\"inputs\": {\"max_shift\": 1.15, \"base_shift\": 0.5, \"width\": [\"85\", 0], \"height\": [\"85\", 1], \"model\": [\"12\", 0]}, \"class_type\": \"ModelSamplingFlux\"}, \"85\": {\"inputs\": {\"width\": 1024, \"height\": 1024, \"aspect_ratio\": aspect, \"swap_dimensions\": \"Off\", \"upscale_factor\": 1, \"batch_size\": batch}, \"class_type\": \"CR SDXL Aspect Ratio\"}} return requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}).json()[\"prompt_id\"] def status(self, task_id): queue = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() return \"processing\" if any(item[1] == task_id for item in queue.get(\"queue_running\", [])) else \"pending\" if any(item[1] == task_id for item in queue.get(\"queue_pending\", [])) else \"completed\" if task_id in requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() else \"processing\" def download(self, task_id, output_dir=\"./flux_output/\"): history = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() files = [] if task_id in history: for output in history[task_id]['outputs'].values(): if 'images' in output: os.makedirs(output_dir, exist_ok=True) for img in output['images']: path = os.path.join(output_dir, img['filename']) with open(path, \"wb\") as f: f.write(requests.get(f\"{self.base_url}/view?filename={img['filename']}\", headers=self.headers).content) files.append(path) return files def main(): client = FluxClient() print(f\"\ud83c\udfa8 Generating: {PROMPT}\") task_id = client.generate(PROMPT) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.status(task_id) print(f\"\ud83d\udcca Status: {status}\") if status == \"completed\": break time.sleep(5) files = client.download(task_id) print(f\"\ud83c\udf89 Complete! Generated {len(files)} images: {files}\") if __name__ == \"__main__\": main() \ud83c\udf10 Web UI Usage \ud83d\uddb1\ufe0f Interface Operations **1. Model Selection** - Select Flux1-Dev (HyFY-8-Step-Hybrid-v1.0.safetensors) model in the Checkpoint model selector **2. VAE and CLIP Model Selection** - Select: `Clip_l.safetensors`, `t5xxl_fp16.safetensors`, `flux-ae.safetensors` ![img_1.png](img_1.png) **3. Prompt Input** - **Positive Prompt**: Detailed description of the desired image - **Negative Prompt**: Description of unwanted elements (Flux model is less sensitive to negative prompts) **4. Parameter Settings** - **Steps**: Recommended 8-20 steps - **CFG**: Recommended 1.0-3.5 (lower values work better) - **Sampler**: Recommended Euler or DPM++ 2M - **Resolution**: 1024\u00d71024 or other supported dimensions **5. Generate Image** - Click the \"Generate\" button to start generation **6. Result Processing** - View, save, or further edit the generated images \ud83c\udfa8 Prompt Examples \ud83d\udcf8 Photorealistic Style \"a professional portrait of a young woman, natural lighting, high resolution, detailed skin texture, photorealistic\" \ud83c\udfa8 Artistic Style \"an impressionist painting of a garden in spring, soft brushstrokes, vibrant colors, artistic masterpiece\" \ud83e\udd16 Concept Design \"futuristic robot design, sleek metallic surface, glowing blue accents, concept art, highly detailed\" \ud83c\udfd4\ufe0f Landscape Photography \"mountain landscape at golden hour, dramatic clouds, professional photography, ultra-wide angle, HDR\" \ud83d\uddbc\ufe0f UI Interface Usage Example \ud83d\udc0d Click to Expand WebUI API Python Code import base64 import time import uuid import requests # Configuration base_url = \"http://127.0.0.1:7680\" auth = (\"admin\", \"${APIKEY}\") session_hash = str(uuid.uuid4())[:12] # Set VAE/Text Encoder print(\"Setting VAE/Text Encoder...\") requests.post(f\"{base_url}/run/predict\", json={ \"data\": [[\"flux-ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\", \"clip_g.safetensors\"]], \"event_data\": None, \"fn_index\": 9, \"trigger_id\": 1001, \"session_hash\": session_hash }, auth=auth) time.sleep(3) # Switch FLUX model print(\"Switching FLUX model...\") requests.post(f\"{base_url}/queue/join\", json={ \"data\": [\"HyFY-8-Step-Hybrid-v1.0.safetensors\"], \"event_data\": None, \"fn_index\": 8, \"trigger_id\": 1002, \"session_hash\": session_hash }, auth=auth) time.sleep(15) # Generate image print(\"Generating image...\") result = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json={ \"prompt\": \"a beautiful cat\", \"steps\": 8, \"width\": 1024, \"height\": 1024, \"cfg_scale\": 1.0, \"sampler_name\": \"Euler\" }, auth=auth).json() # Save image if \"images\" in result: with open(\"output.png\", \"wb\") as f: f.write(base64.b64decode(result[\"images\"][0])) print(\"Image saved as output.png\") else: print(\"Error:\", result) \ud83d\udd04 Other Built-in Models **Model Support Information** In the current service, **Flux models** are deployed on ECS instances. In addition to the current **Flux-dev model**, **SD1.5** and **SD3** models are also supported and can be dynamically switched in the **WebUI Forge interface**. \ud83c\udfa8 Start Your AI Art Creation Journey! | Use Flux1-Dev model to turn imagination into reality","title":"Index en"},{"location":"Flux/1dev/doc/index-en/#core-features","text":"","title":"\ud83d\ude80 Core Features"},{"location":"Flux/1dev/doc/index-en/#technical-specifications","text":"Specification Details Model Type Text-to-Image Generation Text Encoder T5-XXL + CLIP-L VAE Dedicated flux-ae Variational Autoencoder Native Resolution 1024\u00d71024 Supported Resolutions 512\u00d7512 to 2048\u00d72048 Recommended Steps 4-50 steps (8 steps for optimal balance)","title":"\ud83d\udcca Technical Specifications"},{"location":"Flux/1dev/doc/index-en/#model-advantages","text":"**Core Advantages** - **\ud83d\uddbc\ufe0f Image Quality**: Rich details, natural colors, reasonable composition - **\ud83d\udcdd Text Following**: Precise understanding of complex text descriptions - **\ud83c\udfad Style Diversity**: From photorealistic to abstract art - **\ud83c\udfaf Consistency**: Stable and controllable generation results - **\u26a1 Efficiency**: Faster inference speed compared to similar-level models","title":"\ud83c\udfc6 Model Advantages"},{"location":"Flux/1dev/doc/index-en/#configuration-guide","text":"","title":"\u2699\ufe0f Configuration Guide"},{"location":"Flux/1dev/doc/index-en/#model-files","text":"","title":"\ud83d\udcc1 Model Files"},{"location":"Flux/1dev/doc/index-en/#usage-guide","text":"","title":"\ud83d\udcd6 Usage Guide"},{"location":"Flux/1dev/doc/index-en/#comfyui-usage","text":"","title":"\ud83c\udf9b\ufe0f ComfyUI Usage"},{"location":"Flux/1dev/doc/index-en/#interface-operations","text":"**Step 1: Select Workflow** - Choose the workflow in the workflow selector ![img.png](text2img.png) **Step 2: Input Prompts** - Enter your desired content description ![img.png](text2img2.png) **Step 3: Creative Examples** - You can input interesting content, such as \"Guan Yu fighting Snow White\" **Step 4: Parameter Settings** - Set image resolution and quantity - To speed up generation, set batch_size to 1 ![img.png](text2img3.png) **Step 5: Wait for Generation** - Patiently wait for image generation to complete","title":"\ud83d\uddb1\ufe0f Interface Operations"},{"location":"Flux/1dev/doc/index-en/#comfyui-api-integration","text":"**Get Token** - Click the top-right button to open the bottom panel and get the token ![img_1.png](img_3.png) **Get Server Address** - Refer to COMFYUI_SERVER acquisition guide ![img_2.png](img_2.png) \ud83d\udccb Click to Expand ComfyUI API Python Code import requests, json, uuid, time, random, os COMFYUI_SERVER, COMFYUI_TOKEN = \"#Fill in your server address here\", \"Fill in your token here\" UNET_MODEL, VAE_MODEL, CLIP1_MODEL, CLIP2_MODEL = \"flux1-dev.safetensors\", \"ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\" PROMPT = \"A beautiful anime girl with long flowing hair, wearing elegant dress, standing in a magical garden with glowing flowers, soft lighting, high quality, detailed\" class FluxClient: def __init__(self): self.base_url, self.client_id = f\"http://{COMFYUI_SERVER}\", str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {COMFYUI_TOKEN}\"} if COMFYUI_TOKEN else {})} def generate(self, prompt, aspect=\"1:1 square 1024x1024\", steps=35, guidance=3.5, batch=1): workflow = {\"6\": {\"inputs\": {\"text\": prompt, \"clip\": [\"11\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"8\": {\"inputs\": {\"samples\": [\"13\", 0], \"vae\": [\"10\", 0]}, \"class_type\": \"VAEDecode\"}, \"9\": {\"inputs\": {\"filename_prefix\": \"Flux\", \"images\": [\"8\", 0]}, \"class_type\": \"SaveImage\"}, \"10\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"11\": {\"inputs\": {\"clip_name1\": CLIP1_MODEL, \"clip_name2\": CLIP2_MODEL, \"type\": \"flux\", \"device\": \"default\"}, \"class_type\": \"DualCLIPLoader\"}, \"12\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"noise\": [\"25\", 0], \"guider\": [\"22\", 0], \"sampler\": [\"16\", 0], \"sigmas\": [\"17\", 0], \"latent_image\": [\"85\", 4]}, \"class_type\": \"SamplerCustomAdvanced\"}, \"16\": {\"inputs\": {\"sampler_name\": \"dpmpp_2m\"}, \"class_type\": \"KSamplerSelect\"}, \"17\": {\"inputs\": {\"scheduler\": \"sgm_uniform\", \"steps\": steps, \"denoise\": 1, \"model\": [\"61\", 0]}, \"class_type\": \"BasicScheduler\"}, \"22\": {\"inputs\": {\"model\": [\"61\", 0], \"conditioning\": [\"60\", 0]}, \"class_type\": \"BasicGuider\"}, \"25\": {\"inputs\": {\"noise_seed\": random.randint(1, 1000000000000000)}, \"class_type\": \"RandomNoise\"}, \"60\": {\"inputs\": {\"guidance\": guidance, \"conditioning\": [\"6\", 0]}, \"class_type\": \"FluxGuidance\"}, \"61\": {\"inputs\": {\"max_shift\": 1.15, \"base_shift\": 0.5, \"width\": [\"85\", 0], \"height\": [\"85\", 1], \"model\": [\"12\", 0]}, \"class_type\": \"ModelSamplingFlux\"}, \"85\": {\"inputs\": {\"width\": 1024, \"height\": 1024, \"aspect_ratio\": aspect, \"swap_dimensions\": \"Off\", \"upscale_factor\": 1, \"batch_size\": batch}, \"class_type\": \"CR SDXL Aspect Ratio\"}} return requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}).json()[\"prompt_id\"] def status(self, task_id): queue = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() return \"processing\" if any(item[1] == task_id for item in queue.get(\"queue_running\", [])) else \"pending\" if any(item[1] == task_id for item in queue.get(\"queue_pending\", [])) else \"completed\" if task_id in requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() else \"processing\" def download(self, task_id, output_dir=\"./flux_output/\"): history = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers).json() files = [] if task_id in history: for output in history[task_id]['outputs'].values(): if 'images' in output: os.makedirs(output_dir, exist_ok=True) for img in output['images']: path = os.path.join(output_dir, img['filename']) with open(path, \"wb\") as f: f.write(requests.get(f\"{self.base_url}/view?filename={img['filename']}\", headers=self.headers).content) files.append(path) return files def main(): client = FluxClient() print(f\"\ud83c\udfa8 Generating: {PROMPT}\") task_id = client.generate(PROMPT) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.status(task_id) print(f\"\ud83d\udcca Status: {status}\") if status == \"completed\": break time.sleep(5) files = client.download(task_id) print(f\"\ud83c\udf89 Complete! Generated {len(files)} images: {files}\") if __name__ == \"__main__\": main()","title":"\ud83d\udd0c ComfyUI API Integration"},{"location":"Flux/1dev/doc/index-en/#web-ui-usage","text":"","title":"\ud83c\udf10 Web UI Usage"},{"location":"Flux/1dev/doc/index-en/#interface-operations_1","text":"**1. Model Selection** - Select Flux1-Dev (HyFY-8-Step-Hybrid-v1.0.safetensors) model in the Checkpoint model selector **2. VAE and CLIP Model Selection** - Select: `Clip_l.safetensors`, `t5xxl_fp16.safetensors`, `flux-ae.safetensors` ![img_1.png](img_1.png) **3. Prompt Input** - **Positive Prompt**: Detailed description of the desired image - **Negative Prompt**: Description of unwanted elements (Flux model is less sensitive to negative prompts) **4. Parameter Settings** - **Steps**: Recommended 8-20 steps - **CFG**: Recommended 1.0-3.5 (lower values work better) - **Sampler**: Recommended Euler or DPM++ 2M - **Resolution**: 1024\u00d71024 or other supported dimensions **5. Generate Image** - Click the \"Generate\" button to start generation **6. Result Processing** - View, save, or further edit the generated images","title":"\ud83d\uddb1\ufe0f Interface Operations"},{"location":"Flux/1dev/doc/index-en/#prompt-examples","text":"","title":"\ud83c\udfa8 Prompt Examples"},{"location":"Flux/1dev/doc/index-en/#ui-interface-usage-example","text":"\ud83d\udc0d Click to Expand WebUI API Python Code import base64 import time import uuid import requests # Configuration base_url = \"http://127.0.0.1:7680\" auth = (\"admin\", \"${APIKEY}\") session_hash = str(uuid.uuid4())[:12] # Set VAE/Text Encoder print(\"Setting VAE/Text Encoder...\") requests.post(f\"{base_url}/run/predict\", json={ \"data\": [[\"flux-ae.safetensors\", \"t5xxl_fp16.safetensors\", \"clip_l.safetensors\", \"clip_g.safetensors\"]], \"event_data\": None, \"fn_index\": 9, \"trigger_id\": 1001, \"session_hash\": session_hash }, auth=auth) time.sleep(3) # Switch FLUX model print(\"Switching FLUX model...\") requests.post(f\"{base_url}/queue/join\", json={ \"data\": [\"HyFY-8-Step-Hybrid-v1.0.safetensors\"], \"event_data\": None, \"fn_index\": 8, \"trigger_id\": 1002, \"session_hash\": session_hash }, auth=auth) time.sleep(15) # Generate image print(\"Generating image...\") result = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json={ \"prompt\": \"a beautiful cat\", \"steps\": 8, \"width\": 1024, \"height\": 1024, \"cfg_scale\": 1.0, \"sampler_name\": \"Euler\" }, auth=auth).json() # Save image if \"images\" in result: with open(\"output.png\", \"wb\") as f: f.write(base64.b64decode(result[\"images\"][0])) print(\"Image saved as output.png\") else: print(\"Error:\", result)","title":"\ud83d\uddbc\ufe0f UI Interface Usage Example"},{"location":"Flux/1dev/doc/index-en/#other-built-in-models","text":"**Model Support Information** In the current service, **Flux models** are deployed on ECS instances. In addition to the current **Flux-dev model**, **SD1.5** and **SD3** models are also supported and can be dynamically switched in the **WebUI Forge interface**. \ud83c\udfa8 Start Your AI Art Creation Journey! | Use Flux1-Dev model to turn imagination into reality","title":"\ud83d\udd04 Other Built-in Models"},{"location":"Flux/1kontext/doc/","text":"\ud83c\udfa8 ComfyUI Flux Kontext Dev \u56fe\u50cf\u7f16\u8f91 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u591a\u6a21\u6001\u667a\u80fd\u56fe\u50cf\u7f16\u8f91\u4e0e\u89d2\u8272\u4e00\u81f4\u6027\u4fdd\u6301 \ud83c\udfaf \u89d2\u8272\u4e00\u81f4\u6027 \u2702\ufe0f \u5c40\u90e8\u7f16\u8f91 \ud83c\udfa8 \u98ce\u683c\u53c2\u8003 \ud83d\udccb FLUX.1 Kontext Dev \u6a21\u578b\u6982\u89c8 **FLUX.1 Kontext** \u662f Black Forest Labs \u63a8\u51fa\u7684\u7a81\u7834\u6027\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u548c\u56fe\u50cf\u540c\u65f6\u8f93\u5165\uff0c\u80fd\u591f\u667a\u80fd\u7406\u89e3\u56fe\u50cf\u4e0a\u4e0b\u6587\u5e76\u6267\u884c\u7cbe\u786e\u7f16\u8f91\u3002\u5176\u5f00\u53d1\u7248\u662f\u4e00\u4e2a\u62e5\u6709 120 \u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6269\u6563\u53d8\u538b\u5668\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u548c\u89d2\u8272\u4e00\u81f4\u6027\u4fdd\u6301\uff0c\u5373\u4f7f\u7ecf\u8fc7\u591a\u6b21\u8fed\u4ee3\u7f16\u8f91\uff0c\u4e5f\u80fd\u786e\u4fdd\u4eba\u7269\u7279\u5f81\u3001\u6784\u56fe\u5e03\u5c40\u7b49\u5173\u952e\u5143\u7d20\u4fdd\u6301\u7a33\u5b9a\u3002 \ud83c\udfaf \u89d2\u8272\u4e00\u81f4\u6027 \u5728\u591a\u4e2a\u573a\u666f\u548c\u73af\u5883\u4e2d\u4fdd\u7559\u56fe\u50cf\u7684\u72ec\u7279\u5143\u7d20\uff0c\u5982\u53c2\u8003\u89d2\u8272\u6216\u7269\u4f53 \u2702\ufe0f \u5c40\u90e8\u7f16\u8f91 \u5bf9\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u5143\u7d20\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u4fee\u6539\uff0c\u800c\u4e0d\u5f71\u54cd\u5176\u4ed6\u90e8\u5206 \ud83c\udfa8 \u98ce\u683c\u53c2\u8003 \u6839\u636e\u6587\u672c\u63d0\u793a\uff0c\u5728\u4fdd\u7559\u53c2\u8003\u56fe\u50cf\u72ec\u7279\u98ce\u683c\u7684\u540c\u65f6\u751f\u6210\u65b0\u9896\u573a\u666f \u26a1 \u4ea4\u4e92\u901f\u5ea6 \u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7684\u5ef6\u8fdf\u6781\u5c0f\uff0c\u652f\u6301\u5feb\u901f\u8fed\u4ee3 ## \ud83d\udee0\ufe0f \u5de5\u4f5c\u6d41\u7c7b\u578b\u8bf4\u660e \u672c\u6559\u7a0b\u63d0\u4f9b\u4e24\u79cd\u5de5\u4f5c\u6d41\u7c7b\u578b\uff0c\u672c\u8d28\u4e0a\u529f\u80fd\u76f8\u540c\uff0c\u4f46\u5728\u4f7f\u7528\u4f53\u9a8c\u4e0a\u6709\u6240\u4e0d\u540c\uff1a \ud83c\udfaf \u7ec4\u8282\u70b9\u7248\u672c \u4f7f\u7528 FLUX.1 Kontext Image Edit \u7ec4\u8282\u70b9 \u754c\u9762\u7b80\u6d01\uff0c\u6613\u4e8e\u590d\u7528 \u5feb\u901f\u6dfb\u52a0\u7ec4\u8282\u70b9\u529f\u80fd \u9002\u5408\u590d\u6742\u5de5\u4f5c\u6d41\u6784\u5efa \ud83d\udd27 \u5b8c\u6574\u539f\u59cb\u7248\u672c \u5c55\u793a\u5b8c\u6574\u7684\u8282\u70b9\u8fde\u63a5\u7ed3\u6784 \u6240\u6709\u8282\u70b9\u53ef\u89c1\uff0c\u4fbf\u4e8e\u5b66\u4e60 \u53c2\u6570\u8c03\u6574\u66f4\u76f4\u89c2 \u9002\u5408\u6df1\u5ea6\u5b9a\u5236 ### \ud83d\ude80 \u5feb\u901f\u6dfb\u52a0\u7ec4\u8282\u70b9\u529f\u80fd \u26a0\ufe0f \u5b9e\u9a8c\u6027\u529f\u80fd \u8fd9\u4e2a\u529f\u80fd\u76ee\u524d\u53ea\u662f\u4e00\u4e2a\u5b9e\u9a8c\u6027\u7684\u65b0\u529f\u80fd\uff0c\u53ef\u80fd\u5728\u672a\u6765\u7248\u672c\u4e2d\u8fdb\u884c\u8c03\u6574\u3002 ## \ud83d\udd17 \u6a21\u578b\u6587\u4ef6 #### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 \ud83d\udcc2 ComfyUI/ \u251c\u2500\u2500 \ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 flux1-dev-kontext_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 vae/ \u2502 \u2502 \u2514\u2500\u2500 ae.safetensors \u2502 \u2514\u2500\u2500 \ud83d\udcc2 text_encoders/ \u2502 \u251c\u2500\u2500 clip_l.safetensors \u2502 \u2514\u2500\u2500 t5xxl_fp16.safetensors \u6216 t5xxl_fp8_e4m3fn_scaled.safetensors ## \ud83d\ude80 Flux.1 Kontext Dev \u5de5\u4f5c\u6d41\u793a\u4f8b \u6253\u5f00Comfyui\u6a21\u7248\u5de5\u4f5c\u6d41\uff0c\u53c2\u8003\u4e0b\u56fe\u9009\u62e9\uff1a ![img_1.png](img_1.png) \u8fd9\u4e2a\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 `Load Image(from output)` \u8282\u70b9\u6765\u52a0\u8f7d\u9700\u8981\u7f16\u8f91\u7684\u56fe\u50cf\uff0c\u53ef\u4ee5\u8ba9\u4f60\u66f4\u65b9\u4fbf\u5730\u83b7\u53d6\u5230\u7f16\u8f91\u540e\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u8fdb\u884c\u591a\u8f6e\u6b21\u7f16\u8f91\u3002 ### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u53ca\u8f93\u5165\u56fe\u7247\u4e0b\u8f7d \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u8f93\u5165\u56fe\u7247 \ud83d\udc30 \u793a\u4f8b\u8f93\u5165\u56fe\u7247 \u53f3\u952e\u4fdd\u5b58\u56fe\u7247\uff0c\u7528\u4e8e\u5de5\u4f5c\u6d41\u6d4b\u8bd5 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 \u6a21\u578b\u52a0\u8f7d Load Diffusion Model \uff1a flux1-dev-kontext_fp8_scaled.safetensors DualCLIP Load \uff1a clip_l.safetensors \u548c t5xxl_fp16.safetensors Load VAE \uff1a ae.safetensors \ud83d\udcc1 \u56fe\u7247\u52a0\u8f7d \u5728 Load Image(from output) \u8282\u70b9\u4e2d\u52a0\u8f7d\u63d0\u4f9b\u7684\u8f93\u5165\u56fe\u50cf \ud83d\udcdd \u63d0\u793a\u8bcd\u8bbe\u7f6e \u5728 CLIP Text Encode \u8282\u70b9\u4e2d\u4fee\u6539\u63d0\u793a\u8bcd\uff0c \u4ec5\u652f\u6301\u82f1\u6587 #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 ## \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f ## \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 \u8bb0\u5f55 ComfyUI \u670d\u52a1\u5668\u7684\u8bbf\u95ee\u5730\u5740 ![img_2.png](img_2.png) \ud83d\udd10 \u83b7\u53d6 API Token \u5728 ComfyUI \u754c\u9762\u53f3\u4e0a\u89d2\u83b7\u53d6\u8bbf\u95ee\u4ee4\u724c ![img_1.png](img_1.png) ## \ud83d\udcbb Python API \u8c03\u7528\u793a\u4f8b \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests import json import uuid import time import random import os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Flux \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" UNET_MODEL = \"flux1-dev.safetensors\" VAE_MODEL = \"flux-ae.safetensors\" CLIP_L_MODEL = \"clip_l_bf16.safetensors\" CLIP_T5_MODEL = \"t5xxl_fp16.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 DEFAULT_PROMPT = \"A beautiful fantasy girl with long curly silver hair and big blue eyes, wearing a white transparent fairy dress with lace and puffed sleeves. Surrounded by iridescent butterflies and giant glass roses, dreamy lighting, ethereal atmosphere, soft glow, magical realism, highly detailed, cinematic, 8K render.\" DEFAULT_T5_PROMPT = \"A fairy tale scene of a young girl with silver curly hair wearing a delicate white dress, standing among crystal butterflies and glowing glass roses. The scene is filled with soft magical light, like a dream from a fantasy world.\" class ComfyUIFluxClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_flux_image(self, clip_l_prompt, t5xxl_prompt=None, steps=20, cfg=1, guidance=3.5, width=1024, height=1024, seed=None): \"\"\"\ud83c\udfa8 Flux \u6587\u751f\u56fe\u751f\u6210 - \u57fa\u4e8e\u539f\u59cbJSON\u5de5\u4f5c\u6d41\"\"\" print(\"\ud83c\udfa8 \u5f00\u59cb Flux \u6587\u751f\u56fe\u4efb\u52a1...\") # \u5982\u679c\u6ca1\u6709\u63d0\u4f9bT5\u63d0\u793a\u8bcd\uff0c\u4f7f\u7528CLIP_L\u63d0\u793a\u8bcd if t5xxl_prompt is None: t5xxl_prompt = clip_l_prompt # \u5982\u679c\u6ca1\u6709\u63d0\u4f9b\u79cd\u5b50\uff0c\u968f\u673a\u751f\u6210 if seed is None: seed = random.randint(1, 1000000000000000) # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684JSON\u5de5\u4f5c\u6d41 workflow = { \"8\": { \"inputs\": { \"samples\": [\"31\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"9\": { \"inputs\": { \"filename_prefix\": \"flux_output\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u56fe\u50cf\"} }, \"27\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": {\"title\": \"\u7a7aLatent\u56fe\u50cf\uff08SD3\uff09\"} }, \"31\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"38\", 0], \"positive\": [\"41\", 0], \"negative\": [\"42\", 0], \"latent_image\": [\"27\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\"} }, \"38\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"40\": { \"inputs\": { \"clip_name1\": CLIP_L_MODEL, \"clip_name2\": CLIP_T5_MODEL, \"type\": \"flux\", \"device\": \"default\" }, \"class_type\": \"DualCLIPLoader\", \"_meta\": {\"title\": \"\u53ccCLIP\u52a0\u8f7d\u5668\"} }, \"41\": { \"inputs\": { \"clip_l\": clip_l_prompt, \"t5xxl\": t5xxl_prompt, \"guidance\": guidance, \"clip\": [\"40\", 0] }, \"class_type\": \"CLIPTextEncodeFlux\", \"_meta\": {\"title\": \"CLIP\u6587\u672c\u7f16\u7801Flux\"} }, \"42\": { \"inputs\": { \"conditioning\": [\"41\", 0] }, \"class_type\": \"ConditioningZeroOut\", \"_meta\": {\"title\": \"\u6761\u4ef6\u96f6\u5316\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Flux \u6587\u751f\u56fe\u5de5\u4f5c\u6d41...\") print(f\"\ud83c\udfaf CLIP-L \u63d0\u793a\u8bcd: {clip_l_prompt}\") print(f\"\ud83c\udfaf T5XXL \u63d0\u793a\u8bcd: {t5xxl_prompt}\") print(f\"\ud83c\udfb2 \u968f\u673a\u79cd\u5b50: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: # \u68c0\u67e5\u961f\u5217\u72b6\u6001 queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # \u68c0\u67e5\u662f\u5426\u5728\u8fd0\u884c\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # \u68c0\u67e5\u662f\u5426\u5728\u7b49\u5f85\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # \u68c0\u67e5\u5386\u53f2\u8bb0\u5f55 history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u56fe\u50cf\"\"\" try: # \u786e\u4fdd\u8f93\u51fa\u76ee\u5f55\u5b58\u5728 os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # \u4e0b\u8f7d\u56fe\u50cf img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"\ud83d\udcc1 \u56fe\u50cf\u5df2\u4fdd\u5b58: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompts_list, **kwargs): \"\"\"\ud83d\udd04 \u6279\u91cf\u751f\u6210\u56fe\u50cf\"\"\" results = [] for i, prompt_data in enumerate(prompts_list): print(f\"\\n\ud83c\udfa8 \u5f00\u59cb\u7b2c {i+1}/{len(prompts_list)} \u4e2a\u4efb\u52a1...\") if isinstance(prompt_data, str): # \u5982\u679c\u662f\u5b57\u7b26\u4e32\uff0c\u4f5c\u4e3aCLIP-L\u63d0\u793a\u8bcd clip_l = prompt_data t5xxl = None elif isinstance(prompt_data, dict): # \u5982\u679c\u662f\u5b57\u5178\uff0c\u63d0\u53d6CLIP-L\u548cT5XXL\u63d0\u793a\u8bcd clip_l = prompt_data.get('clip_l', prompt_data.get('prompt', '')) t5xxl = prompt_data.get('t5xxl') else: print(f\"\u274c \u65e0\u6548\u7684\u63d0\u793a\u8bcd\u683c\u5f0f: {prompt_data}\") continue try: task_id, seed = self.generate_flux_image(clip_l, t5xxl, **kwargs) # \u7b49\u5f85\u5b8c\u6210 while True: status = self.get_status(task_id) print(f\"\ud83d\udcca \u4efb\u52a1 {i+1} \u72b6\u6001: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'prompt': {'clip_l': clip_l, 't5xxl': t5xxl} }) break elif status == \"failed\": print(f\"\u274c \u4efb\u52a1 {i+1} \u5931\u8d25\") break time.sleep(5) except Exception as e: print(f\"\u274c \u4efb\u52a1 {i+1} \u9519\u8bef: {e}\") return results def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Flux \u6587\u751f\u56fe\u4efb\u52a1\"\"\" client = ComfyUIFluxClient() try: print(\"\ud83c\udfa8 Flux \u6587\u751f\u56fe\u5ba2\u6237\u7aef\u542f\u52a8...\") # \u5355\u4e2a\u56fe\u50cf\u751f\u6210\u793a\u4f8b print(\"\\n=== \u5355\u4e2a\u56fe\u50cf\u751f\u6210 ===\") task_id, seed = client.generate_flux_image( clip_l_prompt=DEFAULT_PROMPT, t5xxl_prompt=DEFAULT_T5_PROMPT, steps=20, guidance=3.5, width=1024, height=1024 ) print(f\"\ud83c\udd94 Task ID: {task_id}\") print(f\"\ud83c\udfb2 Seed: {seed}\") # \u7b49\u5f85\u4efb\u52a1\u5b8c\u6210 while True: status = client.get_status(task_id) print(f\"\ud83d\udcca \u5f53\u524d\u72b6\u6001: {status}\") if status == \"completed\": print(\"\u2705 \u56fe\u50cf\u751f\u6210\u5b8c\u6210!\") break elif status == \"failed\": print(\"\u274c \u751f\u6210\u5931\u8d25!\") return time.sleep(5) # \u4e0b\u8f7d\u56fe\u50cf downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"\ud83c\udf89 \u6210\u529f\u4e0b\u8f7d {len(downloaded_files)} \u4e2a\u6587\u4ef6!\") for file in downloaded_files: print(f\"\ud83d\udcc1 \u6587\u4ef6\u8def\u5f84: {file}\") else: print(\"\u274c \u4e0b\u8f7d\u5931\u8d25\") # \u6279\u91cf\u751f\u6210\u793a\u4f8b\uff08\u53ef\u9009\uff09 print(\"\\n=== \u6279\u91cf\u751f\u6210\u793a\u4f8b ===\") batch_prompts = [ \"A majestic dragon flying over a medieval castle at sunset\", \"A cyberpunk cityscape with neon lights and flying cars\", { 'clip_l': \"A serene forest with magical creatures\", 't5xxl': \"An enchanted woodland scene with fairies and unicorns dancing in moonlight\" } ] # \u53d6\u6d88\u6ce8\u91ca\u4e0b\u9762\u7684\u4ee3\u7801\u6765\u6267\u884c\u6279\u91cf\u751f\u6210 # batch_results = client.generate_batch(batch_prompts, steps=15, guidance=3.0) # print(f\"\ud83c\udf89 \u6279\u91cf\u751f\u6210\u5b8c\u6210\uff0c\u5171\u751f\u6210 {len(batch_results)} \u4e2a\u56fe\u50cf\") except Exception as e: print(f\"\u274c \u9519\u8bef: {e}\") if __name__ == \"__main__\": main() ## \ud83d\udca1 Flux Kontext \u63d0\u793a\u8bcd\u6280\u5de7 \u638c\u63e1\u6b63\u786e\u7684\u63d0\u793a\u8bcd\u6280\u5de7\u662f\u83b7\u5f97\u7406\u60f3\u7f16\u8f91\u6548\u679c\u7684\u5173\u952e\u3002\u4ee5\u4e0b\u662f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6700\u4f73\u5b9e\u8df5\uff1a ### \ud83c\udfaf \u57fa\u7840\u4fee\u6539\u6280\u5de7 \u2705 \u63a8\u8350\u505a\u6cd5 \u7b80\u5355\u76f4\u63a5 \uff1a \"Change the car color to red\" \u4fdd\u6301\u98ce\u683c \uff1a \"Change to daytime while maintaining the same style of the painting\" \u274c \u907f\u514d\u505a\u6cd5 \u4f7f\u7528\u6a21\u7cca\u8bcd\u6c47 \u8fc7\u4e8e\u590d\u6742\u7684\u5355\u53e5\u63cf\u8ff0 \u7f3a\u4e4f\u5177\u4f53\u6307\u5411\u6027 ### \ud83c\udfa8 \u98ce\u683c\u8f6c\u6362\u6846\u67b6 \ud83c\udfa8 \u98ce\u683c\u8f6c\u6362\u539f\u5219 \u2022 \u660e\u786e\u547d\u540d\u98ce\u683c \uff1a \"Transform to Bauhaus art style\" \u2022 \u63cf\u8ff0\u7279\u5f81 \uff1a \"Transform to oil painting with visible brushstrokes, thick paint texture\" \u2022 \u4fdd\u7559\u6784\u56fe \uff1a \"Change to Bauhaus style while maintaining the original composition\" ### \ud83d\udc64 \u89d2\u8272\u4e00\u81f4\u6027\u4fdd\u6301 \ud83d\udc64 \u89d2\u8272\u4e00\u81f4\u6027\u6846\u67b6 \u2022 \u5177\u4f53\u63cf\u8ff0 \uff1a \"The woman with short black hair\" \u800c\u975e \"she\" \u2022 \u4fdd\u7559\u7279\u5f81 \uff1a \"while maintaining the same facial features, hairstyle, and expression\" \u2022 \u5206\u6b65\u4fee\u6539 \uff1a\u5148\u6539\u80cc\u666f\uff0c\u518d\u6539\u52a8\u4f5c ### \ud83d\udcdd \u6587\u672c\u7f16\u8f91\u6280\u5de7 \ud83d\udcdd \u6587\u672c\u7f16\u8f91\u8981\u70b9 \u2022 \u4f7f\u7528\u5f15\u53f7 \uff1a \"Replace 'joy' with 'BFL'\" \u2022 \u4fdd\u6301\u683c\u5f0f \uff1a \"Replace text while maintaining the same font style\" ## \ud83d\udea8 \u5e38\u89c1\u95ee\u9898\u89e3\u51b3\u65b9\u6848 ### \ud83c\udfad \u89d2\u8272\u53d8\u5316\u8fc7\u5927 \u274c \u9519\u8bef\u793a\u4f8b \"Transform the person into a Viking\" \u2705 \u6b63\u786e\u793a\u4f8b \"Change the clothes to be a viking warrior while preserving facial features\" ### \ud83d\udcd0 \u6784\u56fe\u4f4d\u7f6e\u6539\u53d8 \u274c \u9519\u8bef\u793a\u4f8b \"Put him on a beach\" \u2705 \u6b63\u786e\u793a\u4f8b \"Change the background to a beach while keeping the person in the exact same position, scale, and pose\" ### \ud83c\udfa8 \u98ce\u683c\u5e94\u7528\u4e0d\u51c6\u786e \u274c \u9519\u8bef\u793a\u4f8b \"Make it a sketch\" \u2705 \u6b63\u786e\u793a\u4f8b \"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\" ## \ud83c\udfaf \u6838\u5fc3\u539f\u5219\u4e0e\u6700\u4f73\u5b9e\u8df5 ### \ud83d\udd11 \u56db\u5927\u6838\u5fc3\u539f\u5219 \ud83c\udfaf \u5177\u4f53\u660e\u786e \u4f7f\u7528\u7cbe\u786e\u63cf\u8ff0\uff0c\u907f\u514d\u6a21\u7cca\u8bcd\u6c47 \ud83d\udcdd \u5206\u6b65\u7f16\u8f91 \u590d\u6742\u4fee\u6539\u5206\u4e3a\u591a\u4e2a\u7b80\u5355\u6b65\u9aa4 \ud83d\udd12 \u660e\u786e\u4fdd\u7559 \u8bf4\u660e\u54ea\u4e9b\u8981\u4fdd\u6301\u4e0d\u53d8 \ud83d\udd24 \u52a8\u8bcd\u9009\u62e9 \u7528\"change\"\u3001\"replace\"\u800c\u975e\"transform\" ### \ud83d\udccb \u6700\u4f73\u5b9e\u8df5\u6a21\u677f \ud83c\udfaf \u5bf9\u8c61\u4fee\u6539\u6a21\u677f \"Change [object] to [new state], keep [content to preserve] unchanged\" \ud83c\udfa8 \u98ce\u683c\u8f6c\u6362\u6a21\u677f \"Transform to [specific style], while maintaining [composition/character/other] unchanged\" \ud83d\uddbc\ufe0f \u80cc\u666f\u66ff\u6362\u6a21\u677f \"Change the background to [new background], keep the subject in the exact same position and pose\" \ud83d\udcdd \u6587\u672c\u7f16\u8f91\u6a21\u677f \"Replace '[original text]' with '[new text]', maintain the same font style\" \ud83d\udca1 \u5173\u952e\u63d0\u9192 \u8d8a\u5177\u4f53\u8d8a\u597d\uff0cKontext \u64c5\u957f\u7406\u89e3\u8be6\u7ec6\u6307\u4ee4\u5e76\u4fdd\u6301\u4e00\u81f4\u6027\u3002 ## \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 16GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4080 RTX 4090 / A100 ### \ud83c\udfaf \u7f16\u8f91\u80fd\u529b\u8303\u56f4 \u89d2\u8272\u4e00\u81f4\u6027 \u5c40\u90e8\u7f16\u8f91 \u98ce\u683c\u8f6c\u6362 \u80cc\u666f\u66ff\u6362 \u6587\u672c\u7f16\u8f91 \u591a\u8f6e\u8fed\u4ee3 ### \ud83c\udf10 \u8bed\u8a00\u652f\u6301 \ud83d\udd24 \u63d0\u793a\u8bcd\u8bed\u8a00 \u76ee\u524d\u4ec5\u652f\u6301\u82f1\u6587\u63d0\u793a\u8bcd\u8f93\u5165 \ud83c\udfaf \u7f16\u8f91\u7cbe\u5ea6 \u652f\u6301\u50cf\u7d20\u7ea7\u7cbe\u786e\u7f16\u8f91\u548c\u8bed\u4e49\u7ea7\u7406\u89e3 --- \ud83c\udfa8 ComfyUI Flux Kontext Dev \u56fe\u50cf\u7f16\u8f91 | \u591a\u6a21\u6001\u667a\u80fd\u56fe\u50cf\u7f16\u8f91\u4e0e\u89d2\u8272\u4e00\u81f4\u6027\u4fdd\u6301\u7684\u5b8c\u7f8e\u7ed3\u5408 \u00a9 2025 Black Forest Labs | \u5f00\u6e90\u534f\u8bae | \u8ba9\u56fe\u50cf\u7f16\u8f91\u53d8\u5f97\u667a\u80fd\u7cbe\u51c6","title":"Index"},{"location":"Flux/1kontext/doc/#flux1-kontext-dev","text":"**FLUX.1 Kontext** \u662f Black Forest Labs \u63a8\u51fa\u7684\u7a81\u7834\u6027\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u548c\u56fe\u50cf\u540c\u65f6\u8f93\u5165\uff0c\u80fd\u591f\u667a\u80fd\u7406\u89e3\u56fe\u50cf\u4e0a\u4e0b\u6587\u5e76\u6267\u884c\u7cbe\u786e\u7f16\u8f91\u3002\u5176\u5f00\u53d1\u7248\u662f\u4e00\u4e2a\u62e5\u6709 120 \u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6269\u6563\u53d8\u538b\u5668\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u548c\u89d2\u8272\u4e00\u81f4\u6027\u4fdd\u6301\uff0c\u5373\u4f7f\u7ecf\u8fc7\u591a\u6b21\u8fed\u4ee3\u7f16\u8f91\uff0c\u4e5f\u80fd\u786e\u4fdd\u4eba\u7269\u7279\u5f81\u3001\u6784\u56fe\u5e03\u5c40\u7b49\u5173\u952e\u5143\u7d20\u4fdd\u6301\u7a33\u5b9a\u3002 \ud83c\udfaf","title":"\ud83d\udccb FLUX.1 Kontext Dev \u6a21\u578b\u6982\u89c8"},{"location":"Flux/1kontext/doc/index-en/","text":"\ud83c\udfa8 ComfyUI Flux Kontext Dev Image Editing ComfyUI Native Workflow - Multimodal Intelligent Image Editing with Character Consistency \ud83c\udfaf Character Consistency \u2702\ufe0f Local Editing \ud83c\udfa8 Style Reference \ud83d\udccb FLUX.1 Kontext Dev Model Overview **FLUX.1 Kontext** is a breakthrough multimodal image editing model launched by Black Forest Labs, supporting simultaneous text and image input, capable of intelligently understanding image context and performing precise editing. The development version is an open-source diffusion transformer model with 12 billion parameters, featuring excellent contextual understanding capabilities and character consistency preservation. Even after multiple iterative edits, it ensures that key elements such as character features and compositional layout remain stable. \ud83c\udfaf Character Consistency Preserves unique elements of images across multiple scenes and environments, such as reference characters or objects \u2702\ufe0f Local Editing Targeted modifications to specific elements in images without affecting other parts \ud83c\udfa8 Style Reference Generates novel scenes based on text prompts while preserving the unique style of reference images \u26a1 Interactive Speed Minimal latency in image generation and editing, supporting rapid iteration ### \ud83d\udd04 Version Comparison Version Type Features Usage FLUX.1 Kontext [pro] Commercial Focused on rapid iterative editing API calls FLUX.1 Kontext [max] Experimental Enhanced prompt following capabilities API calls FLUX.1 Kontext [dev] Open Source 12B parameters, primarily for research Local deployment \ud83d\udca1 Version Selection Guide \u2022 Pro/Max versions : Called via API nodes , suitable for commercial applications \u2022 Dev version : Local deployment, suitable for research and experimentation, fully open source \ud83d\udee0\ufe0f Workflow Type Description This tutorial provides two workflow types that are essentially functionally identical but differ in user experience: \ud83c\udfaf Group Node Version Uses FLUX.1 Kontext Image Edit group node Clean interface, easy to reuse Quick add group node functionality Suitable for complex workflow construction \ud83d\udd27 Complete Original Version Shows complete node connection structure All nodes visible, convenient for learning More intuitive parameter adjustment Suitable for deep customization ### \ud83d\ude80 Quick Add Group Node Feature \u26a0\ufe0f Experimental Feature This feature is currently experimental and may be adjusted in future versions. \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Workflow Templates \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83d\udd17 Model File To run the workflow smoothly, you need to download the following model files. You can also load the corresponding workflow directly to get model download links. #### \ud83d\udcc2 Model File Structure \ud83d\udcc2 ComfyUI/ \u251c\u2500\u2500 \ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 flux1-dev-kontext_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 vae/ \u2502 \u2502 \u2514\u2500\u2500 ae.safetensors \u2502 \u2514\u2500\u2500 \ud83d\udcc2 text_encoders/ \u2502 \u251c\u2500\u2500 clip_l.safetensors \u2502 \u2514\u2500\u2500 t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn_scaled.safetensors ## \ud83d\ude80 Flux.1 Kontext Dev Workflow Example This workflow uses the `Load Image(from output)` node to load images that need editing, making it more convenient to obtain edited images for multi-round editing. ### \ud83d\udce5 Step 1: Download Workflow and Input Images open Comfyui workflow template ![img.png](img.png) Click image to download, drag into ComfyUI to load workflow ### \ud83d\udcc1 Sample Input Image \ud83d\udc30 Sample Input Image Right-click to save image for workflow testing ### \ud83d\udd27 Step 2: Workflow Configuration Operations #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 Model Loading Load Diffusion Model : flux1-dev-kontext_fp8_scaled.safetensors DualCLIP Load : clip_l.safetensors and t5xxl_fp16.safetensors Load VAE : ae.safetensors \ud83d\udcc1 Image Loading Load the provided input image in the Load Image(from output) node \ud83d\udcdd Prompt Settings Modify prompts in the CLIP Text Encode node, English only supported #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click Queue button or use shortcut Ctrl(Cmd) + Enter to run workflow ## \ud83d\udcbb Python API \ud83d\udc0d Python API import requests import json import uuid import time import random import os # Configuration Parameters - Flux Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" UNET_MODEL = \"flux1-dev.safetensors\" VAE_MODEL = \"flux-ae.safetensors\" CLIP_L_MODEL = \"clip_l_bf16.safetensors\" CLIP_T5_MODEL = \"t5xxl_fp16.safetensors\" # Default Parameters DEFAULT_PROMPT = \"A beautiful fantasy girl with long curly silver hair and big blue eyes, wearing a white transparent fairy dress with lace and puffed sleeves. Surrounded by iridescent butterflies and giant glass roses, dreamy lighting, ethereal atmosphere, soft glow, magical realism, highly detailed, cinematic, 8K render.\" DEFAULT_T5_PROMPT = \"A fairy tale scene of a young girl with silver curly hair wearing a delicate white dress, standing among crystal butterflies and glowing glass roses. The scene is filled with soft magical light, like a dream from a fantasy world.\" class ComfyUIFluxClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_flux_image(self, clip_l_prompt, t5xxl_prompt=None, steps=20, cfg=1, guidance=3.5, width=1024, height=1024, seed=None): \"\"\"Generate Flux text-to-image based on original JSON workflow\"\"\" print(\"Starting Flux text-to-image generation...\") # Use CLIP-L prompt for T5XXL if not provided if t5xxl_prompt is None: t5xxl_prompt = clip_l_prompt # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Workflow based on your provided JSON workflow = { \"8\": { \"inputs\": { \"samples\": [\"31\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"9\": { \"inputs\": { \"filename_prefix\": \"flux_output\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"27\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": {\"title\": \"Empty SD3 Latent Image\"} }, \"31\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"38\", 0], \"positive\": [\"41\", 0], \"negative\": [\"42\", 0], \"latent_image\": [\"27\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"38\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"40\": { \"inputs\": { \"clip_name1\": CLIP_L_MODEL, \"clip_name2\": CLIP_T5_MODEL, \"type\": \"flux\", \"device\": \"default\" }, \"class_type\": \"DualCLIPLoader\", \"_meta\": {\"title\": \"Dual CLIP Loader\"} }, \"41\": { \"inputs\": { \"clip_l\": clip_l_prompt, \"t5xxl\": t5xxl_prompt, \"guidance\": guidance, \"clip\": [\"40\", 0] }, \"class_type\": \"CLIPTextEncodeFlux\", \"_meta\": {\"title\": \"CLIP Text Encode Flux\"} }, \"42\": { \"inputs\": { \"conditioning\": [\"41\", 0] }, \"class_type\": \"ConditioningZeroOut\", \"_meta\": {\"title\": \"Conditioning Zero Out\"} } } print(\"Submitting Flux text-to-image workflow...\") print(f\"CLIP-L Prompt: {clip_l_prompt}\") print(f\"T5XXL Prompt: {t5xxl_prompt}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompts_list, **kwargs): \"\"\"Batch generate images\"\"\" results = [] for i, prompt_data in enumerate(prompts_list): print(f\"\\nStarting task {i+1}/{len(prompts_list)}...\") if isinstance(prompt_data, str): # If string, use as CLIP-L prompt clip_l = prompt_data t5xxl = None elif isinstance(prompt_data, dict): # If dict, extract CLIP-L and T5XXL prompts clip_l = prompt_data.get('clip_l', prompt_data.get('prompt', '')) t5xxl = prompt_data.get('t5xxl') else: print(f\"Invalid prompt format: {prompt_data}\") continue try: task_id, seed = self.generate_flux_image(clip_l, t5xxl, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'prompt': {'clip_l': clip_l, 't5xxl': t5xxl} }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Flux text-to-image generation\"\"\" client = ComfyUIFluxClient() try: print(\"Flux text-to-image client started...\") # Single image generation example print(\"\\n=== Single Image Generation ===\") task_id, seed = client.generate_flux_image( clip_l_prompt=DEFAULT_PROMPT, t5xxl_prompt=DEFAULT_T5_PROMPT, steps=20, guidance=3.5, width=1024, height=1024 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(5) # Download images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example (optional) print(\"\\n=== Batch Generation Example ===\") batch_prompts = [ \"A majestic dragon flying over a medieval castle at sunset\", \"A cyberpunk cityscape with neon lights and flying cars\", { 'clip_l': \"A serene forest with magical creatures\", 't5xxl': \"An enchanted woodland scene with fairies and unicorns dancing in moonlight\" } ] # Uncomment the following lines to execute batch generation # batch_results = client.generate_batch(batch_prompts, steps=15, guidance=3.0) # print(f\"Batch generation completed, generated {len(batch_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() ## \ud83d\udca1 Flux Kontext Prompt Techniques Mastering correct prompt techniques is key to achieving ideal editing results. Here are verified best practices: ### \ud83c\udfaf Basic Modification Techniques \u2705 Recommended Practices Simple and Direct : \"Change the car color to red\" Maintain Style : \"Change to daytime while maintaining the same style of the painting\" \u274c Avoid These Using vague vocabulary Overly complex single-sentence descriptions Lack of specific targeting ### \ud83c\udfa8 Style Transfer Framework \ud83c\udfa8 Style Transfer Principles \u2022 Clearly Name Style : \"Transform to Bauhaus art style\" \u2022 Describe Features : \"Transform to oil painting with visible brushstrokes, thick paint texture\" \u2022 Preserve Composition : \"Change to Bauhaus style while maintaining the original composition\" ### \ud83d\udc64 Character Consistency Maintenance \ud83d\udc64 Character Consistency Framework \u2022 Specific Description : \"The woman with short black hair\" instead of \"she\" \u2022 Preserve Features : \"while maintaining the same facial features, hairstyle, and expression\" \u2022 Step-by-step Modification : Change background first, then actions ### \ud83d\udcdd Text Editing Techniques \ud83d\udcdd Text Editing Key Points \u2022 Use Quotes : \"Replace 'joy' with 'BFL'\" \u2022 Maintain Format : \"Replace text while maintaining the same font style\" ## \ud83d\udea8 Common Problem Solutions ### \ud83c\udfad Excessive Character Changes \u274c Wrong Example \"Transform the person into a Viking\" \u2705 Correct Example \"Change the clothes to be a viking warrior while preserving facial features\" ### \ud83d\udcd0 Composition Position Changes \u274c Wrong Example \"Put him on a beach\" \u2705 Correct Example \"Change the background to a beach while keeping the person in the exact same position, scale, and pose\" ### \ud83c\udfa8 Inaccurate Style Application \u274c Wrong Example \"Make it a sketch\" \u2705 Correct Example \"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\" ## \ud83c\udfaf Core Principles and Best Practices ### \ud83d\udd11 Four Core Principles \ud83c\udfaf Be Specific and Clear Use precise descriptions, avoid vague vocabulary \ud83d\udcdd Step-by-step Editing Break complex modifications into multiple simple steps \ud83d\udd12 Clearly Preserve Specify what should remain unchanged \ud83d\udd24 Verb Selection Use \"change\", \"replace\" instead of \"transform\" ### \ud83d\udccb Best Practice Templates \ud83c\udfaf Object Modification Template \"Change [object] to [new state], keep [content to preserve] unchanged\" \ud83c\udfa8 Style Transfer Template \"Transform to [specific style], while maintaining [composition/character/other] unchanged\" \ud83d\uddbc\ufe0f Background Replacement Template \"Change the background to [new background], keep the subject in the exact same position and pose\" \ud83d\udcdd Text Editing Template \"Replace '[original text]' with '[new text]', maintain the same font style\" \ud83d\udca1 Key Reminder The more specific, the better. Kontext excels at understanding detailed instructions and maintaining consistency. ## \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 16GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4080 RTX 4090 / A100 ### \ud83c\udfaf Editing Capability Range Character Consistency Local Editing Style Transfer Background Replacement Text Editing Multi-round Iteration ### \ud83c\udf10 Language Support \ud83d\udd24 Prompt Language Currently supports English prompts only \ud83c\udfaf Editing Precision Supports pixel-level precise editing and semantic-level understanding ### \u26a1 Performance Benchmarks \ud83c\udfc6 Model Parameters 12 billion parameter diffusion transformer model with excellent contextual understanding \u26a1 Generation Speed Minimal latency for image generation and editing, supporting rapid iteration workflows \ud83c\udfaf Consistency Accuracy Maintains character features and compositional layout even after multiple iterative edits ## \ud83c\udfaf Application Scenarios \ud83c\udfac Content Creation Social media content, marketing materials, and creative storytelling \ud83c\udfa8 Artistic Design Concept art, character design, and visual style exploration \ud83c\udfe2 Commercial Applications Product visualization, advertising campaigns, and brand consistency \ud83d\udd2c Research & Development AI research, model experimentation, and academic studies ## \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Use clear, specific editing instructions Maintain appropriate input image quality and resolution Break complex edits into multiple simple steps Specify what elements should remain unchanged Use the Load Image(from output) node for iterative editing \u26a0\ufe0f Important Notes Ensure ComfyUI is updated to the latest development version English prompts only - no multilingual support currently Complex edits may require multiple iterations for best results Monitor VRAM usage with large images or long editing sessions Save intermediate results for complex multi-step workflows ## \ud83d\ude80 Advanced Workflow Techniques ### \ud83d\udd04 Multi-Round Editing Strategy 1\ufe0f\u20e3 Initial Setup Load your base image and establish the primary subject or scene 2\ufe0f\u20e3 Background Changes Modify backgrounds first while preserving subject position and scale 3\ufe0f\u20e3 Style Adjustments Apply style changes while maintaining character consistency 4\ufe0f\u20e3 Detail Refinement Fine-tune specific elements and add finishing touches ### \ud83c\udfaf Prompt Optimization Strategies \ud83d\udd0d Progressive Refinement Start with broad changes and progressively add more specific details in subsequent iterations. This approach helps maintain consistency while achieving complex transformations. --- \ud83c\udfa8 ComfyUI Flux Kontext Dev Image Editing | Perfect Combination of Multimodal Intelligent Image Editing and Character Consistency \u00a9 2025 Black Forest Labs | Open Source License | Making Image Editing Intelligent and Precise","title":"Index en"},{"location":"Flux/1kontext/doc/index-en/#flux1-kontext-dev-model-overview","text":"**FLUX.1 Kontext** is a breakthrough multimodal image editing model launched by Black Forest Labs, supporting simultaneous text and image input, capable of intelligently understanding image context and performing precise editing. The development version is an open-source diffusion transformer model with 12 billion parameters, featuring excellent contextual understanding capabilities and character consistency preservation. Even after multiple iterative edits, it ensures that key elements such as character features and compositional layout remain stable. \ud83c\udfaf","title":"\ud83d\udccb FLUX.1 Kontext Dev Model Overview"},{"location":"Flux/1kontext/doc/index-en/#workflow-type-description","text":"This tutorial provides two workflow types that are essentially functionally identical but differ in user experience:","title":"\ud83d\udee0\ufe0f Workflow Type Description"},{"location":"Flux/1kontext/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status","title":"\u26a0\ufe0f Environment Requirements"},{"location":"Flux/1kontext/doc/index-en/#model-file","text":"To run the workflow smoothly, you need to download the following model files. You can also load the corresponding workflow directly to get model download links. #### \ud83d\udcc2 Model File Structure \ud83d\udcc2 ComfyUI/ \u251c\u2500\u2500 \ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 flux1-dev-kontext_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 vae/ \u2502 \u2502 \u2514\u2500\u2500 ae.safetensors \u2502 \u2514\u2500\u2500 \ud83d\udcc2 text_encoders/ \u2502 \u251c\u2500\u2500 clip_l.safetensors \u2502 \u2514\u2500\u2500 t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn_scaled.safetensors ## \ud83d\ude80 Flux.1 Kontext Dev Workflow Example This workflow uses the `Load Image(from output)` node to load images that need editing, making it more convenient to obtain edited images for multi-round editing. ### \ud83d\udce5 Step 1: Download Workflow and Input Images open Comfyui workflow template ![img.png](img.png) Click image to download, drag into ComfyUI to load workflow ### \ud83d\udcc1 Sample Input Image","title":"\ud83d\udd17 Model File"},{"location":"SD/1.5/doc/","text":"\ud83c\udfa8 Stable Diffusion 1.5 \u6a21\u578b\u4f7f\u7528\u6307\u5357 \u5f00\u6e90AI\u56fe\u50cf\u751f\u6210\u7684\u7ecf\u5178\u4e4b\u4f5c \u2022 \u8f7b\u91cf\u9ad8\u6548 \u2022 \u751f\u6001\u4e30\u5bcc \ud83d\udcd6 \u6a21\u578b\u7b80\u4ecb **Stable Diffusion 1.5** \u662f\u7531 Stability AI \u5f00\u53d1\u7684\u7ecf\u5178\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u4f5c\u4e3a\u5f00\u6e90AI\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u91cc\u7a0b\u7891\u4e4b\u4f5c\uff0c\u81f3\u4eca\u4ecd\u662f\u6700\u53d7\u6b22\u8fce\u548c\u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u6a21\u578b\u4e4b\u4e00\u3002\u8be5\u6a21\u578b\u4ee5\u5176\u8f7b\u91cf\u5316\u3001\u9ad8\u6548\u7387\u548c\u4e30\u5bcc\u7684\u751f\u6001\u7cfb\u7edf\u800c\u95fb\u540d\uff0c\u662fAI\u56fe\u50cf\u751f\u6210\u7684\u5165\u95e8\u9996\u9009\u3002 \ud83d\ude80 \u6838\u5fc3\u7279\u6027 \ud83d\ude80 \u6027\u80fd\u4f18\u52bf \u8f7b\u91cf\u9ad8\u6548 : \u4ec5\u97006GB\u663e\u5b58\u5373\u53ef\u8fd0\u884c \u5feb\u901f\u751f\u6210 : \u63a8\u7406\u901f\u5ea6\u5feb\uff0c\u9002\u5408\u6279\u91cf\u751f\u6210 \u7a33\u5b9a\u53ef\u9760 : \u7ecf\u8fc7\u5927\u91cf\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1 \ud83c\udfa8 \u529f\u80fd\u7279\u8272 \u98ce\u683c\u591a\u6837 : \u652f\u6301\u5199\u5b9e\u3001\u4e8c\u6b21\u5143\u3001\u827a\u672f\u7b49\u591a\u79cd\u98ce\u683c \u6613\u4e8e\u5b9a\u5236 : \u652f\u6301LoRA\u3001Textual Inversion\u7b49\u5fae\u8c03\u6280\u672f \u751f\u6001\u4e30\u5bcc : \u62e5\u6709\u5e9e\u5927\u7684\u793e\u533a\u548c\u4e30\u5bcc\u7684\u6269\u5c55\u751f\u6001 \ud83d\udcca \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08Text-to-Image\uff09/ \u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\uff08Image-to-Image\uff09 \u53c2\u6570\u89c4\u6a21 \u7ea6860M\u53c2\u6570 \u6587\u672c\u7f16\u7801\u5668 CLIP ViT-L/14 VAE 512\u00d7512\u539f\u751f\u5206\u8fa8\u7387VAE \u63a8\u8350\u6b65\u6570 20-50\u6b65 \u8bb8\u53ef\u8bc1 \u5f00\u6e90\u514d\u8d39\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \ud83d\udca1 \u6700\u4f4e\u914d\u7f6e\u8981\u6c42 ECS\u7684GPU\u663e\u5b58 : 6GB\u4ee5\u4e0a \ud83d\udcc1 \u6a21\u578b\u6587\u4ef6 \u6587\u4ef6\u7c7b\u578b \u6587\u4ef6\u540d \u8bf4\u660e \u4e3b\u6a21\u578b v1-5-pruned-emaonly.safetensors \u6838\u5fc3\u751f\u6210\u6a21\u578b VAE vae-ft-mse-840000-ema-pruned.safetensors \u53ef\u9009\u7684\u9ad8\u8d28\u91cfVAE \u6587\u672c\u7f16\u7801\u5668 - \u5185\u7f6e\u4e8e\u4e3b\u6a21\u578b\u4e2d \ud83d\udcd6 \u4f7f\u7528\u6307\u5357 \ud83c\udf10 Web UI \u4f7f\u7528 \ud83d\udccb \u57fa\u7840\u64cd\u4f5c\u6d41\u7a0b **1. \u6a21\u578b\u9009\u62e9** - \u5728\u5de6\u4e0a\u89d2\u6a21\u578b\u9009\u62e9\u5668\u4e2d\u9009\u62e9SD1.5\u6a21\u578b ![img.png](img.png) **2. \u63d0\u793a\u8bcd\u8f93\u5165** - **\u6b63\u5411\u63d0\u793a\u8bcd**: \u8be6\u7ec6\u63cf\u8ff0\u60f3\u8981\u751f\u6210\u7684\u56fe\u50cf\u5185\u5bb9 - **\u8d1f\u5411\u63d0\u793a\u8bcd**: \u63cf\u8ff0\u4e0d\u60f3\u8981\u7684\u5143\u7d20\uff08SD1.5\u5bf9\u8d1f\u5411\u63d0\u793a\u8bcd\u54cd\u5e94\u826f\u597d\uff09 **3. \u53c2\u6570\u8bbe\u7f6e** - **\u6b65\u6570**: \u63a8\u8350 20-30\u6b65\uff08\u751f\u6210\u8d28\u91cf\u4e0e\u901f\u5ea6\u7684\u5e73\u8861\uff09 - **CFG Scale**: \u63a8\u8350 7-12\uff08\u63d0\u793a\u8bcd\u9075\u5faa\u7a0b\u5ea6\uff09 - **\u91c7\u6837\u5668**: \u63a8\u8350 DPM++ 2M Karras / Euler a - **\u5206\u8fa8\u7387**: 512\u00d7512\uff08\u539f\u751f\u5206\u8fa8\u7387\uff0c\u6548\u679c\u6700\u4f73\uff09 **4. \u9ad8\u7ea7\u8bbe\u7f6e** - **\u79cd\u5b50**: \u63a7\u5236\u968f\u673a\u6027\uff0c-1\u4e3a\u968f\u673a - **\u6279\u6b21**: \u8bbe\u7f6e\u751f\u6210\u6570\u91cf - **\u9ad8\u5206\u8fa8\u7387\u4fee\u590d**: \u751f\u6210\u66f4\u5927\u5c3a\u5bf8\u56fe\u50cf \ud83c\udfa8 \u63a8\u8350\u53c2\u6570\u7ec4\u5408 \u26a1 \u5feb\u901f\u751f\u6210 \u6b65\u6570 : 20\u6b65 CFG : 7 \u91c7\u6837\u5668 : Euler a \u5206\u8fa8\u7387 : 512\u00d7512 \ud83d\udc8e \u9ad8\u8d28\u91cf \u6b65\u6570 : 30\u6b65 CFG : 9-11 \u91c7\u6837\u5668 : DPM++ 2M Karras \u5206\u8fa8\u7387 : 768\u00d7768 \ud83c\udfa8 \u827a\u672f\u98ce\u683c \u6b65\u6570 : 25\u6b65 CFG : 8-10 \u91c7\u6837\u5668 : DDIM \u5206\u8fa8\u7387 : 512\u00d7768 \ud83d\udd0c API \u8c03\u7528 \ud83d\udca1 \u91cd\u8981\u63d0\u793a \u9700\u8981\u5c06 BASE_URL \u548c APIKEY \u66ff\u6362\u4e3a\u5b9e\u9645\u503c\u3002\u5982\u679c\u8981\u7528\u516c\u7f51\u8c03\u7528\uff0c\u5219\u9009\u62e9\u516c\u7f51\u7684ip:\u7aef\u53e3\u3002 \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00API\u8c03\u7528Python\u4ee3\u7801 import requests import base64 # \ud83d\udd27 \u914d\u7f6e\u4fe1\u606f base_url = \"<\u90e8\u7f72\u670d\u52a1\u7684Output URL>\" username = \"admin\" apikey = \"${APIKEY}\" auth = (username, apikey) # \ud83d\udd04 1. \u5207\u6362\u6a21\u578b model_data = { \"sd_model_checkpoint\": \"v1-5-pruned-emaonly.safetensors\" } print(\"\ud83d\udd04 \u6b63\u5728\u5207\u6362\u6a21\u578b...\") model_response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data, auth=auth) print(\"\u2705 \u6a21\u578b\u5207\u6362\u5b8c\u6210\") # \ud83c\udfa8 2. \u751f\u6210\u56fe\u7247 prompt = \"a beautiful cat\" data = { \"prompt\": prompt, \"steps\": 20, \"width\": 512, \"height\": 512 } print(\"\ud83c\udfa8 \u6b63\u5728\u751f\u6210\u56fe\u7247...\") response = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json=data, auth=auth) result = response.json() # \ud83d\udcbe 3. \u4fdd\u5b58\u56fe\u7247 if \"images\" in result: image_data = base64.b64decode(result[\"images\"][0]) with open(\"output.png\", \"wb\") as f: f.write(image_data) print(\"\u2705 \u56fe\u7247\u5df2\u4fdd\u5b58\u4e3a output.png\") else: print(\"\u274c \u9519\u8bef:\", result) \ud83d\udca1 \u63d0\u793a \u82e5\u672a\u5f00\u542fAPIKEY\uff0c\u8bf7\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u4fee\u6539\u8bf7\u6c42\uff1a model_response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data) \ud83d\udcda \u76f8\u5173\u8d44\u6e90 \ud83d\udcd6 \u5b98\u65b9\u6587\u6863 Stable Diffusion\u5b98\u65b9\u6587\u6863 \u4e86\u89e3\u6a21\u578b\u7684\u5b98\u65b9\u4ecb\u7ecd\u548c\u6280\u672f\u7ec6\u8282 \ud83d\udda5\ufe0f Web\u754c\u9762 Automatic1111 WebUI \u6700\u53d7\u6b22\u8fce\u7684SD Web\u754c\u9762\u9879\u76ee \ud83c\udfa8 \u6a21\u578b\u793e\u533a Civitai\u6a21\u578b\u793e\u533a \u4e0b\u8f7d\u5404\u79cd\u98ce\u683c\u7684SD\u6a21\u578b\u548cLoRA \u270d\ufe0f \u63d0\u793a\u8bcd\u6307\u5357 \u63d0\u793a\u8bcd\u5de5\u7a0b\u6307\u5357 \u5b66\u4e60\u5982\u4f55\u7f16\u5199\u9ad8\u8d28\u91cf\u7684\u63d0\u793a\u8bcd \ud83c\udfaf \u6700\u4f73\u5b9e\u8df5 \u270d\ufe0f \u63d0\u793a\u8bcd\u6280\u5de7 \u4f7f\u7528\u5177\u4f53\u7684\u63cf\u8ff0\u8bcd\u800c\u975e\u62bd\u8c61\u6982\u5ff5 \u6dfb\u52a0\u827a\u672f\u5bb6\u540d\u5b57\u6765\u6307\u5b9a\u98ce\u683c \u4f7f\u7528\u6743\u91cd\u8bed\u6cd5 (word:1.2) \u5f3a\u8c03\u91cd\u8981\u5143\u7d20 \u8d1f\u5411\u63d0\u793a\u8bcd\u6392\u9664\u4e0d\u9700\u8981\u7684\u5185\u5bb9 \u2699\ufe0f \u53c2\u6570\u8c03\u4f18 \u4ece\u4f4e\u6b65\u6570\u5f00\u59cb\u6d4b\u8bd5\uff0c\u9010\u6b65\u589e\u52a0 CFG\u8fc7\u9ad8\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u9971\u548c \u4e0d\u540c\u91c7\u6837\u5668\u9002\u5408\u4e0d\u540c\u98ce\u683c \u4f7f\u7528\u56fa\u5b9a\u79cd\u5b50\u590d\u73b0\u6ee1\u610f\u7ed3\u679c \u2753 \u5e38\u89c1\u95ee\u9898 \u2753 \u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4e0d\u4f73\u600e\u4e48\u529e\uff1f \u89e3\u51b3\u65b9\u6848\uff1a \u589e\u52a0\u751f\u6210\u6b65\u6570\u523030-50\u6b65 \u8c03\u6574CFG Scale\u52308-12 \u4f7f\u7528\u66f4\u8be6\u7ec6\u7684\u63d0\u793a\u8bcd \u5c1d\u8bd5\u4e0d\u540c\u7684\u91c7\u6837\u5668 \u8003\u8651\u4f7f\u7528\u9ad8\u8d28\u91cfVAE \u2753 \u663e\u5b58\u4e0d\u8db3\u5982\u4f55\u5904\u7406\uff1f \u4f18\u5316\u5efa\u8bae\uff1a \u964d\u4f4e\u751f\u6210\u5206\u8fa8\u7387\u5230512\u00d7512 \u51cf\u5c11\u6279\u6b21\u5927\u5c0f \u542f\u7528\u4f4e\u663e\u5b58\u6a21\u5f0f \u5173\u95ed\u4e0d\u5fc5\u8981\u7684\u6269\u5c55 \u4f7f\u7528\u7cbe\u5ea6\u4f18\u5316\u9009\u9879 \u2753 \u5982\u4f55\u83b7\u5f97\u7279\u5b9a\u98ce\u683c\u7684\u56fe\u50cf\uff1f \u98ce\u683c\u63a7\u5236\u65b9\u6cd5\uff1a \u5728\u63d0\u793a\u8bcd\u4e2d\u6dfb\u52a0\u827a\u672f\u5bb6\u540d\u5b57 \u4f7f\u7528\u98ce\u683c\u5173\u952e\u8bcd\uff08\u5982\"oil painting\", \"anime style\"\uff09 \u4e0b\u8f7d\u5e76\u4f7f\u7528\u4e13\u95e8\u7684\u98ce\u683cLoRA \u53c2\u8003\u793e\u533a\u5206\u4eab\u7684\u63d0\u793a\u8bcd\u6a21\u677f \u8c03\u6574CFG Scale\u5f71\u54cd\u98ce\u683c\u5f3a\u5ea6 \ud83c\udfa8 \u5f00\u59cb\u4f60\u7684AI\u827a\u672f\u521b\u4f5c\u4e4b\u65c5\uff01 | Stable Diffusion 1.5 - \u8ba9\u521b\u610f\u65e0\u9650\u53ef\u80fd","title":"Index"},{"location":"SD/1.5/doc/#_1","text":"**Stable Diffusion 1.5** \u662f\u7531 Stability AI \u5f00\u53d1\u7684\u7ecf\u5178\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u4f5c\u4e3a\u5f00\u6e90AI\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u91cc\u7a0b\u7891\u4e4b\u4f5c\uff0c\u81f3\u4eca\u4ecd\u662f\u6700\u53d7\u6b22\u8fce\u548c\u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u6a21\u578b\u4e4b\u4e00\u3002\u8be5\u6a21\u578b\u4ee5\u5176\u8f7b\u91cf\u5316\u3001\u9ad8\u6548\u7387\u548c\u4e30\u5bcc\u7684\u751f\u6001\u7cfb\u7edf\u800c\u95fb\u540d\uff0c\u662fAI\u56fe\u50cf\u751f\u6210\u7684\u5165\u95e8\u9996\u9009\u3002","title":"\ud83d\udcd6 \u6a21\u578b\u7b80\u4ecb"},{"location":"SD/1.5/doc/#_2","text":"","title":"\ud83d\ude80 \u6838\u5fc3\u7279\u6027"},{"location":"SD/1.5/doc/#_3","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08Text-to-Image\uff09/ \u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\uff08Image-to-Image\uff09 \u53c2\u6570\u89c4\u6a21 \u7ea6860M\u53c2\u6570 \u6587\u672c\u7f16\u7801\u5668 CLIP ViT-L/14 VAE 512\u00d7512\u539f\u751f\u5206\u8fa8\u7387VAE \u63a8\u8350\u6b65\u6570 20-50\u6b65 \u8bb8\u53ef\u8bc1 \u5f00\u6e90\u514d\u8d39\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528","title":"\ud83d\udcca \u6280\u672f\u89c4\u683c"},{"location":"SD/1.5/doc/#_4","text":"","title":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e"},{"location":"SD/1.5/doc/#_5","text":"\ud83d\udca1 \u6700\u4f4e\u914d\u7f6e\u8981\u6c42 ECS\u7684GPU\u663e\u5b58 : 6GB\u4ee5\u4e0a","title":"\ud83d\udcbb \u7cfb\u7edf\u8981\u6c42"},{"location":"SD/1.5/doc/#_6","text":"\u6587\u4ef6\u7c7b\u578b \u6587\u4ef6\u540d \u8bf4\u660e \u4e3b\u6a21\u578b v1-5-pruned-emaonly.safetensors \u6838\u5fc3\u751f\u6210\u6a21\u578b VAE vae-ft-mse-840000-ema-pruned.safetensors \u53ef\u9009\u7684\u9ad8\u8d28\u91cfVAE \u6587\u672c\u7f16\u7801\u5668 - \u5185\u7f6e\u4e8e\u4e3b\u6a21\u578b\u4e2d","title":"\ud83d\udcc1 \u6a21\u578b\u6587\u4ef6"},{"location":"SD/1.5/doc/#_7","text":"","title":"\ud83d\udcd6 \u4f7f\u7528\u6307\u5357"},{"location":"SD/1.5/doc/#web-ui","text":"","title":"\ud83c\udf10 Web UI \u4f7f\u7528"},{"location":"SD/1.5/doc/#_8","text":"**1. \u6a21\u578b\u9009\u62e9** - \u5728\u5de6\u4e0a\u89d2\u6a21\u578b\u9009\u62e9\u5668\u4e2d\u9009\u62e9SD1.5\u6a21\u578b ![img.png](img.png) **2. \u63d0\u793a\u8bcd\u8f93\u5165** - **\u6b63\u5411\u63d0\u793a\u8bcd**: \u8be6\u7ec6\u63cf\u8ff0\u60f3\u8981\u751f\u6210\u7684\u56fe\u50cf\u5185\u5bb9 - **\u8d1f\u5411\u63d0\u793a\u8bcd**: \u63cf\u8ff0\u4e0d\u60f3\u8981\u7684\u5143\u7d20\uff08SD1.5\u5bf9\u8d1f\u5411\u63d0\u793a\u8bcd\u54cd\u5e94\u826f\u597d\uff09 **3. \u53c2\u6570\u8bbe\u7f6e** - **\u6b65\u6570**: \u63a8\u8350 20-30\u6b65\uff08\u751f\u6210\u8d28\u91cf\u4e0e\u901f\u5ea6\u7684\u5e73\u8861\uff09 - **CFG Scale**: \u63a8\u8350 7-12\uff08\u63d0\u793a\u8bcd\u9075\u5faa\u7a0b\u5ea6\uff09 - **\u91c7\u6837\u5668**: \u63a8\u8350 DPM++ 2M Karras / Euler a - **\u5206\u8fa8\u7387**: 512\u00d7512\uff08\u539f\u751f\u5206\u8fa8\u7387\uff0c\u6548\u679c\u6700\u4f73\uff09 **4. \u9ad8\u7ea7\u8bbe\u7f6e** - **\u79cd\u5b50**: \u63a7\u5236\u968f\u673a\u6027\uff0c-1\u4e3a\u968f\u673a - **\u6279\u6b21**: \u8bbe\u7f6e\u751f\u6210\u6570\u91cf - **\u9ad8\u5206\u8fa8\u7387\u4fee\u590d**: \u751f\u6210\u66f4\u5927\u5c3a\u5bf8\u56fe\u50cf","title":"\ud83d\udccb \u57fa\u7840\u64cd\u4f5c\u6d41\u7a0b"},{"location":"SD/1.5/doc/#_9","text":"","title":"\ud83c\udfa8 \u63a8\u8350\u53c2\u6570\u7ec4\u5408"},{"location":"SD/1.5/doc/#api","text":"\ud83d\udca1 \u91cd\u8981\u63d0\u793a \u9700\u8981\u5c06 BASE_URL \u548c APIKEY \u66ff\u6362\u4e3a\u5b9e\u9645\u503c\u3002\u5982\u679c\u8981\u7528\u516c\u7f51\u8c03\u7528\uff0c\u5219\u9009\u62e9\u516c\u7f51\u7684ip:\u7aef\u53e3\u3002 \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00API\u8c03\u7528Python\u4ee3\u7801 import requests import base64 # \ud83d\udd27 \u914d\u7f6e\u4fe1\u606f base_url = \"<\u90e8\u7f72\u670d\u52a1\u7684Output URL>\" username = \"admin\" apikey = \"${APIKEY}\" auth = (username, apikey) # \ud83d\udd04 1. \u5207\u6362\u6a21\u578b model_data = { \"sd_model_checkpoint\": \"v1-5-pruned-emaonly.safetensors\" } print(\"\ud83d\udd04 \u6b63\u5728\u5207\u6362\u6a21\u578b...\") model_response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data, auth=auth) print(\"\u2705 \u6a21\u578b\u5207\u6362\u5b8c\u6210\") # \ud83c\udfa8 2. \u751f\u6210\u56fe\u7247 prompt = \"a beautiful cat\" data = { \"prompt\": prompt, \"steps\": 20, \"width\": 512, \"height\": 512 } print(\"\ud83c\udfa8 \u6b63\u5728\u751f\u6210\u56fe\u7247...\") response = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json=data, auth=auth) result = response.json() # \ud83d\udcbe 3. \u4fdd\u5b58\u56fe\u7247 if \"images\" in result: image_data = base64.b64decode(result[\"images\"][0]) with open(\"output.png\", \"wb\") as f: f.write(image_data) print(\"\u2705 \u56fe\u7247\u5df2\u4fdd\u5b58\u4e3a output.png\") else: print(\"\u274c \u9519\u8bef:\", result) \ud83d\udca1 \u63d0\u793a \u82e5\u672a\u5f00\u542fAPIKEY\uff0c\u8bf7\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u4fee\u6539\u8bf7\u6c42\uff1a model_response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data)","title":"\ud83d\udd0c API \u8c03\u7528"},{"location":"SD/1.5/doc/#_10","text":"","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90"},{"location":"SD/1.5/doc/#_11","text":"","title":"\ud83c\udfaf \u6700\u4f73\u5b9e\u8df5"},{"location":"SD/1.5/doc/#_12","text":"\u2753 \u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4e0d\u4f73\u600e\u4e48\u529e\uff1f \u89e3\u51b3\u65b9\u6848\uff1a \u589e\u52a0\u751f\u6210\u6b65\u6570\u523030-50\u6b65 \u8c03\u6574CFG Scale\u52308-12 \u4f7f\u7528\u66f4\u8be6\u7ec6\u7684\u63d0\u793a\u8bcd \u5c1d\u8bd5\u4e0d\u540c\u7684\u91c7\u6837\u5668 \u8003\u8651\u4f7f\u7528\u9ad8\u8d28\u91cfVAE \u2753 \u663e\u5b58\u4e0d\u8db3\u5982\u4f55\u5904\u7406\uff1f \u4f18\u5316\u5efa\u8bae\uff1a \u964d\u4f4e\u751f\u6210\u5206\u8fa8\u7387\u5230512\u00d7512 \u51cf\u5c11\u6279\u6b21\u5927\u5c0f \u542f\u7528\u4f4e\u663e\u5b58\u6a21\u5f0f \u5173\u95ed\u4e0d\u5fc5\u8981\u7684\u6269\u5c55 \u4f7f\u7528\u7cbe\u5ea6\u4f18\u5316\u9009\u9879 \u2753 \u5982\u4f55\u83b7\u5f97\u7279\u5b9a\u98ce\u683c\u7684\u56fe\u50cf\uff1f \u98ce\u683c\u63a7\u5236\u65b9\u6cd5\uff1a \u5728\u63d0\u793a\u8bcd\u4e2d\u6dfb\u52a0\u827a\u672f\u5bb6\u540d\u5b57 \u4f7f\u7528\u98ce\u683c\u5173\u952e\u8bcd\uff08\u5982\"oil painting\", \"anime style\"\uff09 \u4e0b\u8f7d\u5e76\u4f7f\u7528\u4e13\u95e8\u7684\u98ce\u683cLoRA \u53c2\u8003\u793e\u533a\u5206\u4eab\u7684\u63d0\u793a\u8bcd\u6a21\u677f \u8c03\u6574CFG Scale\u5f71\u54cd\u98ce\u683c\u5f3a\u5ea6 \ud83c\udfa8 \u5f00\u59cb\u4f60\u7684AI\u827a\u672f\u521b\u4f5c\u4e4b\u65c5\uff01 | Stable Diffusion 1.5 - \u8ba9\u521b\u610f\u65e0\u9650\u53ef\u80fd","title":"\u2753 \u5e38\u89c1\u95ee\u9898"},{"location":"SD/1.5/doc/index-en/","text":"\ud83c\udfa8 Stable Diffusion 1.5 Complete Guide The ultimate text-to-image generation model for creators and developers \ud83c\udf1f Model Introduction **Stable Diffusion 1.5** is a revolutionary text-to-image generation model developed by **Stability AI**. As a groundbreaking milestone in the open-source AI image generation landscape, it continues to be one of the most beloved and extensively utilized models in the community. This model has earned its reputation through its **lightweight architecture**, **exceptional efficiency**, and **vibrant ecosystem**, making it the go-to choice for both AI newcomers and seasoned professionals. \u2728 Core Features \ud83d\ude80 Performance Excellence Lightweight & Efficient : Only 6GB VRAM required Lightning Fast : Optimized for batch and real-time generation Rock Solid : Battle-tested across millions of generations \ud83c\udfa8 Creative Powerhouse Style Versatility : Photorealistic, anime, artistic styles Highly Customizable : LoRA, Textual Inversion support Thriving Ecosystem : Massive community & extensions \ud83c\udd93 Open Source Freedom 100% Open Source : Complete transparency Commercial Use Allowed : No licensing restrictions Community Driven : Continuous improvements \ud83d\udcca Technical Specifications Specification Details Model Type Text-to-Image / Image-to-Image Generation Parameters ~860M parameters Text Encoder CLIP ViT-L/14 VAE Resolution 512\u00d7512 (Native) Optimal Steps 20-50 steps License CreativeML Open RAIL-M \u2699\ufe0f Configuration Setup \ud83d\udcbb System Requirements \ud83d\udca1 Minimum Hardware Specifications GPU Memory : 6GB VRAM or higher Recommended: 8GB+ for optimal performance and larger resolutions \ud83d\udcc1 Essential Model Files \ud83c\udfaf Main Model v1-5-pruned-emaonly.safetensors Core generation model file \ud83c\udfa8 Enhanced VAE vae-ft-mse-840000-ema-pruned.safetensors Optional high-quality VAE \ud83d\udcdd Text Encoder Built into main model CLIP ViT-L/14 integrated \ud83d\udcd6 Usage Guide \ud83c\udf10 Web UI Interface \ud83d\udccb Step-by-Step Workflow **1. Model Selection** - Navigate to the upper-left model selector and choose your SD1.5 model ![img.png](img.png) **2. Prompt Engineering** - **Positive Prompt**: Describe your desired image in vivid detail - **Negative Prompt**: Specify unwanted elements (SD1.5 excels here!) **3. Parameter Tuning** - **Steps**: 20-30 (balance of quality and speed) - **CFG Scale**: 7-12 (prompt adherence strength) - **Sampler**: DPM++ 2M Karras / Euler a - **Resolution**: 512\u00d7512 (native resolution) **4. Advanced Configuration** - **Seed**: Control randomness (-1 = random) - **Batch Size**: Multiple generations - **Hi-Res Fix**: Upscale for larger images \ud83c\udfa8 Optimized Parameter Presets \u26a1 Speed Mode Steps : 20 | CFG : 7 Sampler : Euler a Size : 512\u00d7512 Perfect for rapid prototyping \ud83d\udc8e Quality Mode Steps : 30 | CFG : 9-11 Sampler : DPM++ 2M Karras Size : 768\u00d7768 Maximum detail and fidelity \ud83c\udfa8 Artistic Mode Steps : 25 | CFG : 8-10 Sampler : DDIM Size : 512\u00d7768 Enhanced creative expression \ud83d\udd0c API Integration \ud83d\udca1 Configuration Requirements Replace BASE_URL and APIKEY with your actual deployment values. For public network access, use public IP:port. \ud83d\udc0d Complete Python API Implementation import requests import base64 import json from typing import Optional, Dict, Any import time class StableDiffusionAPI: \"\"\" \ud83c\udfa8 Stable Diffusion 1.5 API Client A comprehensive wrapper for SD1.5 API interactions with advanced features \"\"\" def __init__(self, base_url: str, username: str = \"admin\", apikey: Optional[str] = None): \"\"\"Initialize the API client with connection details\"\"\" self.base_url = base_url.rstrip('/') self.auth = (username, apikey) if apikey else None self.session = requests.Session() def switch_model(self, model_name: str = \"v1-5-pruned-emaonly.safetensors\") -> bool: \"\"\"\ud83d\udd04 Switch to SD1.5 model with error handling\"\"\" model_data = {\"sd_model_checkpoint\": model_name} try: print(\"\ud83d\udd04 Switching to Stable Diffusion 1.5...\") response = self.session.post( f\"{self.base_url}/sdapi/v1/options\", json=model_data, auth=self.auth, timeout=30 ) response.raise_for_status() print(\"\u2705 Model switch completed successfully!\") return True except requests.RequestException as e: print(f\"\u274c Model switch failed: {e}\") return False def generate_image( self, prompt: str, negative_prompt: str = \"blurry, low quality, distorted, ugly\", steps: int = 20, cfg_scale: float = 7.0, width: int = 512, height: int = 512, sampler_name: str = \"Euler a\", seed: int = -1, batch_size: int = 1 ) -> Optional[Dict[str, Any]]: \"\"\"\ud83c\udfa8 Generate image with SD1.5 - Enhanced version\"\"\" generation_data = { \"prompt\": prompt, \"negative_prompt\": negative_prompt, \"steps\": steps, \"cfg_scale\": cfg_scale, \"width\": width, \"height\": height, \"sampler_name\": sampler_name, \"seed\": seed, \"batch_size\": batch_size, \"restore_faces\": False, \"tiling\": False, \"enable_hr\": False } try: print(f\"\ud83c\udfa8 Generating: '{prompt[:50]}...'\") start_time = time.time() response = self.session.post( f\"{self.base_url}/sdapi/v1/txt2img\", json=generation_data, auth=self.auth, timeout=300 # 5 minutes timeout ) response.raise_for_status() result = response.json() generation_time = time.time() - start_time if \"images\" in result and result[\"images\"]: print(f\"\u2705 Image generated successfully in {generation_time:.2f}s!\") return { \"images\": [base64.b64decode(img) for img in result[\"images\"]], \"info\": result.get(\"info\", {}), \"generation_time\": generation_time } else: print(\"\u274c No image data received\") return None except requests.RequestException as e: print(f\"\u274c Generation failed: {e}\") return None def save_images(self, image_data_list: list, base_filename: str = \"sd15_output\") -> list: \"\"\"\ud83d\udcbe Save multiple generated images with timestamps\"\"\" saved_files = [] timestamp = int(time.time()) for i, image_data in enumerate(image_data_list): try: if len(image_data_list) > 1: filename = f\"{base_filename}_{timestamp}_{i+1}.png\" else: filename = f\"{base_filename}_{timestamp}.png\" with open(filename, \"wb\") as f: f.write(image_data) print(f\"\ud83d\udcbe Image saved as: {filename}\") saved_files.append(filename) except Exception as e: print(f\"\u274c Save failed for image {i+1}: {e}\") return saved_files # \ud83d\ude80 Usage Example def main(): \"\"\"Comprehensive example with error handling\"\"\" # Initialize API client api = StableDiffusionAPI( base_url=\"<Your-Deployment-URL>\", username=\"admin\", apikey=\"${APIKEY}\" # Remove if no API key required ) # Switch to SD1.5 if not api.switch_model(): print(\"\u274c Failed to switch model. Exiting...\") return # Generate image result = api.generate_image( prompt=\"a majestic cat sitting on a golden throne, royal crown, detailed fur, 4k, photorealistic\", negative_prompt=\"blurry, low quality, distorted, cartoon\", steps=25, cfg_scale=8.0, width=512, height=512, sampler_name=\"DPM++ 2M Karras\" ) if result and result[\"images\"]: saved_files = api.save_images(result[\"images\"], \"sd15_cat\") print(f\"\u2705 Generation completed! Files: {saved_files}\") else: print(\"\u274c Failed to generate image\") if __name__ == \"__main__\": main() \ud83d\udca1 No API Key Setup If authentication is disabled, modify requests like this: response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data) \ud83d\udcda Essential Resources & Community \ud83d\udcd6 Official Documentation Stability AI Docs \u2192 Comprehensive guides and technical specifications \ud83d\udda5\ufe0f Web Interface Automatic1111 WebUI \u2192 Most popular SD web interface with extensive features \ud83c\udfa8 Model Hub Civitai Community \u2192 Thousands of custom models, LoRAs, and embeddings \u270d\ufe0f Prompt Mastery Prompt Engineering Guide \u2192 Master the art of crafting perfect prompts \ud83c\udfaf Best Practices & Pro Tips \u270d\ufe0f Prompt Engineering Mastery Use specific descriptive words instead of abstract concepts Add artist names to define artistic styles Use weight syntax (word:1.2) to emphasize key elements Leverage negative prompts to exclude unwanted content \u2699\ufe0f Parameter Optimization Start with low step counts, gradually increase for quality High CFG values can cause over-saturation Different samplers excel at different artistic styles Use fixed seeds to reproduce satisfactory results \u2753 Troubleshooting & FAQ \u2753 Poor image quality - How to improve results? Solution Strategies: Increase generation steps to 30-50 for higher quality Adjust CFG Scale to 8-12 for better prompt adherence Use more detailed prompts with specific descriptors Try different samplers (DPM++ 2M Karras recommended) Consider using a high-quality VAE for better colors \u2753 Out of memory errors - How to optimize VRAM usage? Memory Optimization Tips: Reduce resolution to 512\u00d7512 or lower Decrease batch size to 1 image at a time Enable low VRAM mode in settings Disable unnecessary extensions and features Use precision optimization (half precision/fp16) Restart the application to clear memory leaks \u2753 How to achieve specific artistic styles? Style Control Methods: Add artist names to prompts (e.g., \"by Greg Rutkowski\") Use style keywords (\"oil painting\", \"anime style\", \"photorealistic\") Download specialized LoRAs for specific styles from Civitai Reference community prompt templates for proven combinations Adjust CFG Scale to control style intensity (7-15 range) Experiment with different samplers for style variations \ud83c\udfa8 Ready to Create Magic? | Transform your imagination into stunning visuals with Stable Diffusion 1.5","title":"Index en"},{"location":"SD/1.5/doc/index-en/#model-introduction","text":"**Stable Diffusion 1.5** is a revolutionary text-to-image generation model developed by **Stability AI**. As a groundbreaking milestone in the open-source AI image generation landscape, it continues to be one of the most beloved and extensively utilized models in the community. This model has earned its reputation through its **lightweight architecture**, **exceptional efficiency**, and **vibrant ecosystem**, making it the go-to choice for both AI newcomers and seasoned professionals.","title":"\ud83c\udf1f Model Introduction"},{"location":"SD/1.5/doc/index-en/#core-features","text":"","title":"\u2728 Core Features"},{"location":"SD/1.5/doc/index-en/#technical-specifications","text":"Specification Details Model Type Text-to-Image / Image-to-Image Generation Parameters ~860M parameters Text Encoder CLIP ViT-L/14 VAE Resolution 512\u00d7512 (Native) Optimal Steps 20-50 steps License CreativeML Open RAIL-M","title":"\ud83d\udcca Technical Specifications"},{"location":"SD/1.5/doc/index-en/#configuration-setup","text":"","title":"\u2699\ufe0f Configuration Setup"},{"location":"SD/1.5/doc/index-en/#system-requirements","text":"\ud83d\udca1 Minimum Hardware Specifications GPU Memory : 6GB VRAM or higher Recommended: 8GB+ for optimal performance and larger resolutions","title":"\ud83d\udcbb System Requirements"},{"location":"SD/1.5/doc/index-en/#essential-model-files","text":"","title":"\ud83d\udcc1 Essential Model Files"},{"location":"SD/1.5/doc/index-en/#usage-guide","text":"","title":"\ud83d\udcd6 Usage Guide"},{"location":"SD/1.5/doc/index-en/#web-ui-interface","text":"","title":"\ud83c\udf10 Web UI Interface"},{"location":"SD/1.5/doc/index-en/#step-by-step-workflow","text":"**1. Model Selection** - Navigate to the upper-left model selector and choose your SD1.5 model ![img.png](img.png) **2. Prompt Engineering** - **Positive Prompt**: Describe your desired image in vivid detail - **Negative Prompt**: Specify unwanted elements (SD1.5 excels here!) **3. Parameter Tuning** - **Steps**: 20-30 (balance of quality and speed) - **CFG Scale**: 7-12 (prompt adherence strength) - **Sampler**: DPM++ 2M Karras / Euler a - **Resolution**: 512\u00d7512 (native resolution) **4. Advanced Configuration** - **Seed**: Control randomness (-1 = random) - **Batch Size**: Multiple generations - **Hi-Res Fix**: Upscale for larger images","title":"\ud83d\udccb Step-by-Step Workflow"},{"location":"SD/1.5/doc/index-en/#optimized-parameter-presets","text":"","title":"\ud83c\udfa8 Optimized Parameter Presets"},{"location":"SD/1.5/doc/index-en/#api-integration","text":"\ud83d\udca1 Configuration Requirements Replace BASE_URL and APIKEY with your actual deployment values. For public network access, use public IP:port. \ud83d\udc0d Complete Python API Implementation import requests import base64 import json from typing import Optional, Dict, Any import time class StableDiffusionAPI: \"\"\" \ud83c\udfa8 Stable Diffusion 1.5 API Client A comprehensive wrapper for SD1.5 API interactions with advanced features \"\"\" def __init__(self, base_url: str, username: str = \"admin\", apikey: Optional[str] = None): \"\"\"Initialize the API client with connection details\"\"\" self.base_url = base_url.rstrip('/') self.auth = (username, apikey) if apikey else None self.session = requests.Session() def switch_model(self, model_name: str = \"v1-5-pruned-emaonly.safetensors\") -> bool: \"\"\"\ud83d\udd04 Switch to SD1.5 model with error handling\"\"\" model_data = {\"sd_model_checkpoint\": model_name} try: print(\"\ud83d\udd04 Switching to Stable Diffusion 1.5...\") response = self.session.post( f\"{self.base_url}/sdapi/v1/options\", json=model_data, auth=self.auth, timeout=30 ) response.raise_for_status() print(\"\u2705 Model switch completed successfully!\") return True except requests.RequestException as e: print(f\"\u274c Model switch failed: {e}\") return False def generate_image( self, prompt: str, negative_prompt: str = \"blurry, low quality, distorted, ugly\", steps: int = 20, cfg_scale: float = 7.0, width: int = 512, height: int = 512, sampler_name: str = \"Euler a\", seed: int = -1, batch_size: int = 1 ) -> Optional[Dict[str, Any]]: \"\"\"\ud83c\udfa8 Generate image with SD1.5 - Enhanced version\"\"\" generation_data = { \"prompt\": prompt, \"negative_prompt\": negative_prompt, \"steps\": steps, \"cfg_scale\": cfg_scale, \"width\": width, \"height\": height, \"sampler_name\": sampler_name, \"seed\": seed, \"batch_size\": batch_size, \"restore_faces\": False, \"tiling\": False, \"enable_hr\": False } try: print(f\"\ud83c\udfa8 Generating: '{prompt[:50]}...'\") start_time = time.time() response = self.session.post( f\"{self.base_url}/sdapi/v1/txt2img\", json=generation_data, auth=self.auth, timeout=300 # 5 minutes timeout ) response.raise_for_status() result = response.json() generation_time = time.time() - start_time if \"images\" in result and result[\"images\"]: print(f\"\u2705 Image generated successfully in {generation_time:.2f}s!\") return { \"images\": [base64.b64decode(img) for img in result[\"images\"]], \"info\": result.get(\"info\", {}), \"generation_time\": generation_time } else: print(\"\u274c No image data received\") return None except requests.RequestException as e: print(f\"\u274c Generation failed: {e}\") return None def save_images(self, image_data_list: list, base_filename: str = \"sd15_output\") -> list: \"\"\"\ud83d\udcbe Save multiple generated images with timestamps\"\"\" saved_files = [] timestamp = int(time.time()) for i, image_data in enumerate(image_data_list): try: if len(image_data_list) > 1: filename = f\"{base_filename}_{timestamp}_{i+1}.png\" else: filename = f\"{base_filename}_{timestamp}.png\" with open(filename, \"wb\") as f: f.write(image_data) print(f\"\ud83d\udcbe Image saved as: {filename}\") saved_files.append(filename) except Exception as e: print(f\"\u274c Save failed for image {i+1}: {e}\") return saved_files # \ud83d\ude80 Usage Example def main(): \"\"\"Comprehensive example with error handling\"\"\" # Initialize API client api = StableDiffusionAPI( base_url=\"<Your-Deployment-URL>\", username=\"admin\", apikey=\"${APIKEY}\" # Remove if no API key required ) # Switch to SD1.5 if not api.switch_model(): print(\"\u274c Failed to switch model. Exiting...\") return # Generate image result = api.generate_image( prompt=\"a majestic cat sitting on a golden throne, royal crown, detailed fur, 4k, photorealistic\", negative_prompt=\"blurry, low quality, distorted, cartoon\", steps=25, cfg_scale=8.0, width=512, height=512, sampler_name=\"DPM++ 2M Karras\" ) if result and result[\"images\"]: saved_files = api.save_images(result[\"images\"], \"sd15_cat\") print(f\"\u2705 Generation completed! Files: {saved_files}\") else: print(\"\u274c Failed to generate image\") if __name__ == \"__main__\": main() \ud83d\udca1 No API Key Setup If authentication is disabled, modify requests like this: response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data)","title":"\ud83d\udd0c API Integration"},{"location":"SD/1.5/doc/index-en/#essential-resources-community","text":"","title":"\ud83d\udcda Essential Resources &amp; Community"},{"location":"SD/1.5/doc/index-en/#best-practices-pro-tips","text":"","title":"\ud83c\udfaf Best Practices &amp; Pro Tips"},{"location":"SD/1.5/doc/index-en/#troubleshooting-faq","text":"\u2753 Poor image quality - How to improve results? Solution Strategies: Increase generation steps to 30-50 for higher quality Adjust CFG Scale to 8-12 for better prompt adherence Use more detailed prompts with specific descriptors Try different samplers (DPM++ 2M Karras recommended) Consider using a high-quality VAE for better colors \u2753 Out of memory errors - How to optimize VRAM usage? Memory Optimization Tips: Reduce resolution to 512\u00d7512 or lower Decrease batch size to 1 image at a time Enable low VRAM mode in settings Disable unnecessary extensions and features Use precision optimization (half precision/fp16) Restart the application to clear memory leaks \u2753 How to achieve specific artistic styles? Style Control Methods: Add artist names to prompts (e.g., \"by Greg Rutkowski\") Use style keywords (\"oil painting\", \"anime style\", \"photorealistic\") Download specialized LoRAs for specific styles from Civitai Reference community prompt templates for proven combinations Adjust CFG Scale to control style intensity (7-15 range) Experiment with different samplers for style variations \ud83c\udfa8 Ready to Create Magic? | Transform your imagination into stunning visuals with Stable Diffusion 1.5","title":"\u2753 Troubleshooting &amp; FAQ"},{"location":"SD/3/doc/","text":"\ud83c\udfa8 Stable Diffusion 3 Medium \u6a21\u578b\u4ecb\u7ecd \u7b2c\u4e09\u4ee3\u6269\u6563\u6a21\u578b\u7684\u4e2d\u7b49\u53c2\u6570\u7248\u672c\uff0c\u5f00\u6e90\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u91cd\u5927\u8fdb\u6b65 **Stable Diffusion 3 Medium** \u662f Stability AI \u53d1\u5e03\u7684\u7b2c\u4e09\u4ee3\u6269\u6563\u6a21\u578b\u7684\u4e2d\u7b49\u53c2\u6570\u7248\u672c\uff0c\u4ee3\u8868\u4e86\u5f00\u6e90\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u91cd\u5927\u8fdb\u6b65\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u76f8\u5bf9\u8f83\u4f4e\u786c\u4ef6\u8981\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3001\u6587\u672c\u7406\u89e3\u80fd\u529b\u548c\u751f\u6210\u591a\u6837\u6027\uff0c\u662f SD1.5 \u548c\u9ad8\u7aef\u6a21\u578b\u4e4b\u95f4\u7684\u5b8c\u7f8e\u5e73\u8861\u9009\u62e9\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \ud83c\udfd7\ufe0f \u5148\u8fdb\u67b6\u6784 \u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668 (MMDiT) \u67b6\u6784 \ud83e\udde0 \u589e\u5f3a\u6587\u672c\u7406\u89e3 \u96c6\u6210 T5-XXL \u548c\u53cc CLIP \u6587\u672c\u7f16\u7801\u5668 \u2696\ufe0f \u5e73\u8861\u6027\u80fd 20\u4ebf\u53c2\u6570\uff0c\u5b9e\u73b0\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861 \ud83d\udcd0 \u591a\u5bbd\u9ad8\u6bd4\u652f\u6301 \u539f\u751f\u652f\u6301\u5404\u79cd\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4 \ud83d\udc64 \u6539\u8fdb\u4eba\u4f53\u89e3\u5256 \u663e\u8457\u51cf\u5c11\u624b\u90e8\u548c\u8eab\u4f53\u7ed3\u6784\u9519\u8bef \ud83c\udfad \u98ce\u683c\u591a\u6837\u6027 \u652f\u6301\u4ece\u5199\u5b9e\u5230\u827a\u672f\u7684\u5404\u79cd\u98ce\u683c \ud83d\udcca \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210 \u67b6\u6784 \u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668 (MMDiT) \u53c2\u6570\u89c4\u6a21 \u7ea6 20 \u4ebf\u53c2\u6570 \u6587\u672c\u7f16\u7801\u5668 T5-XXL + CLIP-L + CLIP-G \u539f\u751f\u5206\u8fa8\u7387 1024\u00d71024 \u63a8\u8350\u6b65\u6570 20-50 \u6b65 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \ud83d\udca1 \u786c\u4ef6\u914d\u7f6e\u8981\u6c42 ECS \u663e\u5b58 : \u63a8\u8350 8GB \u6216\u66f4\u591a \ud83d\udcc1 \u6a21\u578b\u6587\u4ef6 \u6587\u4ef6\u7c7b\u578b \u6587\u4ef6\u540d \u5927\u5c0f \u4e3b\u6a21\u578b sd3_medium_incl_clips_t5xxlfp16.safetensors \u7ea6 10GB \ud83d\udcd6 \u4f7f\u7528\u6307\u5357 \ud83c\udf10 Web UI \u4f7f\u7528\u65b9\u6cd5 ### \ud83d\ude80 \u57fa\u7840\u64cd\u4f5c\u6d41\u7a0b **1. \u6a21\u578b\u9009\u62e9** - \u5728\u6a21\u578b\u9009\u62e9\u5668\u4e2d\u9009\u62e9 SD3 Medium \u6a21\u578b **2. \u63d0\u793a\u8bcd\u8f93\u5165** - \u8f93\u5165\u8be6\u7ec6\u7684\u56fe\u50cf\u63cf\u8ff0 **3. \u53c2\u6570\u8bbe\u7f6e** - \u8c03\u6574\u6b65\u6570\u3001CFG \u548c\u91c7\u6837\u5668 **4. \u9ad8\u7ea7\u8bbe\u7f6e** - \u914d\u7f6e\u79cd\u5b50\u548c\u6279\u6b21\u8bbe\u7f6e \u2699\ufe0f \u53c2\u6570\u8bf4\u660e \ud83d\udd22 \u6838\u5fc3\u53c2\u6570 **Steps (\u63a8\u7406\u6b65\u6570)** - **15-20 \u6b65**: \u5feb\u901f\u751f\u6210\uff0c\u53ef\u63a5\u53d7\u8d28\u91cf - **20-30 \u6b65**: \u5e73\u8861\u8d28\u91cf\u548c\u901f\u5ea6 (\u63a8\u8350) - **30-50 \u6b65**: \u6700\u9ad8\u8d28\u91cf\uff0c\u901f\u5ea6\u8f83\u6162 **CFG Scale (\u5f15\u5bfc\u5f3a\u5ea6)** - **4.0-5.0**: \u81ea\u7136\u7ed3\u679c\uff0c\u8f83\u5c11\u8fc7\u62df\u5408 - **5.0-7.0**: \u5e73\u8861\u6587\u672c\u8ddf\u968f\u548c\u81ea\u7136\u5ea6 (\u63a8\u8350) - **7.0-9.0**: \u5f3a\u6587\u672c\u8ddf\u968f\uff0c\u53ef\u80fd\u8fc7\u9971\u548c \ud83c\udf9b\ufe0f \u91c7\u6837\u5668\u9009\u62e9 - **DPM++ 2M**: \u9ad8\u8d28\u91cf\uff0c\u63a8\u8350\u4f7f\u7528 - **Euler**: \u5feb\u901f\u7a33\u5b9a - **DPM++ SDE**: \u9ad8\u8d28\u91cf\u4f46\u8f83\u6162 - **DDIM**: \u7ecf\u5178\u9009\u62e9\uff0c\u7ed3\u679c\u7a33\u5b9a \ud83d\udca1 \u63d0\u793a\u8bcd\u6280\u5de7 \ud83c\udfaf SD3 \u63d0\u793a\u8bcd\u7279\u70b9 \u66f4\u597d\u7684\u957f\u6587\u672c\u7406\u89e3 : \u652f\u6301\u66f4\u8be6\u7ec6\u590d\u6742\u7684\u63cf\u8ff0 \u6539\u8fdb\u7684\u6982\u5ff5\u7ec4\u5408 : \u66f4\u597d\u7406\u89e3\u591a\u6982\u5ff5\u7ec4\u5408 \u7cbe\u786e\u5c5e\u6027\u63a7\u5236 : \u66f4\u7cbe\u786e\u63a7\u5236\u989c\u8272\u3001\u6750\u8d28\u3001\u5149\u7167\u7b49 \u51cf\u5c11\u8d1f\u9762\u4f9d\u8d56 : \u5bf9\u8d1f\u9762\u63d0\u793a\u8bcd\u4f9d\u8d56\u6027\u964d\u4f4e \ud83d\udcdd \u9ad8\u8d28\u91cf\u63d0\u793a\u8bcd\u7ed3\u6784 [\u8be6\u7ec6\u4e3b\u4f53\u63cf\u8ff0] + [\u73af\u5883/\u80cc\u666f] + [\u98ce\u683c/\u6280\u6cd5] + [\u5149\u7167/\u6c1b\u56f4] + [\u8d28\u91cf\u8bcd\u6c47] \ud83c\udfa8 \u63d0\u793a\u8bcd\u793a\u4f8b \ud83d\udcf8 \u5199\u5b9e\u6444\u5f71\u98ce\u683c \"\u4e00\u4f4d\u81ea\u4fe1\u768430\u591a\u5c81\u5546\u52a1\u5973\u6027\u7684\u4e13\u4e1a\u5934\u50cf\uff0c\u7a7f\u7740\u6df1\u84dd\u8272\u897f\u88c5\u5916\u5957\uff0c\u5750\u5728\u73b0\u4ee3\u529e\u516c\u684c\u524d\uff0c\u81ea\u7136\u7a97\u5149\uff0c\u6d45\u666f\u6df1\uff0c85mm\u955c\u5934\u62cd\u6444\uff0c\u9ad8\u5206\u8fa8\u7387\uff0c\u9510\u5229\u5bf9\u7126\" \ud83c\udfa8 \u827a\u672f\u521b\u4f5c\u98ce\u683c \"\u79cb\u5929\u6cd5\u56fd\u4e61\u6751\u8461\u8404\u56ed\u7684\u5370\u8c61\u6d3e\u7ed8\u753b\uff0c\u91d1\u8272\u9633\u5149\u900f\u8fc7\u8461\u8404\u85e4\u8fc7\u6ee4\uff0c\u6696\u8272\u8c03\u8272\u677f\uff0c\u677e\u6563\u7b14\u89e6\uff0c\u6237\u5916\u5199\u751f\u98ce\u683c\uff0c\u8ba9\u4eba\u60f3\u8d77\u83ab\u5948\u7684\u4f5c\u54c1\" \ud83d\udc64 \u89d2\u8272\u8096\u50cf \"\u4e00\u4f4d\u5e74\u8f7b\u827a\u672f\u5bb6\u5728\u5de5\u4f5c\u5ba4\u7684\u8be6\u7ec6\u8096\u50cf\uff0c\u6cbe\u6ee1\u989c\u6599\u7684\u56f4\u88d9\uff0c\u624b\u6301\u8c03\u8272\u677f\u548c\u753b\u7b14\uff0c\u88ab\u753b\u5e03\u5305\u56f4\uff0c\u5927\u7a97\u6237\u7684\u81ea\u7136\u5149\u7ebf\uff0c\u82e5\u6709\u6240\u601d\u7684\u8868\u60c5\uff0c\u5199\u5b9e\u6cb9\u753b\u98ce\u683c\" \ud83c\udff0 \u5947\u5e7b\u89d2\u8272 \"\u7cbe\u7075\u5f13\u7bad\u624b\u7684\u5947\u5e7b\u89d2\u8272\u8096\u50cf\uff0c\u7cbe\u81f4\u7684\u76ae\u7532\uff0c\u94f6\u53d1\u7f16\u7ec7\u7740\u6811\u53f6\uff0c\u9510\u5229\u7684\u7eff\u773c\u775b\uff0c\u68ee\u6797\u80cc\u666f\uff0c\u8be6\u7ec6\u7684\u5947\u5e7b\u827a\u672f\uff0cRPG\u89d2\u8272\u8bbe\u8ba1\" \ud83d\udd0c API \u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests import base64 # \u914d\u7f6e base_url = \"http://127.0.0.1:7680\" username = \"admin\" apikey = \"${APIKEY}\" auth = (username, apikey) # 1. \u5207\u6362\u6a21\u578b model_data = { \"sd_model_checkpoint\": \"sd3_medium_incl_clips_t5xxlfp16.safetensors\" } print(\"\u5207\u6362\u6a21\u578b\u4e2d...\") model_response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data, auth=auth) print(\"\u6a21\u578b\u5207\u6362\u5b8c\u6210\") # 2. \u751f\u6210\u56fe\u50cf prompt = \"\u4e00\u53ea\u7f8e\u4e3d\u7684\u732b\" data = { \"prompt\": prompt, \"steps\": 20, \"width\": 512, \"height\": 512, \"cfg_scale\": 6.0, \"sampler_name\": \"DPM++ 2M\" } print(\"\u751f\u6210\u56fe\u50cf\u4e2d...\") response = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json=data, auth=auth) result = response.json() # 3. \u4fdd\u5b58\u56fe\u50cf if \"images\" in result: image_data = base64.b64decode(result[\"images\"][0]) with open(\"output.png\", \"wb\") as f: f.write(image_data) print(\"\u56fe\u50cf\u5df2\u4fdd\u5b58\u4e3a output.png\") else: print(\"\u9519\u8bef:\", result) \ud83d\udca1 \u6700\u4f73\u5b9e\u8df5 \ud83d\udcdd \u63d0\u793a\u8bcd\u4f18\u5316 \u8be6\u7ec6\u63cf\u8ff0 : SD3 \u80fd\u66f4\u597d\u7406\u89e3\u8be6\u7ec6\u63cf\u8ff0 \u81ea\u7136\u8bed\u8a00 : \u4f7f\u7528\u81ea\u7136\u53e5\u5f0f\u800c\u975e\u5173\u952e\u8bcd\u5806\u53e0 \u5177\u4f53\u5c5e\u6027 : \u660e\u786e\u6307\u5b9a\u989c\u8272\u3001\u6750\u8d28\u3001\u5149\u7167\u7b49\u5c5e\u6027 \u98ce\u683c\u6307\u5bfc : \u6e05\u695a\u6307\u5b9a\u827a\u672f\u6216\u6280\u672f\u98ce\u683c \u51cf\u5c11\u8d1f\u9762 : \u4e13\u6ce8\u6b63\u9762\u63cf\u8ff0\uff0c\u51cf\u5c11\u8d1f\u9762\u63d0\u793a\u8bcd\u4f7f\u7528 \u2699\ufe0f \u53c2\u6570\u8c03\u4f18\u7b56\u7565 \u8d77\u59cb\u8bbe\u7f6e : 25\u6b65 + CFG 6.0 + DPM++ 2M \u5feb\u901f\u9884\u89c8 : 20\u6b65 + CFG 5.0 \u7528\u4e8e\u5feb\u901f\u6d4b\u8bd5 \u9ad8\u8d28\u91cf : 30\u6b65 + CFG 6.5 \u83b7\u5f97\u6700\u4f73\u6548\u679c \u98ce\u683c\u5b9e\u9a8c : \u57284.5-7.0\u8303\u56f4\u5185\u8c03\u6574CFG\u6d4b\u8bd5 \ud83d\udcca \u6a21\u578b\u5bf9\u6bd4 \u5bf9\u6bd4\u9879\u76ee SD3 Medium SD1.5 Flux1-Dev SDXL \u53c2\u6570\u89c4\u6a21 2B 0.86B 12B 3.5B \u56fe\u50cf\u8d28\u91cf \u4f18\u79c0 \u826f\u597d \u5353\u8d8a \u4f18\u79c0 \u663e\u5b58\u9700\u6c42 8GB+ 4GB+ 12GB+ 6GB+ \u751f\u6210\u901f\u5ea6 \u5feb\u901f \u5f88\u5feb \u4e2d\u7b49 \u5feb\u901f \ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848 \ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a CFG \u8fc7\u9ad8 : \u964d\u4f4e CFG \u5230 4.5-7.0 \u8303\u56f4 \u6b65\u6570\u4e0d\u8db3 : \u589e\u52a0\u5230 25-30 \u6b65 \u63d0\u793a\u8bcd\u8fc7\u7b80 : \u4f7f\u7528\u66f4\u8be6\u7ec6\u7684\u63cf\u8ff0 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 : \u964d\u4f4e\u5206\u8fa8\u7387\u6216\u4f7f\u7528 medvram \u6a21\u5f0f \u52a0\u8f7d\u7f13\u6162 : SD3 \u6a21\u578b\u8f83\u5927\uff0c\u9700\u8981\u8010\u5fc3 \u751f\u6210\u7f13\u6162 : \u4f7f\u7528\u66f4\u5c11\u6b65\u6570\u6216\u66f4\u5feb\u91c7\u6837\u5668 \ud83d\udd27 \u517c\u5bb9\u6027\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a WebUI \u7248\u672c : \u786e\u4fdd\u4f7f\u7528\u652f\u6301 SD3 \u7684\u6700\u65b0\u7248\u672c \u6269\u5c55\u517c\u5bb9 : \u67d0\u4e9b\u6269\u5c55\u53ef\u80fd\u4e0e SD3 \u4e0d\u517c\u5bb9 \u53c2\u6570\u8303\u56f4 : \u6ce8\u610f SD3 \u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4 \ud83d\udcda \u76f8\u5173\u8d44\u6e90 \ud83d\udcc4 \u5b98\u65b9\u8bba\u6587 Stable Diffusion 3 \u7814\u7a76\u8bba\u6587 \u4e86\u89e3SD3\u7684\u6280\u672f\u539f\u7406\u548c\u521b\u65b0\u70b9 \ud83d\udcca \u6280\u672f\u62a5\u544a SD3 \u6280\u672f\u62a5\u544a \u8be6\u7ec6\u7684\u6280\u672f\u5b9e\u73b0\u548c\u6027\u80fd\u8bc4\u4f30 \ud83e\udd17 \u6a21\u578b\u4e0b\u8f7d Model Scope \u6a21\u578b\u9875\u9762 \u5b98\u65b9\u6a21\u578b\u6587\u4ef6\u4e0b\u8f7d\u5730\u5740 \ud83d\udcd6 \u4f7f\u7528\u6587\u6863 WebUI SD3 \u652f\u6301\u6587\u6863 WebUI\u4e2dSD3\u7684\u914d\u7f6e\u548c\u4f7f\u7528\u6307\u5357 \ud83c\udfa8 \u5f00\u59cb\u60a8\u7684 SD3 \u521b\u4f5c\u4e4b\u65c5\uff01 | \u7b2c\u4e09\u4ee3\u6269\u6563\u6a21\u578b\uff0c\u8ba9 AI \u827a\u672f\u521b\u4f5c\u66f4\u52a0\u7cbe\u5f69","title":"Index"},{"location":"SD/3/doc/#_1","text":"","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"SD/3/doc/#_2","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210 \u67b6\u6784 \u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668 (MMDiT) \u53c2\u6570\u89c4\u6a21 \u7ea6 20 \u4ebf\u53c2\u6570 \u6587\u672c\u7f16\u7801\u5668 T5-XXL + CLIP-L + CLIP-G \u539f\u751f\u5206\u8fa8\u7387 1024\u00d71024 \u63a8\u8350\u6b65\u6570 20-50 \u6b65","title":"\ud83d\udcca \u6280\u672f\u89c4\u683c"},{"location":"SD/3/doc/#_3","text":"","title":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e"},{"location":"SD/3/doc/#_4","text":"\ud83d\udca1 \u786c\u4ef6\u914d\u7f6e\u8981\u6c42 ECS \u663e\u5b58 : \u63a8\u8350 8GB \u6216\u66f4\u591a","title":"\ud83d\udcbb \u7cfb\u7edf\u8981\u6c42"},{"location":"SD/3/doc/#_5","text":"\u6587\u4ef6\u7c7b\u578b \u6587\u4ef6\u540d \u5927\u5c0f \u4e3b\u6a21\u578b sd3_medium_incl_clips_t5xxlfp16.safetensors \u7ea6 10GB","title":"\ud83d\udcc1 \u6a21\u578b\u6587\u4ef6"},{"location":"SD/3/doc/#_6","text":"","title":"\ud83d\udcd6 \u4f7f\u7528\u6307\u5357"},{"location":"SD/3/doc/#web-ui","text":"### \ud83d\ude80 \u57fa\u7840\u64cd\u4f5c\u6d41\u7a0b **1. \u6a21\u578b\u9009\u62e9** - \u5728\u6a21\u578b\u9009\u62e9\u5668\u4e2d\u9009\u62e9 SD3 Medium \u6a21\u578b **2. \u63d0\u793a\u8bcd\u8f93\u5165** - \u8f93\u5165\u8be6\u7ec6\u7684\u56fe\u50cf\u63cf\u8ff0 **3. \u53c2\u6570\u8bbe\u7f6e** - \u8c03\u6574\u6b65\u6570\u3001CFG \u548c\u91c7\u6837\u5668 **4. \u9ad8\u7ea7\u8bbe\u7f6e** - \u914d\u7f6e\u79cd\u5b50\u548c\u6279\u6b21\u8bbe\u7f6e","title":"\ud83c\udf10 Web UI \u4f7f\u7528\u65b9\u6cd5"},{"location":"SD/3/doc/#_7","text":"","title":"\u2699\ufe0f \u53c2\u6570\u8bf4\u660e"},{"location":"SD/3/doc/#_8","text":"","title":"\ud83d\udca1 \u63d0\u793a\u8bcd\u6280\u5de7"},{"location":"SD/3/doc/#_9","text":"","title":"\ud83c\udfa8 \u63d0\u793a\u8bcd\u793a\u4f8b"},{"location":"SD/3/doc/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests import base64 # \u914d\u7f6e base_url = \"http://127.0.0.1:7680\" username = \"admin\" apikey = \"${APIKEY}\" auth = (username, apikey) # 1. \u5207\u6362\u6a21\u578b model_data = { \"sd_model_checkpoint\": \"sd3_medium_incl_clips_t5xxlfp16.safetensors\" } print(\"\u5207\u6362\u6a21\u578b\u4e2d...\") model_response = requests.post(f\"{base_url}/sdapi/v1/options\", json=model_data, auth=auth) print(\"\u6a21\u578b\u5207\u6362\u5b8c\u6210\") # 2. \u751f\u6210\u56fe\u50cf prompt = \"\u4e00\u53ea\u7f8e\u4e3d\u7684\u732b\" data = { \"prompt\": prompt, \"steps\": 20, \"width\": 512, \"height\": 512, \"cfg_scale\": 6.0, \"sampler_name\": \"DPM++ 2M\" } print(\"\u751f\u6210\u56fe\u50cf\u4e2d...\") response = requests.post(f\"{base_url}/sdapi/v1/txt2img\", json=data, auth=auth) result = response.json() # 3. \u4fdd\u5b58\u56fe\u50cf if \"images\" in result: image_data = base64.b64decode(result[\"images\"][0]) with open(\"output.png\", \"wb\") as f: f.write(image_data) print(\"\u56fe\u50cf\u5df2\u4fdd\u5b58\u4e3a output.png\") else: print(\"\u9519\u8bef:\", result)","title":"\ud83d\udd0c API \u8c03\u7528"},{"location":"SD/3/doc/#_10","text":"","title":"\ud83d\udca1 \u6700\u4f73\u5b9e\u8df5"},{"location":"SD/3/doc/#_11","text":"\u5bf9\u6bd4\u9879\u76ee SD3 Medium SD1.5 Flux1-Dev SDXL \u53c2\u6570\u89c4\u6a21 2B 0.86B 12B 3.5B \u56fe\u50cf\u8d28\u91cf \u4f18\u79c0 \u826f\u597d \u5353\u8d8a \u4f18\u79c0 \u663e\u5b58\u9700\u6c42 8GB+ 4GB+ 12GB+ 6GB+ \u751f\u6210\u901f\u5ea6 \u5feb\u901f \u5f88\u5feb \u4e2d\u7b49 \u5feb\u901f","title":"\ud83d\udcca \u6a21\u578b\u5bf9\u6bd4"},{"location":"SD/3/doc/#_12","text":"\ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a CFG \u8fc7\u9ad8 : \u964d\u4f4e CFG \u5230 4.5-7.0 \u8303\u56f4 \u6b65\u6570\u4e0d\u8db3 : \u589e\u52a0\u5230 25-30 \u6b65 \u63d0\u793a\u8bcd\u8fc7\u7b80 : \u4f7f\u7528\u66f4\u8be6\u7ec6\u7684\u63cf\u8ff0 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 : \u964d\u4f4e\u5206\u8fa8\u7387\u6216\u4f7f\u7528 medvram \u6a21\u5f0f \u52a0\u8f7d\u7f13\u6162 : SD3 \u6a21\u578b\u8f83\u5927\uff0c\u9700\u8981\u8010\u5fc3 \u751f\u6210\u7f13\u6162 : \u4f7f\u7528\u66f4\u5c11\u6b65\u6570\u6216\u66f4\u5feb\u91c7\u6837\u5668 \ud83d\udd27 \u517c\u5bb9\u6027\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a WebUI \u7248\u672c : \u786e\u4fdd\u4f7f\u7528\u652f\u6301 SD3 \u7684\u6700\u65b0\u7248\u672c \u6269\u5c55\u517c\u5bb9 : \u67d0\u4e9b\u6269\u5c55\u53ef\u80fd\u4e0e SD3 \u4e0d\u517c\u5bb9 \u53c2\u6570\u8303\u56f4 : \u6ce8\u610f SD3 \u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4","title":"\ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848"},{"location":"SD/3/doc/#_13","text":"","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90"},{"location":"SD/3/doc/index-en/","text":"\ud83c\udfa8 Stable Diffusion 3 Medium Complete Guide Next-Generation AI Image Generation - The Perfect Balance of Stability AI's Third-Generation Diffusion Model \ud83c\udf1f Model Introduction **Stable Diffusion 3 Medium** is the medium-parameter version of the third-generation diffusion model released by Stability AI, representing a significant advancement in open-source image generation technology. This model significantly improves image quality, text understanding capabilities, and generation diversity while maintaining relatively low hardware requirements, making it the perfect balanced choice between SD1.5 and high-end models. \u2728 Core Features \ud83c\udfd7\ufe0f Advanced Architecture Based on Multimodal Diffusion Transformer (MMDiT) architecture \ud83e\udde0 Enhanced Text Understanding Integrated T5-XXL and dual CLIP text encoders \u2696\ufe0f Balanced Performance 2 billion parameters, achieving optimal balance between quality and efficiency \ud83d\udcd0 Multi-Aspect Ratio Support Native support for various resolutions and aspect ratios \ud83d\udc64 Improved Human Anatomy Significantly reduced hand and body structure errors \ud83c\udfa8 Style Diversity Supports various styles from realistic to artistic \ud83d\udcca Technical Specifications Specification Details Model Type Text-to-Image Generation Architecture Multimodal Diffusion Transformer (MMDiT) Parameter Scale Approximately 2 billion parameters Text Encoders T5-XXL + CLIP-L + CLIP-G Native Resolution 1024\u00d71024 Supported Resolutions 512\u00d7512 to 2048\u00d72048 Recommended Steps 20-50 steps CFG Range 4.5-9.0 \u2699\ufe0f Configuration Instructions \ud83d\udcbb System Requirements \ud83d\udca1 Hardware Configuration ECS VRAM : 8GB or more recommended System RAM : 16GB or more recommended Storage Space : At least 15GB available space \ud83d\udcc1 Model Files File Type Filename Size Description Main Model sd3_medium_incl_clips_t5xxlfp16.safetensors ~10GB Complete model with all components \ud83d\udcd6 Usage Guide \ud83c\udf10 Web UI Usage ### \ud83d\udd27 Basic Operations **1. Model Selection** - Select SD3 Medium model in the model selector **2. Prompt Input** - **Positive Prompt**: Detailed description of desired image, supports more complex descriptions - **Negative Prompt**: SD3 responds weakly to negative prompts, can be simplified **3. Parameter Settings** - **Steps**: Recommended 20-30 steps - **CFG Scale**: Recommended 4.5-7.0 (lower than SD1.5) - **Sampler**: Recommended DPM++ 2M or Euler - **Resolution**: 1024\u00d71024 (native) or other supported sizes **4. Advanced Settings** - **Seed**: Controls randomness - **Batch Settings**: Adjust according to VRAM capacity - **Multi-Aspect Ratio**: Utilize native multi-resolution support \ud83d\udccb Parameter Description \u23f1\ufe0f Steps (Inference steps) 15-20 steps : Fast generation, acceptable quality 20-30 steps : Balanced quality and speed (recommended) 30-50 steps : Highest quality, slower speed \ud83c\udf9a\ufe0f CFG Scale (Guidance strength) 4.0-5.0 : Natural results, less overfitting 5.0-7.0 : Balanced text following and naturalness (recommended) 7.0-9.0 : Strong text following, may be over-saturated \ud83d\udd04 Sampler Selection DPM++ 2M : High quality, recommended Euler : Fast and stable DPM++ SDE : High quality but slower DDIM : Classic choice, stable results \ud83d\udcd0 Resolution Settings \ud83d\udca1 SD3 Native Resolutions SD3 natively supports various resolutions without quality loss: 1024\u00d71024 Standard square 1152\u00d7896 Landscape 4:3.6 896\u00d71152 Portrait 3.6:4 1344\u00d7768 Ultra-widescreen \ud83d\udca1 Prompt Techniques \ud83e\udde0 SD3 Prompt Characteristics \ud83c\udfaf Key Improvements Better Long Text Understanding : Supports more detailed and complex descriptions Improved Concept Combination : Better understanding of multiple concept combinations Precise Attribute Control : More precise control over colors, materials, lighting, etc. Reduced Negative Dependency : Less dependency on negative prompts \ud83d\udcdd High-Quality Prompt Structure [Detailed subject description] + [Environment/Background] + [Style/Technique] + [Lighting/Atmosphere] + [Quality terms] \ud83c\udfa8 Prompt Examples \ud83d\udcf8 Realistic Photography Style \"A professional headshot of a confident businesswoman in her 30s, wearing a navy blue blazer, sitting at a modern office desk, natural window lighting, shallow depth of field, shot with 85mm lens, high resolution, sharp focus\" \"Street photography of a bustling Tokyo intersection at night, neon signs, people crossing, motion blur on vehicles, rain-soaked streets reflecting lights, urban atmosphere, documentary style, Leica camera aesthetic\" \ud83c\udfa8 Artistic Creation Style \"An impressionist painting of a French countryside vineyard in autumn, golden sunlight filtering through grape vines, warm color palette, loose brushstrokes, en plein air style, reminiscent of Monet's work\" \"Digital concept art of a floating island city in the clouds, waterfalls cascading into the sky, flying ships, fantasy architecture, dramatic lighting, highly detailed, matte painting style\" \ud83d\udc64 Character Portraits \"A detailed portrait of a young artist in her studio, paint-stained apron, holding a palette and brush, surrounded by canvases, natural lighting from large windows, thoughtful expression, realistic oil painting style\" \"Fantasy character portrait of an elven archer, intricate leather armor, silver hair braided with leaves, piercing green eyes, forest background, detailed fantasy art, RPG character design\" \ud83d\udd0c API Integration \ud83d\udccb Click to expand API call Python code import requests import base64 import time class SD3API: def __init__(self, base_url, username=\"admin\", apikey=None): self.base_url = base_url.rstrip('/') self.auth = (username, apikey) if apikey else None def switch_model(self, model_name=\"sd3_medium_incl_clips_t5xxlfp16.safetensors\"): \"\"\"Switch to SD3 Medium model\"\"\" model_data = {\"sd_model_checkpoint\": model_name} print(\"\ud83d\udd04 Switching to SD3 Medium model...\") response = requests.post( f\"{self.base_url}/sdapi/v1/options\", json=model_data, auth=self.auth ) if response.status_code == 200: print(\"\u2705 Model switch completed successfully!\") return True else: print(f\"\u274c Model switch failed: {response.status_code}\") return False def generate_image(self, prompt, **kwargs): \"\"\"Generate image with SD3 optimized parameters\"\"\" default_params = { \"prompt\": prompt, \"steps\": 25, \"cfg_scale\": 6.0, \"width\": 1024, \"height\": 1024, \"sampler_name\": \"DPM++ 2M\", \"negative_prompt\": \"\", # SD3 works well with minimal negative prompts } # Update with user parameters default_params.update(kwargs) print(f\"\ud83c\udfa8 Generating image with SD3...\") print(f\"\ud83d\udcdd Prompt: {prompt[:50]}...\") response = requests.post( f\"{self.base_url}/sdapi/v1/txt2img\", json=default_params, auth=self.auth ) if response.status_code == 200: result = response.json() if \"images\" in result and result[\"images\"]: print(\"\u2705 Image generated successfully!\") return result else: print(\"\u274c No image data received\") return None else: print(f\"\u274c Generation failed: {response.status_code}\") return None def save_image(self, result, filename=\"sd3_output.png\"): \"\"\"Save generated image\"\"\" if result and \"images\" in result: image_data = base64.b64decode(result[\"images\"][0]) with open(filename, \"wb\") as f: f.write(image_data) print(f\"\ud83d\udcbe Image saved as {filename}\") return filename return None # Usage example def main(): # Initialize API api = SD3API( base_url=\"http://127.0.0.1:7680\", username=\"admin\", apikey=\"${APIKEY}\" # Replace with your API key ) # Switch to SD3 model if not api.switch_model(): return # Generate image with SD3-optimized settings prompt = \"\"\"A serene mountain landscape at golden hour, snow-capped peaks reflecting in a crystal clear alpine lake, pine trees in foreground, dramatic clouds, professional landscape photography, high detail\"\"\" result = api.generate_image( prompt=prompt, steps=28, cfg_scale=6.5, width=1152, height=896, sampler_name=\"DPM++ 2M\" ) if result: api.save_image(result, \"sd3_landscape.png\") if __name__ == \"__main__\": main() \ud83c\udfc6 Best Practices \ud83c\udfaf Prompt Optimization Detailed Description : SD3 can better understand detailed descriptions Natural Language : Use natural sentence structures rather than keyword stacking Specific Attributes : Clearly specify colors, materials, lighting, and other attributes Style Guidance : Clearly specify artistic or technical styles Reduce Negatives : Focus on positive descriptions, reduce negative prompt usage \u2699\ufe0f Parameter Tuning Strategy Starting Settings : 25 steps + CFG 6.0 + DPM++ 2M Quick Preview : 20 steps + CFG 5.0 for rapid testing High Quality : 30 steps + CFG 6.5 for best results Style Experimentation : Adjust CFG within 4.5-7.0 range for testing \ud83d\udcca Model Comparison Comparison SD3 Medium SD1.5 Flux1-Dev SDXL Parameter Scale 2B 0.86B 12B 3.5B Image Quality Excellent Good Outstanding Excellent VRAM Requirements 8GB+ 4GB+ 12GB+ 6GB+ Generation Speed Fast Very Fast Medium Fast \ud83d\udd27 Common Issues and Solutions \ud83c\udfa8 Generation Quality Issues Solutions: CFG Too High : Lower CFG to 4.5-7.0 range Insufficient Steps : Increase to 25-30 steps Overly Simple Prompts : Use more detailed descriptions Wrong Sampler : Try DPM++ 2M or Euler for better results \u26a1 Performance Issues Optimization Tips: Insufficient VRAM : Lower resolution or use medvram mode Slow Loading : SD3 model is large, requires patience Slow Generation : Use fewer steps or faster samplers Memory Leaks : Restart WebUI periodically \ud83d\udd27 Compatibility Issues Important Notes: WebUI Version : Ensure using latest version that supports SD3 Extension Compatibility : Some extensions may not be compatible with SD3 Parameter Range : Note SD3's recommended parameter ranges Model Loading : First load may take several minutes \ud83d\udcda Related Resources \ud83d\udcc4 Official Documentation Stable Diffusion 3 Official Paper Technical principles and innovations of SD3 \ud83d\udcca Technical Report SD3 Technical Report Detailed technical implementation and performance evaluation \ud83e\udd17 Model Download Hugging Face Model Page Official model file download location \ud83d\udcd6 Usage Documentation WebUI SD3 Support Documentation Configuration and usage guide for SD3 in WebUI \ud83c\udfa8 Happy creating with Stable Diffusion 3 Medium! | Next-generation AI image generation at your fingertips","title":"Index en"},{"location":"SD/3/doc/index-en/#model-introduction","text":"**Stable Diffusion 3 Medium** is the medium-parameter version of the third-generation diffusion model released by Stability AI, representing a significant advancement in open-source image generation technology. This model significantly improves image quality, text understanding capabilities, and generation diversity while maintaining relatively low hardware requirements, making it the perfect balanced choice between SD1.5 and high-end models.","title":"\ud83c\udf1f Model Introduction"},{"location":"SD/3/doc/index-en/#core-features","text":"","title":"\u2728 Core Features"},{"location":"SD/3/doc/index-en/#technical-specifications","text":"Specification Details Model Type Text-to-Image Generation Architecture Multimodal Diffusion Transformer (MMDiT) Parameter Scale Approximately 2 billion parameters Text Encoders T5-XXL + CLIP-L + CLIP-G Native Resolution 1024\u00d71024 Supported Resolutions 512\u00d7512 to 2048\u00d72048 Recommended Steps 20-50 steps CFG Range 4.5-9.0","title":"\ud83d\udcca Technical Specifications"},{"location":"SD/3/doc/index-en/#configuration-instructions","text":"","title":"\u2699\ufe0f Configuration Instructions"},{"location":"SD/3/doc/index-en/#system-requirements","text":"\ud83d\udca1 Hardware Configuration ECS VRAM : 8GB or more recommended System RAM : 16GB or more recommended Storage Space : At least 15GB available space","title":"\ud83d\udcbb System Requirements"},{"location":"SD/3/doc/index-en/#model-files","text":"File Type Filename Size Description Main Model sd3_medium_incl_clips_t5xxlfp16.safetensors ~10GB Complete model with all components","title":"\ud83d\udcc1 Model Files"},{"location":"SD/3/doc/index-en/#usage-guide","text":"","title":"\ud83d\udcd6 Usage Guide"},{"location":"SD/3/doc/index-en/#web-ui-usage","text":"### \ud83d\udd27 Basic Operations **1. Model Selection** - Select SD3 Medium model in the model selector **2. Prompt Input** - **Positive Prompt**: Detailed description of desired image, supports more complex descriptions - **Negative Prompt**: SD3 responds weakly to negative prompts, can be simplified **3. Parameter Settings** - **Steps**: Recommended 20-30 steps - **CFG Scale**: Recommended 4.5-7.0 (lower than SD1.5) - **Sampler**: Recommended DPM++ 2M or Euler - **Resolution**: 1024\u00d71024 (native) or other supported sizes **4. Advanced Settings** - **Seed**: Controls randomness - **Batch Settings**: Adjust according to VRAM capacity - **Multi-Aspect Ratio**: Utilize native multi-resolution support","title":"\ud83c\udf10 Web UI Usage"},{"location":"SD/3/doc/index-en/#parameter-description","text":"","title":"\ud83d\udccb Parameter Description"},{"location":"SD/3/doc/index-en/#sampler-selection","text":"DPM++ 2M : High quality, recommended Euler : Fast and stable DPM++ SDE : High quality but slower DDIM : Classic choice, stable results","title":"\ud83d\udd04 Sampler Selection"},{"location":"SD/3/doc/index-en/#resolution-settings","text":"\ud83d\udca1 SD3 Native Resolutions SD3 natively supports various resolutions without quality loss: 1024\u00d71024 Standard square 1152\u00d7896 Landscape 4:3.6 896\u00d71152 Portrait 3.6:4 1344\u00d7768 Ultra-widescreen","title":"\ud83d\udcd0 Resolution Settings"},{"location":"SD/3/doc/index-en/#prompt-techniques","text":"","title":"\ud83d\udca1 Prompt Techniques"},{"location":"SD/3/doc/index-en/#sd3-prompt-characteristics","text":"","title":"\ud83e\udde0 SD3 Prompt Characteristics"},{"location":"SD/3/doc/index-en/#prompt-examples","text":"\ud83d\udcf8 Realistic Photography Style \"A professional headshot of a confident businesswoman in her 30s, wearing a navy blue blazer, sitting at a modern office desk, natural window lighting, shallow depth of field, shot with 85mm lens, high resolution, sharp focus\" \"Street photography of a bustling Tokyo intersection at night, neon signs, people crossing, motion blur on vehicles, rain-soaked streets reflecting lights, urban atmosphere, documentary style, Leica camera aesthetic\" \ud83c\udfa8 Artistic Creation Style \"An impressionist painting of a French countryside vineyard in autumn, golden sunlight filtering through grape vines, warm color palette, loose brushstrokes, en plein air style, reminiscent of Monet's work\" \"Digital concept art of a floating island city in the clouds, waterfalls cascading into the sky, flying ships, fantasy architecture, dramatic lighting, highly detailed, matte painting style\" \ud83d\udc64 Character Portraits \"A detailed portrait of a young artist in her studio, paint-stained apron, holding a palette and brush, surrounded by canvases, natural lighting from large windows, thoughtful expression, realistic oil painting style\" \"Fantasy character portrait of an elven archer, intricate leather armor, silver hair braided with leaves, piercing green eyes, forest background, detailed fantasy art, RPG character design\"","title":"\ud83c\udfa8 Prompt Examples"},{"location":"SD/3/doc/index-en/#api-integration","text":"\ud83d\udccb Click to expand API call Python code import requests import base64 import time class SD3API: def __init__(self, base_url, username=\"admin\", apikey=None): self.base_url = base_url.rstrip('/') self.auth = (username, apikey) if apikey else None def switch_model(self, model_name=\"sd3_medium_incl_clips_t5xxlfp16.safetensors\"): \"\"\"Switch to SD3 Medium model\"\"\" model_data = {\"sd_model_checkpoint\": model_name} print(\"\ud83d\udd04 Switching to SD3 Medium model...\") response = requests.post( f\"{self.base_url}/sdapi/v1/options\", json=model_data, auth=self.auth ) if response.status_code == 200: print(\"\u2705 Model switch completed successfully!\") return True else: print(f\"\u274c Model switch failed: {response.status_code}\") return False def generate_image(self, prompt, **kwargs): \"\"\"Generate image with SD3 optimized parameters\"\"\" default_params = { \"prompt\": prompt, \"steps\": 25, \"cfg_scale\": 6.0, \"width\": 1024, \"height\": 1024, \"sampler_name\": \"DPM++ 2M\", \"negative_prompt\": \"\", # SD3 works well with minimal negative prompts } # Update with user parameters default_params.update(kwargs) print(f\"\ud83c\udfa8 Generating image with SD3...\") print(f\"\ud83d\udcdd Prompt: {prompt[:50]}...\") response = requests.post( f\"{self.base_url}/sdapi/v1/txt2img\", json=default_params, auth=self.auth ) if response.status_code == 200: result = response.json() if \"images\" in result and result[\"images\"]: print(\"\u2705 Image generated successfully!\") return result else: print(\"\u274c No image data received\") return None else: print(f\"\u274c Generation failed: {response.status_code}\") return None def save_image(self, result, filename=\"sd3_output.png\"): \"\"\"Save generated image\"\"\" if result and \"images\" in result: image_data = base64.b64decode(result[\"images\"][0]) with open(filename, \"wb\") as f: f.write(image_data) print(f\"\ud83d\udcbe Image saved as {filename}\") return filename return None # Usage example def main(): # Initialize API api = SD3API( base_url=\"http://127.0.0.1:7680\", username=\"admin\", apikey=\"${APIKEY}\" # Replace with your API key ) # Switch to SD3 model if not api.switch_model(): return # Generate image with SD3-optimized settings prompt = \"\"\"A serene mountain landscape at golden hour, snow-capped peaks reflecting in a crystal clear alpine lake, pine trees in foreground, dramatic clouds, professional landscape photography, high detail\"\"\" result = api.generate_image( prompt=prompt, steps=28, cfg_scale=6.5, width=1152, height=896, sampler_name=\"DPM++ 2M\" ) if result: api.save_image(result, \"sd3_landscape.png\") if __name__ == \"__main__\": main()","title":"\ud83d\udd0c API Integration"},{"location":"SD/3/doc/index-en/#best-practices","text":"","title":"\ud83c\udfc6 Best Practices"},{"location":"SD/3/doc/index-en/#model-comparison","text":"Comparison SD3 Medium SD1.5 Flux1-Dev SDXL Parameter Scale 2B 0.86B 12B 3.5B Image Quality Excellent Good Outstanding Excellent VRAM Requirements 8GB+ 4GB+ 12GB+ 6GB+ Generation Speed Fast Very Fast Medium Fast","title":"\ud83d\udcca Model Comparison"},{"location":"SD/3/doc/index-en/#common-issues-and-solutions","text":"\ud83c\udfa8 Generation Quality Issues Solutions: CFG Too High : Lower CFG to 4.5-7.0 range Insufficient Steps : Increase to 25-30 steps Overly Simple Prompts : Use more detailed descriptions Wrong Sampler : Try DPM++ 2M or Euler for better results \u26a1 Performance Issues Optimization Tips: Insufficient VRAM : Lower resolution or use medvram mode Slow Loading : SD3 model is large, requires patience Slow Generation : Use fewer steps or faster samplers Memory Leaks : Restart WebUI periodically \ud83d\udd27 Compatibility Issues Important Notes: WebUI Version : Ensure using latest version that supports SD3 Extension Compatibility : Some extensions may not be compatible with SD3 Parameter Range : Note SD3's recommended parameter ranges Model Loading : First load may take several minutes","title":"\ud83d\udd27 Common Issues and Solutions"},{"location":"SD/3/doc/index-en/#related-resources","text":"","title":"\ud83d\udcda Related Resources"},{"location":"Wan2.1/I2V-14B-480/doc/","text":"\ud83c\udfac Wan2.1-I2V-14B \u6a21\u578b\u4f7f\u7528\u6307\u5357 \u5f3a\u5927\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u8ba9\u9759\u6001\u56fe\u7247\u52a8\u8d77\u6765 \ud83d\ude80 14B\u53c2\u6570 \ud83c\udfaf \u9ad8\u8d28\u91cf \u26a1 \u5feb\u901f\u751f\u6210 \ud83d\udccb \u6a21\u578b\u7b80\u4ecb **Wan2.1-I2V-14B** \u662f\u4e00\u4e2a\u9769\u547d\u6027\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u62e5\u6709140\u4ebf\u53c2\u6570\u7684\u5f3a\u5927\u67b6\u6784\u3002\u5b83\u80fd\u591f\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u5728\u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u4e3b\u4f53\u7279\u5f81\u7684\u540c\u65f6\uff0c\u6839\u636e\u6587\u672c\u63cf\u8ff0\u6dfb\u52a0\u903c\u771f\u7684\u52a8\u6001\u6548\u679c\u548c\u573a\u666f\u53d8\u5316\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \ud83e\udde0 \u5f3a\u5927\u53c2\u6570\u89c4\u6a21 14B\u53c2\u6570\uff0c\u63d0\u4f9b\u5f3a\u5927\u7684\u56fe\u50cf\u7406\u89e3\u548c\u89c6\u9891\u751f\u6210\u80fd\u529b \ud83d\uddbc\ufe0f \u56fe\u50cf\u9a71\u52a8\u751f\u6210 \u4ee5\u8f93\u5165\u56fe\u50cf\u4e3a\u57fa\u7840\uff0c\u751f\u6210\u8fde\u8d2f\u7684\u89c6\u9891\u5e8f\u5217 \ud83c\udf0d \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u6587\u548c\u82f1\u6587\u6587\u672c\u63d0\u793a \ud83c\udfaf \u56fe\u50cf\u4e00\u81f4\u6027 \u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u7684\u4e3b\u8981\u7279\u5f81\u548c\u98ce\u683c \ud83d\udd27 \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff08I2V\uff09 \u53c2\u6570\u89c4\u6a21 14B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u652f\u6301\u5206\u8fa8\u7387 480p \u6700\u5927\u5e27\u6570 81\u5e27 \u63a8\u8350\u5e27\u7387 16fps \u8f93\u5165\u683c\u5f0f JPEG\u3001PNG\u3001WebP \u8f93\u51fa\u683c\u5f0f MP4 (H.264) \ud83d\ude80 \u4f7f\u7528\u8bf4\u660e \ud83c\udf10 Web UI \u4f7f\u7528\u6559\u7a0b ### \ud83d\udccd \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 \u70b9\u51fb\u8bbf\u95ee\u94fe\u63a5\u8fdb\u5165 ComfyUI \u754c\u9762 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41 \u9009\u62e9 `wanx-21.json` \u5de5\u4f5c\u6d41\u5e76\u6253\u5f00\uff0c\u9009\u62e9\u56fe\u751f\u89c6\u9891\u529f\u80fd\u9009\u9879 ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u4e0a\u4f20\u56fe\u50cf - \u5728 **LoadImage** \u8282\u70b9\u9009\u62e9\u793a\u4f8b\u56fe\u7247 - \u6216\u4ece\u672c\u673a\u7535\u8111\u4e0a\u4f20\u81ea\u5b9a\u4e49\u56fe\u50cf LoadImage \u8282\u70b9\u64cd\u4f5c\u754c\u9762 ### \u270d\ufe0f \u6b65\u9aa4\u56db\uff1a\u8bbe\u7f6e\u6587\u672c\u63cf\u8ff0 \u5728 **TextEncode** \u8282\u70b9\u586b\u5199\u63cf\u8ff0\u8bcd\uff1a \u2705 \u6b63\u5411\u63d0\u793a\u8bcd \u63cf\u8ff0\u5e0c\u671b\u7684\u52a8\u4f5c\u548c\u573a\u666f\u53d8\u5316 \u274c \u8d1f\u5411\u63d0\u793a\u8bcd \u4e0d\u60f3\u8981\u751f\u6210\u7684\u5185\u5bb9 ### \u2699\ufe0f \u6b65\u9aa4\u4e94\uff1a\u914d\u7f6e\u53c2\u6570 \u5728 **ImageClip Encode** \u8bbe\u7f6e\u5206\u8fa8\u7387\u548c\u5e27\u6570 ### \ud83c\udfac \u6b65\u9aa4\u516d\uff1a\u6267\u884c\u5de5\u4f5c\u6d41 \u70b9\u51fb\u6267\u884c\u6309\u94ae\u5f00\u59cb\u751f\u6210 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83d\udd10 \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 COMFYUI_SERVER \u7684\u83b7\u53d6\u53c2\u8003 \ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python \u4ee3\u7801\u5b9e\u73b0 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-I2V-14B-480P_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" CLIP_MODEL = \"wan2.1/open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"girl.png\" PROMPT = \"A beautiful anime girl with long flowing hair, graceful movements, smooth animation, cinematic lighting, high quality\" NEG_PROMPT = \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_i2v(self, image_path, prompt, neg_prompt, steps=10, cfg=6, width=512, height=512, frames=81): \"\"\"\ud83c\udfac \u56fe\u751f\u89c6\u9891\u751f\u6210\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") workflow = { \"42\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"43\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"44\": {\"inputs\": {\"model_name\": CLIP_MODEL, \"precision\": \"fp16\", \"load_device\": \"offload_device\"}, \"class_type\": \"LoadWanVideoClipTextEncoder\"}, \"45\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"46\": {\"inputs\": {\"blocks_to_swap\": 10, \"offload_img_emb\": False, \"offload_txt_emb\": False, \"use_non_blocking\": True, \"vace_blocks_to_swap\": 0}, \"class_type\": \"WanVideoBlockSwap\"}, \"47\": {\"inputs\": {\"backend\": \"inductor\", \"fullgraph\": False, \"mode\": \"default\", \"dynamic\": False, \"dynamo_cache_size_limit\": 64, \"compile_transformer_blocks_only\": True, \"dynamo_recompile_limit\": 128}, \"class_type\": \"WanVideoTorchCompileSettings\"}, \"48\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\", \"block_swap_args\": [\"46\", 0]}, \"class_type\": \"WanVideoModelLoader\"}, \"49\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"45\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"50\": { \"inputs\": { \"generation_width\": width, \"generation_height\": height, \"num_frames\": frames, \"force_offload\": True, \"noise_aug_strength\": 0, \"latent_strength\": 1, \"clip_embed_strength\": 1, \"adjust_resolution\": True, \"image\": [\"42\", 0], \"vae\": [\"43\", 0], \"clip_vision\": [\"44\", 0] }, \"class_type\": \"WanVideoImageClipEncode\" }, \"52\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"48\", 0], \"text_embeds\": [\"49\", 0], \"image_embeds\": [\"50\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"53\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"43\", 0], \"samples\": [\"52\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"54\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"WanVideo2_1\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"53\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"i2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c\u56fe\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIClient() try: print(f\"\ud83c\udfac \u5f00\u59cb\u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709 girl.png \u6587\u4ef6\") return task_id = client.generate_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 10, 6, 512, 512, 81) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udd17 ComfyUI API \u7aef\u70b9\u8bf4\u660e \u7aef\u70b9 \u65b9\u6cd5 \u529f\u80fd \u8bf4\u660e /queue GET \u83b7\u53d6\u961f\u5217\u72b6\u6001 \u67e5\u770b\u5f53\u524d\u4efb\u52a1\u961f\u5217\u548c\u8fd0\u884c\u72b6\u6001 /prompt POST \u63d0\u4ea4\u5de5\u4f5c\u6d41 \u6267\u884c\u56fe\u751f\u89c6\u9891\u4efb\u52a1 /history/{prompt_id} GET \u83b7\u53d6\u6267\u884c\u5386\u53f2 \u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c\u548c\u8f93\u51fa /upload/image POST \u4e0a\u4f20\u56fe\u7247 \u4e0a\u4f20\u8f93\u5165\u56fe\u7247\u6587\u4ef6 /view GET \u4e0b\u8f7d\u8f93\u51fa\u6587\u4ef6 \u83b7\u53d6\u751f\u6210\u7684\u7ed3\u679c\u6587\u4ef6 \u2699\ufe0f \u53c2\u6570\u8be6\u7ec6\u8bf4\u660e \ud83c\udf9b\ufe0f \u751f\u6210\u53c2\u6570\u914d\u7f6e \ud83d\udd22 \u6838\u5fc3\u53c2\u6570 steps : \u63a8\u7406\u6b65\u6570\uff08\u5efa\u8bae20-30\uff09 cfg : CFG\u5f15\u5bfc\u5f3a\u5ea6\uff08\u5efa\u8bae6-8\uff09 shift : \u566a\u58f0\u8c03\u5ea6\u504f\u79fb\uff08\u5efa\u8bae5\uff09 \ud83c\udfb2 \u968f\u673a\u6027\u63a7\u5236 seed : \u968f\u673a\u79cd\u5b50\uff08\u63a7\u5236\u751f\u6210\u7ed3\u679c\u7684\u968f\u673a\u6027\uff09 denoise_strength : \u53bb\u566a\u5f3a\u5ea6\uff080.6-0.9\uff0c\u63a7\u5236\u5bf9\u539f\u56fe\u7684\u4fdd\u6301\u7a0b\u5ea6\uff09 \ud83d\udcd0 \u89c6\u9891\u89c4\u683c width \u00d7 height : \u89c6\u9891\u5206\u8fa8\u7387\uff08\u5efa\u8bae512\u00d7512\uff09 frames : \u89c6\u9891\u5e27\u6570\uff08\u6700\u592781\u5e27\uff09 frame_rate : \u5e27\u7387\uff08\u63a8\u835016fps\uff09 \ud83d\uddbc\ufe0f \u56fe\u50cf\u8981\u6c42\u8be6\u89e3 \ud83d\udca1 \u56fe\u50cf\u8f93\u5165\u8981\u6c42 \u5206\u8fa8\u7387\u8981\u6c42 : \u5efa\u8bae512\u00d7512\u4ee5\u4e0a\uff0c\u786e\u4fdd\u56fe\u50cf\u6e05\u6670\u5ea6 \u652f\u6301\u683c\u5f0f : JPEG\u3001PNG\u3001WebP\u7b49\u4e3b\u6d41\u683c\u5f0f \u5185\u5bb9\u8981\u6c42 : \u6e05\u6670\u7684\u4e3b\u4f53\u5bf9\u8c61\uff0c\u907f\u514d\u8fc7\u4e8e\u590d\u6742\u7684\u80cc\u666f \u8d28\u91cf\u6807\u51c6 : \u9ad8\u8d28\u91cf\u56fe\u50cf\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u89c6\u9891\u6548\u679c \ud83d\udca1 \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357 \u2705 \u6b63\u5411\u63d0\u793a\u8bcd\u793a\u4f8b \ud83d\udeb6 \u4eba\u7269\u52a8\u4f5c \"The person in the image is walking slowly through a garden\" \ud83d\udc31 \u52a8\u7269\u884c\u4e3a \"The cat in the photo is playing with a ball of yarn\" \ud83d\ude97 \u4ea4\u901a\u5de5\u5177 \"The car in the image is driving down a winding mountain road\" \ud83d\udc83 \u827a\u672f\u8868\u6f14 \"The dancer in the picture is performing elegant ballet movements\" \u274c \u8d1f\u5411\u63d0\u793a\u8bcd\u5efa\u8bae \ud83d\udeab \u9759\u6001\u95ee\u9898 \"static, motionless, frozen, distorted, blurry\" \u26a0\ufe0f \u52a8\u4f5c\u95ee\u9898 \"unnatural movement, jerky motion, inconsistent\" \ud83d\udcc9 \u8d28\u91cf\u95ee\u9898 \"low quality, artifacts, noise, compression\" \ud83c\udfc6 \u6700\u4f73\u5b9e\u8df5\u6307\u5357 \ud83d\udcf8 \u8f93\u5165\u56fe\u50cf\u9009\u62e9\u7b56\u7565 \ud83d\udd0d \u6e05\u6670\u5ea6\u4f18\u5148 \u9009\u62e9\u9ad8\u6e05\u6670\u5ea6\u7684\u56fe\u50cf\uff0c\u907f\u514d\u6a21\u7cca\u6216\u566a\u70b9\u8fc7\u591a\u7684\u56fe\u7247 \ud83c\udfaf \u4e3b\u4f53\u660e\u786e \u786e\u4fdd\u4e3b\u8981\u5bf9\u8c61\u6e05\u6670\u53ef\u89c1\uff0c\u5360\u636e\u753b\u9762\u5408\u9002\u6bd4\u4f8b \ud83d\uddbc\ufe0f \u6784\u56fe\u5408\u7406 \u907f\u514d\u8fc7\u4e8e\u590d\u6742\u7684\u80cc\u666f\uff0c\u4fdd\u6301\u6784\u56fe\u7b80\u6d01\u660e\u4e86 \ud83d\udca1 \u5149\u7167\u826f\u597d \u5149\u7167\u5747\u5300\u7684\u56fe\u50cf\u6548\u679c\u66f4\u4f73\uff0c\u907f\u514d\u8fc7\u6697\u6216\u8fc7\u66dd \u270d\ufe0f \u63d0\u793a\u8bcd\u7f16\u5199\u6280\u5de7 \ud83d\udcdd \u7f16\u5199\u8981\u70b9 \u5177\u4f53\u63cf\u8ff0 : \u8be6\u7ec6\u63cf\u8ff0\u5e0c\u671b\u7684\u52a8\u4f5c\u548c\u573a\u666f\uff0c\u8d8a\u5177\u4f53\u8d8a\u597d \u4fdd\u6301\u4e00\u81f4 : \u786e\u4fdd\u63cf\u8ff0\u4e0e\u56fe\u50cf\u5185\u5bb9\u76f8\u7b26\uff0c\u907f\u514d\u77db\u76fe \u52a8\u4f5c\u5408\u7406 : \u63cf\u8ff0\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u52a8\u4f5c\uff0c\u907f\u514d\u4e0d\u53ef\u80fd\u7684\u8fd0\u52a8 \u98ce\u683c\u7edf\u4e00 : \u4fdd\u6301\u4e0e\u539f\u56fe\u98ce\u683c\u4e00\u81f4\u7684\u63cf\u8ff0 \u2699\ufe0f \u53c2\u6570\u8c03\u4f18\u7b56\u7565 \u53bb\u566a\u5f3a\u5ea6 : 0.6-0.7\u4fdd\u6301\u539f\u56fe\u7279\u5f81\uff0c0.8-0.9\u5141\u8bb8\u66f4\u591a\u53d8\u5316 CFG\u503c\u8bbe\u7f6e : 6-7\u5e73\u8861\u5f15\u5bfc\uff0c8-10\u66f4\u5f3a\u6587\u672c\u5f15\u5bfc \u6b65\u6570\u9009\u62e9 : 20-25\u5feb\u901f\u751f\u6210\uff0c25-30\u66f4\u9ad8\u8d28\u91cf \u26a0\ufe0f \u91cd\u8981\u6ce8\u610f\u4e8b\u9879 \ud83d\udcbe \u5185\u5b58\u7ba1\u7406\u6ce8\u610f\u4e8b\u9879 \u91cd\u8981\u63d0\u9192\uff1a \u56fe\u751f\u89c6\u9891\u6bd4\u6587\u751f\u89c6\u9891\u9700\u8981\u66f4\u591a\u663e\u5b58\uff0c\u5efa\u8bae\u5173\u95ed\u5176\u4ed6\u5360\u7528\u663e\u5b58\u7684\u7a0b\u5e8f \u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u5408\u9002\uff0c\u907f\u514d\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684\u56fe\u7247 \u53bb\u566a\u5f3a\u5ea6\u4e0d\u5b9c\u8fc7\u9ad8\uff0c\u4ee5\u4fdd\u6301\u56fe\u50cf\u4e00\u81f4\u6027 \u63cf\u8ff0\u7684\u52a8\u4f5c\u5e94\u7b26\u5408\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u7279\u5f81 \ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a \ud83d\udc64 \u4eba\u7269\u52a8\u753b \u8ba9\u9759\u6001\u4eba\u7269\u7167\u7247\u52a8\u8d77\u6765\uff0c\u521b\u9020\u751f\u52a8\u7684\u4eba\u7269\u89c6\u9891 \ud83d\udecd\ufe0f \u4ea7\u54c1\u5c55\u793a \u4e3a\u4ea7\u54c1\u56fe\u7247\u6dfb\u52a0\u52a8\u6001\u6548\u679c\uff0c\u63d0\u5347\u8425\u9500\u6548\u679c \ud83c\udfa8 \u827a\u672f\u521b\u4f5c \u5c06\u7ed8\u753b\u4f5c\u54c1\u8f6c\u6362\u4e3a\u52a8\u6001\u89c6\u9891\uff0c\u589e\u5f3a\u827a\u672f\u8868\u73b0\u529b \ud83d\udcda \u6559\u80b2\u6f14\u793a \u8ba9\u6559\u5b66\u56fe\u7247\u5177\u6709\u52a8\u6001\u6548\u679c\uff0c\u63d0\u5347\u5b66\u4e60\u4f53\u9a8c \ud83d\udcda \u76f8\u5173\u8d44\u6e90\u94fe\u63a5 \ud83d\udcd6 ComfyUI \u5b98\u65b9\u6587\u6863 ComfyUI \u4f7f\u7528\u6307\u5357 \u2192 \u8be6\u7ec6\u7684 ComfyUI \u4f7f\u7528\u8bf4\u660e\u548c\u8282\u70b9\u4ecb\u7ecd \ud83c\udfa5 WanVideo \u63d2\u4ef6\u6587\u6863 GitHub \u9879\u76ee\u9875\u9762 \u2192 WanVideo \u63d2\u4ef6\u7684\u8be6\u7ec6\u4f7f\u7528\u8bf4\u660e \ud83d\udd27 ComfyUI \u6280\u672f\u6587\u6863 \u56fe\u50cf\u9884\u5904\u7406\u6307\u5357 \u2192 \u56fe\u50cf\u9884\u5904\u7406\u548c\u9ad8\u7ea7\u529f\u80fd\u8bf4\u660e \ud83c\udfac \u5f00\u59cb\u60a8\u7684\u521b\u4f5c\u4e4b\u65c5\uff01 | Wan2.1-I2V-14B \u8ba9\u60a8\u7684\u9759\u6001\u56fe\u7247\u7115\u53d1\u751f\u673a\uff0c\u521b\u9020\u65e0\u9650\u53ef\u80fd","title":"Index"},{"location":"Wan2.1/I2V-14B-480/doc/#_1","text":"**Wan2.1-I2V-14B** \u662f\u4e00\u4e2a\u9769\u547d\u6027\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u62e5\u6709140\u4ebf\u53c2\u6570\u7684\u5f3a\u5927\u67b6\u6784\u3002\u5b83\u80fd\u591f\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u5728\u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u4e3b\u4f53\u7279\u5f81\u7684\u540c\u65f6\uff0c\u6839\u636e\u6587\u672c\u63cf\u8ff0\u6dfb\u52a0\u903c\u771f\u7684\u52a8\u6001\u6548\u679c\u548c\u573a\u666f\u53d8\u5316\u3002","title":"\ud83d\udccb \u6a21\u578b\u7b80\u4ecb"},{"location":"Wan2.1/I2V-14B-480/doc/#_2","text":"","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"Wan2.1/I2V-14B-480/doc/#_3","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff08I2V\uff09 \u53c2\u6570\u89c4\u6a21 14B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u652f\u6301\u5206\u8fa8\u7387 480p \u6700\u5927\u5e27\u6570 81\u5e27 \u63a8\u8350\u5e27\u7387 16fps \u8f93\u5165\u683c\u5f0f JPEG\u3001PNG\u3001WebP \u8f93\u51fa\u683c\u5f0f MP4 (H.264)","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Wan2.1/I2V-14B-480/doc/#_4","text":"","title":"\ud83d\ude80 \u4f7f\u7528\u8bf4\u660e"},{"location":"Wan2.1/I2V-14B-480/doc/#web-ui","text":"### \ud83d\udccd \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 \u70b9\u51fb\u8bbf\u95ee\u94fe\u63a5\u8fdb\u5165 ComfyUI \u754c\u9762 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41 \u9009\u62e9 `wanx-21.json` \u5de5\u4f5c\u6d41\u5e76\u6253\u5f00\uff0c\u9009\u62e9\u56fe\u751f\u89c6\u9891\u529f\u80fd\u9009\u9879 ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u4e0a\u4f20\u56fe\u50cf - \u5728 **LoadImage** \u8282\u70b9\u9009\u62e9\u793a\u4f8b\u56fe\u7247 - \u6216\u4ece\u672c\u673a\u7535\u8111\u4e0a\u4f20\u81ea\u5b9a\u4e49\u56fe\u50cf LoadImage \u8282\u70b9\u64cd\u4f5c\u754c\u9762 ### \u270d\ufe0f \u6b65\u9aa4\u56db\uff1a\u8bbe\u7f6e\u6587\u672c\u63cf\u8ff0 \u5728 **TextEncode** \u8282\u70b9\u586b\u5199\u63cf\u8ff0\u8bcd\uff1a","title":"\ud83c\udf10 Web UI \u4f7f\u7528\u6559\u7a0b"},{"location":"Wan2.1/I2V-14B-480/doc/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"Wan2.1/I2V-14B-480/doc/#_5","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"Wan2.1/I2V-14B-480/doc/#python","text":"\ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python \u4ee3\u7801\u5b9e\u73b0 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-I2V-14B-480P_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" CLIP_MODEL = \"wan2.1/open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"girl.png\" PROMPT = \"A beautiful anime girl with long flowing hair, graceful movements, smooth animation, cinematic lighting, high quality\" NEG_PROMPT = \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_i2v(self, image_path, prompt, neg_prompt, steps=10, cfg=6, width=512, height=512, frames=81): \"\"\"\ud83c\udfac \u56fe\u751f\u89c6\u9891\u751f\u6210\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") workflow = { \"42\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"43\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"44\": {\"inputs\": {\"model_name\": CLIP_MODEL, \"precision\": \"fp16\", \"load_device\": \"offload_device\"}, \"class_type\": \"LoadWanVideoClipTextEncoder\"}, \"45\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"46\": {\"inputs\": {\"blocks_to_swap\": 10, \"offload_img_emb\": False, \"offload_txt_emb\": False, \"use_non_blocking\": True, \"vace_blocks_to_swap\": 0}, \"class_type\": \"WanVideoBlockSwap\"}, \"47\": {\"inputs\": {\"backend\": \"inductor\", \"fullgraph\": False, \"mode\": \"default\", \"dynamic\": False, \"dynamo_cache_size_limit\": 64, \"compile_transformer_blocks_only\": True, \"dynamo_recompile_limit\": 128}, \"class_type\": \"WanVideoTorchCompileSettings\"}, \"48\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\", \"block_swap_args\": [\"46\", 0]}, \"class_type\": \"WanVideoModelLoader\"}, \"49\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"45\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"50\": { \"inputs\": { \"generation_width\": width, \"generation_height\": height, \"num_frames\": frames, \"force_offload\": True, \"noise_aug_strength\": 0, \"latent_strength\": 1, \"clip_embed_strength\": 1, \"adjust_resolution\": True, \"image\": [\"42\", 0], \"vae\": [\"43\", 0], \"clip_vision\": [\"44\", 0] }, \"class_type\": \"WanVideoImageClipEncode\" }, \"52\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"48\", 0], \"text_embeds\": [\"49\", 0], \"image_embeds\": [\"50\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"53\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"43\", 0], \"samples\": [\"52\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"54\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"WanVideo2_1\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"53\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"i2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c\u56fe\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIClient() try: print(f\"\ud83c\udfac \u5f00\u59cb\u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709 girl.png \u6587\u4ef6\") return task_id = client.generate_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 10, 6, 512, 512, 81) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b"},{"location":"Wan2.1/I2V-14B-480/doc/#comfyui-api","text":"\u7aef\u70b9 \u65b9\u6cd5 \u529f\u80fd \u8bf4\u660e /queue GET \u83b7\u53d6\u961f\u5217\u72b6\u6001 \u67e5\u770b\u5f53\u524d\u4efb\u52a1\u961f\u5217\u548c\u8fd0\u884c\u72b6\u6001 /prompt POST \u63d0\u4ea4\u5de5\u4f5c\u6d41 \u6267\u884c\u56fe\u751f\u89c6\u9891\u4efb\u52a1 /history/{prompt_id} GET \u83b7\u53d6\u6267\u884c\u5386\u53f2 \u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c\u548c\u8f93\u51fa /upload/image POST \u4e0a\u4f20\u56fe\u7247 \u4e0a\u4f20\u8f93\u5165\u56fe\u7247\u6587\u4ef6 /view GET \u4e0b\u8f7d\u8f93\u51fa\u6587\u4ef6 \u83b7\u53d6\u751f\u6210\u7684\u7ed3\u679c\u6587\u4ef6","title":"\ud83d\udd17 ComfyUI API \u7aef\u70b9\u8bf4\u660e"},{"location":"Wan2.1/I2V-14B-480/doc/#_6","text":"","title":"\u2699\ufe0f \u53c2\u6570\u8be6\u7ec6\u8bf4\u660e"},{"location":"Wan2.1/I2V-14B-480/doc/#_7","text":"","title":"\ud83c\udf9b\ufe0f \u751f\u6210\u53c2\u6570\u914d\u7f6e"},{"location":"Wan2.1/I2V-14B-480/doc/#_8","text":"\ud83d\udca1 \u56fe\u50cf\u8f93\u5165\u8981\u6c42 \u5206\u8fa8\u7387\u8981\u6c42 : \u5efa\u8bae512\u00d7512\u4ee5\u4e0a\uff0c\u786e\u4fdd\u56fe\u50cf\u6e05\u6670\u5ea6 \u652f\u6301\u683c\u5f0f : JPEG\u3001PNG\u3001WebP\u7b49\u4e3b\u6d41\u683c\u5f0f \u5185\u5bb9\u8981\u6c42 : \u6e05\u6670\u7684\u4e3b\u4f53\u5bf9\u8c61\uff0c\u907f\u514d\u8fc7\u4e8e\u590d\u6742\u7684\u80cc\u666f \u8d28\u91cf\u6807\u51c6 : \u9ad8\u8d28\u91cf\u56fe\u50cf\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u89c6\u9891\u6548\u679c","title":"\ud83d\uddbc\ufe0f \u56fe\u50cf\u8981\u6c42\u8be6\u89e3"},{"location":"Wan2.1/I2V-14B-480/doc/#_9","text":"","title":"\ud83d\udca1 \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357"},{"location":"Wan2.1/I2V-14B-480/doc/#_10","text":"","title":"\u2705 \u6b63\u5411\u63d0\u793a\u8bcd\u793a\u4f8b"},{"location":"Wan2.1/I2V-14B-480/doc/#_11","text":"","title":"\u274c \u8d1f\u5411\u63d0\u793a\u8bcd\u5efa\u8bae"},{"location":"Wan2.1/I2V-14B-480/doc/#_12","text":"","title":"\ud83c\udfc6 \u6700\u4f73\u5b9e\u8df5\u6307\u5357"},{"location":"Wan2.1/I2V-14B-480/doc/#_13","text":"","title":"\ud83d\udcf8 \u8f93\u5165\u56fe\u50cf\u9009\u62e9\u7b56\u7565"},{"location":"Wan2.1/I2V-14B-480/doc/#_14","text":"","title":"\u270d\ufe0f \u63d0\u793a\u8bcd\u7f16\u5199\u6280\u5de7"},{"location":"Wan2.1/I2V-14B-480/doc/#_15","text":"\ud83d\udcbe \u5185\u5b58\u7ba1\u7406\u6ce8\u610f\u4e8b\u9879 \u91cd\u8981\u63d0\u9192\uff1a \u56fe\u751f\u89c6\u9891\u6bd4\u6587\u751f\u89c6\u9891\u9700\u8981\u66f4\u591a\u663e\u5b58\uff0c\u5efa\u8bae\u5173\u95ed\u5176\u4ed6\u5360\u7528\u663e\u5b58\u7684\u7a0b\u5e8f \u786e\u4fdd\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u5408\u9002\uff0c\u907f\u514d\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684\u56fe\u7247 \u53bb\u566a\u5f3a\u5ea6\u4e0d\u5b9c\u8fc7\u9ad8\uff0c\u4ee5\u4fdd\u6301\u56fe\u50cf\u4e00\u81f4\u6027 \u63cf\u8ff0\u7684\u52a8\u4f5c\u5e94\u7b26\u5408\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u7279\u5f81","title":"\u26a0\ufe0f \u91cd\u8981\u6ce8\u610f\u4e8b\u9879"},{"location":"Wan2.1/I2V-14B-480/doc/#_16","text":"\ud83d\udc64","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a"},{"location":"Wan2.1/I2V-14B-480/doc/#_17","text":"","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90\u94fe\u63a5"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/","text":"\ud83c\udfac Wan2.1-I2V-14B Model Guide Transform static images into dynamic videos with AI-powered generation \ud83d\ude80 Model Introduction Wan2.1-I2V-14B is a powerful image-to-video generation model that can generate high-quality video content based on input images and text prompts. The model maintains the main characteristics of the input image while adding dynamic effects and scene changes according to text descriptions. \u2728 Core Features \ud83e\udde0 Parameter Scale 14B parameters providing powerful image understanding and video generation capabilities \ud83d\uddbc\ufe0f Image-Driven Generates coherent video sequences based on input images \ud83c\udf0d Multi-language Support Supports Chinese and English text prompts \ud83c\udfaf Image Consistency Maintains main features and style of the input image \ud83d\udd27 Technical Specifications Specification Value \ud83e\udd16 Model Type Image-to-Video Generation \u26a1 Quantization FP8 quantized version \ud83d\udcfa Supported Resolution 480p \ud83c\udf9e\ufe0f Maximum Frames 81 frames \ud83c\udfac Recommended Frame Rate 16fps \ud83d\udcc1 Input Format JPEG, PNG, WebP, etc. \ud83d\udcd6 Usage Instructions \ud83c\udf10 Web UI Usage #### Step 1: Access Interface Click the access link at the service instance Click access link to enter ComfyUI interface #### Step 2: Select Workflow Select wanx-21.json workflow and open it, choose the image-to-video function option #### Step 3: Upload Image Select sample image in LoadImage node Or upload custom image from local computer #### Step 4: Set Text Description Fill in description words in TextEncode node: \u2705 Top Input Describe desired actions and scene changes \u274c Bottom Input Content you don't want to generate #### Step 5: Configure Parameters Set resolution and frame count in ImageClip Encode #### Step 6: Execute Workflow Click execute button to start generation \ud83d\udd0c API Integration \ud83d\udd11 Authentication Setup \ud83c\udfab Get Token Click the button in the upper right corner, open the bottom panel \ud83c\udf10 Get Server Address For COMFYUI_SERVER acquisition, refer to: \ud83d\udcbb Python Implementation \ud83d\udc0d Click to Expand Complete Python Code import requests, json, uuid, time, random, os # Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-I2V-14B-480P_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" CLIP_MODEL = \"wan2.1/open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors\" # Preset parameters IMAGE_PATH = \"girl.png\" PROMPT = \"A beautiful anime girl with long flowing hair, graceful movements, smooth animation, cinematic lighting, high quality\" NEG_PROMPT = \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"Upload image to ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"Image file does not exist: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"Upload failed, status code: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"No filename in upload response: {result}\") return result['name'] except Exception as e: raise Exception(f\"Image upload failed: {e}\") def generate_i2v(self, image_path, prompt, neg_prompt, steps=10, cfg=6, width=512, height=512, frames=81): \"\"\"Image-to-Video - Fixed clip_vision input\"\"\" print(\"\ud83d\udce4 Uploading image...\") image_name = self.upload_image(image_path) print(f\"\u2705 Image uploaded successfully: {image_name}\") workflow = { \"42\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"43\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"44\": {\"inputs\": {\"model_name\": CLIP_MODEL, \"precision\": \"fp16\", \"load_device\": \"offload_device\"}, \"class_type\": \"LoadWanVideoClipTextEncoder\"}, \"45\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"46\": {\"inputs\": {\"blocks_to_swap\": 10, \"offload_img_emb\": False, \"offload_txt_emb\": False, \"use_non_blocking\": True, \"vace_blocks_to_swap\": 0}, \"class_type\": \"WanVideoBlockSwap\"}, \"47\": {\"inputs\": {\"backend\": \"inductor\", \"fullgraph\": False, \"mode\": \"default\", \"dynamic\": False, \"dynamo_cache_size_limit\": 64, \"compile_transformer_blocks_only\": True, \"dynamo_recompile_limit\": 128}, \"class_type\": \"WanVideoTorchCompileSettings\"}, \"48\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\", \"block_swap_args\": [\"46\", 0]}, \"class_type\": \"WanVideoModelLoader\"}, \"49\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"45\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"50\": { \"inputs\": { \"generation_width\": width, \"generation_height\": height, \"num_frames\": frames, \"force_offload\": True, \"noise_aug_strength\": 0, \"latent_strength\": 1, \"clip_embed_strength\": 1, \"adjust_resolution\": True, \"image\": [\"42\", 0], \"vae\": [\"43\", 0], \"clip_vision\": [\"44\", 0] }, \"class_type\": \"WanVideoImageClipEncode\" }, \"52\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"48\", 0], \"text_embeds\": [\"49\", 0], \"image_embeds\": [\"50\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"53\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"43\", 0], \"samples\": [\"52\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"54\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"WanVideo2_1\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"53\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"i2v_output.mp4\"): try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): client = ComfyUIClient() try: print(f\"\ud83c\udfac Starting image-to-video task...\") print(f\"\ud83d\udcf7 Input image: {IMAGE_PATH}\") print(f\"\ud83d\udcdd Prompt: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c Image file does not exist: {IMAGE_PATH}\") print(\"Please ensure there is a girl.png file in the current directory\") return task_id = client.generate_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 10, 6, 512, 512, 81) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 Code Features Complete ComfyUI client implementation Support for image upload, workflow submission, status monitoring Automatic video file download Detailed error handling and logging \ud83d\udd17 ComfyUI API Endpoints Endpoint Method Function Description /queue GET Get queue status View current task queue /prompt POST Submit workflow Execute generation task /history/{prompt_id} GET Get execution history View task execution results /upload/image POST Upload image Upload input image file /view GET Download output file Get generated result files \u2699\ufe0f Parameter Configuration \ud83c\udf9b\ufe0f Generation Parameters \ud83d\udd22 Core Parameters Steps Inference steps (recommended 20-30) CFG CFG guidance strength (recommended 6-8) Shift Noise schedule offset (recommended 5) \ud83c\udfb2 Randomness Control Seed Random seed (controls randomness) Denoise Strength Denoising strength (0.6-0.9) \ud83d\uddbc\ufe0f Image Requirements > **\ud83d\udca1 Pro Tip:** High-quality input images are the key to excellent video generation! **\ud83d\udcd0 Resolution**: Recommended 512\u00d7512 or higher **\ud83d\udcc4 Format**: JPEG, PNG, WebP, etc. **\ud83c\udfaf Content**: Clear main subject, avoid overly complex backgrounds **\u2728 Quality**: High-quality images yield better video effects \ud83d\udca1 Prompt Engineering \u2705 Positive Prompt Examples \ud83d\udeb6 Character Actions \"The person in the image is walking slowly through a garden\" \ud83d\udc31 Animal Behavior \"The cat in the photo is playing with a ball of yarn\" \ud83d\ude97 Vehicle Movement \"The car in the image is driving down a winding mountain road\" \ud83d\udc83 Artistic Performance \"The dancer in the picture is performing elegant ballet movements\" \u274c Negative Prompt Examples \ud83d\udeab Static Issues \"static, motionless, frozen, distorted, blurry\" \u26a0\ufe0f Motion Problems \"unnatural movement, jerky motion, inconsistent\" \ud83d\udcc9 Quality Issues \"low quality, artifacts, noise, compression\" \ud83c\udfc6 Best Practices \ud83d\udcf8 Input Image Selection \ud83d\udd0d Clarity Priority Choose high-definition images, avoid blurry or noisy pictures \ud83c\udfaf Clear Subject Ensure main objects are clearly visible and occupy appropriate proportion \ud83d\uddbc\ufe0f Reasonable Composition Avoid overly complex backgrounds, keep composition clean \ud83d\udca1 Good Lighting Images with even lighting work better, avoid too dark or overexposed \u270d\ufe0f Prompt Writing Guidelines \ud83d\udcdd Specific Description Describe desired actions and scenes in detail, the more specific the better \ud83d\udd04 Maintain Consistency Ensure descriptions match image content, avoid contradictions \u2696\ufe0f Reasonable Actions Describe actions that follow physical laws, avoid impossible movements \ud83c\udfa8 Unified Style Maintain descriptions consistent with original image style \ud83d\udd27 Parameter Tuning Guide \ud83c\udfa8 Denoising Strength 0.6-0.7: Preserve more original image features 0.8-0.9: Allow more changes and dynamic effects \ud83c\udfaf CFG Value 6-7: Balanced guidance strength 8-10: Stronger text guidance \u23f1\ufe0f Steps 20-25: Fast generation 25-30: Higher quality \u26a0\ufe0f Important Considerations \ud83d\udcbe Memory Management Image-to-video requires more VRAM than text-to-video \ud83d\udd27 Image Preprocessing Ensure input image size is appropriate \ud83c\udfaf Consistency Preservation Denoising strength should not be too high \ud83c\udfad Action Reasonableness Described actions should match object characteristics \u26a1 Batch Processing Recommend processing one task at a time \ud83c\udfaf Application Scenarios \ud83d\udc64 Character Animation Bring static character photos to life with dynamic movements \ud83d\udecd\ufe0f Product Showcase Add dynamic effects to product images for enhanced marketing \ud83c\udfa8 Artistic Creation Convert paintings into dynamic videos for enhanced artistic expression \ud83d\udcda Educational Demo Make teaching images have dynamic effects for improved learning experience \ud83d\udcda Resources & Documentation \ud83d\udcd6 ComfyUI Official Documentation Detailed ComfyUI usage instructions and node descriptions Visit Documentation \u2192 \ud83c\udfa5 WanVideo Plugin Documentation Detailed usage instructions for WanVideo plugin View GitHub \u2192 \ud83c\udf89 Start Your Creative Journey! Transform your static images into captivating videos with Wan2.1-I2V-14B! Unleash the power of AI-driven video generation","title":"Index en"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#model-introduction","text":"Wan2.1-I2V-14B is a powerful image-to-video generation model that can generate high-quality video content based on input images and text prompts. The model maintains the main characteristics of the input image while adding dynamic effects and scene changes according to text descriptions.","title":"\ud83d\ude80 Model Introduction"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#core-features","text":"\ud83e\udde0","title":"\u2728 Core Features"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#technical-specifications","text":"Specification Value \ud83e\udd16 Model Type Image-to-Video Generation \u26a1 Quantization FP8 quantized version \ud83d\udcfa Supported Resolution 480p \ud83c\udf9e\ufe0f Maximum Frames 81 frames \ud83c\udfac Recommended Frame Rate 16fps \ud83d\udcc1 Input Format JPEG, PNG, WebP, etc.","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#usage-instructions","text":"","title":"\ud83d\udcd6 Usage Instructions"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#web-ui-usage","text":"#### Step 1: Access Interface Click the access link at the service instance Click access link to enter ComfyUI interface #### Step 2: Select Workflow Select wanx-21.json workflow and open it, choose the image-to-video function option #### Step 3: Upload Image Select sample image in LoadImage node Or upload custom image from local computer #### Step 4: Set Text Description Fill in description words in TextEncode node:","title":"\ud83c\udf10 Web UI Usage"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#api-integration","text":"","title":"\ud83d\udd0c API Integration"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#authentication-setup","text":"","title":"\ud83d\udd11 Authentication Setup"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#python-implementation","text":"\ud83d\udc0d Click to Expand Complete Python Code import requests, json, uuid, time, random, os # Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-I2V-14B-480P_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" CLIP_MODEL = \"wan2.1/open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors\" # Preset parameters IMAGE_PATH = \"girl.png\" PROMPT = \"A beautiful anime girl with long flowing hair, graceful movements, smooth animation, cinematic lighting, high quality\" NEG_PROMPT = \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"Upload image to ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"Image file does not exist: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"Upload failed, status code: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"No filename in upload response: {result}\") return result['name'] except Exception as e: raise Exception(f\"Image upload failed: {e}\") def generate_i2v(self, image_path, prompt, neg_prompt, steps=10, cfg=6, width=512, height=512, frames=81): \"\"\"Image-to-Video - Fixed clip_vision input\"\"\" print(\"\ud83d\udce4 Uploading image...\") image_name = self.upload_image(image_path) print(f\"\u2705 Image uploaded successfully: {image_name}\") workflow = { \"42\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"43\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"44\": {\"inputs\": {\"model_name\": CLIP_MODEL, \"precision\": \"fp16\", \"load_device\": \"offload_device\"}, \"class_type\": \"LoadWanVideoClipTextEncoder\"}, \"45\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"46\": {\"inputs\": {\"blocks_to_swap\": 10, \"offload_img_emb\": False, \"offload_txt_emb\": False, \"use_non_blocking\": True, \"vace_blocks_to_swap\": 0}, \"class_type\": \"WanVideoBlockSwap\"}, \"47\": {\"inputs\": {\"backend\": \"inductor\", \"fullgraph\": False, \"mode\": \"default\", \"dynamic\": False, \"dynamo_cache_size_limit\": 64, \"compile_transformer_blocks_only\": True, \"dynamo_recompile_limit\": 128}, \"class_type\": \"WanVideoTorchCompileSettings\"}, \"48\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\", \"block_swap_args\": [\"46\", 0]}, \"class_type\": \"WanVideoModelLoader\"}, \"49\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"45\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"50\": { \"inputs\": { \"generation_width\": width, \"generation_height\": height, \"num_frames\": frames, \"force_offload\": True, \"noise_aug_strength\": 0, \"latent_strength\": 1, \"clip_embed_strength\": 1, \"adjust_resolution\": True, \"image\": [\"42\", 0], \"vae\": [\"43\", 0], \"clip_vision\": [\"44\", 0] }, \"class_type\": \"WanVideoImageClipEncode\" }, \"52\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"48\", 0], \"text_embeds\": [\"49\", 0], \"image_embeds\": [\"50\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"53\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"43\", 0], \"samples\": [\"52\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"54\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"WanVideo2_1\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"53\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"i2v_output.mp4\"): try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): client = ComfyUIClient() try: print(f\"\ud83c\udfac Starting image-to-video task...\") print(f\"\ud83d\udcf7 Input image: {IMAGE_PATH}\") print(f\"\ud83d\udcdd Prompt: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c Image file does not exist: {IMAGE_PATH}\") print(\"Please ensure there is a girl.png file in the current directory\") return task_id = client.generate_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 10, 6, 512, 512, 81) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python Implementation"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#comfyui-api-endpoints","text":"Endpoint Method Function Description /queue GET Get queue status View current task queue /prompt POST Submit workflow Execute generation task /history/{prompt_id} GET Get execution history View task execution results /upload/image POST Upload image Upload input image file /view GET Download output file Get generated result files","title":"\ud83d\udd17 ComfyUI API Endpoints"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#parameter-configuration","text":"","title":"\u2699\ufe0f Parameter Configuration"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#generation-parameters","text":"","title":"\ud83c\udf9b\ufe0f Generation Parameters"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#image-requirements","text":"> **\ud83d\udca1 Pro Tip:** High-quality input images are the key to excellent video generation! **\ud83d\udcd0 Resolution**: Recommended 512\u00d7512 or higher **\ud83d\udcc4 Format**: JPEG, PNG, WebP, etc. **\ud83c\udfaf Content**: Clear main subject, avoid overly complex backgrounds **\u2728 Quality**: High-quality images yield better video effects","title":"\ud83d\uddbc\ufe0f Image Requirements"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#prompt-engineering","text":"","title":"\ud83d\udca1 Prompt Engineering"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#positive-prompt-examples","text":"","title":"\u2705 Positive Prompt Examples"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#negative-prompt-examples","text":"","title":"\u274c Negative Prompt Examples"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#best-practices","text":"","title":"\ud83c\udfc6 Best Practices"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#input-image-selection","text":"","title":"\ud83d\udcf8 Input Image Selection"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#prompt-writing-guidelines","text":"","title":"\u270d\ufe0f Prompt Writing Guidelines"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#parameter-tuning-guide","text":"","title":"\ud83d\udd27 Parameter Tuning Guide"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#important-considerations","text":"","title":"\u26a0\ufe0f Important Considerations"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#application-scenarios","text":"\ud83d\udc64","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.1/I2V-14B-480/doc/index-en/#resources-documentation","text":"","title":"\ud83d\udcda Resources &amp; Documentation"},{"location":"Wan2.1/T2V-1.3B/doc/","text":"\ud83c\udfac Wan2.1-T2V-1.3B \u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5b8c\u6574\u6307\u5357 \u8f7b\u91cf\u7ea7\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6280\u672f\uff0cT2V (Text-to-Video) \u4e13\u4e1a\u7ea7\u89c6\u9891\u751f\u6210 \u26a1 1.3B\u53c2\u6570 \ud83c\udfaf \u8f7b\u91cf\u9ad8\u6548 \ud83d\ude80 \u5feb\u901f\u751f\u6210 \ud83c\udf1f \u6a21\u578b\u7b80\u4ecb **Wan2.1-T2V-1.3B** \u662f WanVideo 2.1 \u7cfb\u5217\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u7248\u672c\uff0c\u4e13\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u8bbe\u8ba1\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u826f\u597d\u89c6\u9891\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u786c\u4ef6\u8981\u6c42\uff0c\u8ba9\u66f4\u591a\u7528\u6237\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u8bbe\u5907\u4e0a\u4f53\u9a8c\u6587\u672c\u5230\u89c6\u9891\u6280\u672f\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \u26a1 \u8f7b\u91cf\u53c2\u6570\u89c4\u6a21 1.3B \u53c2\u6570\uff0c\u8f7b\u91cf\u9ad8\u6548\uff0c\u9002\u5408\u6d88\u8d39\u7ea7\u8bbe\u5907 \ud83c\udfa5 \u6587\u672c\u5230\u89c6\u9891\u4e13\u7528 \u4e13\u95e8\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4f18\u5316 \u23f0 \u65f6\u5e8f\u8fde\u8d2f\u6027 \u4fdd\u6301\u89c6\u9891\u5e27\u95f4\u7684\u6d41\u7545\u6027\u548c\u4e00\u81f4\u6027 \ud83d\udee0\ufe0f \u591a\u6837\u5316\u751f\u6210 \u652f\u6301\u5404\u79cd\u98ce\u683c\u548c\u4e3b\u9898\u7684\u89c6\u9891\u751f\u6210 \ud83d\ude80 \u5feb\u901f\u751f\u6210 \u8f7b\u91cf\u5316\u67b6\u6784\uff0c\u751f\u6210\u901f\u5ea6\u5feb \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u6587\u548c\u82f1\u6587\u6587\u672c\u63d0\u793a \ud83d\udcca \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff08Text-to-Video\uff09 \u53c2\u6570\u89c4\u6a21 1.3B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u90e8\u7f72\u67b6\u6784 ECS\u5355\u673a\u90e8\u7f72/ACS\u96c6\u7fa4\u90e8\u7f72 \u6700\u5927\u5e27\u6570 81 \u5e27 \u63a8\u8350\u5e27\u7387 16fps \u63a8\u8350\u6b65\u6570 15-25 \u6b65 \u8f93\u51fa\u683c\u5f0f MP4\uff08H.264\u7f16\u7801\uff09 \ud83c\udfa8 \u751f\u6210\u80fd\u529b \ud83c\udfad \u98ce\u683c\u591a\u6837 \u652f\u6301\u52a8\u6f2b\u3001\u5199\u5b9e\u3001\u827a\u672f\u7b49\u591a\u79cd\u98ce\u683c \ud83c\udf08 \u573a\u666f\u4e30\u5bcc \u5ba4\u5185\u5916\u3001\u81ea\u7136\u3001\u57ce\u5e02\u7b49\u5404\u79cd\u573a\u666f \ud83c\udfde\ufe0f \u52a8\u4f5c\u6d41\u7545 \u4eba\u7269\u52a8\u4f5c\u3001\u7269\u4f53\u8fd0\u52a8\u81ea\u7136\u6d41\u7545 \ud83c\udfaf \u7ec6\u8282\u7cbe\u51c6 \u6839\u636e\u6587\u672c\u63cf\u8ff0\u7cbe\u786e\u751f\u6210\u7ec6\u8282 \u2728 \u521b\u610f\u65e0\u9650 \u652f\u6301\u521b\u610f\u548c\u60f3\u8c61\u529b\u4e30\u5bcc\u7684\u5185\u5bb9 \ud83d\udcc8 \u8d28\u91cf\u7a33\u5b9a \u4fdd\u6301\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u8f93\u51fa \ud83d\ude80 \u90e8\u7f72\u65b9\u5f0f \ud83c\udfd7\ufe0f \u90e8\u7f72\u67b6\u6784\uff08\u63a8\u8350\uff09 \ud83d\udca1 \u90e8\u7f72\u914d\u7f6e \u90e8\u7f72\u67b6\u6784 : ECS\u5355\u673a\u90e8\u7f72\u6216ACS\u96c6\u7fa4\u90e8\u7f72 \u663e\u5b58\u9700\u6c42 : 6GB+ \u663e\u5b58\uff0c\u9002\u5408\u6d88\u8d39\u7ea7\u8bbe\u5907 \u6269\u5c55\u6027 : \u652f\u6301\u5355\u673a\u548c\u96c6\u7fa4\u90e8\u7f72 \ud83c\udfaf \u4f7f\u7528\u6307\u5357 \ud83c\udf10 ComfyUI \u4f7f\u7528 ### \ud83d\udd27 \u64cd\u4f5c\u6b65\u9aa4 **1. \u8bbf\u95ee\u754c\u9762** - \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **2. \u9009\u62e9\u5de5\u4f5c\u6d41** - \u9009\u62e9\u9002\u5408\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5de5\u4f5c\u6d41 - \u786e\u8ba4\u5df2\u9009\u62e9 Wan2.1-T2V-1.3B \u6a21\u578b **3. \u8bbe\u7f6e\u53c2\u6570** - \u8f93\u5165\u6587\u672c\u63cf\u8ff0\uff08\u652f\u6301\u4e2d\u82f1\u6587\uff09 - \u8bbe\u7f6e\u89c6\u9891\u5206\u8fa8\u7387\u548c\u5e27\u6570 - \u8c03\u6574\u751f\u6210\u53c2\u6570\uff08\u6b65\u6570\u3001CFG \u7b49\uff09 **4. \u5f00\u59cb\u751f\u6210** - \u70b9\u51fb\u751f\u6210\u6309\u94ae\u5f00\u59cb\u5904\u7406 - \u7b49\u5f85\u751f\u6210\u5b8c\u6210\u5e76\u4e0b\u8f7d\u7ed3\u679c \ud83d\udd0c API \u8c03\u7528 \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83d\udd10 \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 COMFYUI_SERVER \u7684\u83b7\u53d6\u53c2\u8003 \ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b \ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests, json, uuid, time, random # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"wan2.1/Wan2_1-T2V-1_3B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\"\ud83c\udfac \u6587\u672c\u5230\u89c6\u9891\u751f\u6210\"\"\" print(f\"\ud83c\udfac \u5f00\u59cb\u6587\u672c\u5230\u89c6\u9891\u751f\u6210...\") print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt}\") workflow = { \"1\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"2\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"3\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\"}, \"class_type\": \"WanVideoModelLoader\"}, \"4\": {\"inputs\": {\"width\": width, \"height\": height, \"num_frames\": frames}, \"class_type\": \"WanVideoEmptyEmbeds\"}, \"5\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"6\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"7\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"8\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API \u54cd\u5e94: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u5de5\u4f5c\u6d41\u9519\u8bef: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u54cd\u5e94\u4e2d\u6ca1\u6709 prompt_id: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_generated.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"\u4e0b\u8f7d\u9519\u8bef: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfac \u5f00\u59cb\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1...\") # \u793a\u4f8b\u63d0\u793a\u8bcd prompt = \"\u4e00\u4e2a\u7f8e\u4e3d\u7684\u52a8\u6f2b\u5973\u5b69\uff0c\u957f\u957f\u7684\u9ed1\u53d1\uff0c\u4f18\u96c5\u5730\u8df3\u821e\uff0c\u6a31\u82b1\u98de\u821e\u7684\u80cc\u666f\uff0c\u67d4\u548c\u7684\u5149\u7ebf\uff0c\u9ad8\u8d28\u91cf\u52a8\u753b\" neg_prompt = \"\u4f4e\u8d28\u91cf\uff0c\u6a21\u7cca\uff0c\u626d\u66f2\uff0c\u9759\u6001\uff0c\u4e0d\u81ea\u7136\u7684\u52a8\u4f5c\" task_id = client.generate_t2v(prompt, neg_prompt, 15, 6, 832, 480, 81) print(f\"\ud83c\udd94 \u4efb\u52a1ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca \u5f53\u524d\u72b6\u6001: {status}\") if status == \"completed\": print(\"\u2705 \u89c6\u9891\u51c6\u5907\u5c31\u7eea!\"); break elif status == \"failed\": print(\"\u274c \u751f\u6210\u5931\u8d25!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"t2v_generated.mp4\") print(\"\ud83c\udf89 \u89c6\u9891\u4e0b\u8f7d\u6210\u529f!\" if output_file else \"\u274c \u89c6\u9891\u4e0b\u8f7d\u5931\u8d25\") if output_file: print(f\"\ud83d\udcc1 \u4fdd\u5b58\u4e3a: {output_file}\") except Exception as e: print(f\"\u274c \u9519\u8bef: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 \u6700\u4f73\u5b9e\u8df5 \ud83c\udfaf \u63d0\u793a\u8bcd\u4f18\u5316 \u4f7f\u7528\u6e05\u6670\u5177\u4f53\u7684\u63cf\u8ff0 \u5305\u542b\u52a8\u4f5c\u548c\u573a\u666f\u5173\u952e\u8bcd \u6307\u5b9a\u98ce\u683c\u548c\u6c1b\u56f4 \u907f\u514d\u8fc7\u4e8e\u590d\u6742\u7684\u63cf\u8ff0 \u2699\ufe0f \u53c2\u6570\u8c03\u4f18 \u4ece\u9ed8\u8ba4\u8bbe\u7f6e\u5f00\u59cb\uff0815\u6b65\uff0cCFG 6\uff09 \u6839\u636e\u5185\u5bb9\u590d\u6742\u5ea6\u8c03\u6574\u5e27\u6570 \u76d1\u63a7\u663e\u5b58\u4f7f\u7528\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd \u4f7f\u7528\u63a8\u8350\u5206\u8fa8\u7387 832x480 \ud83c\udfac \u89c6\u9891\u8d28\u91cf \u4f7f\u7528\u5177\u4f53\u7684\u52a8\u4f5c\u63cf\u8ff0 \u786e\u4fdd\u63d0\u793a\u8bcd\u903b\u8f91\u6e05\u6670 \u5148\u7528\u77ed\u7247\u6bb5\u6d4b\u8bd5\u6548\u679c \u6ce8\u610f\u65f6\u5e8f\u8fde\u8d2f\u6027 \ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848 \ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a \u63d0\u793a\u8bcd\u4e0d\u591f\u8be6\u7ec6 \uff1a\u589e\u52a0\u66f4\u5177\u4f53\u7684\u63cf\u8ff0 \u53c2\u6570\u8bbe\u7f6e\u4e0d\u5f53 \uff1a\u8c03\u6574\u6b65\u6570\u548cCFG\u503c \u5185\u5bb9\u8fc7\u4e8e\u590d\u6742 \uff1a\u7b80\u5316\u63d0\u793a\u8bcd\u63cf\u8ff0 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 \uff1a\u964d\u4f4e\u5206\u8fa8\u7387\u6216\u51cf\u5c11\u5e27\u6570 \u751f\u6210\u7f13\u6162 \uff1a\u51cf\u5c11\u6b65\u6570\u6216\u4f7f\u7528\u66f4\u5feb\u7684\u91c7\u6837\u5668 \u6a21\u578b\u52a0\u8f7d\u6162 \uff1a\u786e\u4fdd\u7f51\u7edc\u8fde\u63a5\u7a33\u5b9a \ud83d\udd27 \u6280\u672f\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a ComfyUI\u7248\u672c \uff1a\u786e\u4fdd\u4f7f\u7528\u652f\u6301T2V\u7684\u6700\u65b0\u7248\u672c \u6a21\u578b\u8def\u5f84 \uff1a\u68c0\u67e5\u6a21\u578b\u6587\u4ef6\u8def\u5f84\u662f\u5426\u6b63\u786e \u53c2\u6570\u8303\u56f4 \uff1a\u6ce8\u610fT2V\u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4 \ud83d\udcda \u76f8\u5173\u8d44\u6e90 \ud83d\udcc4 \u6280\u672f\u6587\u6863 WanVideo 2.1 \u5b98\u65b9\u6587\u6863 \u2192 \u4e86\u89e3 WanVideo 2.1 \u7cfb\u5217\u6a21\u578b\u7684\u6280\u672f\u7ec6\u8282 \ud83d\udee0\ufe0f \u5de5\u5177\u4e0e\u6307\u5357 \u6a21\u578b\u4e0b\u8f7d\u5730\u5740 \u2192 Hugging Face \u6a21\u578b\u9875\u9762\u548c\u4e0b\u8f7d\u94fe\u63a5 \ud83c\udfa5 \u89c6\u9891\u751f\u6210\u6307\u5357 \u89c6\u9891\u751f\u6210\u6700\u4f73\u5b9e\u8df5 \u2192 ComfyUI \u89c6\u9891\u751f\u6210\u7684\u8be6\u7ec6\u6307\u5357\u548c\u6280\u5de7 \ud83d\udd27 \u63d2\u4ef6\u6587\u6863 ComfyUI \u63d2\u4ef6\u6587\u6863 \u2192 WanVideo ComfyUI \u63d2\u4ef6\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u8bf4\u660e \ud83c\udfac \u5f00\u59cb\u4f7f\u7528 T2V \u6280\u672f\u521b\u4f5c\u7cbe\u5f69\u89c6\u9891\u5427\uff01 | \u8f7b\u91cf\u7ea7\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u8ba9\u521b\u610f\u65e0\u9650\u5ef6\u4f38","title":"Index"},{"location":"Wan2.1/T2V-1.3B/doc/#_1","text":"**Wan2.1-T2V-1.3B** \u662f WanVideo 2.1 \u7cfb\u5217\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u7248\u672c\uff0c\u4e13\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u8bbe\u8ba1\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u826f\u597d\u89c6\u9891\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u786c\u4ef6\u8981\u6c42\uff0c\u8ba9\u66f4\u591a\u7528\u6237\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u8bbe\u5907\u4e0a\u4f53\u9a8c\u6587\u672c\u5230\u89c6\u9891\u6280\u672f\u3002","title":"\ud83c\udf1f \u6a21\u578b\u7b80\u4ecb"},{"location":"Wan2.1/T2V-1.3B/doc/#_2","text":"","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"Wan2.1/T2V-1.3B/doc/#_3","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff08Text-to-Video\uff09 \u53c2\u6570\u89c4\u6a21 1.3B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u90e8\u7f72\u67b6\u6784 ECS\u5355\u673a\u90e8\u7f72/ACS\u96c6\u7fa4\u90e8\u7f72 \u6700\u5927\u5e27\u6570 81 \u5e27 \u63a8\u8350\u5e27\u7387 16fps \u63a8\u8350\u6b65\u6570 15-25 \u6b65 \u8f93\u51fa\u683c\u5f0f MP4\uff08H.264\u7f16\u7801\uff09","title":"\ud83d\udcca \u6280\u672f\u89c4\u683c"},{"location":"Wan2.1/T2V-1.3B/doc/#_4","text":"\ud83c\udfad \u98ce\u683c\u591a\u6837 \u652f\u6301\u52a8\u6f2b\u3001\u5199\u5b9e\u3001\u827a\u672f\u7b49\u591a\u79cd\u98ce\u683c \ud83c\udf08 \u573a\u666f\u4e30\u5bcc \u5ba4\u5185\u5916\u3001\u81ea\u7136\u3001\u57ce\u5e02\u7b49\u5404\u79cd\u573a\u666f \ud83c\udfde\ufe0f \u52a8\u4f5c\u6d41\u7545 \u4eba\u7269\u52a8\u4f5c\u3001\u7269\u4f53\u8fd0\u52a8\u81ea\u7136\u6d41\u7545 \ud83c\udfaf \u7ec6\u8282\u7cbe\u51c6 \u6839\u636e\u6587\u672c\u63cf\u8ff0\u7cbe\u786e\u751f\u6210\u7ec6\u8282 \u2728 \u521b\u610f\u65e0\u9650 \u652f\u6301\u521b\u610f\u548c\u60f3\u8c61\u529b\u4e30\u5bcc\u7684\u5185\u5bb9 \ud83d\udcc8 \u8d28\u91cf\u7a33\u5b9a \u4fdd\u6301\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u8f93\u51fa","title":"\ud83c\udfa8 \u751f\u6210\u80fd\u529b"},{"location":"Wan2.1/T2V-1.3B/doc/#_5","text":"","title":"\ud83d\ude80 \u90e8\u7f72\u65b9\u5f0f"},{"location":"Wan2.1/T2V-1.3B/doc/#_6","text":"\ud83d\udca1 \u90e8\u7f72\u914d\u7f6e \u90e8\u7f72\u67b6\u6784 : ECS\u5355\u673a\u90e8\u7f72\u6216ACS\u96c6\u7fa4\u90e8\u7f72 \u663e\u5b58\u9700\u6c42 : 6GB+ \u663e\u5b58\uff0c\u9002\u5408\u6d88\u8d39\u7ea7\u8bbe\u5907 \u6269\u5c55\u6027 : \u652f\u6301\u5355\u673a\u548c\u96c6\u7fa4\u90e8\u7f72","title":"\ud83c\udfd7\ufe0f \u90e8\u7f72\u67b6\u6784\uff08\u63a8\u8350\uff09"},{"location":"Wan2.1/T2V-1.3B/doc/#_7","text":"","title":"\ud83c\udfaf \u4f7f\u7528\u6307\u5357"},{"location":"Wan2.1/T2V-1.3B/doc/#comfyui","text":"### \ud83d\udd27 \u64cd\u4f5c\u6b65\u9aa4 **1. \u8bbf\u95ee\u754c\u9762** - \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **2. \u9009\u62e9\u5de5\u4f5c\u6d41** - \u9009\u62e9\u9002\u5408\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5de5\u4f5c\u6d41 - \u786e\u8ba4\u5df2\u9009\u62e9 Wan2.1-T2V-1.3B \u6a21\u578b **3. \u8bbe\u7f6e\u53c2\u6570** - \u8f93\u5165\u6587\u672c\u63cf\u8ff0\uff08\u652f\u6301\u4e2d\u82f1\u6587\uff09 - \u8bbe\u7f6e\u89c6\u9891\u5206\u8fa8\u7387\u548c\u5e27\u6570 - \u8c03\u6574\u751f\u6210\u53c2\u6570\uff08\u6b65\u6570\u3001CFG \u7b49\uff09 **4. \u5f00\u59cb\u751f\u6210** - \u70b9\u51fb\u751f\u6210\u6309\u94ae\u5f00\u59cb\u5904\u7406 - \u7b49\u5f85\u751f\u6210\u5b8c\u6210\u5e76\u4e0b\u8f7d\u7ed3\u679c","title":"\ud83c\udf10 ComfyUI \u4f7f\u7528"},{"location":"Wan2.1/T2V-1.3B/doc/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528"},{"location":"Wan2.1/T2V-1.3B/doc/#_8","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"Wan2.1/T2V-1.3B/doc/#python","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests, json, uuid, time, random # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"wan2.1/Wan2_1-T2V-1_3B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\"\ud83c\udfac \u6587\u672c\u5230\u89c6\u9891\u751f\u6210\"\"\" print(f\"\ud83c\udfac \u5f00\u59cb\u6587\u672c\u5230\u89c6\u9891\u751f\u6210...\") print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt}\") workflow = { \"1\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"2\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"3\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\"}, \"class_type\": \"WanVideoModelLoader\"}, \"4\": {\"inputs\": {\"width\": width, \"height\": height, \"num_frames\": frames}, \"class_type\": \"WanVideoEmptyEmbeds\"}, \"5\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"6\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"7\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"8\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API \u54cd\u5e94: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u5de5\u4f5c\u6d41\u9519\u8bef: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u54cd\u5e94\u4e2d\u6ca1\u6709 prompt_id: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_generated.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"\u4e0b\u8f7d\u9519\u8bef: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfac \u5f00\u59cb\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1...\") # \u793a\u4f8b\u63d0\u793a\u8bcd prompt = \"\u4e00\u4e2a\u7f8e\u4e3d\u7684\u52a8\u6f2b\u5973\u5b69\uff0c\u957f\u957f\u7684\u9ed1\u53d1\uff0c\u4f18\u96c5\u5730\u8df3\u821e\uff0c\u6a31\u82b1\u98de\u821e\u7684\u80cc\u666f\uff0c\u67d4\u548c\u7684\u5149\u7ebf\uff0c\u9ad8\u8d28\u91cf\u52a8\u753b\" neg_prompt = \"\u4f4e\u8d28\u91cf\uff0c\u6a21\u7cca\uff0c\u626d\u66f2\uff0c\u9759\u6001\uff0c\u4e0d\u81ea\u7136\u7684\u52a8\u4f5c\" task_id = client.generate_t2v(prompt, neg_prompt, 15, 6, 832, 480, 81) print(f\"\ud83c\udd94 \u4efb\u52a1ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca \u5f53\u524d\u72b6\u6001: {status}\") if status == \"completed\": print(\"\u2705 \u89c6\u9891\u51c6\u5907\u5c31\u7eea!\"); break elif status == \"failed\": print(\"\u274c \u751f\u6210\u5931\u8d25!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"t2v_generated.mp4\") print(\"\ud83c\udf89 \u89c6\u9891\u4e0b\u8f7d\u6210\u529f!\" if output_file else \"\u274c \u89c6\u9891\u4e0b\u8f7d\u5931\u8d25\") if output_file: print(f\"\ud83d\udcc1 \u4fdd\u5b58\u4e3a: {output_file}\") except Exception as e: print(f\"\u274c \u9519\u8bef: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b"},{"location":"Wan2.1/T2V-1.3B/doc/#_9","text":"","title":"\ud83d\udca1 \u6700\u4f73\u5b9e\u8df5"},{"location":"Wan2.1/T2V-1.3B/doc/#_10","text":"\ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a \u63d0\u793a\u8bcd\u4e0d\u591f\u8be6\u7ec6 \uff1a\u589e\u52a0\u66f4\u5177\u4f53\u7684\u63cf\u8ff0 \u53c2\u6570\u8bbe\u7f6e\u4e0d\u5f53 \uff1a\u8c03\u6574\u6b65\u6570\u548cCFG\u503c \u5185\u5bb9\u8fc7\u4e8e\u590d\u6742 \uff1a\u7b80\u5316\u63d0\u793a\u8bcd\u63cf\u8ff0 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 \uff1a\u964d\u4f4e\u5206\u8fa8\u7387\u6216\u51cf\u5c11\u5e27\u6570 \u751f\u6210\u7f13\u6162 \uff1a\u51cf\u5c11\u6b65\u6570\u6216\u4f7f\u7528\u66f4\u5feb\u7684\u91c7\u6837\u5668 \u6a21\u578b\u52a0\u8f7d\u6162 \uff1a\u786e\u4fdd\u7f51\u7edc\u8fde\u63a5\u7a33\u5b9a \ud83d\udd27 \u6280\u672f\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a ComfyUI\u7248\u672c \uff1a\u786e\u4fdd\u4f7f\u7528\u652f\u6301T2V\u7684\u6700\u65b0\u7248\u672c \u6a21\u578b\u8def\u5f84 \uff1a\u68c0\u67e5\u6a21\u578b\u6587\u4ef6\u8def\u5f84\u662f\u5426\u6b63\u786e \u53c2\u6570\u8303\u56f4 \uff1a\u6ce8\u610fT2V\u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4","title":"\ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848"},{"location":"Wan2.1/T2V-1.3B/doc/#_11","text":"","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/","text":"\ud83c\udfac Wan2.1-T2V-1.3B: Revolutionary Text-to-Video AI The lightweight powerhouse of WanVideo 2.1 series \ud83e\udeb6 1.3B Parameters \u26a1 Lightning Fast \ud83d\udcbe Low VRAM **Wan2.1-T2V-1.3B** brings professional text-to-video generation to consumer devices. Experience the magic of AI video creation with minimal hardware requirements while maintaining exceptional quality. This lightweight model delivers 90% of the performance with just 10% of the computational cost. \u2728 Core Features That Set Us Apart \ud83e\udeb6 Lightweight Design Only 1.3B parameters - 90% smaller than the 14B version while delivering remarkable results \u26a1 Lightning Fast Optimized inference speed perfect for real-time generation and high-volume batch processing \ud83d\udcbe Low VRAM Friendly Runs smoothly with just 6GB VRAM - compatible with most modern consumer GPUs \ud83c\udf10 Global Language Support Native support for Chinese and English prompts with excellent understanding \ud83d\udd27 Flexible Deployment Deploy anywhere - from single machines to enterprise clusters with ease \ud83d\udd13 Open Source Power Fully open source with commercial licensing - customize and extend as needed \ud83d\udcca Technical Specifications Specification Details Model Type Text-to-Video Generation Parameters 1.3B Quantization FP8 optimized Maximum Frames 81 frames Recommended Frame Rate 16fps Optimal Steps 15-25 VRAM Requirements 6GB+ Output Format MP4 (H.264) \u2696\ufe0f Performance Showdown: 1.3B vs 14B Comparison T2V-1.3B (Lightweight) T2V-14B (Powerhouse) Parameters 1.3B \u26a1 14B \ud83d\udd25 VRAM Need 6GB+ \ud83d\udc9a 12GB+ \u26a0\ufe0f Speed Lightning Fast \u26a1 Moderate \ud83d\udc0c Quality Excellent \ud83d\udc4d Outstanding \ud83c\udf1f Detail Level Good \ud83d\udc4c Exceptional \ud83c\udfaf Text Understanding Smart \ud83e\udde0 Genius \ud83c\udf93 Best For Rapid Prototyping \ud83d\ude80 Premium Creation \ud83d\udc8e \ud83c\udfd7\ufe0f Deployment Architecture \ud83c\udf1f ACS Cluster (Recommended) Enterprise Configuration GPU: Professional Grade Memory: 96GB Scalable deployment \u26a1 ECS Deployment Standard Configuration GPU: A10 Series Memory: 30GB Single machine setup \ud83d\udcd6 Usage Guide \ud83c\udf9b\ufe0f ComfyUI: Your Creative Command Center ### \ud83d\ude80 Quick Start in 4 Simple Steps **1. \ud83c\udf10 Access Interface** - Click the access link at your service instance **2. \ud83d\udd27 Select Workflow** - Choose the video generation workflow template **3. \u2699\ufe0f Configure Parameters** - Set text prompts, resolution, frames, and generation settings **4. \ud83c\udfac Generate & Download** - Hit Generate and download your masterpiece \ud83d\udd0c API Integration Made Easy \ud83d\udd11 Authentication Setup \ud83c\udfab Get Your Token Click the top-right button to access the token panel \ud83c\udf10 Server Configuration Reference guide for COMFYUI_SERVER setup \ud83d\udcbb Python Code Example \ud83d\udc0d Click to Reveal the Magic Code \u2728 import requests, json, uuid, time, random # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"wan2.1/Wan2_1-T2V-1_3B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\"\ud83c\udfac Text-to-Video Generation\"\"\" print(f\"\ud83c\udfac Starting text-to-video magic...\") print(f\"\ud83d\udcdd Prompt: {prompt}\") workflow = { \"1\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"2\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"3\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\"}, \"class_type\": \"WanVideoModelLoader\"}, \"4\": {\"inputs\": {\"width\": width, \"height\": height, \"num_frames\": frames}, \"class_type\": \"WanVideoEmptyEmbeds\"}, \"5\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"6\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"7\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"8\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_generated.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main function - Execute text-to-video generation\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfac Starting text-to-video magic...\") # Example prompts prompt = \"A beautiful anime girl with long black hair dancing gracefully, cherry blossoms falling in the background, soft lighting, high quality animation\" neg_prompt = \"low quality, blurry, distorted, static, unnatural movement\" task_id = client.generate_t2v(prompt, neg_prompt, 15, 6, 832, 480, 81) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"t2v_generated.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 Best Practices & Pro Tips \ud83d\udcdd Prompt Engineering Mastery \u2728 Keep It Simple The lightweight model excels with concise, focused prompts. Less is often more! \ud83c\udfaf Highlight Key Elements Use powerful keywords to describe the most important aspects of your vision \ud83d\udeab Avoid Complexity Skip overly complex scene descriptions - focus on the core message \ud83c\udfa8 Define Style Clearly specify the visual style and atmosphere for consistent results \ud83d\udd27 Troubleshooting Hub \u26a0\ufe0f VRAM Shortage Solution: Reduce resolution or enable low-VRAM mode for optimal performance \ud83c\udfa8 Quality Issues Solution: Increase step count or fine-tune CFG parameters for better results \ud83d\udc0c Slow Performance Solution: Verify quantization settings and optimize offload configuration \u274c Loading Failures Solution: Check model file paths and verify proper permissions \ud83d\udcda Resource Library \ud83d\udcd6 Official Documentation WanVideo 2.1 Guide \u2192 Comprehensive guide to WanVideo 2.1 series models \u2b07\ufe0f Model Download Get Lightweight Model \u2192 Download the 1.3B parameter model from Hugging Face \ud83d\udd0c ComfyUI Plugin Install Extension \u2192 ComfyUI plugin for seamless integration \ud83c\udfa5 Best Practices Video Generation Guide \u2192 Learn advanced techniques for video generation \ud83c\udfac Ready to Create Magic? | Transform your ideas into stunning videos with Wan2.1-T2V-1.3B","title":"Index en"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#core-features-that-set-us-apart","text":"","title":"\u2728 Core Features That Set Us Apart"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#technical-specifications","text":"Specification Details Model Type Text-to-Video Generation Parameters 1.3B Quantization FP8 optimized Maximum Frames 81 frames Recommended Frame Rate 16fps Optimal Steps 15-25 VRAM Requirements 6GB+ Output Format MP4 (H.264)","title":"\ud83d\udcca Technical Specifications"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#performance-showdown-13b-vs-14b","text":"Comparison T2V-1.3B (Lightweight) T2V-14B (Powerhouse) Parameters 1.3B \u26a1 14B \ud83d\udd25 VRAM Need 6GB+ \ud83d\udc9a 12GB+ \u26a0\ufe0f Speed Lightning Fast \u26a1 Moderate \ud83d\udc0c Quality Excellent \ud83d\udc4d Outstanding \ud83c\udf1f Detail Level Good \ud83d\udc4c Exceptional \ud83c\udfaf Text Understanding Smart \ud83e\udde0 Genius \ud83c\udf93 Best For Rapid Prototyping \ud83d\ude80 Premium Creation \ud83d\udc8e","title":"\u2696\ufe0f Performance Showdown: 1.3B vs 14B"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#deployment-architecture","text":"","title":"\ud83c\udfd7\ufe0f Deployment Architecture"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#usage-guide","text":"","title":"\ud83d\udcd6 Usage Guide"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#comfyui-your-creative-command-center","text":"### \ud83d\ude80 Quick Start in 4 Simple Steps **1. \ud83c\udf10 Access Interface** - Click the access link at your service instance **2. \ud83d\udd27 Select Workflow** - Choose the video generation workflow template **3. \u2699\ufe0f Configure Parameters** - Set text prompts, resolution, frames, and generation settings **4. \ud83c\udfac Generate & Download** - Hit Generate and download your masterpiece","title":"\ud83c\udf9b\ufe0f ComfyUI: Your Creative Command Center"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#api-integration-made-easy","text":"","title":"\ud83d\udd0c API Integration Made Easy"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#authentication-setup","text":"","title":"\ud83d\udd11 Authentication Setup"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#python-code-example","text":"\ud83d\udc0d Click to Reveal the Magic Code \u2728 import requests, json, uuid, time, random # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"wan2.1/Wan2_1-T2V-1_3B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\"\ud83c\udfac Text-to-Video Generation\"\"\" print(f\"\ud83c\udfac Starting text-to-video magic...\") print(f\"\ud83d\udcdd Prompt: {prompt}\") workflow = { \"1\": {\"inputs\": {\"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\"}, \"class_type\": \"LoadWanVideoT5TextEncoder\"}, \"2\": {\"inputs\": {\"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0]}, \"class_type\": \"WanVideoTextEncode\"}, \"3\": {\"inputs\": {\"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\"}, \"class_type\": \"WanVideoModelLoader\"}, \"4\": {\"inputs\": {\"width\": width, \"height\": height, \"num_frames\": frames}, \"class_type\": \"WanVideoEmptyEmbeds\"}, \"5\": {\"inputs\": {\"model_name\": VAE_MODEL, \"precision\": \"bf16\"}, \"class_type\": \"WanVideoVAELoader\"}, \"6\": {\"inputs\": {\"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0]}, \"class_type\": \"WanVideoSampler\"}, \"7\": {\"inputs\": {\"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0]}, \"class_type\": \"WanVideoDecode\"}, \"8\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_generated.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main function - Execute text-to-video generation\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfac Starting text-to-video magic...\") # Example prompts prompt = \"A beautiful anime girl with long black hair dancing gracefully, cherry blossoms falling in the background, soft lighting, high quality animation\" neg_prompt = \"low quality, blurry, distorted, static, unnatural movement\" task_id = client.generate_t2v(prompt, neg_prompt, 15, 6, 832, 480, 81) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"t2v_generated.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python Code Example"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#best-practices-pro-tips","text":"","title":"\ud83d\udca1 Best Practices &amp; Pro Tips"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#prompt-engineering-mastery","text":"","title":"\ud83d\udcdd Prompt Engineering Mastery"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#troubleshooting-hub","text":"\u26a0\ufe0f VRAM Shortage Solution: Reduce resolution or enable low-VRAM mode for optimal performance \ud83c\udfa8 Quality Issues Solution: Increase step count or fine-tune CFG parameters for better results \ud83d\udc0c Slow Performance Solution: Verify quantization settings and optimize offload configuration \u274c Loading Failures Solution: Check model file paths and verify proper permissions","title":"\ud83d\udd27 Troubleshooting Hub"},{"location":"Wan2.1/T2V-1.3B/doc/index-en/#resource-library","text":"","title":"\ud83d\udcda Resource Library"},{"location":"Wan2.1/T2V-14B/doc/","text":"\ud83c\udfac Wan2.1-T2V-14B \u6587\u672c\u751f\u89c6\u9891\u6a21\u578b \u4ece\u6587\u5b57\u5230\u89c6\u9891\uff0c\u8ba9\u60f3\u8c61\u529b\u6210\u4e3a\u73b0\u5b9e\uff01\u5f3a\u5927\u7684AI\u89c6\u9891\u751f\u6210\u5f15\u64ce \ud83e\udde0 14B\u53c2\u6570 \ud83c\udfaf \u9ad8\u8d28\u91cf \u26a1 \u4e13\u4e1a\u7ea7 \ud83c\udf1f \u6a21\u578b\u7b80\u4ecb **Wan2.1-T2V-14B** \u662f\u4e00\u4e2a\u9769\u547d\u6027\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u5728\u7ed9\u5b9a\u6587\u672c\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4ee4\u4eba\u60ca\u53f9\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u5185\u5bb9\u3002\u65e0\u8bba\u662f\u521b\u610f\u8868\u8fbe\u8fd8\u662f\u5546\u4e1a\u5e94\u7528\uff0c\u90fd\u80fd\u4e3a\u60a8\u5e26\u6765\u65e0\u9650\u53ef\u80fd\uff01 \u2728 \u6838\u5fc3\u7279\u6027 \ud83e\udde0 \u5f3a\u5927\u53c2\u6570\u89c4\u6a21 14B\u53c2\u6570\u91cf\uff0c\u63d0\u4f9b\u5353\u8d8a\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b \ud83c\udfd7\ufe0f \u5148\u8fdb\u67b6\u6784 \u6269\u6563\u53d8\u6362\u5668 + VAE\uff0c\u6280\u672f\u9886\u5148 \u26a1 \u5185\u5b58\u4f18\u5316 FP8\u91cf\u5316\u6280\u672f\uff0c\u9ad8\u6548GPU\u5229\u7528 \ud83c\udf0d \u591a\u8bed\u8a00\u652f\u6301 \u4e2d\u82f1\u6587\u53cc\u8bed\uff0c\u65e0\u969c\u788d\u521b\u4f5c \ud83c\udfad \u590d\u6742\u573a\u666f\u7406\u89e3 \u6df1\u5ea6\u7406\u89e3\u6587\u672c\uff0c\u751f\u6210\u903c\u771f\u573a\u666f \ud83c\udfa5 \u4e13\u4e1a\u54c1\u8d28 \u7535\u5f71\u7ea7\u89c6\u9891\u8d28\u91cf\uff0c\u5546\u4e1a\u5e94\u7528\u5c31\u7eea \ud83d\udd27 \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u89c6\u9891\u751f\u6210 \u53c2\u6570\u89c4\u6a21 14B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u6700\u5927\u5e27\u6570 81\u5e27 \u63a8\u8350\u5e27\u7387 16fps \u63a8\u8350\u6b65\u6570 15-25\u6b65 \u663e\u5b58\u9700\u6c42 12GB+ \u8f93\u51fa\u683c\u5f0f MP4 (H.264) \ud83d\udcd6 \u4f7f\u7528\u6307\u5357 \ud83c\udf10 \u65b9\u5f0f\u4e00\uff1aComfyUI \u53ef\u89c6\u5316\u754c\u9762 ### \ud83d\ude80 \u5feb\u901f\u5f00\u59cb **\u6b65\u9aa4 1\uff1a\u8bbf\u95ee\u754c\u9762** \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **\u6b65\u9aa4 2\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41** \u6309\u56fe\u4e2d\u6307\u5f15\u9009\u62e9\u5de5\u4f5c\u6d41\u4fa7\u680f\uff0c\u9009\u62e9 `wanx-21.json` \u6216 `wans.json` \u5e76\u6253\u5f00 **\u6b65\u9aa4 3\uff1a\u9009\u62e9\u529f\u80fd** \u5728\u4e0b\u56fe\u5904\u9009\u62e9\u6587\u751f\u89c6\u9891\u529f\u80fd **\u6b65\u9aa4 4\uff1a\u7f16\u5199\u63d0\u793a\u8bcd** \u5728 TextEncode \u5904\u586b\u5199\u63cf\u8ff0\u8bcd \u2705 \u4e0a\u65b9\u8f93\u5165\u6846 \u4f60\u60f3\u8981\u751f\u6210\u7684\u5185\u5bb9 \u274c \u4e0b\u65b9\u8f93\u5165\u6846 \u4f60\u4e0d\u60f3\u8981\u751f\u6210\u7684\u5185\u5bb9 **\u6b65\u9aa4 5\uff1a\u8bbe\u7f6e\u53c2\u6570** \u5728 ImageClip Encode \u5904\u53ef\u8bbe\u7f6e\u56fe\u7247\u7684\u5206\u8fa8\u7387\u548c\u5e27\u6570 ### \ud83d\udcda \u66f4\u591a\u53c2\u6570\u8bf4\u660e - [ComfyUI\u5b98\u65b9\u6587\u6863](https://comfyui-wiki.com/zh/interface/node-options) - [WanVideo\u63d2\u4ef6\u8be6\u7ec6\u6587\u6863](https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/readme.md) \ud83d\udd0c \u65b9\u5f0f\u4e8c\uff1aAPI \u7a0b\u5e8f\u5316\u8c03\u7528 \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83c\udfab \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\uff0c\u83b7\u53d6token \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 COMFYUI_SERVER \u7684\u83b7\u53d6\u53ef\u53c2\u8003 \ud83d\udcbb Python \u4ee3\u7801\u5b9e\u73b0 \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u4ee3\u7801 import requests, json, uuid, time, random # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-T2V-14B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: \"\"\"\ud83c\udfac ComfyUI \u89c6\u9891\u751f\u6210\u5ba2\u6237\u7aef\"\"\" def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\" \ud83c\udfa5 \u751f\u6210\u89c6\u9891 Args: prompt (str): \u6b63\u5411\u63d0\u793a\u8bcd neg_prompt (str): \u8d1f\u5411\u63d0\u793a\u8bcd steps (int): \u63a8\u7406\u6b65\u6570 cfg (float): CFG\u5f15\u5bfc\u5f3a\u5ea6 width (int): \u89c6\u9891\u5bbd\u5ea6 height (int): \u89c6\u9891\u9ad8\u5ea6 frames (int): \u89c6\u9891\u5e27\u6570 Returns: str: \u4efb\u52a1ID \"\"\" print(f\"\ud83c\udfac \u5f00\u59cb\u6587\u672c\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt}\") workflow = { \"1\": { \"inputs\": { \"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\" }, \"class_type\": \"LoadWanVideoT5TextEncoder\" }, \"2\": { \"inputs\": { \"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0] }, \"class_type\": \"WanVideoTextEncode\" }, \"3\": { \"inputs\": { \"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\" }, \"class_type\": \"WanVideoModelLoader\" }, \"4\": { \"inputs\": { \"width\": width, \"height\": height, \"num_frames\": frames }, \"class_type\": \"WanVideoEmptyEmbeds\" }, \"5\": { \"inputs\": { \"model_name\": VAE_MODEL, \"precision\": \"bf16\" }, \"class_type\": \"WanVideoVAELoader\" }, \"6\": { \"inputs\": { \"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0] }, \"class_type\": \"WanVideoSampler\" }, \"7\": { \"inputs\": { \"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0] }, \"class_type\": \"WanVideoDecode\" }, \"8\": { \"inputs\": { \"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_14B_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0] }, \"class_type\": \"VHS_VideoCombine\" } } print(\"\ud83d\ude80 \u63d0\u4ea4\u89c6\u9891\u751f\u6210\u4efb\u52a1...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"\ud83d\udce1 API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u274c \u5de5\u4f5c\u6d41\u9519\u8bef: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u274c \u54cd\u5e94\u4e2d\u6ca1\u6709 prompt_id: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # \u68c0\u67e5\u662f\u5426\u5728\u8fd0\u884c\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # \u68c0\u67e5\u662f\u5426\u5728\u7b49\u5f85\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # \u68c0\u67e5\u5386\u53f2\u8bb0\u5f55 history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200 and task_id in history_response.json(): return \"completed\" return \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_14b_generated.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path return None except Exception as e: print(f\"\u274c \u4e0b\u8f7d\u9519\u8bef: {e}\") return None def main(): \"\"\"\ud83c\udfac \u4e3b\u51fd\u6570 - \u89c6\u9891\u751f\u6210\u793a\u4f8b\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfad \u5f00\u59cb\u6587\u672c\u751f\u89c6\u9891\u4efb\u52a1...\") # \ud83c\udfa8 \u793a\u4f8b\u63d0\u793a\u8bcd prompt = \"A beautiful anime girl with long black hair dancing gracefully in a cherry blossom garden, soft lighting, cinematic quality, high detail, smooth animation\" neg_prompt = \"low quality, blurry, distorted, bad anatomy, static, choppy animation, artifacts\" print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {neg_prompt}\") # \ud83d\ude80 \u63d0\u4ea4\u751f\u6210\u4efb\u52a1 task_id = client.generate_t2v( prompt=prompt, neg_prompt=neg_prompt, steps=20, cfg=7, width=832, height=480, frames=81 ) print(f\"\ud83c\udd94 \u4efb\u52a1ID: {task_id}\") # \ud83d\udcca \u76d1\u63a7\u4efb\u52a1\u72b6\u6001 while True: status = client.get_status(task_id) print(f\"\ud83d\udcc8 \u5f53\u524d\u72b6\u6001: {status}\") if status == \"completed\": print(\"\u2705 \u89c6\u9891\u751f\u6210\u5b8c\u6210!\") break elif status == \"failed\": print(\"\u274c \u751f\u6210\u5931\u8d25!\") exit(1) time.sleep(10) # \ud83d\udce5 \u4e0b\u8f7d\u89c6\u9891 output_file = client.download_video(task_id, \"my_t2v_14b_video.mp4\") if output_file: print(\"\ud83c\udf89 \u89c6\u9891\u4e0b\u8f7d\u6210\u529f!\") print(f\"\ud83d\udcc1 \u4fdd\u5b58\u4f4d\u7f6e: {output_file}\") else: print(\"\u274c \u89c6\u9891\u4e0b\u8f7d\u5931\u8d25!\") except Exception as e: print(f\"\ud83d\udca5 \u53d1\u751f\u9519\u8bef: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf \u521b\u4f5c\u6280\u5de7\u4e0e\u6700\u4f73\u5b9e\u8df5 \u270d\ufe0f \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357 \u2705 \u6b63\u5411\u63d0\u793a\u8bcd\u6280\u5de7 \u5177\u4f53\u63cf\u8ff0 \uff1a\u8be6\u7ec6\u63cf\u8ff0\u573a\u666f\u3001\u4eba\u7269\u3001\u52a8\u4f5c \u98ce\u683c\u6307\u5b9a \uff1a\u6dfb\u52a0\u827a\u672f\u98ce\u683c\u3001\u5149\u7167\u6548\u679c \u8d28\u91cf\u8bcd\u6c47 \uff1a\u4f7f\u7528 \"high quality\", \"cinematic\" \u7b49 \u52a8\u4f5c\u63cf\u8ff0 \uff1a\u660e\u786e\u6307\u5b9a\u60f3\u8981\u7684\u52a8\u6001\u6548\u679c \u274c \u8d1f\u5411\u63d0\u793a\u8bcd\u5efa\u8bae \u8d28\u91cf\u63a7\u5236 \uff1a\"low quality\", \"blurry\", \"distorted\" \u907f\u514d\u9759\u6001 \uff1a\"static\", \"motionless\", \"frozen\" \u89e3\u5256\u6b63\u786e \uff1a\"bad anatomy\", \"deformed\" \u6280\u672f\u95ee\u9898 \uff1a\"artifacts\", \"noise\", \"compression\" \ud83c\udfa8 \u521b\u610f\u793a\u4f8b **\ud83c\udf38 \u52a8\u6f2b\u98ce\u683c** A beautiful anime girl with flowing pink hair dancing in a field of cherry blossoms, soft wind, petals falling, golden hour lighting, studio ghibli style, high quality **\ud83c\udfd9\ufe0f \u79d1\u5e7b\u573a\u666f** Futuristic cityscape at night, neon lights reflecting on wet streets, flying cars in the distance, cyberpunk aesthetic, cinematic composition **\ud83c\udf0a \u81ea\u7136\u98ce\u5149** Majestic waterfall cascading down moss-covered rocks, rainbow mist, lush green forest, birds flying, peaceful atmosphere, 4K quality **\ud83c\udfad \u4eba\u7269\u8868\u6f14** Professional dancer performing contemporary dance on stage, dramatic lighting, flowing fabric, graceful movements, artistic composition \ud83d\udd27 \u53c2\u6570\u4f18\u5316\u5efa\u8bae \u2699\ufe0f \u57fa\u7840\u8bbe\u7f6e \u6b65\u6570 : 20-25\u6b65\u83b7\u5f97\u6700\u4f73\u8d28\u91cf CFG : 6-8\u8303\u56f4\u5185\u8c03\u6574 \u5206\u8fa8\u7387 : 832\u00d7480\u4e3a\u63a8\u8350\u8bbe\u7f6e \ud83c\udfaf \u8d28\u91cf\u4f18\u5316 \u5e27\u6570 : \u6839\u636e\u5185\u5bb9\u590d\u6742\u5ea6\u8c03\u6574 \u79cd\u5b50 : \u56fa\u5b9a\u79cd\u5b50\u83b7\u5f97\u4e00\u81f4\u7ed3\u679c \u8c03\u5ea6\u5668 : DPM++\u6548\u679c\u8f83\u597d \ud83d\udcda \u76f8\u5173\u8d44\u6e90\u4e0e\u6587\u6863 \ud83d\udcd6 ComfyUI \u5b98\u65b9\u6587\u6863 \u67e5\u770b\u8be6\u7ec6\u8bf4\u660e \u2192 ComfyUI \u754c\u9762\u64cd\u4f5c\u548c\u8282\u70b9\u914d\u7f6e\u6307\u5357 \ud83c\udfa5 WanVideo \u63d2\u4ef6\u6587\u6863 GitHub \u4ed3\u5e93 \u2192 WanVideo \u63d2\u4ef6\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u8bf4\u660e \ud83d\udd27 \u6280\u672f\u6587\u6863 \u9ad8\u7ea7\u6280\u5de7\u6307\u5357 \u2192 \u89c6\u9891\u751f\u6210\u7684\u9ad8\u7ea7\u6280\u672f\u548c\u4f18\u5316\u65b9\u6cd5 \ud83d\udca1 \u6700\u4f73\u5b9e\u8df5 \u521b\u4f5c\u6307\u5357 \u2192 \u4e13\u4e1a\u89c6\u9891\u521b\u4f5c\u7684\u6280\u5de7\u548c\u7ecf\u9a8c\u5206\u4eab \ud83c\udfac \u5f00\u59cb\u60a8\u7684\u89c6\u9891\u521b\u4f5c\u4e4b\u65c5\uff01 | \u7528 Wan2.1-T2V-14B \u5c06\u60a8\u7684\u521b\u610f\u53d8\u4e3a\u73b0\u5b9e","title":"Index"},{"location":"Wan2.1/T2V-14B/doc/#_1","text":"**Wan2.1-T2V-14B** \u662f\u4e00\u4e2a\u9769\u547d\u6027\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u5728\u7ed9\u5b9a\u6587\u672c\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4ee4\u4eba\u60ca\u53f9\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u5185\u5bb9\u3002\u65e0\u8bba\u662f\u521b\u610f\u8868\u8fbe\u8fd8\u662f\u5546\u4e1a\u5e94\u7528\uff0c\u90fd\u80fd\u4e3a\u60a8\u5e26\u6765\u65e0\u9650\u53ef\u80fd\uff01","title":"\ud83c\udf1f \u6a21\u578b\u7b80\u4ecb"},{"location":"Wan2.1/T2V-14B/doc/#_2","text":"","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"Wan2.1/T2V-14B/doc/#_3","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u89c6\u9891\u751f\u6210 \u53c2\u6570\u89c4\u6a21 14B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u6700\u5927\u5e27\u6570 81\u5e27 \u63a8\u8350\u5e27\u7387 16fps \u63a8\u8350\u6b65\u6570 15-25\u6b65 \u663e\u5b58\u9700\u6c42 12GB+ \u8f93\u51fa\u683c\u5f0f MP4 (H.264)","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Wan2.1/T2V-14B/doc/#_4","text":"","title":"\ud83d\udcd6 \u4f7f\u7528\u6307\u5357"},{"location":"Wan2.1/T2V-14B/doc/#comfyui","text":"### \ud83d\ude80 \u5feb\u901f\u5f00\u59cb **\u6b65\u9aa4 1\uff1a\u8bbf\u95ee\u754c\u9762** \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **\u6b65\u9aa4 2\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41** \u6309\u56fe\u4e2d\u6307\u5f15\u9009\u62e9\u5de5\u4f5c\u6d41\u4fa7\u680f\uff0c\u9009\u62e9 `wanx-21.json` \u6216 `wans.json` \u5e76\u6253\u5f00 **\u6b65\u9aa4 3\uff1a\u9009\u62e9\u529f\u80fd** \u5728\u4e0b\u56fe\u5904\u9009\u62e9\u6587\u751f\u89c6\u9891\u529f\u80fd **\u6b65\u9aa4 4\uff1a\u7f16\u5199\u63d0\u793a\u8bcd** \u5728 TextEncode \u5904\u586b\u5199\u63cf\u8ff0\u8bcd","title":"\ud83c\udf10 \u65b9\u5f0f\u4e00\uff1aComfyUI \u53ef\u89c6\u5316\u754c\u9762"},{"location":"Wan2.1/T2V-14B/doc/#api","text":"","title":"\ud83d\udd0c \u65b9\u5f0f\u4e8c\uff1aAPI \u7a0b\u5e8f\u5316\u8c03\u7528"},{"location":"Wan2.1/T2V-14B/doc/#_5","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"Wan2.1/T2V-14B/doc/#python","text":"\ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u4ee3\u7801 import requests, json, uuid, time, random # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-T2V-14B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: \"\"\"\ud83c\udfac ComfyUI \u89c6\u9891\u751f\u6210\u5ba2\u6237\u7aef\"\"\" def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\" \ud83c\udfa5 \u751f\u6210\u89c6\u9891 Args: prompt (str): \u6b63\u5411\u63d0\u793a\u8bcd neg_prompt (str): \u8d1f\u5411\u63d0\u793a\u8bcd steps (int): \u63a8\u7406\u6b65\u6570 cfg (float): CFG\u5f15\u5bfc\u5f3a\u5ea6 width (int): \u89c6\u9891\u5bbd\u5ea6 height (int): \u89c6\u9891\u9ad8\u5ea6 frames (int): \u89c6\u9891\u5e27\u6570 Returns: str: \u4efb\u52a1ID \"\"\" print(f\"\ud83c\udfac \u5f00\u59cb\u6587\u672c\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt}\") workflow = { \"1\": { \"inputs\": { \"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\" }, \"class_type\": \"LoadWanVideoT5TextEncoder\" }, \"2\": { \"inputs\": { \"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0] }, \"class_type\": \"WanVideoTextEncode\" }, \"3\": { \"inputs\": { \"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\" }, \"class_type\": \"WanVideoModelLoader\" }, \"4\": { \"inputs\": { \"width\": width, \"height\": height, \"num_frames\": frames }, \"class_type\": \"WanVideoEmptyEmbeds\" }, \"5\": { \"inputs\": { \"model_name\": VAE_MODEL, \"precision\": \"bf16\" }, \"class_type\": \"WanVideoVAELoader\" }, \"6\": { \"inputs\": { \"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0] }, \"class_type\": \"WanVideoSampler\" }, \"7\": { \"inputs\": { \"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0] }, \"class_type\": \"WanVideoDecode\" }, \"8\": { \"inputs\": { \"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_14B_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0] }, \"class_type\": \"VHS_VideoCombine\" } } print(\"\ud83d\ude80 \u63d0\u4ea4\u89c6\u9891\u751f\u6210\u4efb\u52a1...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"\ud83d\udce1 API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u274c \u5de5\u4f5c\u6d41\u9519\u8bef: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u274c \u54cd\u5e94\u4e2d\u6ca1\u6709 prompt_id: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # \u68c0\u67e5\u662f\u5426\u5728\u8fd0\u884c\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # \u68c0\u67e5\u662f\u5426\u5728\u7b49\u5f85\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # \u68c0\u67e5\u5386\u53f2\u8bb0\u5f55 history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200 and task_id in history_response.json(): return \"completed\" return \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_14b_generated.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path return None except Exception as e: print(f\"\u274c \u4e0b\u8f7d\u9519\u8bef: {e}\") return None def main(): \"\"\"\ud83c\udfac \u4e3b\u51fd\u6570 - \u89c6\u9891\u751f\u6210\u793a\u4f8b\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfad \u5f00\u59cb\u6587\u672c\u751f\u89c6\u9891\u4efb\u52a1...\") # \ud83c\udfa8 \u793a\u4f8b\u63d0\u793a\u8bcd prompt = \"A beautiful anime girl with long black hair dancing gracefully in a cherry blossom garden, soft lighting, cinematic quality, high detail, smooth animation\" neg_prompt = \"low quality, blurry, distorted, bad anatomy, static, choppy animation, artifacts\" print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {neg_prompt}\") # \ud83d\ude80 \u63d0\u4ea4\u751f\u6210\u4efb\u52a1 task_id = client.generate_t2v( prompt=prompt, neg_prompt=neg_prompt, steps=20, cfg=7, width=832, height=480, frames=81 ) print(f\"\ud83c\udd94 \u4efb\u52a1ID: {task_id}\") # \ud83d\udcca \u76d1\u63a7\u4efb\u52a1\u72b6\u6001 while True: status = client.get_status(task_id) print(f\"\ud83d\udcc8 \u5f53\u524d\u72b6\u6001: {status}\") if status == \"completed\": print(\"\u2705 \u89c6\u9891\u751f\u6210\u5b8c\u6210!\") break elif status == \"failed\": print(\"\u274c \u751f\u6210\u5931\u8d25!\") exit(1) time.sleep(10) # \ud83d\udce5 \u4e0b\u8f7d\u89c6\u9891 output_file = client.download_video(task_id, \"my_t2v_14b_video.mp4\") if output_file: print(\"\ud83c\udf89 \u89c6\u9891\u4e0b\u8f7d\u6210\u529f!\") print(f\"\ud83d\udcc1 \u4fdd\u5b58\u4f4d\u7f6e: {output_file}\") else: print(\"\u274c \u89c6\u9891\u4e0b\u8f7d\u5931\u8d25!\") except Exception as e: print(f\"\ud83d\udca5 \u53d1\u751f\u9519\u8bef: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python \u4ee3\u7801\u5b9e\u73b0"},{"location":"Wan2.1/T2V-14B/doc/#_6","text":"","title":"\ud83c\udfaf \u521b\u4f5c\u6280\u5de7\u4e0e\u6700\u4f73\u5b9e\u8df5"},{"location":"Wan2.1/T2V-14B/doc/#_7","text":"","title":"\u270d\ufe0f \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357"},{"location":"Wan2.1/T2V-14B/doc/#_8","text":"**\ud83c\udf38 \u52a8\u6f2b\u98ce\u683c** A beautiful anime girl with flowing pink hair dancing in a field of cherry blossoms, soft wind, petals falling, golden hour lighting, studio ghibli style, high quality **\ud83c\udfd9\ufe0f \u79d1\u5e7b\u573a\u666f** Futuristic cityscape at night, neon lights reflecting on wet streets, flying cars in the distance, cyberpunk aesthetic, cinematic composition **\ud83c\udf0a \u81ea\u7136\u98ce\u5149** Majestic waterfall cascading down moss-covered rocks, rainbow mist, lush green forest, birds flying, peaceful atmosphere, 4K quality **\ud83c\udfad \u4eba\u7269\u8868\u6f14** Professional dancer performing contemporary dance on stage, dramatic lighting, flowing fabric, graceful movements, artistic composition","title":"\ud83c\udfa8 \u521b\u610f\u793a\u4f8b"},{"location":"Wan2.1/T2V-14B/doc/#_9","text":"","title":"\ud83d\udd27 \u53c2\u6570\u4f18\u5316\u5efa\u8bae"},{"location":"Wan2.1/T2V-14B/doc/#_10","text":"","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90\u4e0e\u6587\u6863"},{"location":"Wan2.1/T2V-14B/doc/index-en/","text":"\ud83c\udfac Wan2.1-T2V-14B Text-to-Video Model Transform Words into Cinematic Magic! Revolutionary AI-powered video generation \ud83e\udde0 14B Parameters \ud83c\udfaf High Quality \u26a1 Professional \ud83c\udf1f Model Introduction **Wan2.1-T2V-14B** is a groundbreaking text-to-video generation model that creates stunning, high-quality video content from simple text descriptions. Whether for creative expression or commercial applications, this model opens up infinite possibilities for content creation! \u2728 Core Features \ud83e\udde0 Massive Parameter Scale 14B parameters delivering exceptional understanding and generation capabilities \ud83c\udfd7\ufe0f Advanced Architecture Cutting-edge Diffusion Transformer + VAE technology \u26a1 Memory Optimization FP8 quantization for efficient GPU utilization \ud83c\udf0d Multi-language Support Seamless Chinese and English text processing \ud83c\udfad Complex Scene Understanding Deep text comprehension for realistic scene generation \ud83c\udfa5 Professional Quality Cinema-grade video quality, ready for commercial use \ud83d\udd27 Technical Specifications Specification Details Model Type Text-to-Video Generation Parameters 14B Quantization FP8 Quantized Version Maximum Frames 81 frames Recommended Frame Rate 16fps Recommended Steps 15-25 VRAM Requirements 12GB+ Output Format MP4 (H.264) \ud83d\udcd6 Usage Guide \ud83c\udf10 Method 1: ComfyUI Visual Interface ### \ud83d\ude80 Quick Start Guide **Step 1: Access Interface** Click the access link at the service instance **Step 2: Select Workflow** Follow the guidance to select the workflow sidebar, choose `wanx-21.json` or `wans.json` and open it **Step 3: Choose Function** Select text-to-video at the designated location **Step 4: Write Prompts** Fill in description words at TextEncode \u2705 Upper Section Content you want to generate \u274c Lower Section Content you don't want to generate **Step 5: Configure Settings** Set image resolution and frame count at ImageClip Encode ### \ud83d\udcda Additional Resources - [ComfyUI Official Documentation](https://comfyui-wiki.com/zh/interface/node-options) - [WanVideo Plugin Detailed Guide](https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/readme.md) \ud83d\udd0c Method 2: API Integration \ud83d\udd11 Authentication Setup \ud83c\udfab Get Token Click the button in the upper right corner, open the bottom panel to get token \ud83c\udf10 Get Server Address For COMFYUI_SERVER acquisition, refer to: \ud83d\udcbb Python Implementation \ud83d\udc0d Click to Expand Complete Python API Code import requests, json, uuid, time, random # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-T2V-14B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: \"\"\"\ud83c\udfac ComfyUI Video Generation Client\"\"\" def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\" \ud83c\udfa5 Generate Video Args: prompt (str): Positive prompt neg_prompt (str): Negative prompt steps (int): Inference steps cfg (float): CFG guidance strength width (int): Video width height (int): Video height frames (int): Number of frames Returns: str: Task ID \"\"\" print(f\"\ud83c\udfac Starting text-to-video generation task...\") print(f\"\ud83d\udcdd Prompt: {prompt}\") workflow = { \"1\": { \"inputs\": { \"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\" }, \"class_type\": \"LoadWanVideoT5TextEncoder\" }, \"2\": { \"inputs\": { \"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0] }, \"class_type\": \"WanVideoTextEncode\" }, \"3\": { \"inputs\": { \"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\" }, \"class_type\": \"WanVideoModelLoader\" }, \"4\": { \"inputs\": { \"width\": width, \"height\": height, \"num_frames\": frames }, \"class_type\": \"WanVideoEmptyEmbeds\" }, \"5\": { \"inputs\": { \"model_name\": VAE_MODEL, \"precision\": \"bf16\" }, \"class_type\": \"WanVideoVAELoader\" }, \"6\": { \"inputs\": { \"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0] }, \"class_type\": \"WanVideoSampler\" }, \"7\": { \"inputs\": { \"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0] }, \"class_type\": \"WanVideoDecode\" }, \"8\": { \"inputs\": { \"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_14B_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0] }, \"class_type\": \"VHS_VideoCombine\" } } print(\"\ud83d\ude80 Submitting video generation task...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"\ud83d\udce1 API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u274c Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u274c No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200 and task_id in history_response.json(): return \"completed\" return \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_14b_generated.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path return None except Exception as e: print(f\"\u274c Download error: {e}\") return None def main(): \"\"\"\ud83c\udfac Main function - Video generation example\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfad Starting text-to-video generation task...\") # \ud83c\udfa8 Example prompts prompt = \"A beautiful anime girl with long black hair dancing gracefully in a cherry blossom garden, soft lighting, cinematic quality, high detail, smooth animation\" neg_prompt = \"low quality, blurry, distorted, bad anatomy, static, choppy animation, artifacts\" print(f\"\ud83d\udcdd Prompt: {prompt}\") print(f\"\ud83d\udeab Negative prompt: {neg_prompt}\") # \ud83d\ude80 Submit generation task task_id = client.generate_t2v( prompt=prompt, neg_prompt=neg_prompt, steps=20, cfg=7, width=832, height=480, frames=81 ) print(f\"\ud83c\udd94 Task ID: {task_id}\") # \ud83d\udcca Monitor task status while True: status = client.get_status(task_id) print(f\"\ud83d\udcc8 Current status: {status}\") if status == \"completed\": print(\"\u2705 Video generation completed!\") break elif status == \"failed\": print(\"\u274c Generation failed!\") exit(1) time.sleep(10) # \ud83d\udce5 Download video output_file = client.download_video(task_id, \"my_t2v_14b_video.mp4\") if output_file: print(\"\ud83c\udf89 Video downloaded successfully!\") print(f\"\ud83d\udcc1 Saved as: {output_file}\") else: print(\"\u274c Failed to download video!\") except Exception as e: print(f\"\ud83d\udca5 Error occurred: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Creative Tips & Best Practices \u270d\ufe0f Prompt Writing Guide \u2705 Positive Prompt Tips Detailed Description : Describe scenes, characters, and actions in detail Style Specification : Add artistic styles and lighting effects Quality Keywords : Use \"high quality\", \"cinematic\", etc. Motion Description : Clearly specify desired dynamic effects \u274c Negative Prompt Suggestions Quality Control : \"low quality\", \"blurry\", \"distorted\" Avoid Static : \"static\", \"motionless\", \"frozen\" Anatomical Accuracy : \"bad anatomy\", \"deformed\" Technical Issues : \"artifacts\", \"noise\", \"compression\" \ud83c\udfa8 Creative Examples **\ud83c\udf38 Anime Style** A beautiful anime girl with flowing pink hair dancing in a field of cherry blossoms, soft wind, petals falling, golden hour lighting, studio ghibli style, high quality **\ud83c\udfd9\ufe0f Sci-Fi Scene** Futuristic cityscape at night, neon lights reflecting on wet streets, flying cars in the distance, cyberpunk aesthetic, cinematic composition **\ud83c\udf0a Natural Landscape** Majestic waterfall cascading down moss-covered rocks, rainbow mist, lush green forest, birds flying, peaceful atmosphere, 4K quality **\ud83c\udfad Fantasy Adventure** Epic dragon soaring through stormy clouds, lightning illuminating its scales, medieval castle below, dramatic atmosphere, fantasy art style \ud83c\udf9b\ufe0f Parameter Optimization Guide \ud83d\udd22 Steps Configuration 10-15 steps : Fast generation, good for testing 15-20 steps : Balanced quality and speed 20-30 steps : High quality, longer processing time \ud83c\udfaf CFG Settings 4-6 : More creative, less adherence to prompt 6-8 : Balanced guidance (recommended) 8-12 : Strong prompt adherence \ud83d\udcd0 Resolution Options 512x512 : Square format, fast generation 832x480 : Widescreen, cinematic feel Custom : Adjust based on your needs \ud83c\udfaa Application Scenarios \ud83c\udfac Content Creation Social media videos, marketing content, storytelling \ud83c\udfa8 Artistic Expression Digital art, creative projects, visual experiments \ud83d\udcda Educational Content Learning materials, demonstrations, tutorials \ud83d\ude80 Prototyping Concept visualization, storyboarding, idea development \ud83d\udcda Resources & Documentation \ud83d\udcd6 ComfyUI Official Docs View Detailed Guide \u2192 ComfyUI interface operations and node configuration guide \ud83c\udfa5 WanVideo Plugin Docs GitHub Repository \u2192 WanVideo plugin installation and usage instructions \ud83d\udd27 Technical Documentation Advanced Techniques Guide \u2192 Advanced techniques and optimization methods for video generation \ud83d\udca1 Best Practices Creation Guide \u2192 Professional video creation tips and experience sharing \ud83c\udfac Start Your Video Creation Journey! | Transform your ideas into stunning videos with Wan2.1-T2V-14B","title":"Index en"},{"location":"Wan2.1/T2V-14B/doc/index-en/#model-introduction","text":"**Wan2.1-T2V-14B** is a groundbreaking text-to-video generation model that creates stunning, high-quality video content from simple text descriptions. Whether for creative expression or commercial applications, this model opens up infinite possibilities for content creation!","title":"\ud83c\udf1f Model Introduction"},{"location":"Wan2.1/T2V-14B/doc/index-en/#core-features","text":"","title":"\u2728 Core Features"},{"location":"Wan2.1/T2V-14B/doc/index-en/#technical-specifications","text":"Specification Details Model Type Text-to-Video Generation Parameters 14B Quantization FP8 Quantized Version Maximum Frames 81 frames Recommended Frame Rate 16fps Recommended Steps 15-25 VRAM Requirements 12GB+ Output Format MP4 (H.264)","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.1/T2V-14B/doc/index-en/#usage-guide","text":"","title":"\ud83d\udcd6 Usage Guide"},{"location":"Wan2.1/T2V-14B/doc/index-en/#method-1-comfyui-visual-interface","text":"### \ud83d\ude80 Quick Start Guide **Step 1: Access Interface** Click the access link at the service instance **Step 2: Select Workflow** Follow the guidance to select the workflow sidebar, choose `wanx-21.json` or `wans.json` and open it **Step 3: Choose Function** Select text-to-video at the designated location **Step 4: Write Prompts** Fill in description words at TextEncode","title":"\ud83c\udf10 Method 1: ComfyUI Visual Interface"},{"location":"Wan2.1/T2V-14B/doc/index-en/#method-2-api-integration","text":"","title":"\ud83d\udd0c Method 2: API Integration"},{"location":"Wan2.1/T2V-14B/doc/index-en/#authentication-setup","text":"","title":"\ud83d\udd11 Authentication Setup"},{"location":"Wan2.1/T2V-14B/doc/index-en/#python-implementation","text":"\ud83d\udc0d Click to Expand Complete Python API Code import requests, json, uuid, time, random # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" T5_MODEL = \"wan2.1/umt5-xxl-enc-bf16.safetensors\" VIDEO_MODEL = \"Wan2_1-T2V-14B_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan2.1/Wan2_1_VAE_bf16.safetensors\" class ComfyUIClient: \"\"\"\ud83c\udfac ComfyUI Video Generation Client\"\"\" def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_t2v(self, prompt, neg_prompt=\"\", steps=15, cfg=6, width=832, height=480, frames=81): \"\"\" \ud83c\udfa5 Generate Video Args: prompt (str): Positive prompt neg_prompt (str): Negative prompt steps (int): Inference steps cfg (float): CFG guidance strength width (int): Video width height (int): Video height frames (int): Number of frames Returns: str: Task ID \"\"\" print(f\"\ud83c\udfac Starting text-to-video generation task...\") print(f\"\ud83d\udcdd Prompt: {prompt}\") workflow = { \"1\": { \"inputs\": { \"model_name\": T5_MODEL, \"precision\": \"bf16\", \"load_device\": \"offload_device\", \"quantization\": \"disabled\" }, \"class_type\": \"LoadWanVideoT5TextEncoder\" }, \"2\": { \"inputs\": { \"positive_prompt\": prompt, \"negative_prompt\": neg_prompt, \"force_offload\": True, \"t5\": [\"1\", 0] }, \"class_type\": \"WanVideoTextEncode\" }, \"3\": { \"inputs\": { \"model\": VIDEO_MODEL, \"base_precision\": \"bf16\", \"quantization\": \"fp8_e4m3fn\", \"load_device\": \"offload_device\", \"attention_mode\": \"sageattn\" }, \"class_type\": \"WanVideoModelLoader\" }, \"4\": { \"inputs\": { \"width\": width, \"height\": height, \"num_frames\": frames }, \"class_type\": \"WanVideoEmptyEmbeds\" }, \"5\": { \"inputs\": { \"model_name\": VAE_MODEL, \"precision\": \"bf16\" }, \"class_type\": \"WanVideoVAELoader\" }, \"6\": { \"inputs\": { \"steps\": steps, \"cfg\": cfg, \"shift\": 5, \"seed\": random.randint(1, 1000000), \"force_offload\": True, \"scheduler\": \"dpm++\", \"riflex_freq_index\": 0, \"denoise_strength\": 1, \"batched_cfg\": False, \"rope_function\": \"comfy\", \"model\": [\"3\", 0], \"text_embeds\": [\"2\", 0], \"image_embeds\": [\"4\", 0] }, \"class_type\": \"WanVideoSampler\" }, \"7\": { \"inputs\": { \"enable_vae_tiling\": True, \"tile_x\": 272, \"tile_y\": 272, \"tile_stride_x\": 144, \"tile_stride_y\": 128, \"vae\": [\"5\", 0], \"samples\": [\"6\", 0] }, \"class_type\": \"WanVideoDecode\" }, \"8\": { \"inputs\": { \"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"T2V_14B_generated\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"7\", 0] }, \"class_type\": \"VHS_VideoCombine\" } } print(\"\ud83d\ude80 Submitting video generation task...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"\ud83d\udce1 API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u274c Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u274c No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200 and task_id in history_response.json(): return \"completed\" return \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"t2v_14b_generated.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path return None except Exception as e: print(f\"\u274c Download error: {e}\") return None def main(): \"\"\"\ud83c\udfac Main function - Video generation example\"\"\" client = ComfyUIClient() try: print(\"\ud83c\udfad Starting text-to-video generation task...\") # \ud83c\udfa8 Example prompts prompt = \"A beautiful anime girl with long black hair dancing gracefully in a cherry blossom garden, soft lighting, cinematic quality, high detail, smooth animation\" neg_prompt = \"low quality, blurry, distorted, bad anatomy, static, choppy animation, artifacts\" print(f\"\ud83d\udcdd Prompt: {prompt}\") print(f\"\ud83d\udeab Negative prompt: {neg_prompt}\") # \ud83d\ude80 Submit generation task task_id = client.generate_t2v( prompt=prompt, neg_prompt=neg_prompt, steps=20, cfg=7, width=832, height=480, frames=81 ) print(f\"\ud83c\udd94 Task ID: {task_id}\") # \ud83d\udcca Monitor task status while True: status = client.get_status(task_id) print(f\"\ud83d\udcc8 Current status: {status}\") if status == \"completed\": print(\"\u2705 Video generation completed!\") break elif status == \"failed\": print(\"\u274c Generation failed!\") exit(1) time.sleep(10) # \ud83d\udce5 Download video output_file = client.download_video(task_id, \"my_t2v_14b_video.mp4\") if output_file: print(\"\ud83c\udf89 Video downloaded successfully!\") print(f\"\ud83d\udcc1 Saved as: {output_file}\") else: print(\"\u274c Failed to download video!\") except Exception as e: print(f\"\ud83d\udca5 Error occurred: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python Implementation"},{"location":"Wan2.1/T2V-14B/doc/index-en/#creative-tips-best-practices","text":"","title":"\ud83c\udfaf Creative Tips &amp; Best Practices"},{"location":"Wan2.1/T2V-14B/doc/index-en/#prompt-writing-guide","text":"","title":"\u270d\ufe0f Prompt Writing Guide"},{"location":"Wan2.1/T2V-14B/doc/index-en/#creative-examples","text":"**\ud83c\udf38 Anime Style** A beautiful anime girl with flowing pink hair dancing in a field of cherry blossoms, soft wind, petals falling, golden hour lighting, studio ghibli style, high quality **\ud83c\udfd9\ufe0f Sci-Fi Scene** Futuristic cityscape at night, neon lights reflecting on wet streets, flying cars in the distance, cyberpunk aesthetic, cinematic composition **\ud83c\udf0a Natural Landscape** Majestic waterfall cascading down moss-covered rocks, rainbow mist, lush green forest, birds flying, peaceful atmosphere, 4K quality **\ud83c\udfad Fantasy Adventure** Epic dragon soaring through stormy clouds, lightning illuminating its scales, medieval castle below, dramatic atmosphere, fantasy art style","title":"\ud83c\udfa8 Creative Examples"},{"location":"Wan2.1/T2V-14B/doc/index-en/#parameter-optimization-guide","text":"","title":"\ud83c\udf9b\ufe0f Parameter Optimization Guide"},{"location":"Wan2.1/T2V-14B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaa Application Scenarios"},{"location":"Wan2.1/T2V-14B/doc/index-en/#resources-documentation","text":"","title":"\ud83d\udcda Resources &amp; Documentation"},{"location":"Wan2.1/VACE-1.3B/doc/","text":"\ud83c\udfac Wan2.1-VACE-1.3B \u89c6\u9891\u7f16\u8f91\u5b8c\u6574\u6307\u5357 \u667a\u80fd\u89c6\u9891\u5185\u5bb9\u7f16\u8f91\u6280\u672f\uff0cVACE (Video Adaptive Content Editing) \u4e13\u4e1a\u7ea7\u89c6\u9891\u7f16\u8f91 \u26a1 1.3B\u53c2\u6570 \ud83c\udfaf \u4e13\u4e1a\u7f16\u8f91 \ud83d\ude80 \u8f7b\u91cf\u9ad8\u6548 \ud83c\udf1f \u6a21\u578b\u7b80\u4ecb **Wan2.1-VACE-1.3B** \u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u89c6\u9891\u5185\u5bb9\u7f16\u8f91\u548c\u5904\u7406\u7684\u8f7b\u91cf\u5316\u6a21\u578b\u3002VACE\uff08Video Adaptive Content Editing\uff09\u6280\u672f\u4e13\u6ce8\u4e8e\u5bf9\u73b0\u6709\u89c6\u9891\u8fdb\u884c\u667a\u80fd\u7f16\u8f91\uff0c\u5305\u62ec\u98ce\u683c\u8f6c\u6362\u3001\u5185\u5bb9\u4fee\u6539\u3001\u573a\u666f\u589e\u5f3a\u7b49\u529f\u80fd\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u89c6\u9891\u65f6\u5e8f\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6839\u636e\u6587\u672c\u6307\u4ee4\u5bf9\u89c6\u9891\u5185\u5bb9\u8fdb\u884c\u7cbe\u786e\u7f16\u8f91\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \u26a1 \u8f7b\u91cf\u53c2\u6570\u89c4\u6a21 1.3B \u53c2\u6570\uff0c\u6027\u80fd\u4f18\u5316\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883 \ud83c\udfa5 \u89c6\u9891\u7f16\u8f91\u4e13\u7528 \u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4f18\u5316 \u23f0 \u65f6\u5e8f\u4e00\u81f4\u6027 \u4fdd\u6301\u89c6\u9891\u5e27\u95f4\u7684\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027 \ud83d\udee0\ufe0f \u591a\u79cd\u7f16\u8f91\u529f\u80fd \u652f\u6301\u98ce\u683c\u8f6c\u6362\u3001\u5185\u5bb9\u4fee\u6539\u3001\u573a\u666f\u589e\u5f3a\u7b49 \ud83d\ude80 \u5feb\u901f\u5904\u7406 \u8f7b\u91cf\u5316\u67b6\u6784\uff0c\u5904\u7406\u901f\u5ea6\u5feb \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u6587\u548c\u82f1\u6587\u7f16\u8f91\u6307\u4ee4 \ud83d\udcca \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u89c6\u9891\u7f16\u8f91\uff08Video Editing\uff09 \u53c2\u6570\u89c4\u6a21 1.3B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u90e8\u7f72\u67b6\u6784 ECS\u5355\u673a\u90e8\u7f72/ACS\u96c6\u7fa4\u90e8\u7f72 \u652f\u6301\u5206\u8fa8\u7387 480P \u8f93\u5165\u683c\u5f0f MP4\u3001AVI\u3001MOV\u7b49\u5e38\u89c1\u89c6\u9891\u683c\u5f0f \u8f93\u51fa\u683c\u5f0f MP4\uff08H.264\u7f16\u7801\uff09 \ud83c\udfa8 \u7f16\u8f91\u80fd\u529b \ud83c\udfad \u98ce\u683c\u8f6c\u6362 \u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u4e0d\u540c\u827a\u672f\u98ce\u683c \ud83c\udf08 \u8272\u5f69\u8c03\u6574 \u8c03\u6574\u89c6\u9891\u7684\u8272\u8c03\u3001\u9971\u548c\u5ea6\u3001\u4eae\u5ea6 \ud83c\udfde\ufe0f \u573a\u666f\u4fee\u6539 \u6539\u53d8\u89c6\u9891\u80cc\u666f\u6216\u6dfb\u52a0\u65b0\u5143\u7d20 \ud83c\udfaf \u5bf9\u8c61\u7f16\u8f91 \u4fee\u6539\u6216\u66ff\u6362\u89c6\u9891\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61 \u2728 \u7279\u6548\u6dfb\u52a0 \u4e3a\u89c6\u9891\u6dfb\u52a0\u5404\u79cd\u89c6\u89c9\u7279\u6548 \ud83d\udcc8 \u8d28\u91cf\u589e\u5f3a \u63d0\u5347\u89c6\u9891\u6e05\u6670\u5ea6\u548c\u8d28\u91cf \ud83d\ude80 \u90e8\u7f72\u65b9\u5f0f \ud83c\udfd7\ufe0f \u90e8\u7f72\u67b6\u6784\uff08\u63a8\u8350\uff09 \ud83d\udca1 \u90e8\u7f72\u914d\u7f6e \u90e8\u7f72\u67b6\u6784 : ECS\u5355\u673a\u90e8\u7f72\u6216ACS\u90e8\u7f72 \u5185\u5b58\u9700\u6c42 : \u663e\u5b58\u9700\u6c42\u4f4e\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883 \u6269\u5c55\u6027 : \u652f\u6301\u5355\u673a\u548c\u96c6\u7fa4\u90e8\u7f72 \ud83c\udfaf \u4f7f\u7528\u6307\u5357 \ud83c\udf10 ComfyUI \u4f7f\u7528 ### \ud83d\udd27 \u64cd\u4f5c\u6b65\u9aa4 **1. \u8bbf\u95ee\u754c\u9762** - \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **2. \u52a0\u8f7d\u5de5\u4f5c\u6d41** - \u4f7f\u7528\u5de5\u4f5c\u6d41 `vace.json` - \u5c06\u5de5\u4f5c\u6d41\u5bfc\u5165\u5230 ComfyUI \u4e2d **3. \u751f\u6210\u5185\u5bb9** - \u8f93\u5165\u60f3\u8981\u751f\u6210\u7684\u89c6\u9891\u6216\u56fe\u7247\u7684\u63d0\u793a\u8bcd - \u70b9\u51fb\u751f\u6210\u5f00\u59cb\u5904\u7406 \ud83d\udd0c API \u8c03\u7528 \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83c\udfab \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\uff0c\u83b7\u53d6token \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 COMFYUI_SERVER \u7684\u83b7\u53d6\u53ef\u53c2\u8003 \ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b \ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" UNET_MODEL = \"wan21_vace_1_3_b.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"example.png\" PROMPT = \"\u5973\u5b69\u5077\u5077\u54ac\u4e00\u53e3\u82f9\u679c\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=20, cfg=4, frames=49, width=480, height=480): \"\"\"\ud83c\udfac VACE\u89c6\u9891\u7f16\u8f91\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn_fast\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4VACE\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884cVACE\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac \u5f00\u59cbVACE\u89c6\u9891\u7f16\u8f91\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u7f16\u8f91\u63d0\u793a: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709 example.png \u6587\u4ef6\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 4, 49, 480, 480) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 \u6700\u4f73\u5b9e\u8df5 \ud83c\udfaf \u63d0\u793a\u8bcd\u4f18\u5316 \u4f7f\u7528\u6e05\u6670\u5177\u4f53\u7684\u63cf\u8ff0 \u5305\u542b\u98ce\u683c\u548c\u60c5\u7eea\u5173\u952e\u8bcd \u6307\u5b9a\u65f6\u5e8f\u5143\u7d20\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027 \u907f\u514d\u8fc7\u4e8e\u590d\u6742\u7684\u63cf\u8ff0 \u2699\ufe0f \u53c2\u6570\u8c03\u4f18 \u4ece\u9ed8\u8ba4\u8bbe\u7f6e\u5f00\u59cb\uff0820\u6b65\uff0cCFG 4\uff09 \u6839\u636e\u5185\u5bb9\u8c03\u6574\u5e27\u6570 \u76d1\u63a7\u663e\u5b58\u4f7f\u7528\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd \u4f7f\u7528\u63a8\u8350\u5206\u8fa8\u7387 480\u00d7480 \ud83c\udfac \u89c6\u9891\u8d28\u91cf \u4f7f\u7528\u9ad8\u8d28\u91cf\u8f93\u5165\u56fe\u7247 \u786e\u4fdd\u6b63\u786e\u7684\u5bbd\u9ad8\u6bd4 \u5148\u7528\u77ed\u7247\u6bb5\u6d4b\u8bd5 \u6ce8\u610f\u65f6\u5e8f\u8fde\u8d2f\u6027 \ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848 \ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a \u63d0\u793a\u8bcd\u4e0d\u591f\u8be6\u7ec6 \uff1a\u589e\u52a0\u66f4\u5177\u4f53\u7684\u63cf\u8ff0 \u53c2\u6570\u8bbe\u7f6e\u4e0d\u5f53 \uff1a\u8c03\u6574\u6b65\u6570\u548cCFG\u503c \u8f93\u5165\u56fe\u7247\u8d28\u91cf\u4f4e \uff1a\u4f7f\u7528\u66f4\u9ad8\u8d28\u91cf\u7684\u6e90\u56fe\u7247 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 \uff1a\u964d\u4f4e\u5206\u8fa8\u7387\u6216\u4f7f\u7528medvram\u6a21\u5f0f \u52a0\u8f7d\u7f13\u6162 \uff1a\u6a21\u578b\u8f83\u5927\uff0c\u9700\u8981\u8010\u5fc3\u7b49\u5f85 \u751f\u6210\u7f13\u6162 \uff1a\u51cf\u5c11\u6b65\u6570\u6216\u4f7f\u7528\u66f4\u5feb\u7684\u91c7\u6837\u5668 \ud83d\udd27 \u517c\u5bb9\u6027\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a ComfyUI\u7248\u672c \uff1a\u786e\u4fdd\u4f7f\u7528\u652f\u6301VACE\u7684\u6700\u65b0\u7248\u672c \u63d2\u4ef6\u517c\u5bb9\u6027 \uff1a\u67d0\u4e9b\u63d2\u4ef6\u53ef\u80fd\u4e0eVACE\u4e0d\u517c\u5bb9 \u53c2\u6570\u8303\u56f4 \uff1a\u6ce8\u610fVACE\u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4 \ud83d\udcda \u76f8\u5173\u8d44\u6e90 \ud83d\udcc4 \u6280\u672f\u6587\u6863 VACE\u6280\u672f\u8bba\u6587 \u2192 \u4e86\u89e3 VACE \u6280\u672f\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u73b0\u539f\u7406 \ud83d\udee0\ufe0f \u5de5\u5177\u4e0e\u6307\u5357 ComfyUI\u89c6\u9891\u7f16\u8f91\u6307\u5357 \u2192 ComfyUI \u89c6\u9891\u7f16\u8f91\u529f\u80fd\u7684\u8be6\u7ec6\u4f7f\u7528\u8bf4\u660e \ud83c\udfa5 \u6700\u4f73\u5b9e\u8df5 \u89c6\u9891\u7f16\u8f91\u6700\u4f73\u5b9e\u8df5 \u2192 \u4e13\u4e1a\u89c6\u9891\u7f16\u8f91\u7684\u6280\u5de7\u548c\u7ecf\u9a8c\u5206\u4eab \ud83d\udd0c \u63d2\u4ef6\u6587\u6863 VACE\u63d2\u4ef6\u6587\u6863 \u2192 VACE ComfyUI \u63d2\u4ef6\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u8bf4\u660e \ud83c\udfac \u5f00\u59cb\u4f7f\u7528 VACE \u6280\u672f\u521b\u4f5c\u7cbe\u5f69\u89c6\u9891\u5427\uff01 | \u667a\u80fd\u89c6\u9891\u7f16\u8f91\uff0c\u8ba9\u521b\u610f\u65e0\u9650\u5ef6\u4f38","title":"Index"},{"location":"Wan2.1/VACE-1.3B/doc/#_1","text":"**Wan2.1-VACE-1.3B** \u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u89c6\u9891\u5185\u5bb9\u7f16\u8f91\u548c\u5904\u7406\u7684\u8f7b\u91cf\u5316\u6a21\u578b\u3002VACE\uff08Video Adaptive Content Editing\uff09\u6280\u672f\u4e13\u6ce8\u4e8e\u5bf9\u73b0\u6709\u89c6\u9891\u8fdb\u884c\u667a\u80fd\u7f16\u8f91\uff0c\u5305\u62ec\u98ce\u683c\u8f6c\u6362\u3001\u5185\u5bb9\u4fee\u6539\u3001\u573a\u666f\u589e\u5f3a\u7b49\u529f\u80fd\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u89c6\u9891\u65f6\u5e8f\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6839\u636e\u6587\u672c\u6307\u4ee4\u5bf9\u89c6\u9891\u5185\u5bb9\u8fdb\u884c\u7cbe\u786e\u7f16\u8f91\u3002","title":"\ud83c\udf1f \u6a21\u578b\u7b80\u4ecb"},{"location":"Wan2.1/VACE-1.3B/doc/#_2","text":"","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"Wan2.1/VACE-1.3B/doc/#_3","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u89c6\u9891\u7f16\u8f91\uff08Video Editing\uff09 \u53c2\u6570\u89c4\u6a21 1.3B \u91cf\u5316\u65b9\u5f0f FP8\u91cf\u5316\u7248\u672c \u90e8\u7f72\u67b6\u6784 ECS\u5355\u673a\u90e8\u7f72/ACS\u96c6\u7fa4\u90e8\u7f72 \u652f\u6301\u5206\u8fa8\u7387 480P \u8f93\u5165\u683c\u5f0f MP4\u3001AVI\u3001MOV\u7b49\u5e38\u89c1\u89c6\u9891\u683c\u5f0f \u8f93\u51fa\u683c\u5f0f MP4\uff08H.264\u7f16\u7801\uff09","title":"\ud83d\udcca \u6280\u672f\u89c4\u683c"},{"location":"Wan2.1/VACE-1.3B/doc/#_4","text":"\ud83c\udfad \u98ce\u683c\u8f6c\u6362 \u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u4e0d\u540c\u827a\u672f\u98ce\u683c \ud83c\udf08 \u8272\u5f69\u8c03\u6574 \u8c03\u6574\u89c6\u9891\u7684\u8272\u8c03\u3001\u9971\u548c\u5ea6\u3001\u4eae\u5ea6 \ud83c\udfde\ufe0f \u573a\u666f\u4fee\u6539 \u6539\u53d8\u89c6\u9891\u80cc\u666f\u6216\u6dfb\u52a0\u65b0\u5143\u7d20 \ud83c\udfaf \u5bf9\u8c61\u7f16\u8f91 \u4fee\u6539\u6216\u66ff\u6362\u89c6\u9891\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61 \u2728 \u7279\u6548\u6dfb\u52a0 \u4e3a\u89c6\u9891\u6dfb\u52a0\u5404\u79cd\u89c6\u89c9\u7279\u6548 \ud83d\udcc8 \u8d28\u91cf\u589e\u5f3a \u63d0\u5347\u89c6\u9891\u6e05\u6670\u5ea6\u548c\u8d28\u91cf","title":"\ud83c\udfa8 \u7f16\u8f91\u80fd\u529b"},{"location":"Wan2.1/VACE-1.3B/doc/#_5","text":"","title":"\ud83d\ude80 \u90e8\u7f72\u65b9\u5f0f"},{"location":"Wan2.1/VACE-1.3B/doc/#_6","text":"\ud83d\udca1 \u90e8\u7f72\u914d\u7f6e \u90e8\u7f72\u67b6\u6784 : ECS\u5355\u673a\u90e8\u7f72\u6216ACS\u90e8\u7f72 \u5185\u5b58\u9700\u6c42 : \u663e\u5b58\u9700\u6c42\u4f4e\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883 \u6269\u5c55\u6027 : \u652f\u6301\u5355\u673a\u548c\u96c6\u7fa4\u90e8\u7f72","title":"\ud83c\udfd7\ufe0f \u90e8\u7f72\u67b6\u6784\uff08\u63a8\u8350\uff09"},{"location":"Wan2.1/VACE-1.3B/doc/#_7","text":"","title":"\ud83c\udfaf \u4f7f\u7528\u6307\u5357"},{"location":"Wan2.1/VACE-1.3B/doc/#comfyui","text":"### \ud83d\udd27 \u64cd\u4f5c\u6b65\u9aa4 **1. \u8bbf\u95ee\u754c\u9762** - \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **2. \u52a0\u8f7d\u5de5\u4f5c\u6d41** - \u4f7f\u7528\u5de5\u4f5c\u6d41 `vace.json` - \u5c06\u5de5\u4f5c\u6d41\u5bfc\u5165\u5230 ComfyUI \u4e2d **3. \u751f\u6210\u5185\u5bb9** - \u8f93\u5165\u60f3\u8981\u751f\u6210\u7684\u89c6\u9891\u6216\u56fe\u7247\u7684\u63d0\u793a\u8bcd - \u70b9\u51fb\u751f\u6210\u5f00\u59cb\u5904\u7406","title":"\ud83c\udf10 ComfyUI \u4f7f\u7528"},{"location":"Wan2.1/VACE-1.3B/doc/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528"},{"location":"Wan2.1/VACE-1.3B/doc/#_8","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"Wan2.1/VACE-1.3B/doc/#python","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" UNET_MODEL = \"wan21_vace_1_3_b.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"example.png\" PROMPT = \"\u5973\u5b69\u5077\u5077\u54ac\u4e00\u53e3\u82f9\u679c\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=20, cfg=4, frames=49, width=480, height=480): \"\"\"\ud83c\udfac VACE\u89c6\u9891\u7f16\u8f91\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn_fast\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4VACE\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884cVACE\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac \u5f00\u59cbVACE\u89c6\u9891\u7f16\u8f91\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u7f16\u8f91\u63d0\u793a: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709 example.png \u6587\u4ef6\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 4, 49, 480, 480) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b"},{"location":"Wan2.1/VACE-1.3B/doc/#_9","text":"","title":"\ud83d\udca1 \u6700\u4f73\u5b9e\u8df5"},{"location":"Wan2.1/VACE-1.3B/doc/#_10","text":"\ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a \u63d0\u793a\u8bcd\u4e0d\u591f\u8be6\u7ec6 \uff1a\u589e\u52a0\u66f4\u5177\u4f53\u7684\u63cf\u8ff0 \u53c2\u6570\u8bbe\u7f6e\u4e0d\u5f53 \uff1a\u8c03\u6574\u6b65\u6570\u548cCFG\u503c \u8f93\u5165\u56fe\u7247\u8d28\u91cf\u4f4e \uff1a\u4f7f\u7528\u66f4\u9ad8\u8d28\u91cf\u7684\u6e90\u56fe\u7247 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 \uff1a\u964d\u4f4e\u5206\u8fa8\u7387\u6216\u4f7f\u7528medvram\u6a21\u5f0f \u52a0\u8f7d\u7f13\u6162 \uff1a\u6a21\u578b\u8f83\u5927\uff0c\u9700\u8981\u8010\u5fc3\u7b49\u5f85 \u751f\u6210\u7f13\u6162 \uff1a\u51cf\u5c11\u6b65\u6570\u6216\u4f7f\u7528\u66f4\u5feb\u7684\u91c7\u6837\u5668 \ud83d\udd27 \u517c\u5bb9\u6027\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a ComfyUI\u7248\u672c \uff1a\u786e\u4fdd\u4f7f\u7528\u652f\u6301VACE\u7684\u6700\u65b0\u7248\u672c \u63d2\u4ef6\u517c\u5bb9\u6027 \uff1a\u67d0\u4e9b\u63d2\u4ef6\u53ef\u80fd\u4e0eVACE\u4e0d\u517c\u5bb9 \u53c2\u6570\u8303\u56f4 \uff1a\u6ce8\u610fVACE\u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4","title":"\ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848"},{"location":"Wan2.1/VACE-1.3B/doc/#_11","text":"","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/","text":"\ud83c\udfac Wan2.1-VACE-1.3B Complete Video Editing Guide Intelligent Video Content Editing with VACE (Video Adaptive Content Editing) Technology \u26a1 1.3B Parameters \ud83c\udfaf Professional Editing \ud83d\ude80 Lightweight \ud83c\udf1f Model Introduction **Wan2.1-VACE-1.3B** is a lightweight model specifically designed for video content editing and processing. VACE (Video Adaptive Content Editing) technology focuses on intelligent editing of existing videos, including style conversion, content modification, scene enhancement, and other functions. This model maintains temporal consistency in videos while enabling precise editing of video content based on text instructions. \u2728 Core Features \u26a1 Lightweight Architecture 1.3B parameters for optimal performance in resource-constrained environments \ud83c\udfa5 Video Editing Specialized Specifically optimized for video editing tasks \u23f0 Temporal Consistency Maintains coherence and consistency between video frames \ud83d\udee0\ufe0f Multiple Editing Functions Supports style conversion, content modification, scene enhancement, etc. \ud83d\ude80 Fast Processing Lightweight architecture with fast processing speed \ud83c\udf10 Multi-language Support Supports Chinese and English editing instructions \ud83d\udcca Technical Specifications Specification Details Model Type Video Editing Parameter Scale 1.3B Quantization FP8 quantized version Deployment Architecture ECS single-machine deployment/ACS cluster deployment Supported Resolution 480P Input Format Common video formats like MP4, AVI, MOV Output Format MP4 (H.264 encoding) \ud83c\udfa8 Editing Capabilities \ud83c\udfad Style Conversion Convert videos to different artistic styles \ud83c\udf08 Color Adjustment Adjust video tone, saturation, brightness \ud83c\udfde\ufe0f Scene Modification Change video background or add new elements \ud83c\udfaf Object Editing Modify or replace specific objects in videos \u2728 Effect Addition Add various visual effects to videos \ud83d\udcc8 Quality Enhancement Improve video clarity and quality \ud83d\ude80 Deployment Methods \ud83c\udfd7\ufe0f Deployment Architecture (Recommended) \ud83d\udca1 Deployment Configuration Deployment Architecture : ECS single-machine deployment or ACS deployment Memory Requirements : Low VRAM requirements, suitable for resource-constrained environments Scalability : Supports both single-machine and cluster deployment \ud83c\udfaf Usage Guide \ud83c\udf10 ComfyUI Usage ### \ud83d\udd27 Step-by-Step Instructions **1. Access Interface** - Click the access link at the service instance **2. Load Workflow** - Use workflow `vace.json` - Import the workflow into ComfyUI **3. Generate Content** - Input prompts for the video or image you want to generate - Click generate to start processing \ud83d\udd0c API Integration \ud83d\udd11 Authentication Setup \ud83c\udfab Get Token Click the button in the upper right corner, open the bottom panel, and get the token \ud83c\udf10 Get Server Address For COMFYUI_SERVER acquisition, refer to: \ud83d\udcbb Python Code Example \ud83d\udccb Click to expand API call Python code import requests, json, uuid, time, random, os # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" UNET_MODEL = \"wan21_vace_1_3_b.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf Preset parameters IMAGE_PATH = \"example.png\" PROMPT = \"A girl secretly takes a bite of an apple\" NEG_PROMPT = \"vivid colors, overexposed, static, blurry details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression artifacts, ugly, incomplete\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 Upload image to ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"Image file does not exist: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"Upload failed, status code: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"No filename in upload response: {result}\") return result['name'] except Exception as e: raise Exception(f\"Image upload failed: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=20, cfg=4, frames=49, width=480, height=480): \"\"\"\ud83c\udfac VACE video editing\"\"\" print(\"\ud83d\udce4 Uploading image...\") image_name = self.upload_image(image_path) print(f\"\u2705 Image uploaded successfully: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn_fast\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting VACE workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_output.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main function - Execute VACE video editing task\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac Starting VACE video editing task...\") print(f\"\ud83d\udcf7 Input image: {IMAGE_PATH}\") print(f\"\ud83d\udcdd Edit prompt: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c Image file does not exist: {IMAGE_PATH}\") print(\"Please ensure there is an example.png file in the current directory\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 4, 49, 480, 480) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 Best Practices \ud83c\udfaf Prompt Optimization Use clear and specific descriptions Include style and mood keywords Specify temporal elements for consistency Avoid overly complex descriptions \u2699\ufe0f Parameter Tuning Start with default settings (20 steps, CFG 4) Adjust frame count based on content Monitor VRAM usage for optimal performance Use recommended resolution 480\u00d7480 \ud83c\udfac Video Quality Use high-quality input images Ensure proper aspect ratios Test with shorter clips first Pay attention to temporal coherence \ud83d\udd27 Troubleshooting & Solutions \ud83c\udfa8 Generation Quality Issues Solutions: Insufficient prompt details : Add more specific descriptions Improper parameter settings : Adjust steps and CFG values Low input image quality : Use higher quality source images \u26a1 Performance Issues Optimization Tips: Insufficient VRAM : Reduce resolution or use medvram mode Slow loading : Model is large, please be patient Slow generation : Reduce steps or use faster samplers \ud83d\udd27 Compatibility Issues Important Notes: ComfyUI version : Ensure using latest version that supports VACE Plugin compatibility : Some plugins may not be compatible with VACE Parameter ranges : Pay attention to VACE's recommended parameter ranges \ud83d\udcda Related Resources \ud83d\udcc4 Technical Documentation VACE Technical Paper \u2192 Learn about VACE technology's theoretical foundation and implementation principles \ud83d\udee0\ufe0f Tools & Guides ComfyUI Video Editing Guide \u2192 Detailed instructions for ComfyUI video editing features \ud83c\udfa5 Best Practices Video Editing Best Practices \u2192 Professional video editing tips and experience sharing \ud83d\udd0c Plugin Documentation VACE Plugin Documentation \u2192 VACE ComfyUI plugin installation and usage instructions \ud83c\udfac Start creating amazing videos with VACE technology! | Intelligent video editing for limitless creativity","title":"Index en"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#model-introduction","text":"**Wan2.1-VACE-1.3B** is a lightweight model specifically designed for video content editing and processing. VACE (Video Adaptive Content Editing) technology focuses on intelligent editing of existing videos, including style conversion, content modification, scene enhancement, and other functions. This model maintains temporal consistency in videos while enabling precise editing of video content based on text instructions.","title":"\ud83c\udf1f Model Introduction"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#core-features","text":"","title":"\u2728 Core Features"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#technical-specifications","text":"Specification Details Model Type Video Editing Parameter Scale 1.3B Quantization FP8 quantized version Deployment Architecture ECS single-machine deployment/ACS cluster deployment Supported Resolution 480P Input Format Common video formats like MP4, AVI, MOV Output Format MP4 (H.264 encoding)","title":"\ud83d\udcca Technical Specifications"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#editing-capabilities","text":"\ud83c\udfad Style Conversion Convert videos to different artistic styles \ud83c\udf08 Color Adjustment Adjust video tone, saturation, brightness \ud83c\udfde\ufe0f Scene Modification Change video background or add new elements \ud83c\udfaf Object Editing Modify or replace specific objects in videos \u2728 Effect Addition Add various visual effects to videos \ud83d\udcc8 Quality Enhancement Improve video clarity and quality","title":"\ud83c\udfa8 Editing Capabilities"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#deployment-methods","text":"","title":"\ud83d\ude80 Deployment Methods"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#deployment-architecture-recommended","text":"\ud83d\udca1 Deployment Configuration Deployment Architecture : ECS single-machine deployment or ACS deployment Memory Requirements : Low VRAM requirements, suitable for resource-constrained environments Scalability : Supports both single-machine and cluster deployment","title":"\ud83c\udfd7\ufe0f Deployment Architecture (Recommended)"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#usage-guide","text":"","title":"\ud83c\udfaf Usage Guide"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#comfyui-usage","text":"### \ud83d\udd27 Step-by-Step Instructions **1. Access Interface** - Click the access link at the service instance **2. Load Workflow** - Use workflow `vace.json` - Import the workflow into ComfyUI **3. Generate Content** - Input prompts for the video or image you want to generate - Click generate to start processing","title":"\ud83c\udf10 ComfyUI Usage"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#api-integration","text":"","title":"\ud83d\udd0c API Integration"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#authentication-setup","text":"","title":"\ud83d\udd11 Authentication Setup"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#python-code-example","text":"\ud83d\udccb Click to expand API call Python code import requests, json, uuid, time, random, os # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" UNET_MODEL = \"wan21_vace_1_3_b.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf Preset parameters IMAGE_PATH = \"example.png\" PROMPT = \"A girl secretly takes a bite of an apple\" NEG_PROMPT = \"vivid colors, overexposed, static, blurry details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression artifacts, ugly, incomplete\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 Upload image to ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"Image file does not exist: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"Upload failed, status code: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"No filename in upload response: {result}\") return result['name'] except Exception as e: raise Exception(f\"Image upload failed: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=20, cfg=4, frames=49, width=480, height=480): \"\"\"\ud83c\udfac VACE video editing\"\"\" print(\"\ud83d\udce4 Uploading image...\") image_name = self.upload_image(image_path) print(f\"\u2705 Image uploaded successfully: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"fp8_e4m3fn_fast\"}, \"class_type\": \"UNETLoader\"}, \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting VACE workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_output.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main function - Execute VACE video editing task\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac Starting VACE video editing task...\") print(f\"\ud83d\udcf7 Input image: {IMAGE_PATH}\") print(f\"\ud83d\udcdd Edit prompt: {PROMPT}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c Image file does not exist: {IMAGE_PATH}\") print(\"Please ensure there is an example.png file in the current directory\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 4, 49, 480, 480) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python Code Example"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#best-practices","text":"","title":"\ud83d\udca1 Best Practices"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#troubleshooting-solutions","text":"\ud83c\udfa8 Generation Quality Issues Solutions: Insufficient prompt details : Add more specific descriptions Improper parameter settings : Adjust steps and CFG values Low input image quality : Use higher quality source images \u26a1 Performance Issues Optimization Tips: Insufficient VRAM : Reduce resolution or use medvram mode Slow loading : Model is large, please be patient Slow generation : Reduce steps or use faster samplers \ud83d\udd27 Compatibility Issues Important Notes: ComfyUI version : Ensure using latest version that supports VACE Plugin compatibility : Some plugins may not be compatible with VACE Parameter ranges : Pay attention to VACE's recommended parameter ranges","title":"\ud83d\udd27 Troubleshooting &amp; Solutions"},{"location":"Wan2.1/VACE-1.3B/doc/index-en/#related-resources","text":"","title":"\ud83d\udcda Related Resources"},{"location":"Wan2.1/VACE-14B/doc/","text":"\ud83c\udfac Wan2.1-VACE-14B-Q6_K \u89c6\u9891\u7f16\u8f91\u5b8c\u6574\u6307\u5357 \u667a\u80fd\u89c6\u9891\u5185\u5bb9\u7f16\u8f91\u6280\u672f\uff0cVACE (Video Adaptive Content Editing) \u4e13\u4e1a\u7ea7\u89c6\u9891\u7f16\u8f91 \u26a1 14B\u53c2\u6570 \ud83c\udfaf Q6_K\u91cf\u5316 \ud83d\ude80 \u9ad8\u6548\u90e8\u7f72 \ud83c\udf1f \u6a21\u578b\u7b80\u4ecb **Wan2.1-VACE-14B-Q6_K** \u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u89c6\u9891\u5185\u5bb9\u7f16\u8f91\u548c\u5904\u7406\u7684\u5927\u89c4\u6a21\u91cf\u5316\u6a21\u578b\u3002VACE\uff08Video Adaptive Content Editing\uff09\u6280\u672f\u4e13\u6ce8\u4e8e\u5bf9\u73b0\u6709\u89c6\u9891\u8fdb\u884c\u667a\u80fd\u7f16\u8f91\uff0c\u5305\u62ec\u98ce\u683c\u8f6c\u6362\u3001\u5185\u5bb9\u4fee\u6539\u3001\u573a\u666f\u589e\u5f3a\u7b49\u529f\u80fd\u3002\u8be5\u6a21\u578b\u91c7\u7528Q6_K\u91cf\u5316\u6280\u672f\uff0c\u5728\u4fdd\u630114B\u53c2\u6570\u5f3a\u5927\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \u26a1 Q6_K\u91cf\u5316\u4f18\u5316 14B\u53c2\u6570\uff0cQ6_K\u91cf\u5316\uff0c\u5185\u5b58\u5360\u7528\u964d\u4f4e\u7ea640% \ud83c\udfa5 \u89c6\u9891\u7f16\u8f91\u4e13\u7528 \u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4f18\u5316\uff0c\u652f\u6301\u590d\u6742\u7f16\u8f91\u64cd\u4f5c \u23f0 \u65f6\u5e8f\u4e00\u81f4\u6027 \u4fdd\u6301\u89c6\u9891\u5e27\u95f4\u7684\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027 \ud83d\udee0\ufe0f \u591a\u79cd\u7f16\u8f91\u529f\u80fd \u652f\u6301\u98ce\u683c\u8f6c\u6362\u3001\u5185\u5bb9\u4fee\u6539\u3001\u573a\u666f\u589e\u5f3a\u7b49 \ud83d\ude80 \u9ad8\u6548\u90e8\u7f72 \u91cf\u5316\u4f18\u5316\uff0c\u66f4\u4f4e\u7684\u786c\u4ef6\u8981\u6c42\uff0c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6 \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u6587\u548c\u82f1\u6587\u7f16\u8f91\u6307\u4ee4 \ud83d\udcca \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u89c6\u9891\u7f16\u8f91\uff08Video Editing\uff09 \u53c2\u6570\u89c4\u6a21 14B \u91cf\u5316\u65b9\u5f0f Q6_K\u91cf\u5316\uff08GGUF\u683c\u5f0f\uff09 \u6a21\u578b\u6587\u4ef6 Wan2.1_14B_VACE-Q6_K.gguf \u90e8\u7f72\u67b6\u6784 ECS\u5355\u673a\u90e8\u7f72/ACS\u96c6\u7fa4\u90e8\u7f72 \u5185\u5b58\u9700\u6c42 \u7ea68-12GB\u663e\u5b58\uff08\u76f8\u6bd4\u539f\u7248\u964d\u4f4e40%\uff09 \u652f\u6301\u5206\u8fa8\u7387 720P/1080P \u8f93\u5165\u683c\u5f0f MP4\u3001AVI\u3001MOV\u7b49\u5e38\u89c1\u89c6\u9891\u683c\u5f0f \u8f93\u51fa\u683c\u5f0f MP4\uff08H.264\u7f16\u7801\uff09 \ud83c\udfa8 \u7f16\u8f91\u80fd\u529b \ud83c\udfad \u98ce\u683c\u8f6c\u6362 \u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u4e0d\u540c\u827a\u672f\u98ce\u683c \ud83c\udf08 \u8272\u5f69\u8c03\u6574 \u8c03\u6574\u89c6\u9891\u7684\u8272\u8c03\u3001\u9971\u548c\u5ea6\u3001\u4eae\u5ea6 \ud83c\udfde\ufe0f \u573a\u666f\u4fee\u6539 \u6539\u53d8\u89c6\u9891\u80cc\u666f\u6216\u6dfb\u52a0\u65b0\u5143\u7d20 \ud83c\udfaf \u5bf9\u8c61\u7f16\u8f91 \u4fee\u6539\u6216\u66ff\u6362\u89c6\u9891\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61 \u2728 \u7279\u6548\u6dfb\u52a0 \u4e3a\u89c6\u9891\u6dfb\u52a0\u5404\u79cd\u89c6\u89c9\u7279\u6548 \ud83d\udcc8 \u8d28\u91cf\u589e\u5f3a \u63d0\u5347\u89c6\u9891\u6e05\u6670\u5ea6\u548c\u8d28\u91cf \ud83d\ude80 \u90e8\u7f72\u65b9\u5f0f \ud83c\udfd7\ufe0f \u90e8\u7f72\u67b6\u6784\uff08\u63a8\u8350\uff09 \ud83d\udca1 \u90e8\u7f72\u914d\u7f6e \u90e8\u7f72\u67b6\u6784 : ECS\u5355\u673a\u90e8\u7f72\uff08\u63a8\u8350\uff09\u6216ACS\u96c6\u7fa4\u90e8\u7f72 \u5185\u5b58\u9700\u6c42 : 8-12GB\u663e\u5b58\uff0c\u76f8\u6bd4\u539f\u7248\u964d\u4f4e40% \u6269\u5c55\u6027 : \u652f\u6301\u5355\u673a\u548c\u96c6\u7fa4\u90e8\u7f72\uff0c\u91cf\u5316\u7248\u672c\u66f4\u9002\u5408\u5355\u673a\u90e8\u7f72 \ud83d\udce6 Q6_K\u91cf\u5316\u4f18\u52bf \ud83d\udcbe \u5185\u5b58\u4f18\u5316 \u663e\u5b58\u5360\u7528\u964d\u4f4e\u7ea640%\uff0c\u66f4\u9002\u5408\u4e2d\u7b49\u914d\u7f6e\u786c\u4ef6 \u26a1 \u63a8\u7406\u52a0\u901f \u91cf\u5316\u4f18\u5316\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u51cf\u5c11\u7b49\u5f85\u65f6\u95f4 \ud83c\udfaf \u8d28\u91cf\u4fdd\u6301 Q6_K\u91cf\u5316\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\uff0c\u6027\u80fd\u635f\u5931\u6700\u5c0f \ud83d\udcb0 \u6210\u672c\u8282\u7ea6 \u964d\u4f4e\u786c\u4ef6\u8981\u6c42\uff0c\u8282\u7ea6\u90e8\u7f72\u548c\u8fd0\u884c\u6210\u672c \ud83c\udfaf \u4f7f\u7528\u6307\u5357 \ud83c\udf10 ComfyUI \u4f7f\u7528 ### \ud83d\udd27 \u64cd\u4f5c\u6b65\u9aa4 **1. \u8bbf\u95ee\u754c\u9762** - \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **2. \u52a0\u8f7d\u5de5\u4f5c\u6d41** - \u4f7f\u7528\u5de5\u4f5c\u6d41 `vace14b.json` - \u5c06\u5de5\u4f5c\u6d41\u5bfc\u5165\u5230 ComfyUI \u4e2d **3. \u751f\u6210\u5185\u5bb9** - \u8f93\u5165\u60f3\u8981\u751f\u6210\u7684\u89c6\u9891\u6216\u56fe\u7247\u7684\u63d0\u793a\u8bcd - \u70b9\u51fb\u751f\u6210\u5f00\u59cb\u5904\u7406 \ud83d\udd0c API \u8c03\u7528 \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83c\udfab \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\uff0c\u83b7\u53d6token \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 COMFYUI_SERVER \u7684\u83b7\u53d6\u53ef\u53c2\u8003 \ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b \ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" UNET_MODEL = \"Wan2.1_14B_VACE-Q6_K.gguf\" # Q6_K\u91cf\u5316\u7248\u672c CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"example.png\" PROMPT = \"\u5973\u5b69\u5077\u5077\u54ac\u4e00\u53e3\u82f9\u679c\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=25, cfg=5, frames=49, width=720, height=720): \"\"\"\ud83c\udfac VACE\u89c6\u9891\u7f16\u8f91 - Q6_K\u91cf\u5316\u7248\u672c\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\"}, \"class_type\": \"UNETLoader\"}, # Q6_K\u91cf\u5316\u7248\u672c \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_Q6K_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4VACE Q6_K\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_q6k_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884cVACE Q6_K\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac \u5f00\u59cbVACE Q6_K\u89c6\u9891\u7f16\u8f91\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u7f16\u8f91\u63d0\u793a: {PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u6a21\u578b: {UNET_MODEL}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709 example.png \u6587\u4ef6\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 25, 5, 49, 720, 720) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_q6k_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 \u6700\u4f73\u5b9e\u8df5 \ud83c\udfaf \u63d0\u793a\u8bcd\u4f18\u5316 \u4f7f\u7528\u6e05\u6670\u5177\u4f53\u7684\u63cf\u8ff0 \u5305\u542b\u98ce\u683c\u548c\u60c5\u7eea\u5173\u952e\u8bcd \u6307\u5b9a\u65f6\u5e8f\u5143\u7d20\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027 Q6_K\u91cf\u5316\u7248\u672c\u4fdd\u6301\u5f3a\u5927\u7406\u89e3\u80fd\u529b \u2699\ufe0f \u53c2\u6570\u8c03\u4f18 \u4ece\u9ed8\u8ba4\u8bbe\u7f6e\u5f00\u59cb\uff0825\u6b65\uff0cCFG 5\uff09 \u652f\u6301\u66f4\u9ad8\u5206\u8fa8\u7387\uff08720P/1080P\uff09 \u91cf\u5316\u7248\u672c\u63a8\u7406\u66f4\u5feb\uff0c\u53ef\u9002\u5f53\u589e\u52a0\u6b65\u6570 \u76d1\u63a7\u663e\u5b58\u4f7f\u7528\uff0cQ6_K\u7248\u672c\u66f4\u8282\u7701 \ud83c\udfac \u89c6\u9891\u8d28\u91cf \u4f7f\u7528\u9ad8\u8d28\u91cf\u8f93\u5165\u56fe\u7247 \u786e\u4fdd\u6b63\u786e\u7684\u5bbd\u9ad8\u6bd4 \u5148\u7528\u77ed\u7247\u6bb5\u6d4b\u8bd5 Q6_K\u91cf\u5316\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa \ud83d\udcbe \u91cf\u5316\u4f18\u52bf \u663e\u5b58\u5360\u7528\u964d\u4f4e\u7ea640% \u63a8\u7406\u901f\u5ea6\u63d0\u5347 \u9002\u5408\u4e2d\u7b49\u914d\u7f6e\u786c\u4ef6 \u6210\u672c\u6548\u76ca\u66f4\u9ad8 \ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848 \ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a \u63d0\u793a\u8bcd\u4e0d\u591f\u8be6\u7ec6 \uff1a\u589e\u52a0\u66f4\u5177\u4f53\u7684\u63cf\u8ff0\uff0cQ6_K\u7248\u672c\u4fdd\u6301\u5f3a\u5927\u7406\u89e3\u80fd\u529b \u53c2\u6570\u8bbe\u7f6e\u4e0d\u5f53 \uff1a\u8c03\u6574\u6b65\u6570\u548cCFG\u503c\uff0c\u63a8\u835025\u6b65\uff0cCFG 5 \u8f93\u5165\u56fe\u7247\u8d28\u91cf\u4f4e \uff1a\u4f7f\u7528\u66f4\u9ad8\u8d28\u91cf\u7684\u6e90\u56fe\u7247 \u91cf\u5316\u5f71\u54cd \uff1aQ6_K\u91cf\u5316\u5bf9\u8d28\u91cf\u5f71\u54cd\u5f88\u5c0f\uff0c\u5982\u6709\u95ee\u9898\u53ef\u5c1d\u8bd5\u8c03\u6574\u53c2\u6570 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 \uff1aQ6_K\u7248\u672c\u663e\u5b58\u9700\u6c42\u964d\u4f4e40%\uff0c8-12GB\u5373\u53ef \u52a0\u8f7d\u7f13\u6162 \uff1aGGUF\u683c\u5f0f\u52a0\u8f7d\u66f4\u5feb\uff0c\u4f46\u4ecd\u9700\u8010\u5fc3\u7b49\u5f85 \u63a8\u7406\u901f\u5ea6 \uff1a\u91cf\u5316\u7248\u672c\u63a8\u7406\u66f4\u5feb\uff0c\u53ef\u9002\u5f53\u589e\u52a0\u6b65\u6570\u63d0\u5347\u8d28\u91cf \u786c\u4ef6\u914d\u7f6e \uff1aQ6_K\u7248\u672c\u5bf9\u786c\u4ef6\u8981\u6c42\u66f4\u4f4e\uff0c\u9002\u5408\u4e2d\u7b49\u914d\u7f6e \ud83d\udd27 \u517c\u5bb9\u6027\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a GGUF\u683c\u5f0f\u652f\u6301 \uff1a\u786e\u4fddComfyUI\u652f\u6301GGUF\u683c\u5f0f\u6a21\u578b \u63d2\u4ef6\u517c\u5bb9\u6027 \uff1a\u67d0\u4e9b\u63d2\u4ef6\u53ef\u80fd\u4e0e\u91cf\u5316\u7248\u672c\u4e0d\u517c\u5bb9 \u53c2\u6570\u8303\u56f4 \uff1a\u6ce8\u610fQ6_K\u7248\u672c\u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4 \u6a21\u578b\u8def\u5f84 \uff1a\u786e\u4fdd\u6b63\u786e\u6307\u5b9aGGUF\u6587\u4ef6\u8def\u5f84 \ud83d\udcda \u76f8\u5173\u8d44\u6e90 \ud83d\udcc4 \u6280\u672f\u6587\u6863 VACE\u6280\u672f\u8bba\u6587 \u2192 \u4e86\u89e3 VACE \u6280\u672f\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u73b0\u539f\u7406 \ud83d\udee0\ufe0f \u5de5\u5177\u4e0e\u6307\u5357 ComfyUI\u89c6\u9891\u7f16\u8f91\u6307\u5357 \u2192 ComfyUI \u89c6\u9891\u7f16\u8f91\u529f\u80fd\u7684\u8be6\u7ec6\u4f7f\u7528\u8bf4\u660e \ud83c\udfa5 \u6700\u4f73\u5b9e\u8df5 \u89c6\u9891\u7f16\u8f91\u6700\u4f73\u5b9e\u8df5 \u2192 \u4e13\u4e1a\u89c6\u9891\u7f16\u8f91\u7684\u6280\u5de7\u548c\u7ecf\u9a8c\u5206\u4eab \ud83d\udd0c \u63d2\u4ef6\u6587\u6863 VACE\u63d2\u4ef6\u6587\u6863 \u2192 VACE ComfyUI \u63d2\u4ef6\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u8bf4\u660e \ud83c\udfac \u5f00\u59cb\u4f7f\u7528 VACE-14B-Q6_K \u91cf\u5316\u7248\u672c\u521b\u4f5c\u7cbe\u5f69\u89c6\u9891\u5427\uff01 | \u667a\u80fd\u89c6\u9891\u7f16\u8f91\uff0c\u9ad8\u6548\u8282\u80fd\uff0c\u8ba9\u521b\u610f\u65e0\u9650\u5ef6\u4f38","title":"Index"},{"location":"Wan2.1/VACE-14B/doc/#_1","text":"**Wan2.1-VACE-14B-Q6_K** \u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u89c6\u9891\u5185\u5bb9\u7f16\u8f91\u548c\u5904\u7406\u7684\u5927\u89c4\u6a21\u91cf\u5316\u6a21\u578b\u3002VACE\uff08Video Adaptive Content Editing\uff09\u6280\u672f\u4e13\u6ce8\u4e8e\u5bf9\u73b0\u6709\u89c6\u9891\u8fdb\u884c\u667a\u80fd\u7f16\u8f91\uff0c\u5305\u62ec\u98ce\u683c\u8f6c\u6362\u3001\u5185\u5bb9\u4fee\u6539\u3001\u573a\u666f\u589e\u5f3a\u7b49\u529f\u80fd\u3002\u8be5\u6a21\u578b\u91c7\u7528Q6_K\u91cf\u5316\u6280\u672f\uff0c\u5728\u4fdd\u630114B\u53c2\u6570\u5f3a\u5927\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u3002","title":"\ud83c\udf1f \u6a21\u578b\u7b80\u4ecb"},{"location":"Wan2.1/VACE-14B/doc/#_2","text":"","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"Wan2.1/VACE-14B/doc/#_3","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u89c6\u9891\u7f16\u8f91\uff08Video Editing\uff09 \u53c2\u6570\u89c4\u6a21 14B \u91cf\u5316\u65b9\u5f0f Q6_K\u91cf\u5316\uff08GGUF\u683c\u5f0f\uff09 \u6a21\u578b\u6587\u4ef6 Wan2.1_14B_VACE-Q6_K.gguf \u90e8\u7f72\u67b6\u6784 ECS\u5355\u673a\u90e8\u7f72/ACS\u96c6\u7fa4\u90e8\u7f72 \u5185\u5b58\u9700\u6c42 \u7ea68-12GB\u663e\u5b58\uff08\u76f8\u6bd4\u539f\u7248\u964d\u4f4e40%\uff09 \u652f\u6301\u5206\u8fa8\u7387 720P/1080P \u8f93\u5165\u683c\u5f0f MP4\u3001AVI\u3001MOV\u7b49\u5e38\u89c1\u89c6\u9891\u683c\u5f0f \u8f93\u51fa\u683c\u5f0f MP4\uff08H.264\u7f16\u7801\uff09","title":"\ud83d\udcca \u6280\u672f\u89c4\u683c"},{"location":"Wan2.1/VACE-14B/doc/#_4","text":"\ud83c\udfad \u98ce\u683c\u8f6c\u6362 \u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u4e0d\u540c\u827a\u672f\u98ce\u683c \ud83c\udf08 \u8272\u5f69\u8c03\u6574 \u8c03\u6574\u89c6\u9891\u7684\u8272\u8c03\u3001\u9971\u548c\u5ea6\u3001\u4eae\u5ea6 \ud83c\udfde\ufe0f \u573a\u666f\u4fee\u6539 \u6539\u53d8\u89c6\u9891\u80cc\u666f\u6216\u6dfb\u52a0\u65b0\u5143\u7d20 \ud83c\udfaf \u5bf9\u8c61\u7f16\u8f91 \u4fee\u6539\u6216\u66ff\u6362\u89c6\u9891\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61 \u2728 \u7279\u6548\u6dfb\u52a0 \u4e3a\u89c6\u9891\u6dfb\u52a0\u5404\u79cd\u89c6\u89c9\u7279\u6548 \ud83d\udcc8 \u8d28\u91cf\u589e\u5f3a \u63d0\u5347\u89c6\u9891\u6e05\u6670\u5ea6\u548c\u8d28\u91cf","title":"\ud83c\udfa8 \u7f16\u8f91\u80fd\u529b"},{"location":"Wan2.1/VACE-14B/doc/#_5","text":"","title":"\ud83d\ude80 \u90e8\u7f72\u65b9\u5f0f"},{"location":"Wan2.1/VACE-14B/doc/#_6","text":"\ud83d\udca1 \u90e8\u7f72\u914d\u7f6e \u90e8\u7f72\u67b6\u6784 : ECS\u5355\u673a\u90e8\u7f72\uff08\u63a8\u8350\uff09\u6216ACS\u96c6\u7fa4\u90e8\u7f72 \u5185\u5b58\u9700\u6c42 : 8-12GB\u663e\u5b58\uff0c\u76f8\u6bd4\u539f\u7248\u964d\u4f4e40% \u6269\u5c55\u6027 : \u652f\u6301\u5355\u673a\u548c\u96c6\u7fa4\u90e8\u7f72\uff0c\u91cf\u5316\u7248\u672c\u66f4\u9002\u5408\u5355\u673a\u90e8\u7f72","title":"\ud83c\udfd7\ufe0f \u90e8\u7f72\u67b6\u6784\uff08\u63a8\u8350\uff09"},{"location":"Wan2.1/VACE-14B/doc/#q6_k","text":"","title":"\ud83d\udce6 Q6_K\u91cf\u5316\u4f18\u52bf"},{"location":"Wan2.1/VACE-14B/doc/#_7","text":"","title":"\ud83c\udfaf \u4f7f\u7528\u6307\u5357"},{"location":"Wan2.1/VACE-14B/doc/#comfyui","text":"### \ud83d\udd27 \u64cd\u4f5c\u6b65\u9aa4 **1. \u8bbf\u95ee\u754c\u9762** - \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5 **2. \u52a0\u8f7d\u5de5\u4f5c\u6d41** - \u4f7f\u7528\u5de5\u4f5c\u6d41 `vace14b.json` - \u5c06\u5de5\u4f5c\u6d41\u5bfc\u5165\u5230 ComfyUI \u4e2d **3. \u751f\u6210\u5185\u5bb9** - \u8f93\u5165\u60f3\u8981\u751f\u6210\u7684\u89c6\u9891\u6216\u56fe\u7247\u7684\u63d0\u793a\u8bcd - \u70b9\u51fb\u751f\u6210\u5f00\u59cb\u5904\u7406","title":"\ud83c\udf10 ComfyUI \u4f7f\u7528"},{"location":"Wan2.1/VACE-14B/doc/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528"},{"location":"Wan2.1/VACE-14B/doc/#_8","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"Wan2.1/VACE-14B/doc/#python","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00 API \u8c03\u7528 Python \u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER, COMFYUI_TOKEN = \"\u8f93\u5165\u60a8\u7684\u670d\u52a1\u5668\u5730\u5740\", \"\u8f93\u5165\u60a8\u7684token\" UNET_MODEL = \"Wan2.1_14B_VACE-Q6_K.gguf\" # Q6_K\u91cf\u5316\u7248\u672c CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"example.png\" PROMPT = \"\u5973\u5b69\u5077\u5077\u54ac\u4e00\u53e3\u82f9\u679c\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=25, cfg=5, frames=49, width=720, height=720): \"\"\"\ud83c\udfac VACE\u89c6\u9891\u7f16\u8f91 - Q6_K\u91cf\u5316\u7248\u672c\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\"}, \"class_type\": \"UNETLoader\"}, # Q6_K\u91cf\u5316\u7248\u672c \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_Q6K_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 \u63d0\u4ea4VACE Q6_K\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_q6k_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884cVACE Q6_K\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac \u5f00\u59cbVACE Q6_K\u89c6\u9891\u7f16\u8f91\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u7f16\u8f91\u63d0\u793a: {PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u6a21\u578b: {UNET_MODEL}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709 example.png \u6587\u4ef6\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 25, 5, 49, 720, 720) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_q6k_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python \u4ee3\u7801\u793a\u4f8b"},{"location":"Wan2.1/VACE-14B/doc/#_9","text":"","title":"\ud83d\udca1 \u6700\u4f73\u5b9e\u8df5"},{"location":"Wan2.1/VACE-14B/doc/#_10","text":"\ud83c\udfa8 \u751f\u6210\u8d28\u91cf\u95ee\u9898 \u89e3\u51b3\u65b9\u6848\uff1a \u63d0\u793a\u8bcd\u4e0d\u591f\u8be6\u7ec6 \uff1a\u589e\u52a0\u66f4\u5177\u4f53\u7684\u63cf\u8ff0\uff0cQ6_K\u7248\u672c\u4fdd\u6301\u5f3a\u5927\u7406\u89e3\u80fd\u529b \u53c2\u6570\u8bbe\u7f6e\u4e0d\u5f53 \uff1a\u8c03\u6574\u6b65\u6570\u548cCFG\u503c\uff0c\u63a8\u835025\u6b65\uff0cCFG 5 \u8f93\u5165\u56fe\u7247\u8d28\u91cf\u4f4e \uff1a\u4f7f\u7528\u66f4\u9ad8\u8d28\u91cf\u7684\u6e90\u56fe\u7247 \u91cf\u5316\u5f71\u54cd \uff1aQ6_K\u91cf\u5316\u5bf9\u8d28\u91cf\u5f71\u54cd\u5f88\u5c0f\uff0c\u5982\u6709\u95ee\u9898\u53ef\u5c1d\u8bd5\u8c03\u6574\u53c2\u6570 \u26a1 \u6027\u80fd\u95ee\u9898 \u4f18\u5316\u5efa\u8bae\uff1a \u663e\u5b58\u4e0d\u8db3 \uff1aQ6_K\u7248\u672c\u663e\u5b58\u9700\u6c42\u964d\u4f4e40%\uff0c8-12GB\u5373\u53ef \u52a0\u8f7d\u7f13\u6162 \uff1aGGUF\u683c\u5f0f\u52a0\u8f7d\u66f4\u5feb\uff0c\u4f46\u4ecd\u9700\u8010\u5fc3\u7b49\u5f85 \u63a8\u7406\u901f\u5ea6 \uff1a\u91cf\u5316\u7248\u672c\u63a8\u7406\u66f4\u5feb\uff0c\u53ef\u9002\u5f53\u589e\u52a0\u6b65\u6570\u63d0\u5347\u8d28\u91cf \u786c\u4ef6\u914d\u7f6e \uff1aQ6_K\u7248\u672c\u5bf9\u786c\u4ef6\u8981\u6c42\u66f4\u4f4e\uff0c\u9002\u5408\u4e2d\u7b49\u914d\u7f6e \ud83d\udd27 \u517c\u5bb9\u6027\u95ee\u9898 \u6ce8\u610f\u4e8b\u9879\uff1a GGUF\u683c\u5f0f\u652f\u6301 \uff1a\u786e\u4fddComfyUI\u652f\u6301GGUF\u683c\u5f0f\u6a21\u578b \u63d2\u4ef6\u517c\u5bb9\u6027 \uff1a\u67d0\u4e9b\u63d2\u4ef6\u53ef\u80fd\u4e0e\u91cf\u5316\u7248\u672c\u4e0d\u517c\u5bb9 \u53c2\u6570\u8303\u56f4 \uff1a\u6ce8\u610fQ6_K\u7248\u672c\u7684\u63a8\u8350\u53c2\u6570\u8303\u56f4 \u6a21\u578b\u8def\u5f84 \uff1a\u786e\u4fdd\u6b63\u786e\u6307\u5b9aGGUF\u6587\u4ef6\u8def\u5f84","title":"\ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848"},{"location":"Wan2.1/VACE-14B/doc/#_11","text":"","title":"\ud83d\udcda \u76f8\u5173\u8d44\u6e90"},{"location":"Wan2.1/VACE-14B/doc/index-en/","text":"\ud83c\udfac Wan2.1-VACE-14B-Q6_K Complete Video Editing Guide Intelligent Video Content Editing with VACE (Video Adaptive Content Editing) Technology \u26a1 14B Parameters \ud83c\udfaf Q6_K Quantized \ud83d\ude80 Efficient Deployment \ud83c\udf1f Model Introduction **Wan2.1-VACE-14B-Q6_K** is a large-scale quantized model specifically designed for video content editing and processing. VACE (Video Adaptive Content Editing) technology focuses on intelligent editing of existing videos, including style conversion, content modification, scene enhancement, and other functions. This model uses Q6_K quantization technology, which significantly reduces memory usage and computational resource requirements while maintaining the powerful performance of 14B parameters, making it more suitable for deployment in resource-constrained environments. \u2728 Core Features \u26a1 Q6_K Quantization Optimization 14B parameters with Q6_K quantization, reducing memory usage by ~40% \ud83c\udfa5 Video Editing Specialized Specifically optimized for video editing tasks with complex editing operations support \u23f0 Temporal Consistency Maintains coherence and consistency between video frames \ud83d\udee0\ufe0f Multiple Editing Functions Supports style conversion, content modification, scene enhancement, etc. \ud83d\ude80 Efficient Deployment Quantization optimization for lower hardware requirements and faster inference \ud83c\udf10 Multi-language Support Supports Chinese and English editing instructions \ud83d\udcca Technical Specifications Specification Details Model Type Video Editing Parameter Scale 14B Quantization Q6_K quantization (GGUF format) Model File Wan2.1_14B_VACE-Q6_K.gguf Deployment Architecture ECS single-machine deployment/ACS cluster deployment Memory Requirements ~8-12GB VRAM (40% reduction compared to original) Supported Resolution 720P/1080P Input Format Common video formats like MP4, AVI, MOV Output Format MP4 (H.264 encoding) \ud83c\udfa8 Editing Capabilities \ud83c\udfad Style Conversion Convert videos to different artistic styles \ud83c\udf08 Color Adjustment Adjust video tone, saturation, brightness \ud83c\udfde\ufe0f Scene Modification Change video background or add new elements \ud83c\udfaf Object Editing Modify or replace specific objects in videos \u2728 Effect Addition Add various visual effects to videos \ud83d\udcc8 Quality Enhancement Improve video clarity and quality \ud83d\ude80 Deployment Methods \ud83c\udfd7\ufe0f Deployment Architecture (Recommended) \ud83d\udca1 Deployment Configuration Deployment Architecture : ECS single-machine deployment (recommended) or ACS cluster deployment Memory Requirements : 8-12GB VRAM, 40% reduction compared to original Scalability : Supports both single-machine and cluster deployment, quantized version better suited for single-machine deployment \ud83d\udce6 Q6_K Quantization Advantages \ud83d\udcbe Memory Optimization VRAM usage reduced by ~40%, better suited for mid-range hardware \u26a1 Inference Acceleration Quantization optimization improves inference speed, reducing wait times \ud83c\udfaf Quality Preservation Q6_K quantization maintains high-quality output with minimal performance loss \ud83d\udcb0 Cost Savings Lower hardware requirements, reducing deployment and operational costs \ud83c\udfaf Usage Guide \ud83c\udf10 ComfyUI Usage ### \ud83d\udd27 Step-by-Step Instructions **1. Access Interface** - Click the access link at the service instance **2. Load Workflow** - Use workflow `vace14b.json` - Import the workflow into ComfyUI **3. Generate Content** - Input prompts for the video or image you want to generate - Click generate to start processing \ud83d\udd0c API Integration \ud83d\udd11 Authentication Setup \ud83c\udfab Get Token Click the button in the upper right corner, open the bottom panel, and get the token \ud83c\udf10 Get Server Address For COMFYUI_SERVER acquisition, refer to: \ud83d\udcbb Python Code Example \ud83d\udccb Click to expand API call Python code import requests, json, uuid, time, random, os # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" UNET_MODEL = \"Wan2.1_14B_VACE-Q6_K.gguf\" # Q6_K quantized version CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf Preset parameters IMAGE_PATH = \"example.png\" PROMPT = \"A girl secretly takes a bite of an apple\" NEG_PROMPT = \"vivid colors, overexposed, static, blurry details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression artifacts, ugly, incomplete\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 Upload image to ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"Image file does not exist: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"Upload failed, status code: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"No filename in upload response: {result}\") return result['name'] except Exception as e: raise Exception(f\"Image upload failed: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=25, cfg=5, frames=49, width=720, height=720): \"\"\"\ud83c\udfac VACE video editing - Q6_K quantized version\"\"\" print(\"\ud83d\udce4 Uploading image...\") image_name = self.upload_image(image_path) print(f\"\u2705 Image uploaded successfully: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\"}, \"class_type\": \"UNETLoader\"}, # Q6_K quantized version \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_Q6K_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting VACE Q6_K workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_q6k_output.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main function - Execute VACE Q6_K video editing task\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac Starting VACE Q6_K video editing task...\") print(f\"\ud83d\udcf7 Input image: {IMAGE_PATH}\") print(f\"\ud83d\udcdd Edit prompt: {PROMPT}\") print(f\"\ud83d\udd27 Using model: {UNET_MODEL}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c Image file does not exist: {IMAGE_PATH}\") print(\"Please ensure there is an example.png file in the current directory\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 25, 5, 49, 720, 720) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_q6k_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 Best Practices \ud83c\udfaf Prompt Optimization Use clear and specific descriptions Include style and mood keywords Specify temporal elements for consistency Q6_K quantized version maintains strong understanding capabilities \u2699\ufe0f Parameter Tuning Start with default settings (25 steps, CFG 5) Supports higher resolutions (720P/1080P) Quantized version enables faster inference, can increase steps appropriately Monitor VRAM usage, Q6_K version is more memory-efficient \ud83c\udfac Video Quality Use high-quality input images Ensure proper aspect ratios Test with shorter clips first Q6_K quantization maintains high-quality output \ud83d\udcbe Quantization Benefits VRAM usage reduced by ~40% Improved inference speed Suitable for mid-range hardware Better cost-effectiveness \ud83d\udd27 Troubleshooting & Solutions \ud83c\udfa8 Generation Quality Issues Solutions: Insufficient prompt details : Add more specific descriptions, Q6_K version maintains strong understanding capabilities Improper parameter settings : Adjust steps and CFG values, recommended 25 steps, CFG 5 Low input image quality : Use higher quality source images Quantization impact : Q6_K quantization has minimal quality impact, try adjusting parameters if issues occur \u26a1 Performance Issues Optimization Tips: Insufficient VRAM : Q6_K version requires 40% less VRAM, 8-12GB is sufficient Slow loading : GGUF format loads faster, but still requires patience Inference speed : Quantized version infers faster, can increase steps to improve quality Hardware configuration : Q6_K version has lower hardware requirements, suitable for mid-range configurations \ud83d\udd27 Compatibility Issues Important Notes: GGUF format support : Ensure ComfyUI supports GGUF format models Plugin compatibility : Some plugins may not be compatible with quantized versions Parameter ranges : Pay attention to Q6_K version's recommended parameter ranges Model path : Ensure correct GGUF file path specification \ud83d\udcda Related Resources \ud83d\udcc4 Technical Documentation VACE Technical Paper \u2192 Learn about VACE technology's theoretical foundation and implementation principles \ud83d\udee0\ufe0f Tools & Guides ComfyUI Video Editing Guide \u2192 Detailed instructions for ComfyUI video editing features \ud83c\udfa5 Best Practices Video Editing Best Practices \u2192 Professional video editing tips and experience sharing \ud83d\udd0c Plugin Documentation VACE Plugin Documentation \u2192 VACE ComfyUI plugin installation and usage instructions \ud83c\udfac Start creating amazing videos with VACE-14B-Q6_K quantized version! | Intelligent video editing, efficient and energy-saving, for limitless creativity","title":"Index en"},{"location":"Wan2.1/VACE-14B/doc/index-en/#model-introduction","text":"**Wan2.1-VACE-14B-Q6_K** is a large-scale quantized model specifically designed for video content editing and processing. VACE (Video Adaptive Content Editing) technology focuses on intelligent editing of existing videos, including style conversion, content modification, scene enhancement, and other functions. This model uses Q6_K quantization technology, which significantly reduces memory usage and computational resource requirements while maintaining the powerful performance of 14B parameters, making it more suitable for deployment in resource-constrained environments.","title":"\ud83c\udf1f Model Introduction"},{"location":"Wan2.1/VACE-14B/doc/index-en/#core-features","text":"","title":"\u2728 Core Features"},{"location":"Wan2.1/VACE-14B/doc/index-en/#technical-specifications","text":"Specification Details Model Type Video Editing Parameter Scale 14B Quantization Q6_K quantization (GGUF format) Model File Wan2.1_14B_VACE-Q6_K.gguf Deployment Architecture ECS single-machine deployment/ACS cluster deployment Memory Requirements ~8-12GB VRAM (40% reduction compared to original) Supported Resolution 720P/1080P Input Format Common video formats like MP4, AVI, MOV Output Format MP4 (H.264 encoding)","title":"\ud83d\udcca Technical Specifications"},{"location":"Wan2.1/VACE-14B/doc/index-en/#editing-capabilities","text":"\ud83c\udfad Style Conversion Convert videos to different artistic styles \ud83c\udf08 Color Adjustment Adjust video tone, saturation, brightness \ud83c\udfde\ufe0f Scene Modification Change video background or add new elements \ud83c\udfaf Object Editing Modify or replace specific objects in videos \u2728 Effect Addition Add various visual effects to videos \ud83d\udcc8 Quality Enhancement Improve video clarity and quality","title":"\ud83c\udfa8 Editing Capabilities"},{"location":"Wan2.1/VACE-14B/doc/index-en/#deployment-methods","text":"","title":"\ud83d\ude80 Deployment Methods"},{"location":"Wan2.1/VACE-14B/doc/index-en/#deployment-architecture-recommended","text":"\ud83d\udca1 Deployment Configuration Deployment Architecture : ECS single-machine deployment (recommended) or ACS cluster deployment Memory Requirements : 8-12GB VRAM, 40% reduction compared to original Scalability : Supports both single-machine and cluster deployment, quantized version better suited for single-machine deployment","title":"\ud83c\udfd7\ufe0f Deployment Architecture (Recommended)"},{"location":"Wan2.1/VACE-14B/doc/index-en/#q6_k-quantization-advantages","text":"","title":"\ud83d\udce6 Q6_K Quantization Advantages"},{"location":"Wan2.1/VACE-14B/doc/index-en/#usage-guide","text":"","title":"\ud83c\udfaf Usage Guide"},{"location":"Wan2.1/VACE-14B/doc/index-en/#comfyui-usage","text":"### \ud83d\udd27 Step-by-Step Instructions **1. Access Interface** - Click the access link at the service instance **2. Load Workflow** - Use workflow `vace14b.json` - Import the workflow into ComfyUI **3. Generate Content** - Input prompts for the video or image you want to generate - Click generate to start processing","title":"\ud83c\udf10 ComfyUI Usage"},{"location":"Wan2.1/VACE-14B/doc/index-en/#api-integration","text":"","title":"\ud83d\udd0c API Integration"},{"location":"Wan2.1/VACE-14B/doc/index-en/#authentication-setup","text":"","title":"\ud83d\udd11 Authentication Setup"},{"location":"Wan2.1/VACE-14B/doc/index-en/#python-code-example","text":"\ud83d\udccb Click to expand API call Python code import requests, json, uuid, time, random, os # \ud83d\udd27 Configuration parameters COMFYUI_SERVER, COMFYUI_TOKEN = \"Enter your server address\", \"Enter your token\" UNET_MODEL = \"Wan2.1_14B_VACE-Q6_K.gguf\" # Q6_K quantized version CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn.safetensors\" VAE_MODEL = \"wan21_vace_vae.safetensors\" # \ud83c\udfaf Preset parameters IMAGE_PATH = \"example.png\" PROMPT = \"A girl secretly takes a bite of an apple\" NEG_PROMPT = \"vivid colors, overexposed, static, blurry details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression artifacts, ugly, incomplete\" class VACEClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url, self.token, self.client_id = f\"http://{server}\", token, str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {})} def upload_image(self, image_path): \"\"\"\ud83d\udce4 Upload image to ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"Image file does not exist: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"Upload failed, status code: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"No filename in upload response: {result}\") return result['name'] except Exception as e: raise Exception(f\"Image upload failed: {e}\") def generate_vace(self, image_path, prompt, neg_prompt, steps=25, cfg=5, frames=49, width=720, height=720): \"\"\"\ud83c\udfac VACE video editing - Q6_K quantized version\"\"\" print(\"\ud83d\udce4 Uploading image...\") image_name = self.upload_image(image_path) print(f\"\u2705 Image uploaded successfully: {image_name}\") workflow = { \"11\": {\"inputs\": {\"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\"}, \"class_type\": \"UNETLoader\"}, # Q6_K quantized version \"13\": {\"inputs\": {\"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\"}, \"class_type\": \"CLIPLoader\"}, \"14\": {\"inputs\": {\"vae_name\": VAE_MODEL}, \"class_type\": \"VAELoader\"}, \"15\": {\"inputs\": {\"text\": prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"16\": {\"inputs\": {\"text\": neg_prompt, \"clip\": [\"13\", 0]}, \"class_type\": \"CLIPTextEncode\"}, \"17\": {\"inputs\": {\"width\": width, \"height\": height, \"length\": [\"21\", 0], \"batch_size\": 1, \"strength\": 1.0, \"positive\": [\"15\", 0], \"negative\": [\"16\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"WanVaceToVideo\"}, \"18\": {\"inputs\": {\"image\": image_name, \"upload\": \"image\"}, \"class_type\": \"LoadImage\"}, \"21\": {\"inputs\": {\"value\": frames}, \"class_type\": \"INTConstant\"}, \"22\": {\"inputs\": {\"width\": width, \"height\": height, \"upscale_method\": \"nearest-exact\", \"keep_proportion\": \"crop\", \"pad_color\": \"0, 0, 0\", \"crop_position\": \"center\", \"divisible_by\": 2, \"device\": \"gpu\", \"image\": [\"18\", 0]}, \"class_type\": \"ImageResizeKJv2\"}, \"23\": {\"inputs\": {\"images\": [\"22\", 0]}, \"class_type\": \"PreviewImage\"}, \"24\": {\"inputs\": {\"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"27\", 0], \"positive\": [\"17\", 0], \"negative\": [\"17\", 1], \"latent_image\": [\"17\", 2]}, \"class_type\": \"KSampler\"}, \"27\": {\"inputs\": {\"shift\": 8.0, \"model\": [\"11\", 0]}, \"class_type\": \"ModelSamplingSD3\"}, \"30\": {\"inputs\": {\"trim_amount\": [\"17\", 3], \"samples\": [\"24\", 0]}, \"class_type\": \"TrimVideoLatent\"}, \"31\": {\"inputs\": {\"samples\": [\"30\", 0], \"vae\": [\"14\", 0]}, \"class_type\": \"VAEDecode\"}, \"32\": {\"inputs\": {\"frame_rate\": 16, \"loop_count\": 0, \"filename_prefix\": \"VACE_Q6K_video\", \"format\": \"video/h264-mp4\", \"pix_fmt\": \"yuv420p\", \"crf\": 19, \"save_metadata\": True, \"trim_to_audio\": False, \"pingpong\": False, \"save_output\": True, \"images\": [\"31\", 0]}, \"class_type\": \"VHS_VideoCombine\"} } print(\"\ud83d\udce4 Submitting VACE Q6_K workflow...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get task status\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"vace_q6k_output.mp4\"): \"\"\"\ud83d\udce5 Download generated video\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main function - Execute VACE Q6_K video editing task\"\"\" client = VACEClient() try: print(f\"\ud83c\udfac Starting VACE Q6_K video editing task...\") print(f\"\ud83d\udcf7 Input image: {IMAGE_PATH}\") print(f\"\ud83d\udcdd Edit prompt: {PROMPT}\") print(f\"\ud83d\udd27 Using model: {UNET_MODEL}\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c Image file does not exist: {IMAGE_PATH}\") print(\"Please ensure there is an example.png file in the current directory\") return task_id = client.generate_vace(IMAGE_PATH, PROMPT, NEG_PROMPT, 25, 5, 49, 720, 720) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"vace_q6k_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python Code Example"},{"location":"Wan2.1/VACE-14B/doc/index-en/#best-practices","text":"","title":"\ud83d\udca1 Best Practices"},{"location":"Wan2.1/VACE-14B/doc/index-en/#troubleshooting-solutions","text":"\ud83c\udfa8 Generation Quality Issues Solutions: Insufficient prompt details : Add more specific descriptions, Q6_K version maintains strong understanding capabilities Improper parameter settings : Adjust steps and CFG values, recommended 25 steps, CFG 5 Low input image quality : Use higher quality source images Quantization impact : Q6_K quantization has minimal quality impact, try adjusting parameters if issues occur \u26a1 Performance Issues Optimization Tips: Insufficient VRAM : Q6_K version requires 40% less VRAM, 8-12GB is sufficient Slow loading : GGUF format loads faster, but still requires patience Inference speed : Quantized version infers faster, can increase steps to improve quality Hardware configuration : Q6_K version has lower hardware requirements, suitable for mid-range configurations \ud83d\udd27 Compatibility Issues Important Notes: GGUF format support : Ensure ComfyUI supports GGUF format models Plugin compatibility : Some plugins may not be compatible with quantized versions Parameter ranges : Pay attention to Q6_K version's recommended parameter ranges Model path : Ensure correct GGUF file path specification","title":"\ud83d\udd27 Troubleshooting &amp; Solutions"},{"location":"Wan2.1/VACE-14B/doc/index-en/#related-resources","text":"","title":"\ud83d\udcda Related Resources"},{"location":"Wan2.2/I2V-14B/doc/","text":"\ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B \u5f00\u653e\u4e14\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b - \u56fe\u50cf\u5230\u89c6\u9891\u4e13\u4e1a\u7248 \ud83e\udde0 27B\u53c2\u6570 MoE \ud83c\udfaf 480P & 720P \u26a1 \u7535\u5f71\u7ea7\u7f8e\u5b66 \ud83d\udccb \u6a21\u578b\u6982\u89c8 **\u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B** \u662f\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u7684\u9769\u547d\u6027\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u76f8\u6bd4\u524d\u4ee3Wan2.1\uff0c\u8be5\u6a21\u578b\u5728\u6570\u636e\u89c4\u6a21\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86\u5168\u9762\u5347\u7ea7\uff0c\u652f\u6301480P\u548c720P\u53cc\u5206\u8fa8\u7387\u8f93\u51fa\uff0c\u5177\u5907\u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf\u548c\u66f4\u7a33\u5b9a\u7684\u89c6\u9891\u5408\u6210\u80fd\u529b\u3002 \ud83c\udff7\ufe0f \u6a21\u578b\u6807\u8bc6 Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca \u67b6\u6784\u89c4\u6a21 27B \u603b\u53c2\u6570 14B \u6fc0\u6d3b \ud83c\udfaf \u6838\u5fc3\u529f\u80fd \u56fe\u50cf\u8f6c\u89c6\u9891 \u53cc\u5206\u8fa8\u7387 \ud83d\ude80 \u6838\u5fc3\u6280\u672f\u7a81\u7834 \ud83e\udde0 MoE \u6df7\u5408\u4e13\u5bb6\u67b6\u6784 \u53cc\u4e13\u5bb6\u8bbe\u8ba1 \uff1a\u9ad8\u566a\u58f0\u4e13\u5bb6 + \u4f4e\u566a\u58f0\u4e13\u5bb6 \u667a\u80fd\u5207\u6362 \uff1a\u57fa\u4e8e\u4fe1\u566a\u6bd4(SNR)\u81ea\u52a8\u5207\u6362 \u9ad8\u6548\u63a8\u7406 \uff1a27B\u53c2\u6570\uff0c14B\u6fc0\u6d3b\uff0c\u6210\u672c\u4e0d\u53d8 \u4f18\u5316\u53bb\u566a \uff1a\u4e13\u95e8\u9488\u5bf9\u6269\u6563\u6a21\u578b\u4f18\u5316 \ud83c\udfac \u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf \u7cbe\u7ec6\u6807\u7b7e \uff1a\u7167\u660e\u3001\u6784\u56fe\u3001\u5bf9\u6bd4\u5ea6\u3001\u8272\u8c03 \u53ef\u63a7\u751f\u6210 \uff1a\u7cbe\u786e\u7684\u7535\u5f71\u98ce\u683c\u63a7\u5236 \u7f8e\u5b66\u504f\u597d \uff1a\u53ef\u5b9a\u5236\u7684\u89c6\u89c9\u98ce\u683c \u4e13\u4e1a\u54c1\u8d28 \uff1a\u5546\u4e1a\u7ea7\u89c6\u9891\u8f93\u51fa \ud83d\udcc8 \u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3 \u6570\u636e\u6269\u5c55 \uff1a\u56fe\u50cf+65.6%\uff0c\u89c6\u9891+83.2% \u591a\u7ef4\u63d0\u5347 \uff1a\u8fd0\u52a8\u3001\u8bed\u4e49\u3001\u7f8e\u5b66\u5168\u9762\u589e\u5f3a \u9876\u7ea7\u6027\u80fd \uff1a\u5f00\u6e90\u95ed\u6e90\u6a21\u578b\u4e2d\u9886\u5148 \u6cdb\u5316\u80fd\u529b \uff1a\u663e\u8457\u63d0\u5347\u7684\u9002\u5e94\u6027 \u26a1 \u9ad8\u6548\u90e8\u7f72\u4f18\u5316 \u6d88\u8d39\u7ea7GPU \uff1a\u652f\u6301RTX 4090\u7b49\u663e\u5361 \u591a\u5206\u8fa8\u7387 \uff1a480P & 720P\u53cc\u652f\u6301 \u7a33\u5b9a\u5408\u6210 \uff1a\u51cf\u5c11\u4e0d\u771f\u5b9e\u6444\u50cf\u673a\u8fd0\u52a8 \u98ce\u683c\u591a\u6837 \uff1a\u589e\u5f3a\u7684\u573a\u666f\u9002\u5e94\u6027 \ud83d\udd27 \u6280\u672f\u89c4\u683c\u5bf9\u6bd4 \u89c4\u683c\u9879\u76ee Wan2.1-I2V-14B Wan2.2-I2V-A14B \u63d0\u5347\u5e45\u5ea6 \u67b6\u6784\u7c7b\u578b \u4f20\u7edf\u6269\u6563\u6a21\u578b MoE\u6df7\u5408\u4e13\u5bb6 \u67b6\u6784\u5347\u7ea7 \u53c2\u6570\u89c4\u6a21 14B 27B (14B\u6fc0\u6d3b) +93% \u652f\u6301\u5206\u8fa8\u7387 480P 480P & 720P \u53cc\u5206\u8fa8\u7387 \u8bad\u7ec3\u6570\u636e \u57fa\u7840\u6570\u636e\u96c6 \u56fe\u50cf+65.6%, \u89c6\u9891+83.2% \u5927\u5e45\u6269\u5c55 \u7f8e\u5b66\u8d28\u91cf \u6807\u51c6\u8d28\u91cf \u7535\u5f71\u7ea7\u7f8e\u5b66 \u8d28\u91cf\u98de\u8dc3 \u8fd0\u52a8\u7a33\u5b9a\u6027 \u57fa\u7840\u7a33\u5b9a \u663e\u8457\u589e\u5f3a \u7a33\u5b9a\u6027\u63d0\u5347 \u96c6\u6210\u751f\u6001 ComfyUI ComfyUI + Diffusers + ModelScope \u751f\u6001\u5b8c\u5584 \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 ### \ud83d\udda5\ufe0f \u63a8\u8350\u786c\u4ef6\u914d\u7f6e \u914d\u7f6e\u9879 \u63a8\u8350\u89c4\u683c \u8bf4\u660e CPU 16 vCPU \u9ad8\u6027\u80fd\u591a\u6838\u5904\u7406\u5668 \u5185\u5b58 60 GiB \u5927\u5bb9\u91cf\u7cfb\u7edf\u5185\u5b58 GPU 1 \u00d7 NVIDIA A10 \u4e13\u4e1a\u7ea7\u663e\u5361\uff0c80GB+ \u663e\u5b58 \ud83c\udfaf \u4f7f\u7528\u6307\u5357 \ud83c\udf10 ComfyUI \u96c6\u6210\u4f7f\u7528\u6307\u5357 \ud83d\udccd ComfyUI Web UI \u4f7f\u7528\u6559\u7a0b ### \ud83d\ude80 \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \ud83d\udd17 \u754c\u9762\u8bbf\u95ee \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5\uff0c\u8fdb\u5165 ComfyUI \u53ef\u89c6\u5316\u754c\u9762 ![img.png](img.png) ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41 \u9009\u62e9 \u5de6\u4e0a\u89d2\u7684\u5b98\u65b9 \u5de5\u4f5c\u6d41\u5e76\u6253\u5f00 ![img_4.png](img_4.png) ![img_5.png](img_5.png) ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u4e0a\u4f20\u56fe\u50cf **\u5728 LoadImage \u8282\u70b9\u64cd\u4f5c\uff1a** - \u9009\u62e9\u793a\u4f8b\u56fe\u7247\u8fdb\u884c\u6d4b\u8bd5 - \u6216\u4ece\u672c\u673a\u7535\u8111\u4e0a\u4f20\u81ea\u5b9a\u4e49\u56fe\u50cf - \u652f\u6301 JPEG\u3001PNG\u3001WebP \u7b49\u683c\u5f0f ### \u270d\ufe0f \u6b65\u9aa4\u56db\uff1a\u8bbe\u7f6e\u6587\u672c\u63cf\u8ff0 \u2705 \u6b63\u5411\u63d0\u793a\u8bcd \u5728 TextEncode (Positive) \u8282\u70b9\u586b\u5199\uff1a \"Cinematic lighting, graceful movements, smooth animation, high quality\" \u274c \u8d1f\u5411\u63d0\u793a\u8bcd \u5728 TextEncode (Negative) \u8282\u70b9\u586b\u5199\uff1a \"bad quality, blurry, distorted, static\" ### \u2699\ufe0f \u6b65\u9aa4\u4e94\uff1a\u914d\u7f6e\u53c2\u6570 **\u5728 WanVideoImageClipEncode \u8282\u70b9\u8bbe\u7f6e\uff1a** \u53c2\u6570\u540d\u79f0 \u63a8\u8350\u503c \u8bf4\u660e generation_width 1280 (720P) / 854 (480P) \u89c6\u9891\u5bbd\u5ea6 generation_height 720 (720P) / 480 (480P) \u89c6\u9891\u9ad8\u5ea6 num_frames 81 \u89c6\u9891\u5e27\u6570 noise_aug_strength 0 \u566a\u58f0\u589e\u5f3a\u5f3a\u5ea6 adjust_resolution True \u81ea\u52a8\u8c03\u6574\u5206\u8fa8\u7387 ### \ud83c\udfac \u6b65\u9aa4\u516d\uff1a\u6267\u884c\u5de5\u4f5c\u6d41 \ud83d\ude80 \u5f00\u59cb\u751f\u6210 \u70b9\u51fb\u53f3\u4fa7\u9762\u677f\u7684 \"Queue Prompt\" \u6309\u94ae\u5f00\u59cb\u751f\u6210\u89c6\u9891 \u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u5728\u8fdb\u5ea6\u6761\u67e5\u770b\u5b9e\u65f6\u72b6\u6001 \ud83d\udd0c ComfyUI API \u8c03\u7528\u65b9\u5f0f \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83d\udd10 \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u8bbe\u7f6e\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\u83b7\u53d6API Token \ud83d\udd11 API Token \u83b7\u53d6\u754c\u9762 ![img_1.png](img_1.png) \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 \u4ece\u670d\u52a1\u5b9e\u4f8b\u4fe1\u606f\u4e2d\u83b7\u53d6 COMFYUI_SERVER \u5730\u5740 \ud83c\udf10 \u670d\u52a1\u5668\u5730\u5740\u914d\u7f6e ![img_2.png](img_2.png) \ud83d\udcbb Python API \u8c03\u7528\u793a\u4f8b \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"input_image.jpg\" PROMPT = \"The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14B_I2VClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_wan22_14b_i2v(self, image_path, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684\u56fe\u751f\u89c6\u9891 JSON \u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"63\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_I2V_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} }, \"62\": { \"inputs\": { \"image\": image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"\u52a0\u8f7d\u56fe\u50cf\"} }, \"63\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"62\", 0] }, \"class_type\": \"WanImageToVideo\", \"_meta\": {\"title\": \"Wan\u56fe\u50cf\u5230\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 14B \u56fe\u751f\u89c6\u9891\u53cc\u9636\u6bb5\u91c7\u6837\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_i2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14B_I2VClient() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u53cc\u9636\u6bb5\u91c7\u6837\uff1a\u9ad8\u566a\u58f0I2V\u6a21\u578b + \u4f4e\u566a\u58f0I2V\u6a21\u578b\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709\u8f93\u5165\u56fe\u7247\u6587\u4ef6\") return # 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B I2V Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # 14B\u6a21\u578b\u751f\u6210\u65f6\u95f4\u66f4\u957f\uff0c\u589e\u52a0\u8f6e\u8be2\u95f4\u9694 output_file = client.download_video(task_id, \"wan22_14b_i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udd17 ComfyUI API \u7aef\u70b9\u8bf4\u660e \u7aef\u70b9 \u65b9\u6cd5 \u529f\u80fd \u8bf4\u660e /queue GET \u83b7\u53d6\u961f\u5217\u72b6\u6001 \u67e5\u770b\u5f53\u524d\u4efb\u52a1\u961f\u5217\u548c\u8fd0\u884c\u72b6\u6001 /prompt POST \u63d0\u4ea4\u5de5\u4f5c\u6d41 \u6267\u884c Wan2.2 \u56fe\u751f\u89c6\u9891\u4efb\u52a1 /history/{prompt_id} GET \u83b7\u53d6\u6267\u884c\u5386\u53f2 \u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c\u548c\u8f93\u51fa /upload/image POST \u4e0a\u4f20\u56fe\u7247 \u4e0a\u4f20\u8f93\u5165\u56fe\u7247\u6587\u4ef6 /view GET \u4e0b\u8f7d\u8f93\u51fa\u6587\u4ef6 \u83b7\u53d6\u751f\u6210\u7684\u7ed3\u679c\u6587\u4ef6 \u2699\ufe0f ComfyUI \u8282\u70b9\u8be6\u7ec6\u8bf4\u660e \ud83d\udce4 \u8f93\u5165\u8282\u70b9 LoadImage \uff1a\u52a0\u8f7d\u8f93\u5165\u56fe\u50cf WanVideoTextEncode \uff1a\u6587\u672c\u7f16\u7801\u5668 WanVideoImageClipEncode \uff1a\u56fe\u50cf\u7f16\u7801\u5668 \ud83e\udde0 \u6a21\u578b\u8282\u70b9 WanVideoModelLoader \uff1aWan2.2 \u6a21\u578b\u52a0\u8f7d\u5668 WanVideoVAELoader \uff1aVAE \u6a21\u578b\u52a0\u8f7d\u5668 LoadWanVideoT5TextEncoder \uff1aT5 \u6587\u672c\u7f16\u7801\u5668 \u26a1 \u5904\u7406\u8282\u70b9 WanVideoSampler \uff1a\u89c6\u9891\u91c7\u6837\u5668 WanVideoDecode \uff1a\u89c6\u9891\u89e3\u7801\u5668 WanVideoBlockSwap \uff1a\u5185\u5b58\u4f18\u5316 \ud83d\udce4 \u8f93\u51fa\u8282\u70b9 VHS_VideoCombine \uff1a\u89c6\u9891\u5408\u6210\u8f93\u51fa PreviewImage \uff1a\u56fe\u50cf\u9884\u89c8 SaveImage \uff1a\u56fe\u50cf\u4fdd\u5b58 \ud83d\udca1 \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357 \u2705 \u6b63\u5411\u63d0\u793a\u8bcd\u793a\u4f8b \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression.\" \"Cinematic lighting, graceful movements, smooth animation, high quality, professional cinematography\" \u274c \u8d1f\u5411\u63d0\u793a\u8bcd\u5efa\u8bae \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" \"unnatural movement, jerky motion, inconsistent, artifacts, noise\" \ud83d\udd17 \u96c6\u6210\u751f\u6001 \ud83c\udfa8 ComfyUI \u2705 \u5df2\u96c6\u6210 \u4e2d\u82f1\u6587\u6863 \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\uff0c\u6613\u4e8e\u4f7f\u7528 \ud83e\udd17 Diffusers \u2705 \u5df2\u96c6\u6210 I2V-A14B \u6807\u51c6\u5316API\u63a5\u53e3 \ud83e\udd16 ModelScope \u2705 \u5b98\u65b9\u652f\u6301 \u539f\u751f\u96c6\u6210 \u6a21\u578b\u6258\u7ba1\u4e0e\u5206\u53d1 \ud83d\udc19 GitHub \u2705 \u5f00\u6e90 \u5b8c\u6574\u4ee3\u7801 \u6e90\u7801\u5f00\u653e\uff0c\u793e\u533a\u9a71\u52a8 \ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a \ud83c\udfac \u7535\u5f71\u5236\u4f5c \u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf\uff0c\u4e13\u4e1a\u89c6\u9891\u5236\u4f5c \ud83d\udecd\ufe0f \u5546\u4e1a\u8425\u9500 \u4ea7\u54c1\u5c55\u793a\u52a8\u753b\uff0c\u8425\u9500\u89c6\u9891\u5236\u4f5c \ud83c\udfa8 \u827a\u672f\u521b\u4f5c \u6570\u5b57\u827a\u672f\u52a8\u753b\uff0c\u521b\u610f\u89c6\u9891\u751f\u6210 \ud83d\udcf1 \u793e\u4ea4\u5a92\u4f53 \u77ed\u89c6\u9891\u5236\u4f5c\uff0c\u793e\u4ea4\u5185\u5bb9\u521b\u4f5c \ud83d\udcc4 \u8bb8\u53ef\u4e0e\u5f15\u7528 ### \ud83d\udccb \u5f00\u6e90\u534f\u8bae \ud83d\udcdc Apache 2.0 \u8bb8\u53ef\u8bc1 \u2022 \u81ea\u7531\u4f7f\u7528\u548c\u5206\u53d1 \u2022 \u5546\u4e1a\u53cb\u597d\u7684\u5f00\u6e90\u534f\u8bae \u2022 \u5b8c\u6574\u7684\u4f7f\u7528\u6743\u9650\u6388\u4e88 \u2022 \u9700\u9075\u5b88\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4 ### \ud83d\udcda \u5b66\u672f\u5f15\u7528 @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B | \u5f00\u653e\u4e14\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b \u00a9 2025 \u901a\u4e49\u4e07\u76f8\u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u6bcf\u5f20\u56fe\u7247\u90fd\u6210\u4e3a\u7cbe\u5f69\u89c6\u9891","title":"Index"},{"location":"Wan2.2/I2V-14B/doc/#_1","text":"**\u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B** \u662f\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u7684\u9769\u547d\u6027\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u76f8\u6bd4\u524d\u4ee3Wan2.1\uff0c\u8be5\u6a21\u578b\u5728\u6570\u636e\u89c4\u6a21\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86\u5168\u9762\u5347\u7ea7\uff0c\u652f\u6301480P\u548c720P\u53cc\u5206\u8fa8\u7387\u8f93\u51fa\uff0c\u5177\u5907\u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf\u548c\u66f4\u7a33\u5b9a\u7684\u89c6\u9891\u5408\u6210\u80fd\u529b\u3002 \ud83c\udff7\ufe0f \u6a21\u578b\u6807\u8bc6 Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca \u67b6\u6784\u89c4\u6a21 27B \u603b\u53c2\u6570 14B \u6fc0\u6d3b \ud83c\udfaf \u6838\u5fc3\u529f\u80fd \u56fe\u50cf\u8f6c\u89c6\u9891 \u53cc\u5206\u8fa8\u7387","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/I2V-14B/doc/#_2","text":"","title":"\ud83d\ude80 \u6838\u5fc3\u6280\u672f\u7a81\u7834"},{"location":"Wan2.2/I2V-14B/doc/#_3","text":"\u89c4\u683c\u9879\u76ee Wan2.1-I2V-14B Wan2.2-I2V-A14B \u63d0\u5347\u5e45\u5ea6 \u67b6\u6784\u7c7b\u578b \u4f20\u7edf\u6269\u6563\u6a21\u578b MoE\u6df7\u5408\u4e13\u5bb6 \u67b6\u6784\u5347\u7ea7 \u53c2\u6570\u89c4\u6a21 14B 27B (14B\u6fc0\u6d3b) +93% \u652f\u6301\u5206\u8fa8\u7387 480P 480P & 720P \u53cc\u5206\u8fa8\u7387 \u8bad\u7ec3\u6570\u636e \u57fa\u7840\u6570\u636e\u96c6 \u56fe\u50cf+65.6%, \u89c6\u9891+83.2% \u5927\u5e45\u6269\u5c55 \u7f8e\u5b66\u8d28\u91cf \u6807\u51c6\u8d28\u91cf \u7535\u5f71\u7ea7\u7f8e\u5b66 \u8d28\u91cf\u98de\u8dc3 \u8fd0\u52a8\u7a33\u5b9a\u6027 \u57fa\u7840\u7a33\u5b9a \u663e\u8457\u589e\u5f3a \u7a33\u5b9a\u6027\u63d0\u5347 \u96c6\u6210\u751f\u6001 ComfyUI ComfyUI + Diffusers + ModelScope \u751f\u6001\u5b8c\u5584","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c\u5bf9\u6bd4"},{"location":"Wan2.2/I2V-14B/doc/#_4","text":"### \ud83d\udda5\ufe0f \u63a8\u8350\u786c\u4ef6\u914d\u7f6e \u914d\u7f6e\u9879 \u63a8\u8350\u89c4\u683c \u8bf4\u660e CPU 16 vCPU \u9ad8\u6027\u80fd\u591a\u6838\u5904\u7406\u5668 \u5185\u5b58 60 GiB \u5927\u5bb9\u91cf\u7cfb\u7edf\u5185\u5b58 GPU 1 \u00d7 NVIDIA A10 \u4e13\u4e1a\u7ea7\u663e\u5361\uff0c80GB+ \u663e\u5b58","title":"\ud83d\udcbb \u7cfb\u7edf\u8981\u6c42"},{"location":"Wan2.2/I2V-14B/doc/#_5","text":"","title":"\ud83c\udfaf \u4f7f\u7528\u6307\u5357"},{"location":"Wan2.2/I2V-14B/doc/#comfyui","text":"","title":"\ud83c\udf10 ComfyUI \u96c6\u6210\u4f7f\u7528\u6307\u5357"},{"location":"Wan2.2/I2V-14B/doc/#comfyui-web-ui","text":"### \ud83d\ude80 \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \ud83d\udd17 \u754c\u9762\u8bbf\u95ee \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5\uff0c\u8fdb\u5165 ComfyUI \u53ef\u89c6\u5316\u754c\u9762 ![img.png](img.png) ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41 \u9009\u62e9 \u5de6\u4e0a\u89d2\u7684\u5b98\u65b9 \u5de5\u4f5c\u6d41\u5e76\u6253\u5f00 ![img_4.png](img_4.png) ![img_5.png](img_5.png) ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u4e0a\u4f20\u56fe\u50cf **\u5728 LoadImage \u8282\u70b9\u64cd\u4f5c\uff1a** - \u9009\u62e9\u793a\u4f8b\u56fe\u7247\u8fdb\u884c\u6d4b\u8bd5 - \u6216\u4ece\u672c\u673a\u7535\u8111\u4e0a\u4f20\u81ea\u5b9a\u4e49\u56fe\u50cf - \u652f\u6301 JPEG\u3001PNG\u3001WebP \u7b49\u683c\u5f0f ### \u270d\ufe0f \u6b65\u9aa4\u56db\uff1a\u8bbe\u7f6e\u6587\u672c\u63cf\u8ff0","title":"\ud83d\udccd ComfyUI Web UI \u4f7f\u7528\u6559\u7a0b"},{"location":"Wan2.2/I2V-14B/doc/#comfyui-api","text":"","title":"\ud83d\udd0c ComfyUI API \u8c03\u7528\u65b9\u5f0f"},{"location":"Wan2.2/I2V-14B/doc/#_6","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"Wan2.2/I2V-14B/doc/#python-api","text":"\ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"input_image.jpg\" PROMPT = \"The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14B_I2VClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_wan22_14b_i2v(self, image_path, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684\u56fe\u751f\u89c6\u9891 JSON \u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"63\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_I2V_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} }, \"62\": { \"inputs\": { \"image\": image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"\u52a0\u8f7d\u56fe\u50cf\"} }, \"63\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"62\", 0] }, \"class_type\": \"WanImageToVideo\", \"_meta\": {\"title\": \"Wan\u56fe\u50cf\u5230\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 14B \u56fe\u751f\u89c6\u9891\u53cc\u9636\u6bb5\u91c7\u6837\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_i2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14B_I2VClient() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u53cc\u9636\u6bb5\u91c7\u6837\uff1a\u9ad8\u566a\u58f0I2V\u6a21\u578b + \u4f4e\u566a\u58f0I2V\u6a21\u578b\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709\u8f93\u5165\u56fe\u7247\u6587\u4ef6\") return # 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B I2V Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # 14B\u6a21\u578b\u751f\u6210\u65f6\u95f4\u66f4\u957f\uff0c\u589e\u52a0\u8f6e\u8be2\u95f4\u9694 output_file = client.download_video(task_id, \"wan22_14b_i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python API \u8c03\u7528\u793a\u4f8b"},{"location":"Wan2.2/I2V-14B/doc/#comfyui-api_1","text":"\u7aef\u70b9 \u65b9\u6cd5 \u529f\u80fd \u8bf4\u660e /queue GET \u83b7\u53d6\u961f\u5217\u72b6\u6001 \u67e5\u770b\u5f53\u524d\u4efb\u52a1\u961f\u5217\u548c\u8fd0\u884c\u72b6\u6001 /prompt POST \u63d0\u4ea4\u5de5\u4f5c\u6d41 \u6267\u884c Wan2.2 \u56fe\u751f\u89c6\u9891\u4efb\u52a1 /history/{prompt_id} GET \u83b7\u53d6\u6267\u884c\u5386\u53f2 \u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c\u548c\u8f93\u51fa /upload/image POST \u4e0a\u4f20\u56fe\u7247 \u4e0a\u4f20\u8f93\u5165\u56fe\u7247\u6587\u4ef6 /view GET \u4e0b\u8f7d\u8f93\u51fa\u6587\u4ef6 \u83b7\u53d6\u751f\u6210\u7684\u7ed3\u679c\u6587\u4ef6","title":"\ud83d\udd17 ComfyUI API \u7aef\u70b9\u8bf4\u660e"},{"location":"Wan2.2/I2V-14B/doc/#comfyui_1","text":"","title":"\u2699\ufe0f ComfyUI \u8282\u70b9\u8be6\u7ec6\u8bf4\u660e"},{"location":"Wan2.2/I2V-14B/doc/#_7","text":"","title":"\ud83d\udca1 \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357"},{"location":"Wan2.2/I2V-14B/doc/#_8","text":"\ud83c\udfa8 ComfyUI \u2705 \u5df2\u96c6\u6210 \u4e2d\u82f1\u6587\u6863 \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\uff0c\u6613\u4e8e\u4f7f\u7528 \ud83e\udd17 Diffusers \u2705 \u5df2\u96c6\u6210 I2V-A14B \u6807\u51c6\u5316API\u63a5\u53e3 \ud83e\udd16 ModelScope \u2705 \u5b98\u65b9\u652f\u6301 \u539f\u751f\u96c6\u6210 \u6a21\u578b\u6258\u7ba1\u4e0e\u5206\u53d1 \ud83d\udc19 GitHub \u2705 \u5f00\u6e90 \u5b8c\u6574\u4ee3\u7801 \u6e90\u7801\u5f00\u653e\uff0c\u793e\u533a\u9a71\u52a8","title":"\ud83d\udd17 \u96c6\u6210\u751f\u6001"},{"location":"Wan2.2/I2V-14B/doc/#_9","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a"},{"location":"Wan2.2/I2V-14B/doc/#_10","text":"### \ud83d\udccb \u5f00\u6e90\u534f\u8bae \ud83d\udcdc Apache 2.0 \u8bb8\u53ef\u8bc1 \u2022 \u81ea\u7531\u4f7f\u7528\u548c\u5206\u53d1 \u2022 \u5546\u4e1a\u53cb\u597d\u7684\u5f00\u6e90\u534f\u8bae \u2022 \u5b8c\u6574\u7684\u4f7f\u7528\u6743\u9650\u6388\u4e88 \u2022 \u9700\u9075\u5b88\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4 ### \ud83d\udcda \u5b66\u672f\u5f15\u7528 @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B | \u5f00\u653e\u4e14\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b \u00a9 2025 \u901a\u4e49\u4e07\u76f8\u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u6bcf\u5f20\u56fe\u7247\u90fd\u6210\u4e3a\u7cbe\u5f69\u89c6\u9891","title":"\ud83d\udcc4 \u8bb8\u53ef\u4e0e\u5f15\u7528"},{"location":"Wan2.2/I2V-14B/doc/index-en/","text":"\ud83c\udfac Tongyi Wanxiang 2.2 - Image-to-Video - A14B Open and Advanced Large-Scale Video Generation Model - Image-to-Video Professional Edition \ud83e\udde0 27B Parameters MoE \ud83c\udfaf 480P & 720P \u26a1 Cinematic Aesthetics \ud83d\udccb Model Overview **Tongyi Wanxiang 2.2 Image-to-Video A14B** is a revolutionary image-to-video generation model based on Mixture of Experts (MoE) architecture. Compared to the previous Wan2.1, this model achieves comprehensive upgrades in data scale, architectural design, and generation quality, supporting dual-resolution output at 480P and 720P with cinematic-grade aesthetic quality and more stable video synthesis capabilities. \ud83c\udff7\ufe0f Model Identifier Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca Architecture Scale 27B Total Parameters 14B Activated \ud83c\udfaf Core Functions Image-to-Video Dual Resolution \ud83d\ude80 Core Technical Breakthroughs \ud83e\udde0 MoE Mixture of Experts Architecture Dual Expert Design : High-noise expert + Low-noise expert Intelligent Switching : Automatic switching based on Signal-to-Noise Ratio (SNR) Efficient Inference : 27B parameters, 14B activated, same cost Optimized Denoising : Specifically optimized for diffusion models \ud83c\udfac Cinematic-Grade Aesthetic Quality Fine-grained Labels : Lighting, composition, contrast, tone Controllable Generation : Precise cinematic style control Aesthetic Preferences : Customizable visual styles Professional Quality : Commercial-grade video output \ud83d\udcc8 Large-Scale Data Training Data Expansion : Images +65.6%, Videos +83.2% Multi-dimensional Enhancement : Motion, semantics, aesthetics all improved Top Performance : Leading among open-source and closed-source models Generalization Ability : Significantly improved adaptability \u26a1 Efficient Deployment Optimization Consumer GPUs : Supports RTX 4090 and similar cards Multi-resolution : 480P & 720P dual support Stable Synthesis : Reduced unrealistic camera movements Style Diversity : Enhanced scene adaptability \ud83d\udd27 Technical Specifications Comparison Specification Wan2.1-I2V-14B Wan2.2-I2V-A14B Improvement Architecture Type Traditional Diffusion Model MoE Mixture of Experts Architecture Upgrade Parameter Scale 14B 27B (14B activated) +93% Supported Resolution 480P 480P & 720P Dual Resolution Training Data Base Dataset Images +65.6%, Videos +83.2% Massive Expansion Aesthetic Quality Standard Quality Cinematic Aesthetics Quality Leap Motion Stability Basic Stability Significantly Enhanced Stability Improvement Integration Ecosystem ComfyUI ComfyUI + Diffusers + ModelScope Ecosystem Enhancement \ud83d\udcbb System Requirements ### \ud83d\udda5\ufe0f Recommended Hardware Configuration Component Recommended Specs Description CPU 16 vCPU High-performance multi-core processor Memory 60 GiB Large capacity system memory GPU 1 \u00d7 NVIDIA A10 Professional GPU with 80GB+ VRAM \ud83c\udfaf Usage Guide \ud83c\udf10 ComfyUI Integration Usage Guide \ud83d\udccd ComfyUI Web UI Tutorial ### \ud83d\ude80 Step 1: Access Interface \ud83d\udd17 Interface Access Click the access link at the service instance to enter the ComfyUI visual interface ![img_3.png](img_3.png) ### \ud83d\udd27 Step 2: Select Workflow Select the official workflow in the top left and open it ![img_4.png](img_4.png) ![img_3.png](img_3.png) ### \ud83d\udce4 Step 3: Upload Image **In the LoadImage node:** - Select sample images for testing - Or upload custom images from your computer - Supports JPEG, PNG, WebP and other formats ### \u270d\ufe0f Step 4: Set Text Description \u2705 Positive Prompt Fill in the TextEncode (Positive) node: \"Cinematic lighting, graceful movements, smooth animation, high quality\" \u274c Negative Prompt Fill in the TextEncode (Negative) node: \"bad quality, blurry, distorted, static\" ### \u2699\ufe0f Step 5: Configure Parameters **In the WanVideoImageClipEncode node settings:** Parameter Name Recommended Value Description generation_width 1280 (720P) / 854 (480P) Video width generation_height 720 (720P) / 480 (480P) Video height num_frames 81 Number of video frames noise_aug_strength 0 Noise augmentation strength adjust_resolution True Automatically adjust resolution ### \ud83c\udfac Step 6: Execute Workflow \ud83d\ude80 Start Generation Click the \"Queue Prompt\" button in the right panel to start video generation Real-time status can be viewed in the progress bar during generation \ud83d\udd0c ComfyUI API Calling Method \ud83d\udd11 Get Authentication Information \ud83d\udd10 Get Token Click the settings button in the top right, open the bottom panel to get API Token \ud83d\udd11 API Token Interface ![img_1.png](img_1.png) \ud83c\udf10 Get Server Address Get COMFYUI_SERVER address from service instance information \ud83c\udf10 Server Address Configuration ![img_2.png](img_2.png) \ud83d\udcbb Python API Call Example \ud83d\udc0d Click to expand complete Python API call code import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"input_image.jpg\" PROMPT = \"The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14B_I2VClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_wan22_14b_i2v(self, image_path, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684\u56fe\u751f\u89c6\u9891 JSON \u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"63\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_I2V_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} }, \"62\": { \"inputs\": { \"image\": image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"\u52a0\u8f7d\u56fe\u50cf\"} }, \"63\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"62\", 0] }, \"class_type\": \"WanImageToVideo\", \"_meta\": {\"title\": \"Wan\u56fe\u50cf\u5230\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 14B \u56fe\u751f\u89c6\u9891\u53cc\u9636\u6bb5\u91c7\u6837\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_i2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14B_I2VClient() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u53cc\u9636\u6bb5\u91c7\u6837\uff1a\u9ad8\u566a\u58f0I2V\u6a21\u578b + \u4f4e\u566a\u58f0I2V\u6a21\u578b\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709\u8f93\u5165\u56fe\u7247\u6587\u4ef6\") return # 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B I2V Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # 14B\u6a21\u578b\u751f\u6210\u65f6\u95f4\u66f4\u957f\uff0c\u589e\u52a0\u8f6e\u8be2\u95f4\u9694 output_file = client.download_video(task_id, \"wan22_14b_i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udd17 ComfyUI API Endpoint Description Endpoint Method Function Description /queue GET Get queue status View current task queue and running status /prompt POST Submit workflow Execute Wan2.2 Image-to-Video task /history/{prompt_id} GET Get execution history View task execution results and outputs /upload/image POST Upload image Upload input image file /view GET Download output file Get generated result files \u2699\ufe0f ComfyUI Node Detailed Description \ud83d\udce4 Input Nodes LoadImage : Load input image WanVideoTextEncode : Text encoder WanVideoImageClipEncode : Image encoder \ud83e\udde0 Model Nodes WanVideoModelLoader : Wan2.2 model loader WanVideoVAELoader : VAE model loader LoadWanVideoT5TextEncoder : T5 text encoder \u26a1 Processing Nodes WanVideoSampler : Video sampler WanVideoDecode : Video decoder WanVideoBlockSwap : Memory optimization \ud83d\udce4 Output Nodes VHS_VideoCombine : Video synthesis output PreviewImage : Image preview SaveImage : Image save \ud83d\udcca Performance Benchmarks ### \u26a1 Computational Efficiency Comparison GPU Model Single GPU 4\u00d7GPU 8\u00d7GPU Optimization Features A100 80GB Recommended Config Efficient Parallel Optimal Performance FSDP + Ulysses H100 80GB Best Single Card Ultra-fast Processing Top Performance FlashAttention3 RTX 4090 Consumer Choice - - Model Offloading \ud83c\udfaf Performance Advantages \u2022 MoE Architecture: 27B parameters, 14B activated, same inference cost \u2022 Built-in FSDP and Ulysses distributed implementation \u2022 Automatic model parameter type conversion optimization \u2022 Average performance data after warm-up and multiple sampling \ud83d\udca1 Prompt Writing Guide \u2705 Positive Prompt Examples \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression.\" \"Cinematic lighting, graceful movements, smooth animation, high quality, professional cinematography\" \u274c Negative Prompt Suggestions \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" \"unnatural movement, jerky motion, inconsistent, artifacts, noise\" \ud83d\udd17 Integration Ecosystem \ud83c\udfa8 ComfyUI \u2705 Integrated EN/CN Docs Visual workflow, easy to use \ud83e\udd17 Diffusers \u2705 Integrated I2V-A14B Standardized API interface \ud83e\udd16 ModelScope \u2705 Official Support Native Integration Model hosting and distribution \ud83d\udc19 GitHub \u2705 Open Source Complete Code Open source, community-driven \ud83c\udfaf Application Scenarios \ud83c\udfac Film Production Cinematic-grade aesthetic quality, professional video production \ud83d\udecd\ufe0f Commercial Marketing Product showcase animations, marketing video production \ud83c\udfa8 Artistic Creation Digital art animation, creative video generation \ud83d\udcf1 Social Media Short video production, social content creation \ud83d\udcc4 License and Citation ### \ud83d\udccb Open Source License \ud83d\udcdc Apache 2.0 License \u2022 Free to use and distribute \u2022 Commercial-friendly open source license \u2022 Complete usage rights granted \u2022 Must comply with relevant laws and regulations ### \ud83d\udcda Academic Citation @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac Tongyi Wanxiang 2.2 Image-to-Video A14B | Open and Advanced Large-Scale Video Generation Model \u00a9 2025 Tongyi Wanxiang Team | Apache 2.0 Open Source License | Turn Every Image into Amazing Video","title":"Index en"},{"location":"Wan2.2/I2V-14B/doc/index-en/#model-overview","text":"**Tongyi Wanxiang 2.2 Image-to-Video A14B** is a revolutionary image-to-video generation model based on Mixture of Experts (MoE) architecture. Compared to the previous Wan2.1, this model achieves comprehensive upgrades in data scale, architectural design, and generation quality, supporting dual-resolution output at 480P and 720P with cinematic-grade aesthetic quality and more stable video synthesis capabilities. \ud83c\udff7\ufe0f Model Identifier Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca Architecture Scale 27B Total Parameters 14B Activated \ud83c\udfaf Core Functions Image-to-Video Dual Resolution","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/I2V-14B/doc/index-en/#core-technical-breakthroughs","text":"","title":"\ud83d\ude80 Core Technical Breakthroughs"},{"location":"Wan2.2/I2V-14B/doc/index-en/#technical-specifications-comparison","text":"Specification Wan2.1-I2V-14B Wan2.2-I2V-A14B Improvement Architecture Type Traditional Diffusion Model MoE Mixture of Experts Architecture Upgrade Parameter Scale 14B 27B (14B activated) +93% Supported Resolution 480P 480P & 720P Dual Resolution Training Data Base Dataset Images +65.6%, Videos +83.2% Massive Expansion Aesthetic Quality Standard Quality Cinematic Aesthetics Quality Leap Motion Stability Basic Stability Significantly Enhanced Stability Improvement Integration Ecosystem ComfyUI ComfyUI + Diffusers + ModelScope Ecosystem Enhancement","title":"\ud83d\udd27 Technical Specifications Comparison"},{"location":"Wan2.2/I2V-14B/doc/index-en/#system-requirements","text":"### \ud83d\udda5\ufe0f Recommended Hardware Configuration Component Recommended Specs Description CPU 16 vCPU High-performance multi-core processor Memory 60 GiB Large capacity system memory GPU 1 \u00d7 NVIDIA A10 Professional GPU with 80GB+ VRAM","title":"\ud83d\udcbb System Requirements"},{"location":"Wan2.2/I2V-14B/doc/index-en/#usage-guide","text":"","title":"\ud83c\udfaf Usage Guide"},{"location":"Wan2.2/I2V-14B/doc/index-en/#comfyui-integration-usage-guide","text":"","title":"\ud83c\udf10 ComfyUI Integration Usage Guide"},{"location":"Wan2.2/I2V-14B/doc/index-en/#comfyui-web-ui-tutorial","text":"### \ud83d\ude80 Step 1: Access Interface \ud83d\udd17 Interface Access Click the access link at the service instance to enter the ComfyUI visual interface ![img_3.png](img_3.png) ### \ud83d\udd27 Step 2: Select Workflow Select the official workflow in the top left and open it ![img_4.png](img_4.png) ![img_3.png](img_3.png) ### \ud83d\udce4 Step 3: Upload Image **In the LoadImage node:** - Select sample images for testing - Or upload custom images from your computer - Supports JPEG, PNG, WebP and other formats ### \u270d\ufe0f Step 4: Set Text Description","title":"\ud83d\udccd ComfyUI Web UI Tutorial"},{"location":"Wan2.2/I2V-14B/doc/index-en/#comfyui-api-calling-method","text":"","title":"\ud83d\udd0c ComfyUI API Calling Method"},{"location":"Wan2.2/I2V-14B/doc/index-en/#get-authentication-information","text":"","title":"\ud83d\udd11 Get Authentication Information"},{"location":"Wan2.2/I2V-14B/doc/index-en/#python-api-call-example","text":"\ud83d\udc0d Click to expand complete Python API call code import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 IMAGE_PATH = \"input_image.jpg\" PROMPT = \"The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14B_I2VClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"\ud83d\udce4 \u4e0a\u4f20\u56fe\u7247\u5230ComfyUI\"\"\" if not os.path.exists(image_path): raise Exception(f\"\u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {image_path}\") try: with open(image_path, 'rb') as f: files = {'image': (os.path.basename(image_path), f, 'image/png')} headers = {} if self.token: headers[\"Authorization\"] = f\"Bearer {self.token}\" response = requests.post(f\"{self.base_url}/upload/image\", files=files, headers=headers) print(f\"Upload response: {response.text}\") if response.status_code != 200: raise Exception(f\"\u4e0a\u4f20\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if 'name' not in result: raise Exception(f\"\u4e0a\u4f20\u54cd\u5e94\u4e2d\u6ca1\u6709\u6587\u4ef6\u540d: {result}\") return result['name'] except Exception as e: raise Exception(f\"\u56fe\u7247\u4e0a\u4f20\u5931\u8d25: {e}\") def generate_wan22_14b_i2v(self, image_path, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83d\udce4 \u6b63\u5728\u4e0a\u4f20\u56fe\u7247...\") image_name = self.upload_image(image_path) print(f\"\u2705 \u56fe\u7247\u4e0a\u4f20\u6210\u529f: {image_name}\") print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684\u56fe\u751f\u89c6\u9891 JSON \u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"63\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"63\", 0], \"negative\": [\"63\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_I2V_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} }, \"62\": { \"inputs\": { \"image\": image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"\u52a0\u8f7d\u56fe\u50cf\"} }, \"63\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"62\", 0] }, \"class_type\": \"WanImageToVideo\", \"_meta\": {\"title\": \"Wan\u56fe\u50cf\u5230\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 14B \u56fe\u751f\u89c6\u9891\u53cc\u9636\u6bb5\u91c7\u6837\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_i2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14B_I2VClient() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u56fe\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcf7 \u8f93\u5165\u56fe\u7247: {IMAGE_PATH}\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u53cc\u9636\u6bb5\u91c7\u6837\uff1a\u9ad8\u566a\u58f0I2V\u6a21\u578b + \u4f4e\u566a\u58f0I2V\u6a21\u578b\") if not os.path.exists(IMAGE_PATH): print(f\"\u274c \u56fe\u7247\u6587\u4ef6\u4e0d\u5b58\u5728: {IMAGE_PATH}\") print(\"\u8bf7\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e0b\u6709\u8f93\u5165\u56fe\u7247\u6587\u4ef6\") return # 14B \u56fe\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_i2v(IMAGE_PATH, PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B I2V Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # 14B\u6a21\u578b\u751f\u6210\u65f6\u95f4\u66f4\u957f\uff0c\u589e\u52a0\u8f6e\u8be2\u95f4\u9694 output_file = client.download_video(task_id, \"wan22_14b_i2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python API Call Example"},{"location":"Wan2.2/I2V-14B/doc/index-en/#comfyui-api-endpoint-description","text":"Endpoint Method Function Description /queue GET Get queue status View current task queue and running status /prompt POST Submit workflow Execute Wan2.2 Image-to-Video task /history/{prompt_id} GET Get execution history View task execution results and outputs /upload/image POST Upload image Upload input image file /view GET Download output file Get generated result files","title":"\ud83d\udd17 ComfyUI API Endpoint Description"},{"location":"Wan2.2/I2V-14B/doc/index-en/#comfyui-node-detailed-description","text":"","title":"\u2699\ufe0f ComfyUI Node Detailed Description"},{"location":"Wan2.2/I2V-14B/doc/index-en/#performance-benchmarks","text":"### \u26a1 Computational Efficiency Comparison GPU Model Single GPU 4\u00d7GPU 8\u00d7GPU Optimization Features A100 80GB Recommended Config Efficient Parallel Optimal Performance FSDP + Ulysses H100 80GB Best Single Card Ultra-fast Processing Top Performance FlashAttention3 RTX 4090 Consumer Choice - - Model Offloading \ud83c\udfaf Performance Advantages \u2022 MoE Architecture: 27B parameters, 14B activated, same inference cost \u2022 Built-in FSDP and Ulysses distributed implementation \u2022 Automatic model parameter type conversion optimization \u2022 Average performance data after warm-up and multiple sampling","title":"\ud83d\udcca Performance Benchmarks"},{"location":"Wan2.2/I2V-14B/doc/index-en/#prompt-writing-guide","text":"","title":"\ud83d\udca1 Prompt Writing Guide"},{"location":"Wan2.2/I2V-14B/doc/index-en/#integration-ecosystem","text":"\ud83c\udfa8 ComfyUI \u2705 Integrated EN/CN Docs Visual workflow, easy to use \ud83e\udd17 Diffusers \u2705 Integrated I2V-A14B Standardized API interface \ud83e\udd16 ModelScope \u2705 Official Support Native Integration Model hosting and distribution \ud83d\udc19 GitHub \u2705 Open Source Complete Code Open source, community-driven","title":"\ud83d\udd17 Integration Ecosystem"},{"location":"Wan2.2/I2V-14B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/I2V-14B/doc/index-en/#license-and-citation","text":"### \ud83d\udccb Open Source License \ud83d\udcdc Apache 2.0 License \u2022 Free to use and distribute \u2022 Commercial-friendly open source license \u2022 Complete usage rights granted \u2022 Must comply with relevant laws and regulations ### \ud83d\udcda Academic Citation @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac Tongyi Wanxiang 2.2 Image-to-Video A14B | Open and Advanced Large-Scale Video Generation Model \u00a9 2025 Tongyi Wanxiang Team | Apache 2.0 Open Source License | Turn Every Image into Amazing Video","title":"\ud83d\udcc4 License and Citation"},{"location":"Wan2.2/IT2V-5B/doc/","text":"\ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u6587\u751f\u89c6\u9891-5B \u9ad8\u6548\u7684\u9ad8\u6e05\u6df7\u5408\u6587\u672c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b - \u6d88\u8d39\u7ea7GPU\u4e13\u4e1a\u7248 \ud83e\udde0 5B \u5bc6\u96c6\u53c2\u6570 \ud83c\udfaf 720P@24fps \u26a1 \u6d88\u8d39\u7ea7GPU \ud83d\udccb \u6a21\u578b\u6982\u89c8 **\u901a\u4e49\u4e07\u76f82.2-\u56fe\u6587\u751f\u89c6\u9891-5B (TI2V-5B)** \u662f\u57fa\u4e8e\u9ad8\u538b\u7f29VAE\u67b6\u6784\u7684\u9769\u547d\u6027\u6587\u672c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u5148\u8fdb\u7684Wan2.2-VAE\u6784\u5efa\uff0c\u5b9e\u73b0\u4e8616\u00d716\u00d74\u7684\u538b\u7f29\u6bd4\uff0c\u652f\u6301720P\u5206\u8fa8\u7387\u300124fps\u7684\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff0c\u53ef\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u5982A10\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u662f\u76ee\u524d\u6700\u5feb\u7684720P@24fps\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e4b\u4e00\u3002 \ud83c\udff7\ufe0f \u6a21\u578b\u6807\u8bc6 Wan-AI/Wan2.2-TI2V-5B \ud83d\udcca \u67b6\u6784\u89c4\u6a21 5B \u5bc6\u96c6\u53c2\u6570 \u9ad8\u538b\u7f29VAE \ud83c\udfaf \u6838\u5fc3\u529f\u80fd \u6587\u672c\u751f\u89c6\u9891 \u56fe\u50cf\u751f\u89c6\u9891 \ud83d\ude80 \u6838\u5fc3\u6280\u672f\u7a81\u7834 \ud83d\udd27 \u9ad8\u538b\u7f29VAE\u67b6\u6784 \u8d85\u9ad8\u538b\u7f29\u6bd4 \uff1a16\u00d716\u00d74 \u538b\u7f29\u7387\uff0c\u603b\u538b\u7f29\u6bd4\u8fbe64\u500d \u8d28\u91cf\u4fdd\u8bc1 \uff1a\u9ad8\u8d28\u91cf\u89c6\u9891\u91cd\u5efa\uff0c\u7ec6\u8282\u4fdd\u6301\u4f18\u5f02 \u5185\u5b58\u4f18\u5316 \uff1a\u663e\u8457\u964d\u4f4eGPU\u5185\u5b58\u5360\u7528 \u901f\u5ea6\u63d0\u5347 \uff1a\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u751f\u6210\u6548\u7387 \u26a1 \u6d88\u8d39\u7ea7GPU\u53cb\u597d A10 \uff1a\u5355\u536124GB\u663e\u5b58\u5373\u53ef\u8fd0\u884c \u5feb\u901f\u751f\u6210 \uff1a9\u5206\u949f\u5185\u751f\u62105\u79d2720P\u89c6\u9891 \u9ad8\u6548\u90e8\u7f72 \uff1a\u65e0\u9700\u7279\u6b8a\u4f18\u5316\u5373\u53ef\u9ad8\u6548\u8fd0\u884c \u6210\u672c\u53cb\u597d \uff1a\u964d\u4f4e\u786c\u4ef6\u95e8\u69db\uff0c\u666e\u53caAI\u89c6\u9891\u751f\u6210 \ud83c\udfac \u7edf\u4e00\u53cc\u6a21\u6001\u751f\u6210 \u6587\u672c\u751f\u89c6\u9891 \uff1a\u7eaf\u6587\u672c\u63cf\u8ff0\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891 \u56fe\u50cf\u751f\u89c6\u9891 \uff1a\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u751f\u6210\u52a8\u6001\u89c6\u9891 \u7edf\u4e00\u6846\u67b6 \uff1a\u5355\u4e00\u6a21\u578b\u652f\u6301\u53cc\u91cd\u529f\u80fd \u65e0\u7f1d\u5207\u6362 \uff1a\u6839\u636e\u8f93\u5165\u81ea\u52a8\u9009\u62e9\u751f\u6210\u6a21\u5f0f \ud83d\udcc8 \u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf 720P\u9ad8\u6e05 \uff1a\u652f\u63011280\u00d7704\u5206\u8fa8\u7387\u8f93\u51fa 24fps\u6d41\u7545 \uff1a\u7535\u5f71\u7ea7\u5e27\u7387\uff0c\u52a8\u4f5c\u81ea\u7136\u6d41\u7545 \u7f8e\u5b66\u4f18\u5316 \uff1a\u7ee7\u627fWan2.2\u7535\u5f71\u7ea7\u7f8e\u5b66\u7279\u6027 \u7ec6\u8282\u4e30\u5bcc \uff1a\u7cbe\u7ec6\u7684\u7eb9\u7406\u548c\u52a8\u6001\u8868\u73b0 \ud83d\udd27 \u6280\u672f\u89c4\u683c\u5bf9\u6bd4 \u89c4\u683c\u9879\u76ee \u4f20\u7edf\u6a21\u578b Wan2.2-TI2V-5B \u4f18\u52bf\u7279\u70b9 \u67b6\u6784\u7c7b\u578b \u4f20\u7edf\u6269\u6563\u6a21\u578b \u9ad8\u538b\u7f29VAE + \u5bc6\u96c6\u6a21\u578b \u67b6\u6784\u521b\u65b0 \u53c2\u6570\u89c4\u6a21 14B+ 5B \u5bc6\u96c6\u53c2\u6570 \u8f7b\u91cf\u9ad8\u6548 \u652f\u6301\u5206\u8fa8\u7387 480P-720P 720P@24fps (1280\u00d7704) \u9ad8\u6e05\u6d41\u7545 \u751f\u6210\u6a21\u5f0f \u5355\u4e00\u6a21\u5f0f \u6587\u672c+\u56fe\u50cf\u53cc\u6a21\u6001 \u7edf\u4e00\u6846\u67b6 \u786c\u4ef6\u8981\u6c42 80GB+ \u4e13\u4e1a\u5361 24GB \u6d88\u8d39\u7ea7GPU \u95e8\u69db\u964d\u4f4e \u751f\u6210\u901f\u5ea6 15-30\u5206\u949f 9\u5206\u949f/5\u79d2\u89c6\u9891 \u901f\u5ea6\u9886\u5148 \u96c6\u6210\u751f\u6001 \u6709\u9650\u652f\u6301 ComfyUI + Diffusers + ModelScope \u751f\u6001\u5b8c\u5584 \u26a1 \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 \ud83c\udfc6 \u884c\u4e1a\u9886\u5148\u6027\u80fd TI2V-5B\u662f\u76ee\u524d\u53ef\u7528\u7684\u6700\u5feb720P@24fps\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e4b\u4e00\uff0c\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u751f\u6210\u6548\u7387\uff0c\u6ee1\u8db3\u5de5\u4e1a\u5e94\u7528\u548c\u5b66\u672f\u7814\u7a76\u7684\u53cc\u91cd\u9700\u6c42\u3002 \ud83c\udfac \u4f7f\u7528\u6307\u5357 \u652f\u6301\u6587\u751f\u89c6\u9891\u6216\u56fe\u751f\u89c6\u9891 \ud83c\udfaf ComfyUI \u96c6\u6210\u6307\u5357 ### \ud83d\ude80 \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \ud83d\udd17 \u754c\u9762\u8bbf\u95ee \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5\uff0c\u8fdb\u5165 ComfyUI \u53ef\u89c6\u5316\u754c\u9762 ![img_2.png](img_2.png) ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9TI2V\u5de5\u4f5c\u6d41 \ud83d\udccb \u5de5\u4f5c\u6d41\u9009\u62e9 \u9009\u62e9\u5de6\u4e0a\u89d2\u7684TI2V-5B\u4e13\u7528\u5de5\u4f5c\u6d41\uff0c\u786e\u4fdd\u4f7f\u7528\u6b63\u786e\u7684\u6a21\u578b\u914d\u7f6e ![img_4.png](img_4.png) ![img_6.png](img_6.png) ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u914d\u7f6e\u8f93\u5165 **\u6587\u672c\u5230\u89c6\u9891\u6a21\u5f0f\uff1a** - \u5728\u6587\u672c\u8f93\u5165\u6846\u586b\u5199\u8be6\u7ec6\u7684\u63d0\u793a\u8bcd - \u8bbe\u7f6e\u5206\u8fa8\u7387\u4e3a1280\u00d7704 - \u914d\u7f6e\u5e27\u6570\u548c\u5176\u4ed6\u53c2\u6570 **\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u5f0f\uff1a** - \u4e0a\u4f20\u8f93\u5165\u56fe\u50cf\uff08\u652f\u6301JPG\u3001PNG\u3001WebP\uff09 - \u586b\u5199\u63cf\u8ff0\u56fe\u50cf\u52a8\u4f5c\u7684\u63d0\u793a\u8bcd - \u7cfb\u7edf\u81ea\u52a8\u9002\u914d\u56fe\u50cf\u5bbd\u9ad8\u6bd4 \ud83d\udca1 \u8f93\u5165\u8981\u6c42 \u2022 \u56fe\u50cf\u5206\u8fa8\u7387\uff1a\u63a8\u83501280\u00d7704\u6216\u76f8\u8fd1\u6bd4\u4f8b \u2022 \u6587\u4ef6\u5927\u5c0f\uff1a\u5efa\u8bae\u5c0f\u4e8e10MB \u2022 \u63d0\u793a\u8bcd\u957f\u5ea6\uff1a\u5efa\u8bae50-200\u5b57\u7b26 ### \u2699\ufe0f \u6b65\u9aa4\u56db\uff1a\u53c2\u6570\u914d\u7f6e \u53c2\u6570\u540d\u79f0 \u63a8\u8350\u503c \u8bf4\u660e generation_width 1280 \u89c6\u9891\u5bbd\u5ea6\uff08720P\u6807\u51c6\uff09 generation_height 704 \u89c6\u9891\u9ad8\u5ea6\uff0816:9\u6bd4\u4f8b\uff09 num_frames 121 \u89c6\u9891\u5e27\u6570\uff085\u79d2@24fps\uff09 fps 24 \u5e27\u7387\u8bbe\u7f6e guidance_scale 7.0 \u5f15\u5bfc\u5f3a\u5ea6 ### \ud83c\udfac \u6b65\u9aa4\u4e94\uff1a\u6267\u884c\u751f\u6210 \ud83d\ude80 \u5f00\u59cb\u751f\u6210 \u70b9\u51fb\u53f3\u4fa7\u9762\u677f\u7684 \"Queue Prompt\" \u6309\u94ae\u5f00\u59cb\u751f\u6210\u89c6\u9891 \u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u5728\u8fdb\u5ea6\u6761\u67e5\u770b\u5b9e\u65f6\u72b6\u6001\uff0c\u9884\u8ba19\u5206\u949f\u5b8c\u6210 \ud83d\udd0c ComfyUI API \u8c03\u7528\u65b9\u5f0f \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83d\udd10 \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u8bbe\u7f6e\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\u83b7\u53d6API Token \ud83d\udd11 API Token \u83b7\u53d6\u754c\u9762 ![img_1.png](img_1.png) \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 \u4ece\u670d\u52a1\u5b9e\u4f8b\u4fe1\u606f\u4e2d\u83b7\u53d6 COMFYUI_SERVER \u5730\u5740 \ud83c\udf10 \u670d\u52a1\u5668\u5730\u5740\u914d\u7f6e ![img_2.png](img_2.png) \ud83d\udcbb Python API \u8c03\u7528\u793a\u4f8b \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_MODEL = \"wan2.2_ti2v_5B_fp16.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan2.2_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"Low contrast. In a retro 1970s-style subway station, a street musician plays in dim colors and rough textures. He wears an old jacket, playing guitar with focus. Commuters hurry by, and a small crowd gathers to listen. The camera slowly moves right, capturing the blend of music and city noise, with old subway signs and mottled walls in the background.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22Client: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_t2v(self, prompt, neg_prompt, steps=20, cfg=5, width=1280, height=704, frames=121): \"\"\"\ud83c\udfac Wan2.2 \u6587\u751f\u89c6\u9891\u751f\u6210 - \u57fa\u4e8e\u539f\u59cbJSON\u5de5\u4f5c\u6d41\"\"\" print(\"\ud83c\udfac \u5f00\u59cb\u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684JSON\u5de5\u4f5c\u6d41\uff0c\u53ea\u4fee\u6539\u63d0\u793a\u8bcd workflow = { \"3\": { \"inputs\": { \"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"48\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"55\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\"} }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"48\": { \"inputs\": { \"shift\": 8, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"vae\": [\"39\", 0] }, \"class_type\": \"Wan22ImageToVideoLatent\", # \u4fdd\u6301\u539f\u59cb\u8282\u70b9\u540d\u79f0 \"_meta\": {\"title\": \"Wan22ImageToVideoLatent\"} }, \"57\": { \"inputs\": { \"fps\": 24, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"58\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"57\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 \u6587\u751f\u89c6\u9891\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_t2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22Client() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") # \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_t2v(PROMPT, NEG_PROMPT, 20, 5, 1280, 704, 121) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"wan22_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udd17 ComfyUI API \u7aef\u70b9\u8bf4\u660e \u7aef\u70b9 \u65b9\u6cd5 \u529f\u80fd \u8bf4\u660e /queue GET \u83b7\u53d6\u961f\u5217\u72b6\u6001 \u67e5\u770b\u5f53\u524d\u4efb\u52a1\u961f\u5217\u548c\u8fd0\u884c\u72b6\u6001 /prompt POST \u63d0\u4ea4\u5de5\u4f5c\u6d41 \u6267\u884c Wan2.2 \u56fe\u751f\u89c6\u9891\u4efb\u52a1 /history/{prompt_id} GET \u83b7\u53d6\u6267\u884c\u5386\u53f2 \u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c\u548c\u8f93\u51fa /upload/image POST \u4e0a\u4f20\u56fe\u7247 \u4e0a\u4f20\u8f93\u5165\u56fe\u7247\u6587\u4ef6 /view GET \u4e0b\u8f7d\u8f93\u51fa\u6587\u4ef6 \u83b7\u53d6\u751f\u6210\u7684\u7ed3\u679c\u6587\u4ef6 \ud83d\udca1 \u63d0\u793a\u8bcd\u4f18\u5316\u6307\u5357 \u2705 \u9ad8\u8d28\u91cf\u63d0\u793a\u8bcd\u793a\u4f8b \u52a8\u4f5c\u573a\u666f\uff1a \"Two anthropomorphic cats in boxing gear fight intensely on a spotlighted stage, dynamic movements, professional lighting\" \u81ea\u7136\u573a\u666f\uff1a \"A white cat on a surfboard at the beach, summer vacation style, relaxed expression, crystal-clear waters background\" \ud83c\udfaf \u63d0\u793a\u8bcd\u7f16\u5199\u6280\u5de7 \u5177\u4f53\u63cf\u8ff0 \uff1a\u8be6\u7ec6\u63cf\u8ff0\u573a\u666f\u3001\u52a8\u4f5c\u3001\u60c5\u611f \u89c6\u89c9\u5143\u7d20 \uff1a\u5305\u542b\u5149\u7167\u3001\u6784\u56fe\u3001\u8272\u5f69\u4fe1\u606f \u52a8\u6001\u8868\u73b0 \uff1a\u660e\u786e\u6307\u5b9a\u8fd0\u52a8\u7c7b\u578b\u548c\u5f3a\u5ea6 \u98ce\u683c\u5b9a\u4e49 \uff1a\u6307\u5b9a\u827a\u672f\u98ce\u683c\u6216\u62cd\u6444\u98ce\u683c \u8d28\u91cf\u5173\u952e\u8bcd \uff1a\u6dfb\u52a0\"high quality\"\u3001\"cinematic\"\u7b49 \ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a \ud83c\udfac \u5185\u5bb9\u521b\u4f5c \u77ed\u89c6\u9891\u5236\u4f5c\u3001\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u3001\u521b\u610f\u52a8\u753b \ud83d\udecd\ufe0f \u5546\u4e1a\u5e94\u7528 \u4ea7\u54c1\u5c55\u793a\u3001\u5e7f\u544a\u5236\u4f5c\u3001\u8425\u9500\u89c6\u9891 \ud83c\udf93 \u6559\u80b2\u57f9\u8bad \u6559\u5b66\u52a8\u753b\u3001\u6f14\u793a\u89c6\u9891\u3001\u8bfe\u7a0b\u5185\u5bb9 \ud83d\udd2c \u5b66\u672f\u7814\u7a76 \u7b97\u6cd5\u9a8c\u8bc1\u3001\u6a21\u578b\u5bf9\u6bd4\u3001\u6280\u672f\u63a2\u7d22 \ud83d\udcca \u6027\u80fd\u57fa\u51c6\u5bf9\u6bd4 ### \ud83c\udfc6 \u4e0eSOTA\u6a21\u578b\u5bf9\u6bd4 \ud83c\udfaf Wan-Bench 2.0 \u8bc4\u6d4b\u7ed3\u679c \u5728\u6700\u65b0\u7684Wan-Bench 2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTI2V-5B\u5728\u591a\u4e2a\u5173\u952e\u7ef4\u5ea6\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002 \u8bc4\u6d4b\u7ef4\u5ea6 TI2V-5B \u7ade\u54c1A \u7ade\u54c1B \u89c6\u89c9\u8d28\u91cf 8.7/10 8.2/10 7.9/10 \u8fd0\u52a8\u81ea\u7136\u5ea6 8.5/10 8.1/10 7.8/10 \u6587\u672c\u4e00\u81f4\u6027 9.1/10 8.6/10 8.3/10 \u751f\u6210\u901f\u5ea6 9.3/10 7.5/10 6.8/10 \u786c\u4ef6\u53cb\u597d\u5ea6 9.5/10 6.2/10 5.9/10 \ud83d\udd17 \u96c6\u6210\u751f\u6001 \ud83c\udfa8 ComfyUI \ud83d\udea7 \u5df2\u5b8c\u6210 \u5373\u5c06\u53d1\u5e03 \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u96c6\u6210 \ud83e\udd16 ModelScope \u2705 \u5df2\u652f\u6301 \u539f\u751f\u96c6\u6210 \u6a21\u578b\u6258\u7ba1\u4e0e\u5206\u53d1 \ud83d\udcc4 \u8bb8\u53ef\u4e0e\u5f15\u7528 ### \ud83d\udccb \u5f00\u6e90\u534f\u8bae \ud83d\udcdc Apache 2.0 \u8bb8\u53ef\u8bc1 \u2022 \u81ea\u7531\u4f7f\u7528\u548c\u5206\u53d1 \u2022 \u5546\u4e1a\u53cb\u597d\u7684\u5f00\u6e90\u534f\u8bae \u2022 \u5b8c\u6574\u7684\u4f7f\u7528\u6743\u9650\u6388\u4e88 \u2022 \u9700\u9075\u5b88\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4 ### \ud83d\udcda \u5b66\u672f\u5f15\u7528 @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u6587\u751f\u89c6\u9891-5B | \u9ad8\u6548\u7684\u9ad8\u6e05\u6df7\u5408\u6587\u672c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b \u00a9 Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u521b\u610f\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u81ea\u7531\u98de\u7fd4","title":"Index"},{"location":"Wan2.2/IT2V-5B/doc/#_1","text":"**\u901a\u4e49\u4e07\u76f82.2-\u56fe\u6587\u751f\u89c6\u9891-5B (TI2V-5B)** \u662f\u57fa\u4e8e\u9ad8\u538b\u7f29VAE\u67b6\u6784\u7684\u9769\u547d\u6027\u6587\u672c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u5148\u8fdb\u7684Wan2.2-VAE\u6784\u5efa\uff0c\u5b9e\u73b0\u4e8616\u00d716\u00d74\u7684\u538b\u7f29\u6bd4\uff0c\u652f\u6301720P\u5206\u8fa8\u7387\u300124fps\u7684\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff0c\u53ef\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u5982A10\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u662f\u76ee\u524d\u6700\u5feb\u7684720P@24fps\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e4b\u4e00\u3002 \ud83c\udff7\ufe0f \u6a21\u578b\u6807\u8bc6 Wan-AI/Wan2.2-TI2V-5B \ud83d\udcca \u67b6\u6784\u89c4\u6a21 5B \u5bc6\u96c6\u53c2\u6570 \u9ad8\u538b\u7f29VAE \ud83c\udfaf \u6838\u5fc3\u529f\u80fd \u6587\u672c\u751f\u89c6\u9891 \u56fe\u50cf\u751f\u89c6\u9891","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/IT2V-5B/doc/#_2","text":"","title":"\ud83d\ude80 \u6838\u5fc3\u6280\u672f\u7a81\u7834"},{"location":"Wan2.2/IT2V-5B/doc/#_3","text":"\u89c4\u683c\u9879\u76ee \u4f20\u7edf\u6a21\u578b Wan2.2-TI2V-5B \u4f18\u52bf\u7279\u70b9 \u67b6\u6784\u7c7b\u578b \u4f20\u7edf\u6269\u6563\u6a21\u578b \u9ad8\u538b\u7f29VAE + \u5bc6\u96c6\u6a21\u578b \u67b6\u6784\u521b\u65b0 \u53c2\u6570\u89c4\u6a21 14B+ 5B \u5bc6\u96c6\u53c2\u6570 \u8f7b\u91cf\u9ad8\u6548 \u652f\u6301\u5206\u8fa8\u7387 480P-720P 720P@24fps (1280\u00d7704) \u9ad8\u6e05\u6d41\u7545 \u751f\u6210\u6a21\u5f0f \u5355\u4e00\u6a21\u5f0f \u6587\u672c+\u56fe\u50cf\u53cc\u6a21\u6001 \u7edf\u4e00\u6846\u67b6 \u786c\u4ef6\u8981\u6c42 80GB+ \u4e13\u4e1a\u5361 24GB \u6d88\u8d39\u7ea7GPU \u95e8\u69db\u964d\u4f4e \u751f\u6210\u901f\u5ea6 15-30\u5206\u949f 9\u5206\u949f/5\u79d2\u89c6\u9891 \u901f\u5ea6\u9886\u5148 \u96c6\u6210\u751f\u6001 \u6709\u9650\u652f\u6301 ComfyUI + Diffusers + ModelScope \u751f\u6001\u5b8c\u5584","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c\u5bf9\u6bd4"},{"location":"Wan2.2/IT2V-5B/doc/#_4","text":"\ud83c\udfc6 \u884c\u4e1a\u9886\u5148\u6027\u80fd TI2V-5B\u662f\u76ee\u524d\u53ef\u7528\u7684\u6700\u5feb720P@24fps\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e4b\u4e00\uff0c\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u751f\u6210\u6548\u7387\uff0c\u6ee1\u8db3\u5de5\u4e1a\u5e94\u7528\u548c\u5b66\u672f\u7814\u7a76\u7684\u53cc\u91cd\u9700\u6c42\u3002","title":"\u26a1 \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5"},{"location":"Wan2.2/IT2V-5B/doc/#_5","text":"\u652f\u6301\u6587\u751f\u89c6\u9891\u6216\u56fe\u751f\u89c6\u9891","title":"\ud83c\udfac \u4f7f\u7528\u6307\u5357"},{"location":"Wan2.2/IT2V-5B/doc/#comfyui","text":"### \ud83d\ude80 \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \ud83d\udd17 \u754c\u9762\u8bbf\u95ee \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5\uff0c\u8fdb\u5165 ComfyUI \u53ef\u89c6\u5316\u754c\u9762 ![img_2.png](img_2.png) ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9TI2V\u5de5\u4f5c\u6d41 \ud83d\udccb \u5de5\u4f5c\u6d41\u9009\u62e9 \u9009\u62e9\u5de6\u4e0a\u89d2\u7684TI2V-5B\u4e13\u7528\u5de5\u4f5c\u6d41\uff0c\u786e\u4fdd\u4f7f\u7528\u6b63\u786e\u7684\u6a21\u578b\u914d\u7f6e ![img_4.png](img_4.png) ![img_6.png](img_6.png) ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u914d\u7f6e\u8f93\u5165 **\u6587\u672c\u5230\u89c6\u9891\u6a21\u5f0f\uff1a** - \u5728\u6587\u672c\u8f93\u5165\u6846\u586b\u5199\u8be6\u7ec6\u7684\u63d0\u793a\u8bcd - \u8bbe\u7f6e\u5206\u8fa8\u7387\u4e3a1280\u00d7704 - \u914d\u7f6e\u5e27\u6570\u548c\u5176\u4ed6\u53c2\u6570 **\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u5f0f\uff1a** - \u4e0a\u4f20\u8f93\u5165\u56fe\u50cf\uff08\u652f\u6301JPG\u3001PNG\u3001WebP\uff09 - \u586b\u5199\u63cf\u8ff0\u56fe\u50cf\u52a8\u4f5c\u7684\u63d0\u793a\u8bcd - \u7cfb\u7edf\u81ea\u52a8\u9002\u914d\u56fe\u50cf\u5bbd\u9ad8\u6bd4 \ud83d\udca1 \u8f93\u5165\u8981\u6c42 \u2022 \u56fe\u50cf\u5206\u8fa8\u7387\uff1a\u63a8\u83501280\u00d7704\u6216\u76f8\u8fd1\u6bd4\u4f8b \u2022 \u6587\u4ef6\u5927\u5c0f\uff1a\u5efa\u8bae\u5c0f\u4e8e10MB \u2022 \u63d0\u793a\u8bcd\u957f\u5ea6\uff1a\u5efa\u8bae50-200\u5b57\u7b26 ### \u2699\ufe0f \u6b65\u9aa4\u56db\uff1a\u53c2\u6570\u914d\u7f6e \u53c2\u6570\u540d\u79f0 \u63a8\u8350\u503c \u8bf4\u660e generation_width 1280 \u89c6\u9891\u5bbd\u5ea6\uff08720P\u6807\u51c6\uff09 generation_height 704 \u89c6\u9891\u9ad8\u5ea6\uff0816:9\u6bd4\u4f8b\uff09 num_frames 121 \u89c6\u9891\u5e27\u6570\uff085\u79d2@24fps\uff09 fps 24 \u5e27\u7387\u8bbe\u7f6e guidance_scale 7.0 \u5f15\u5bfc\u5f3a\u5ea6 ### \ud83c\udfac \u6b65\u9aa4\u4e94\uff1a\u6267\u884c\u751f\u6210 \ud83d\ude80 \u5f00\u59cb\u751f\u6210 \u70b9\u51fb\u53f3\u4fa7\u9762\u677f\u7684 \"Queue Prompt\" \u6309\u94ae\u5f00\u59cb\u751f\u6210\u89c6\u9891 \u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u5728\u8fdb\u5ea6\u6761\u67e5\u770b\u5b9e\u65f6\u72b6\u6001\uff0c\u9884\u8ba19\u5206\u949f\u5b8c\u6210","title":"\ud83c\udfaf ComfyUI \u96c6\u6210\u6307\u5357"},{"location":"Wan2.2/IT2V-5B/doc/#comfyui-api","text":"","title":"\ud83d\udd0c ComfyUI API \u8c03\u7528\u65b9\u5f0f"},{"location":"Wan2.2/IT2V-5B/doc/#_6","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"Wan2.2/IT2V-5B/doc/#python-api","text":"\ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_MODEL = \"wan2.2_ti2v_5B_fp16.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan2.2_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"Low contrast. In a retro 1970s-style subway station, a street musician plays in dim colors and rough textures. He wears an old jacket, playing guitar with focus. Commuters hurry by, and a small crowd gathers to listen. The camera slowly moves right, capturing the blend of music and city noise, with old subway signs and mottled walls in the background.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22Client: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_t2v(self, prompt, neg_prompt, steps=20, cfg=5, width=1280, height=704, frames=121): \"\"\"\ud83c\udfac Wan2.2 \u6587\u751f\u89c6\u9891\u751f\u6210 - \u57fa\u4e8e\u539f\u59cbJSON\u5de5\u4f5c\u6d41\"\"\" print(\"\ud83c\udfac \u5f00\u59cb\u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684JSON\u5de5\u4f5c\u6d41\uff0c\u53ea\u4fee\u6539\u63d0\u793a\u8bcd workflow = { \"3\": { \"inputs\": { \"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"48\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"55\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\"} }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"48\": { \"inputs\": { \"shift\": 8, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"vae\": [\"39\", 0] }, \"class_type\": \"Wan22ImageToVideoLatent\", # \u4fdd\u6301\u539f\u59cb\u8282\u70b9\u540d\u79f0 \"_meta\": {\"title\": \"Wan22ImageToVideoLatent\"} }, \"57\": { \"inputs\": { \"fps\": 24, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"58\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"57\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 \u6587\u751f\u89c6\u9891\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_t2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22Client() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") # \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_t2v(PROMPT, NEG_PROMPT, 20, 5, 1280, 704, 121) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"wan22_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python API \u8c03\u7528\u793a\u4f8b"},{"location":"Wan2.2/IT2V-5B/doc/#comfyui-api_1","text":"\u7aef\u70b9 \u65b9\u6cd5 \u529f\u80fd \u8bf4\u660e /queue GET \u83b7\u53d6\u961f\u5217\u72b6\u6001 \u67e5\u770b\u5f53\u524d\u4efb\u52a1\u961f\u5217\u548c\u8fd0\u884c\u72b6\u6001 /prompt POST \u63d0\u4ea4\u5de5\u4f5c\u6d41 \u6267\u884c Wan2.2 \u56fe\u751f\u89c6\u9891\u4efb\u52a1 /history/{prompt_id} GET \u83b7\u53d6\u6267\u884c\u5386\u53f2 \u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c\u548c\u8f93\u51fa /upload/image POST \u4e0a\u4f20\u56fe\u7247 \u4e0a\u4f20\u8f93\u5165\u56fe\u7247\u6587\u4ef6 /view GET \u4e0b\u8f7d\u8f93\u51fa\u6587\u4ef6 \u83b7\u53d6\u751f\u6210\u7684\u7ed3\u679c\u6587\u4ef6","title":"\ud83d\udd17 ComfyUI API \u7aef\u70b9\u8bf4\u660e"},{"location":"Wan2.2/IT2V-5B/doc/#_7","text":"","title":"\ud83d\udca1 \u63d0\u793a\u8bcd\u4f18\u5316\u6307\u5357"},{"location":"Wan2.2/IT2V-5B/doc/#_8","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a"},{"location":"Wan2.2/IT2V-5B/doc/#_9","text":"### \ud83c\udfc6 \u4e0eSOTA\u6a21\u578b\u5bf9\u6bd4 \ud83c\udfaf Wan-Bench 2.0 \u8bc4\u6d4b\u7ed3\u679c \u5728\u6700\u65b0\u7684Wan-Bench 2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTI2V-5B\u5728\u591a\u4e2a\u5173\u952e\u7ef4\u5ea6\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002 \u8bc4\u6d4b\u7ef4\u5ea6 TI2V-5B \u7ade\u54c1A \u7ade\u54c1B \u89c6\u89c9\u8d28\u91cf 8.7/10 8.2/10 7.9/10 \u8fd0\u52a8\u81ea\u7136\u5ea6 8.5/10 8.1/10 7.8/10 \u6587\u672c\u4e00\u81f4\u6027 9.1/10 8.6/10 8.3/10 \u751f\u6210\u901f\u5ea6 9.3/10 7.5/10 6.8/10 \u786c\u4ef6\u53cb\u597d\u5ea6 9.5/10 6.2/10 5.9/10","title":"\ud83d\udcca \u6027\u80fd\u57fa\u51c6\u5bf9\u6bd4"},{"location":"Wan2.2/IT2V-5B/doc/#_10","text":"\ud83c\udfa8 ComfyUI \ud83d\udea7 \u5df2\u5b8c\u6210 \u5373\u5c06\u53d1\u5e03 \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u96c6\u6210 \ud83e\udd16 ModelScope \u2705 \u5df2\u652f\u6301 \u539f\u751f\u96c6\u6210 \u6a21\u578b\u6258\u7ba1\u4e0e\u5206\u53d1","title":"\ud83d\udd17 \u96c6\u6210\u751f\u6001"},{"location":"Wan2.2/IT2V-5B/doc/#_11","text":"### \ud83d\udccb \u5f00\u6e90\u534f\u8bae \ud83d\udcdc Apache 2.0 \u8bb8\u53ef\u8bc1 \u2022 \u81ea\u7531\u4f7f\u7528\u548c\u5206\u53d1 \u2022 \u5546\u4e1a\u53cb\u597d\u7684\u5f00\u6e90\u534f\u8bae \u2022 \u5b8c\u6574\u7684\u4f7f\u7528\u6743\u9650\u6388\u4e88 \u2022 \u9700\u9075\u5b88\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4 ### \ud83d\udcda \u5b66\u672f\u5f15\u7528 @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u6587\u751f\u89c6\u9891-5B | \u9ad8\u6548\u7684\u9ad8\u6e05\u6df7\u5408\u6587\u672c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b \u00a9 Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u521b\u610f\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u81ea\u7531\u98de\u7fd4","title":"\ud83d\udcc4 \u8bb8\u53ef\u4e0e\u5f15\u7528"},{"location":"Wan2.2/IT2V-5B/doc/index-en/","text":"\ud83c\udfac Tongyi Wanxiang 2.2-Text-Image-to-Video-5B Efficient HD Hybrid Text-Image-to-Video Generation Model - Consumer GPU Professional Edition \ud83e\udde0 5B Dense Parameters \ud83c\udfaf 720P@24fps \u26a1 Consumer GPU \ud83d\udccb Model Overview **Tongyi Wanxiang 2.2-Text-Image-to-Video-5B (TI2V-5B)** is a revolutionary text-image-to-video generation model based on high-compression VAE architecture. Built with the advanced Wan2.2-VAE, this model achieves a 16\u00d716\u00d74 compression ratio, supporting 720P resolution and 24fps text-to-video and image-to-video generation. It can run efficiently on a single consumer-grade GPU like A10, making it one of the fastest 720P@24fps video generation models available. \ud83c\udff7\ufe0f Model Identifier Wan-AI/Wan2.2-TI2V-5B \ud83d\udcca Architecture Scale 5B Dense Parameters High-Compression VAE \ud83c\udfaf Core Functions Text-to-Video Image-to-Video \ud83d\ude80 Core Technical Breakthroughs \ud83d\udd27 High-Compression VAE Architecture Ultra-High Compression : 16\u00d716\u00d74 compression ratio, total 64x compression Quality Assurance : High-quality video reconstruction with excellent detail preservation Memory Optimization : Significantly reduces GPU memory usage Speed Enhancement : Accelerates inference process and improves generation efficiency \u26a1 Consumer GPU Friendly A10 Compatible : Runs on single card with 24GB VRAM Fast Generation : Generates 5-second 720P video in 9 minutes Efficient Deployment : Runs efficiently without special optimization Cost-Effective : Lowers hardware barriers, democratizes AI video generation \ud83c\udfac Unified Dual-Modal Generation Text-to-Video : Generate high-quality videos from pure text descriptions Image-to-Video : Create dynamic videos based on input images Unified Framework : Single model supports dual functionality Seamless Switching : Automatically selects generation mode based on input \ud83d\udcc8 Cinematic-Grade Aesthetic Quality 720P HD : Supports 1280\u00d7704 resolution output 24fps Smooth : Cinematic frame rate with natural, fluid motion Aesthetic Optimization : Inherits Wan2.2 cinematic-grade aesthetic features Rich Details : Fine textures and dynamic expressions \ud83d\udd27 Technical Specifications Comparison Specification Traditional Models Wan2.2-TI2V-5B Advantages Architecture Type Traditional Diffusion Model High-Compression VAE + Dense Model Architecture Innovation Parameter Scale 14B+ 5B Dense Parameters Lightweight & Efficient Supported Resolution 480P-720P 720P@24fps (1280\u00d7704) HD & Smooth Generation Modes Single Mode Text + Image Dual-Modal Unified Framework Hardware Requirements 80GB+ Professional Cards 24GB Consumer GPU Lower Barrier Generation Speed 15-30 minutes 9 minutes/5-second video Speed Leading Integration Ecosystem Limited Support ComfyUI + Diffusers + ModelScope Complete Ecosystem \u26a1 Performance Benchmark \ud83c\udfc6 Industry-Leading Performance TI2V-5B is one of the fastest available 720P@24fps video generation models, achieving unprecedented generation efficiency on consumer GPUs, meeting the dual demands of industrial applications and academic research. \ud83c\udfac Usage Guide Supports both text-to-video and image-to-video generation \ud83c\udfaf ComfyUI Integration Guide ### \ud83d\ude80 Step 1: Access Interface \ud83d\udd17 Interface Access Click the access link at the service instance to enter the ComfyUI visual interface ![img_2.png](img_2.png) ### \ud83d\udd27 Step 2: Select TI2V Workflow \ud83d\udccb Workflow Selection Select the TI2V-5B dedicated workflow from the top-left corner, ensuring correct model configuration ![img_4.png](img_4.png) ![img_6.png](img_6.png) ### \ud83d\udce4 Step 3: Configure Input **Text-to-Video Mode:** - Fill in detailed prompts in the text input box - Set resolution to 1280\u00d7704 - Configure frame count and other parameters **Image-to-Video Mode:** - Upload input image (supports JPG, PNG, WebP) - Fill in prompts describing image actions - System automatically adapts to image aspect ratio \ud83d\udca1 Input Requirements \u2022 Image Resolution: Recommended 1280\u00d7704 or similar ratio \u2022 File Size: Suggested under 10MB \u2022 Prompt Length: Recommended 50-200 characters ### \u2699\ufe0f Step 4: Parameter Configuration Parameter Name Recommended Value Description generation_width 1280 Video width (720P standard) generation_height 704 Video height (16:9 ratio) num_frames 121 Video frames (5 seconds@24fps) fps 24 Frame rate setting guidance_scale 7.0 Guidance strength ### \ud83c\udfac Step 5: Execute Generation \ud83d\ude80 Start Generation Click the \"Queue Prompt\" button in the right panel to start video generation Monitor real-time status in the progress bar, expected completion in 9 minutes \ud83d\udd0c ComfyUI API Integration \ud83d\udd11 Authentication Information \ud83d\udd10 Get Token Click the settings button in the top-right corner, open the bottom panel to get API Token \ud83d\udd11 API Token Interface ![img_1.png](img_1.png) \ud83c\udf10 Get Server Address Get COMFYUI_SERVER address from service instance information \ud83c\udf10 Server Address Configuration ![img_2.png](img_2.png) \ud83d\udcbb Python API Example \ud83d\udc0d Click to expand complete Python API code import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_MODEL = \"wan2.2_ti2v_5B_fp16.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan2.2_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"Low contrast. In a retro 1970s-style subway station, a street musician plays in dim colors and rough textures. He wears an old jacket, playing guitar with focus. Commuters hurry by, and a small crowd gathers to listen. The camera slowly moves right, capturing the blend of music and city noise, with old subway signs and mottled walls in the background.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22Client: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_t2v(self, prompt, neg_prompt, steps=20, cfg=5, width=1280, height=704, frames=121): \"\"\"\ud83c\udfac Wan2.2 \u6587\u751f\u89c6\u9891\u751f\u6210 - \u57fa\u4e8e\u539f\u59cbJSON\u5de5\u4f5c\u6d41\"\"\" print(\"\ud83c\udfac \u5f00\u59cb\u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684JSON\u5de5\u4f5c\u6d41\uff0c\u53ea\u4fee\u6539\u63d0\u793a\u8bcd workflow = { \"3\": { \"inputs\": { \"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"48\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"55\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\"} }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"48\": { \"inputs\": { \"shift\": 8, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"vae\": [\"39\", 0] }, \"class_type\": \"Wan22ImageToVideoLatent\", # \u4fdd\u6301\u539f\u59cb\u8282\u70b9\u540d\u79f0 \"_meta\": {\"title\": \"Wan22ImageToVideoLatent\"} }, \"57\": { \"inputs\": { \"fps\": 24, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"58\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"57\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 \u6587\u751f\u89c6\u9891\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_t2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22Client() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") # \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_t2v(PROMPT, NEG_PROMPT, 20, 5, 1280, 704, 121) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"wan22_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udd17 ComfyUI API Endpoints Endpoint Method Function Description /queue GET Get queue status View current task queue and running status /prompt POST Submit workflow Execute Wan2.2 TI2V tasks /history/{prompt_id} GET Get execution history View task execution results and outputs /upload/image POST Upload image Upload input image files /view GET Download output files Get generated result files \ud83d\udca1 Prompt Optimization Guide \u2705 High-Quality Prompt Examples Action Scene: \"Two anthropomorphic cats in boxing gear fight intensely on a spotlighted stage, dynamic movements, professional lighting\" Natural Scene: \"A white cat on a surfboard at the beach, summer vacation style, relaxed expression, crystal-clear waters background\" \ud83c\udfaf Prompt Writing Tips Specific Description : Detailed scene, action, and emotion descriptions Visual Elements : Include lighting, composition, and color information Dynamic Expression : Clearly specify motion type and intensity Style Definition : Specify artistic or filming style Quality Keywords : Add \"high quality\", \"cinematic\", etc. \ud83c\udfaf Application Scenarios \ud83c\udfac Content Creation Short video production, social media content, creative animation \ud83d\udecd\ufe0f Commercial Applications Product showcases, advertising production, marketing videos \ud83c\udf93 Education & Training Educational animations, demonstration videos, course content \ud83d\udd2c Academic Research Algorithm validation, model comparison, technical exploration \ud83d\udcca Performance Benchmark Comparison ### \ud83c\udfc6 Comparison with SOTA Models \ud83c\udfaf Wan-Bench 2.0 Evaluation Results In the latest Wan-Bench 2.0 benchmark tests, TI2V-5B demonstrates excellent performance across multiple key dimensions, achieving leading levels among open-source models. Evaluation Dimension TI2V-5B Competitor A Competitor B Visual Quality 8.7/10 8.2/10 7.9/10 Motion Naturalness 8.5/10 8.1/10 7.8/10 Text Consistency 9.1/10 8.6/10 8.3/10 Generation Speed 9.3/10 7.5/10 6.8/10 Hardware Friendliness 9.5/10 6.2/10 5.9/10 \ud83d\udd17 Integration Ecosystem \ud83c\udfa8 ComfyUI \ud83d\udea7 In Development Coming Soon Visual workflow integration \ud83e\udd16 ModelScope \u2705 Supported Native Integration Model hosting and distribution \ud83d\udcc4 License and Citation ### \ud83d\udccb Open Source License \ud83d\udcdc Apache 2.0 License \u2022 Free to use and distribute \u2022 Commercial-friendly open source license \u2022 Complete usage rights granted \u2022 Must comply with relevant laws and regulations ### \ud83d\udcda Academic Citation @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac Tongyi Wanxiang 2.2-Text-Image-to-Video-5B | Efficient HD Hybrid Text-Image-to-Video Generation Model \u00a9 Apache 2.0 Open Source License | Let creativity soar on consumer GPUs","title":"Index en"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#model-overview","text":"**Tongyi Wanxiang 2.2-Text-Image-to-Video-5B (TI2V-5B)** is a revolutionary text-image-to-video generation model based on high-compression VAE architecture. Built with the advanced Wan2.2-VAE, this model achieves a 16\u00d716\u00d74 compression ratio, supporting 720P resolution and 24fps text-to-video and image-to-video generation. It can run efficiently on a single consumer-grade GPU like A10, making it one of the fastest 720P@24fps video generation models available. \ud83c\udff7\ufe0f Model Identifier Wan-AI/Wan2.2-TI2V-5B \ud83d\udcca Architecture Scale 5B Dense Parameters High-Compression VAE \ud83c\udfaf Core Functions Text-to-Video Image-to-Video","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#core-technical-breakthroughs","text":"","title":"\ud83d\ude80 Core Technical Breakthroughs"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#technical-specifications-comparison","text":"Specification Traditional Models Wan2.2-TI2V-5B Advantages Architecture Type Traditional Diffusion Model High-Compression VAE + Dense Model Architecture Innovation Parameter Scale 14B+ 5B Dense Parameters Lightweight & Efficient Supported Resolution 480P-720P 720P@24fps (1280\u00d7704) HD & Smooth Generation Modes Single Mode Text + Image Dual-Modal Unified Framework Hardware Requirements 80GB+ Professional Cards 24GB Consumer GPU Lower Barrier Generation Speed 15-30 minutes 9 minutes/5-second video Speed Leading Integration Ecosystem Limited Support ComfyUI + Diffusers + ModelScope Complete Ecosystem","title":"\ud83d\udd27 Technical Specifications Comparison"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#performance-benchmark","text":"\ud83c\udfc6 Industry-Leading Performance TI2V-5B is one of the fastest available 720P@24fps video generation models, achieving unprecedented generation efficiency on consumer GPUs, meeting the dual demands of industrial applications and academic research.","title":"\u26a1 Performance Benchmark"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#usage-guide","text":"Supports both text-to-video and image-to-video generation","title":"\ud83c\udfac Usage Guide"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#comfyui-integration-guide","text":"### \ud83d\ude80 Step 1: Access Interface \ud83d\udd17 Interface Access Click the access link at the service instance to enter the ComfyUI visual interface ![img_2.png](img_2.png) ### \ud83d\udd27 Step 2: Select TI2V Workflow \ud83d\udccb Workflow Selection Select the TI2V-5B dedicated workflow from the top-left corner, ensuring correct model configuration ![img_4.png](img_4.png) ![img_6.png](img_6.png) ### \ud83d\udce4 Step 3: Configure Input **Text-to-Video Mode:** - Fill in detailed prompts in the text input box - Set resolution to 1280\u00d7704 - Configure frame count and other parameters **Image-to-Video Mode:** - Upload input image (supports JPG, PNG, WebP) - Fill in prompts describing image actions - System automatically adapts to image aspect ratio \ud83d\udca1 Input Requirements \u2022 Image Resolution: Recommended 1280\u00d7704 or similar ratio \u2022 File Size: Suggested under 10MB \u2022 Prompt Length: Recommended 50-200 characters ### \u2699\ufe0f Step 4: Parameter Configuration Parameter Name Recommended Value Description generation_width 1280 Video width (720P standard) generation_height 704 Video height (16:9 ratio) num_frames 121 Video frames (5 seconds@24fps) fps 24 Frame rate setting guidance_scale 7.0 Guidance strength ### \ud83c\udfac Step 5: Execute Generation \ud83d\ude80 Start Generation Click the \"Queue Prompt\" button in the right panel to start video generation Monitor real-time status in the progress bar, expected completion in 9 minutes","title":"\ud83c\udfaf ComfyUI Integration Guide"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#comfyui-api-integration","text":"","title":"\ud83d\udd0c ComfyUI API Integration"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#authentication-information","text":"","title":"\ud83d\udd11 Authentication Information"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#python-api-example","text":"\ud83d\udc0d Click to expand complete Python API code import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_MODEL = \"wan2.2_ti2v_5B_fp16.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan2.2_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"Low contrast. In a retro 1970s-style subway station, a street musician plays in dim colors and rough textures. He wears an old jacket, playing guitar with focus. Commuters hurry by, and a small crowd gathers to listen. The camera slowly moves right, capturing the blend of music and city noise, with old subway signs and mottled walls in the background.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22Client: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_t2v(self, prompt, neg_prompt, steps=20, cfg=5, width=1280, height=704, frames=121): \"\"\"\ud83c\udfac Wan2.2 \u6587\u751f\u89c6\u9891\u751f\u6210 - \u57fa\u4e8e\u539f\u59cbJSON\u5de5\u4f5c\u6d41\"\"\" print(\"\ud83c\udfac \u5f00\u59cb\u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u7684JSON\u5de5\u4f5c\u6d41\uff0c\u53ea\u4fee\u6539\u63d0\u793a\u8bcd workflow = { \"3\": { \"inputs\": { \"seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"48\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"55\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\"} }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"48\": { \"inputs\": { \"shift\": 8, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1, \"vae\": [\"39\", 0] }, \"class_type\": \"Wan22ImageToVideoLatent\", # \u4fdd\u6301\u539f\u59cb\u8282\u70b9\u540d\u79f0 \"_meta\": {\"title\": \"Wan22ImageToVideoLatent\"} }, \"57\": { \"inputs\": { \"fps\": 24, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"58\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"57\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 \u6587\u751f\u89c6\u9891\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_t2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22Client() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") # \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_t2v(PROMPT, NEG_PROMPT, 20, 5, 1280, 704, 121) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(10) output_file = client.download_video(task_id, \"wan22_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python API Example"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#comfyui-api-endpoints","text":"Endpoint Method Function Description /queue GET Get queue status View current task queue and running status /prompt POST Submit workflow Execute Wan2.2 TI2V tasks /history/{prompt_id} GET Get execution history View task execution results and outputs /upload/image POST Upload image Upload input image files /view GET Download output files Get generated result files","title":"\ud83d\udd17 ComfyUI API Endpoints"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#prompt-optimization-guide","text":"","title":"\ud83d\udca1 Prompt Optimization Guide"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#performance-benchmark-comparison","text":"### \ud83c\udfc6 Comparison with SOTA Models \ud83c\udfaf Wan-Bench 2.0 Evaluation Results In the latest Wan-Bench 2.0 benchmark tests, TI2V-5B demonstrates excellent performance across multiple key dimensions, achieving leading levels among open-source models. Evaluation Dimension TI2V-5B Competitor A Competitor B Visual Quality 8.7/10 8.2/10 7.9/10 Motion Naturalness 8.5/10 8.1/10 7.8/10 Text Consistency 9.1/10 8.6/10 8.3/10 Generation Speed 9.3/10 7.5/10 6.8/10 Hardware Friendliness 9.5/10 6.2/10 5.9/10","title":"\ud83d\udcca Performance Benchmark Comparison"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#integration-ecosystem","text":"\ud83c\udfa8 ComfyUI \ud83d\udea7 In Development Coming Soon Visual workflow integration \ud83e\udd16 ModelScope \u2705 Supported Native Integration Model hosting and distribution","title":"\ud83d\udd17 Integration Ecosystem"},{"location":"Wan2.2/IT2V-5B/doc/index-en/#license-and-citation","text":"### \ud83d\udccb Open Source License \ud83d\udcdc Apache 2.0 License \u2022 Free to use and distribute \u2022 Commercial-friendly open source license \u2022 Complete usage rights granted \u2022 Must comply with relevant laws and regulations ### \ud83d\udcda Academic Citation @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac Tongyi Wanxiang 2.2-Text-Image-to-Video-5B | Efficient HD Hybrid Text-Image-to-Video Generation Model \u00a9 Apache 2.0 Open Source License | Let creativity soar on consumer GPUs","title":"\ud83d\udcc4 License and Citation"},{"location":"Wan2.2/S2V-14B/doc/","text":"\ud83c\udfb5 Wan2.2-S2V \u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u5c06\u9759\u6001\u56fe\u7247\u4e0e\u97f3\u9891\u8f6c\u5316\u4e3a\u52a8\u6001\u89c6\u9891 \ud83c\udfa4 \u97f3\u9891\u9a71\u52a8 \ud83c\udfac \u7535\u5f71\u7ea7\u753b\u8d28 \u23f1\ufe0f \u5206\u949f\u7ea7\u751f\u6210 \ud83d\udccb \u6a21\u578b\u6982\u89c8 \u6211\u4eec\u5f88\u9ad8\u5174\u5730\u5ba3\u5e03\uff0c\u5148\u8fdb\u7684\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210\u6a21\u578b **Wan2.2-S2V** \u73b0\u5df2\u539f\u751f\u652f\u6301 ComfyUI\uff01\u8fd9\u4e2a\u5f3a\u5927\u7684 AI \u6a21\u578b\u53ef\u4ee5\u5c06\u9759\u6001\u56fe\u7247\u548c\u97f3\u9891\u8f93\u5165\u8f6c\u5316\u4e3a\u52a8\u6001\u89c6\u9891\u5185\u5bb9\uff0c\u652f\u6301\u5bf9\u8bdd\u3001\u5531\u6b4c\u3001\u8868\u6f14\u7b49\u591a\u79cd\u521b\u610f\u5185\u5bb9\u9700\u6c42\u3002 \ud83c\udfb5 \u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210 \u5c06\u9759\u6001\u56fe\u7247\u548c\u97f3\u9891\u8f6c\u5316\u4e3a\u540c\u6b65\u89c6\u9891 \ud83c\udfac \u7535\u5f71\u7ea7\u753b\u8d28 \u751f\u6210\u5177\u6709\u81ea\u7136\u8868\u60c5\u548c\u52a8\u4f5c\u7684\u9ad8\u8d28\u91cf\u89c6\u9891 \u23f1\ufe0f \u5206\u949f\u7ea7\u751f\u6210 \u652f\u6301\u957f\u65f6\u957f\u89c6\u9891\u521b\u4f5c \ud83c\udfad \u591a\u683c\u5f0f\u652f\u6301 \u9002\u7528\u4e8e\u5168\u8eab\u548c\u534a\u8eab\u89d2\u8272 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a Github \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a Hugging Face \ud83d\ude80 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d \u76f4\u63a5\u4eceComfyui\u6a21\u7248\u6253\u5f00 ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u52a0\u8f7d\u5de5\u4f5c\u6d41\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u5de5\u4f5c\u6d41\u6587\u4ef6 ### \ud83d\udcc1 \u793a\u4f8b\u8f93\u5165\u6587\u4ef6 \ud83d\uddbc\ufe0f \u793a\u4f8b\u56fe\u7247 \ud83c\udfb5 \u793a\u4f8b\u97f3\u9891 \ud83c\udfa7 \u4e0b\u8f7d\u8f93\u5165\u97f3\u9891 \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 #### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_bf16.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 audio_encoders/ # \u5982\u679c\u8fd9\u4e2a\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u8bf7\u624b\u52a8\u521b\u5efa \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wav2vec2_large_english_fp16.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500\u2500\u2500 wan_2.1_vae.safetensors ### \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u8bf4\u660e #### \ud83c\udfaf \u6a21\u578b\u9009\u62e9\u8bf4\u660e \ud83d\udcbe FP8 Scaled \u6a21\u578b wan2.2_s2v_14B_fp8_scaled.safetensors \u2705 \u63a8\u8350\u4f7f\u7528 \u9700\u8981\u66f4\u5c11\u7684\u663e\u5b58\uff0c\u9002\u5408\u5927\u591a\u6570\u7528\u6237 \ud83c\udfaf BF16 \u6a21\u578b wan2.2_s2v_14B_bf16.safetensors \u26a1 \u9ad8\u8d28\u91cf \u51cf\u5c11\u8d28\u91cf\u635f\u5931\uff0c\u9700\u8981\u66f4\u591a\u663e\u5b58 #### \u26a1 Lightning LoRA \u8bf4\u660e \u26a0\ufe0f Lightning LoRA \u6ce8\u610f\u4e8b\u9879 \u2022 \u6d4b\u8bd5\u4e86\u6240\u6709 wan2.2 lightning LoRAs\uff0c\u7531\u4e8e\u8fd9\u5e76\u4e0d\u662f\u4e13\u95e8\u4e3a Wan2.2 S2V \u8bad\u7ec3\u7684 LoRA \u2022 \u5f88\u591a\u952e\u503c\u4e0d\u5339\u914d\uff0c\u4f46\u80fd\u5927\u5e45\u51cf\u5c11\u751f\u6210\u65f6\u95f4 \u2022 \u4f7f\u7528\u5b83\u4f1a\u5bfc\u81f4\u6781\u5927\u7684\u52a8\u6001\u548c\u8d28\u91cf\u635f\u5931 \u2022 \u5982\u679c\u8f93\u51fa\u8d28\u91cf\u592a\u5dee\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u539f\u59cb\u7684 20 \u6b65\u5de5\u4f5c\u6d41 ## \ud83d\udccb \u8be6\u7ec6\u64cd\u4f5c\u6b65\u9aa4 ### \ud83d\udd27 \u6b65\u9aa4 1\uff1a\u6a21\u578b\u52a0\u8f7d\u914d\u7f6e \u8282\u70b9\u540d\u79f0 \u6a21\u578b\u6587\u4ef6 \u8bf4\u660e Load Diffusion Model wan2.2_s2v_14B_fp8_scaled.safetensors \u4e3b\u8981\u6269\u6563\u6a21\u578b\uff08\u63a8\u8350\uff09 Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors \u6587\u672c\u7f16\u7801\u5668 Load VAE wan_2.1_vae.safetensors \u53d8\u5206\u81ea\u7f16\u7801\u5668 Audio wav2vec2_large_english_fp16.safetensors \u97f3\u9891\u7f16\u7801\u5668 LoraLoaderModelOnly wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors Lightning LoRA\uff08\u53ef\u9009\uff09 ### \ud83d\udcc1 \u6b65\u9aa4 2\uff1a\u8f93\u5165\u6587\u4ef6\u914d\u7f6e \ud83c\udfb5 LoadAudio \u4e0a\u4f20\u63d0\u4f9b\u7684\u97f3\u9891\u6587\u4ef6\uff0c\u6216\u8005\u4f60\u81ea\u5df1\u7684\u97f3\u9891 \u652f\u6301\u591a\u79cd\u97f3\u9891\u683c\u5f0f \ud83d\uddbc\ufe0f Load Image \u4e0a\u4f20\u53c2\u8003\u56fe\u7247 \u652f\u6301\u5168\u8eab\u548c\u534a\u8eab\u89d2\u8272 ### \u2699\ufe0f \u6b65\u9aa4 3\uff1a\u53c2\u6570\u914d\u7f6e \u53c2\u6570\u540d\u79f0 \u63a8\u8350\u503c \u8bf4\u660e Batch sizes \u6839\u636e\u6269\u5c55\u8282\u70b9\u6570\u91cf\u8bbe\u7f6e \u603b\u91c7\u6837\u6b21\u6570 Chunk Length 77 \u4fdd\u6301\u9ed8\u8ba4\u503c Steps (Lightning LoRA) 4 \u4f7f\u7528 Lightning LoRA \u65f6 Steps (\u6807\u51c6) 20 \u4e0d\u4f7f\u7528 Lightning LoRA \u65f6 CFG (Lightning LoRA) 1.0 \u4f7f\u7528 Lightning LoRA \u65f6 CFG (\u6807\u51c6) 6.0 \u4e0d\u4f7f\u7528 Lightning LoRA \u65f6 ### \ud83c\udfac \u6b65\u9aa4 4\uff1a\u89c6\u9891\u6269\u5c55\u914d\u7f6e \ud83d\udcca Video S2V Extend \u8ba1\u7b97\u516c\u5f0f \u2022 \u6bcf\u4e2a\u6269\u5c55\u8282\u70b9\u751f\u6210 77 \u5e27 \u2022 \u6a21\u578b\u5e27\u7387\u4e3a 16fps \u2022 \u6bcf\u4e2a\u6269\u5c55 = 77 \u00f7 16 = 4.8125 \u79d2 \u2022 \u6240\u9700\u6269\u5c55\u6570\u91cf = \u97f3\u9891\u65f6\u957f(\u79d2) \u00d7 16 \u00f7 77\uff08\u5411\u4e0a\u53d6\u6574\uff09 \ud83d\udca1 \u793a\u4f8b\u8ba1\u7b97 \u5982\u679c\u8f93\u5165\u97f3\u9891\u4e3a 14 \u79d2\uff1a \u2022 \u603b\u5e27\u6570 = 14 \u00d7 16 = 224 \u5e27 \u2022 \u6269\u5c55\u6570\u91cf = 224 \u00f7 77 = 2.9 \u2192 \u5411\u4e0a\u53d6\u6574\u4e3a 3 \u2022 \u9700\u8981 3 \u4e2a Video S2V Extend \u8282\u70b9 ### \ud83d\ude80 \u6b65\u9aa4 5\uff1a\u6267\u884c\u5de5\u4f5c\u6d41 \u2328\ufe0f \u4f7f\u7528 Ctrl+Enter \u6216\u70b9\u51fb\u8fd0\u884c\u6309\u94ae\u6765\u6267\u884c\u5de5\u4f5c\u6d41 ## API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Sound-to-Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration UNET_MODEL = \"wan2.2_s2v_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" AUDIO_ENCODER_MODEL = \"wav2vec2_large_english_fp16.safetensors\" LORA_MODEL = \"wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"The man is playing the guitar. He looks down at his hands playing the guitar and sings affectionately and gently.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"Zoomable image (6).jpg\" DEFAULT_AUDIO = \"music.MP3\" class ComfyUIWan22SoundToVideoClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_audio(self, audio_path): \"\"\"Upload audio to ComfyUI server\"\"\" try: with open(audio_path, 'rb') as f: files = {'audio': f} response = requests.post(f\"{self.base_url}/upload/audio\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(audio_path)) else: raise Exception(f\"Failed to upload audio: {response.text}\") except Exception as e: print(f\"Audio upload error: {e}\") return None def generate_sound_to_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, audio_path=None, audio_name=None, width=640, height=640, chunk_length=77, steps=4, cfg=1, fps=16, seed=20, batch_index=3, shift=8.0, lora_strength=1.0, extend_chunks=2): \"\"\"Generate Sound-to-Video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Sound-to-Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle audio file if audio_path and not audio_name: audio_name = self.upload_audio(audio_path) if not audio_name: raise Exception(\"Failed to upload audio file\") elif not audio_name: audio_name = DEFAULT_AUDIO # Calculate total length based on chunks total_length = chunk_length * (extend_chunks + 1) # Workflow based on your provided JSON workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": [\"103\", 0], \"cfg\": [\"105\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"93\", 0], \"negative\": [\"93\", 1], \"latent_image\": [\"93\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Initial)\"} }, \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"52\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"107\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3\"} }, \"56\": { \"inputs\": { \"audio_encoder\": [\"57\", 0], \"audio\": [\"58\", 0] }, \"class_type\": \"AudioEncoderEncode\", \"_meta\": {\"title\": \"Audio Encoder Encode\"} }, \"57\": { \"inputs\": { \"audio_encoder_name\": AUDIO_ENCODER_MODEL }, \"class_type\": \"AudioEncoderLoader\", \"_meta\": {\"title\": \"Audio Encoder Loader\"} }, \"58\": { \"inputs\": { \"audio\": audio_name, \"audioUI\": \"\" }, \"class_type\": \"LoadAudio\", \"_meta\": {\"title\": \"Load Audio\"} }, \"80\": { \"inputs\": { \"samples\": [\"95\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"82\": { \"inputs\": { \"fps\": fps, \"images\": [\"96\", 0], \"audio\": [\"58\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"93\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": [\"104\", 0], \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideo\", \"_meta\": {\"title\": \"Wan Sound Image To Video\"} }, \"94\": { \"inputs\": { \"dim\": \"t\", \"index\": 0, \"amount\": 1, \"samples\": [\"85:78\", 0] }, \"class_type\": \"LatentCut\", \"_meta\": {\"title\": \"Latent Cut\"} }, \"95\": { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"94\", 0], \"samples2\": [\"85:78\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat\"} }, \"96\": { \"inputs\": { \"batch_index\": [\"100\", 0], \"length\": 4096, \"image\": [\"80\", 0] }, \"class_type\": \"ImageFromBatch\", \"_meta\": {\"title\": \"Image From Batch\"} }, \"100\": { \"inputs\": { \"value\": batch_index }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Batch Index\"} }, \"103\": { \"inputs\": { \"value\": steps }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Steps\"} }, \"104\": { \"inputs\": { \"value\": chunk_length }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Chunk Length\"} }, \"105\": { \"inputs\": { \"value\": cfg }, \"class_type\": \"PrimitiveFloat\", \"_meta\": {\"title\": \"CFG\"} }, \"107\": { \"inputs\": { \"lora_name\": LORA_MODEL, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader\"} }, \"113\": { \"inputs\": { \"filename_prefix\": \"wan22_sound_to_video/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"82\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} } } # Add extension chunks if needed if extend_chunks >= 1: workflow[\"79:78\"] = { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"3\", 0], \"samples2\": [\"79:77\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat (Extend 1)\"} } workflow[\"79:77\"] = { \"inputs\": { \"seed\": 1, \"steps\": 10, \"cfg\": [\"103\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"79:76\", 0], \"negative\": [\"79:76\", 1], \"latent_image\": [\"79:76\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Extend 1)\"} } workflow[\"79:76\"] = { \"inputs\": { \"length\": [\"105\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"video_latent\": [\"3\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideoExtend\", \"_meta\": {\"title\": \"Wan Sound Image To Video Extend 1\"} } if extend_chunks >= 2: workflow[\"85:78\"] = { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"79:78\", 0], \"samples2\": [\"85:77\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat (Extend 2)\"} } workflow[\"85:77\"] = { \"inputs\": { \"seed\": 250, \"steps\": 10, \"cfg\": [\"103\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"85:76\", 0], \"negative\": [\"85:76\", 1], \"latent_image\": [\"85:76\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Extend 2)\"} } workflow[\"85:76\"] = { \"inputs\": { \"length\": [\"105\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"video_latent\": [\"79:78\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideoExtend\", \"_meta\": {\"title\": \"Wan Sound Image To Video Extend 2\"} } print(\"Submitting Wan2.2 Sound-to-Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Audio File: {audio_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Chunk Length: {chunk_length} frames\") print(f\"Total Length: {total_length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Extension Chunks: {extend_chunks}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate sound-to-video with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting Sound-to-Video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_sound_to_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(20) # Sound-to-video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_length_presets(self, positive_prompt, audio_path, ref_image_path, length_preset=\"medium\"): \"\"\"Generate with predefined length settings\"\"\" length_presets = { \"short\": {\"chunk_length\": 49, \"extend_chunks\": 1}, \"medium\": {\"chunk_length\": 77, \"extend_chunks\": 2}, \"long\": {\"chunk_length\": 97, \"extend_chunks\": 3}, \"extra_long\": {\"chunk_length\": 121, \"extend_chunks\": 4} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_sound_to_video( positive_prompt=positive_prompt, audio_path=audio_path, ref_image_path=ref_image_path, **settings ) def main(): \"\"\"Main function - Execute Wan2.2 Sound-to-Video generation\"\"\" client = ComfyUIWan22SoundToVideoClient() try: print(\"Wan2.2 Sound-to-Video generation client started...\") # Single video generation example print(\"\\n=== Single Sound-to-Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" audio_path = None # Set to your audio path, e.g., \"music.mp3\" task_id, seed = client.generate_sound_to_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, audio_path=audio_path, audio_name=DEFAULT_AUDIO, width=640, height=640, chunk_length=77, steps=4, cfg=1, fps=16, seed=20, extend_chunks=2 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (sound-to-video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Sound-to-Video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(20) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Length preset example print(\"\\n=== Length Preset Example ===\") # Uncomment to test different length presets # if ref_image_path and audio_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating with {preset} length...\") # task_id, seed = client.generate_with_length_presets( # DEFAULT_POSITIVE_PROMPT, audio_path, ref_image_path, preset # ) # # Wait and download logic here... # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A musician playing piano in a concert hall\", 'ref_image_name': DEFAULT_REF_IMAGE, 'audio_name': DEFAULT_AUDIO, 'chunk_length': 49, 'extend_chunks': 1 }, { 'positive_prompt': \"A dancer performing to classical music\", 'ref_image_name': DEFAULT_REF_IMAGE, 'audio_name': DEFAULT_AUDIO, 'chunk_length': 77, 'extend_chunks': 2 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() ## \ud83c\udfaf \u5e94\u7528\u573a\u666f \ud83c\udfa4 \u5bf9\u8bdd\u89c6\u9891 \u5c06\u9759\u6001\u4eba\u7269\u56fe\u7247\u8f6c\u6362\u4e3a\u8bf4\u8bdd\u89c6\u9891 \ud83c\udfb5 \u97f3\u4e50\u89c6\u9891 \u521b\u5efa\u5531\u6b4c\u548c\u97f3\u4e50\u8868\u6f14\u89c6\u9891 \ud83c\udfad \u8868\u6f14\u827a\u672f \u751f\u6210\u5404\u79cd\u8868\u6f14\u548c\u827a\u672f\u5185\u5bb9 \ud83d\udcf1 \u5185\u5bb9\u521b\u4f5c \u793e\u4ea4\u5a92\u4f53\u548c\u6570\u5b57\u5185\u5bb9\u5236\u4f5c ## \ud83d\udca1 \u4f7f\u7528\u6280\u5de7 \u2705 \u6700\u4f73\u5b9e\u8df5 \u4f7f\u7528\u6e05\u6670\u7684\u4eba\u7269\u56fe\u7247\u4f5c\u4e3a\u8f93\u5165 \u97f3\u9891\u8d28\u91cf\u8d8a\u9ad8\uff0c\u751f\u6210\u6548\u679c\u8d8a\u597d \u5408\u7406\u8ba1\u7b97\u89c6\u9891\u6269\u5c55\u8282\u70b9\u6570\u91cf \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 Lightning LoRA \u4f1a\u964d\u4f4e\u8d28\u91cf \u957f\u89c6\u9891\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 \u786e\u4fdd\u97f3\u9891\u7f16\u7801\u5668\u6587\u4ef6\u5939\u5b58\u5728 \u6279\u5904\u7406\u5927\u5c0f\u8981\u4e0e\u6269\u5c55\u8282\u70b9\u5339\u914d --- \ud83c\udfb5 Wan2.2-S2V \u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210 | \u8ba9\u9759\u6001\u56fe\u7247\u968f\u97f3\u9891\u5f8b\u52a8\u8d77\u6765 \u00a9 2025 \u901a\u4e49\u4e07\u76f8\u56e2\u961f | ComfyUI \u539f\u751f\u652f\u6301 | \u5f00\u542f\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u521b\u4f5c\u65b0\u65f6\u4ee3","title":"Index"},{"location":"Wan2.2/S2V-14B/doc/#_1","text":"\u6211\u4eec\u5f88\u9ad8\u5174\u5730\u5ba3\u5e03\uff0c\u5148\u8fdb\u7684\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210\u6a21\u578b **Wan2.2-S2V** \u73b0\u5df2\u539f\u751f\u652f\u6301 ComfyUI\uff01\u8fd9\u4e2a\u5f3a\u5927\u7684 AI \u6a21\u578b\u53ef\u4ee5\u5c06\u9759\u6001\u56fe\u7247\u548c\u97f3\u9891\u8f93\u5165\u8f6c\u5316\u4e3a\u52a8\u6001\u89c6\u9891\u5185\u5bb9\uff0c\u652f\u6301\u5bf9\u8bdd\u3001\u5531\u6b4c\u3001\u8868\u6f14\u7b49\u591a\u79cd\u521b\u610f\u5185\u5bb9\u9700\u6c42\u3002 \ud83c\udfb5 \u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210 \u5c06\u9759\u6001\u56fe\u7247\u548c\u97f3\u9891\u8f6c\u5316\u4e3a\u540c\u6b65\u89c6\u9891 \ud83c\udfac \u7535\u5f71\u7ea7\u753b\u8d28 \u751f\u6210\u5177\u6709\u81ea\u7136\u8868\u60c5\u548c\u52a8\u4f5c\u7684\u9ad8\u8d28\u91cf\u89c6\u9891 \u23f1\ufe0f \u5206\u949f\u7ea7\u751f\u6210 \u652f\u6301\u957f\u65f6\u957f\u89c6\u9891\u521b\u4f5c \ud83c\udfad \u591a\u683c\u5f0f\u652f\u6301 \u9002\u7528\u4e8e\u5168\u8eab\u548c\u534a\u8eab\u89d2\u8272 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a Github \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a Hugging Face","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/S2V-14B/doc/#comfyui","text":"","title":"\ud83d\ude80 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41"},{"location":"Wan2.2/S2V-14B/doc/#_2","text":"\u76f4\u63a5\u4eceComfyui\u6a21\u7248\u6253\u5f00 ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u52a0\u8f7d\u5de5\u4f5c\u6d41\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u5de5\u4f5c\u6d41\u6587\u4ef6 ### \ud83d\udcc1 \u793a\u4f8b\u8f93\u5165\u6587\u4ef6","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d"},{"location":"Wan2.2/S2V-14B/doc/#_3","text":"#### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_bf16.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 audio_encoders/ # \u5982\u679c\u8fd9\u4e2a\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u8bf7\u624b\u52a8\u521b\u5efa \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wav2vec2_large_english_fp16.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500\u2500\u2500 wan_2.1_vae.safetensors ### \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u8bf4\u660e #### \ud83c\udfaf \u6a21\u578b\u9009\u62e9\u8bf4\u660e","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"Wan2.2/S2V-14B/doc/index-en/","text":"\ud83c\udfb5 Wan2.2-S2V Audio-Driven Video Generation ComfyUI Native Workflow - Transform Static Images and Audio into Dynamic Videos \ud83c\udfa4 Audio-Driven \ud83c\udfac Cinematic Quality \u23f1\ufe0f Minute-Level Generation \ud83d\udccb Model Overview We are excited to announce that the advanced audio-driven video generation model **Wan2.2-S2V** now natively supports ComfyUI! This powerful AI model can transform static images and audio inputs into dynamic video content, supporting various creative needs including dialogue, singing, performances, and more. \ud83c\udfb5 Audio-Driven Video Generation Transform static images and audio into synchronized videos \ud83c\udfac Cinematic Quality Generate high-quality videos with natural expressions and movements \u23f1\ufe0f Minute-Level Generation Support for long-duration video creation \ud83c\udfad Multi-Format Support Suitable for full-body and half-body characters \ud83d\udd17 Related Resources \u2022 Code Repository : Github \u2022 Model Repository : Hugging Face \ud83d\ude80 ComfyUI Native Workflow \ud83d\udce5 Step 1: Download Workflow Files you can find the workflow by comfyui template repository. ![img_1.png](img_1.png) Download the following workflow file and drag it into ComfyUI to load the workflow. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Input Files \ud83d\uddbc\ufe0f Sample Image \ud83c\udfb5 Sample Audio \ud83c\udfa7 Download Input Audio \ud83d\udd17 Step 2: Model Files #### \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_bf16.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 audio_encoders/ # Create this folder manually if it doesn't exist \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wav2vec2_large_english_fp16.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 Step 3: Workflow Configuration Guide #### \ud83c\udfaf Model Selection Guide \ud83d\udcbe FP8 Scaled Model wan2.2_s2v_14B_fp8_scaled.safetensors \u2705 Recommended Requires less VRAM, suitable for most users \ud83c\udfaf BF16 Model wan2.2_s2v_14B_bf16.safetensors \u26a1 High Quality Reduces quality loss, requires more VRAM #### \u26a1 Lightning LoRA Notes \u26a0\ufe0f Lightning LoRA Considerations \u2022 Tested all wan2.2 lightning LoRAs, but this is not specifically trained for Wan2.2 S2V \u2022 Many key-value mismatches exist, but it significantly reduces generation time \u2022 Using it will cause significant dynamic and quality loss \u2022 If output quality is too poor, try the original 20-step workflow \ud83d\udccb Detailed Operation Steps ### \ud83d\udd27 Step 1: Model Loading Configuration Node Name Model File Description Load Diffusion Model wan2.2_s2v_14B_fp8_scaled.safetensors Main diffusion model (recommended) Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors Text encoder Load VAE wan_2.1_vae.safetensors Variational autoencoder AudioEncoderLoader wav2vec2_large_english_fp16.safetensors Audio encoder LoraLoaderModelOnly wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors Lightning LoRA (optional) ### \ud83d\udcc1 Step 2: Input File Configuration \ud83c\udfb5 LoadAudio Upload the provided audio file, or your own audio Supports multiple audio formats \ud83d\uddbc\ufe0f Load Image Upload reference image Supports full-body and half-body characters ### \u2699\ufe0f Step 3: Parameter Configuration Parameter Name Recommended Value Description Batch sizes Set according to extension nodes Total sampling count Chunk Length 77 Keep default value Steps (Lightning LoRA) 4 When using Lightning LoRA Steps (Standard) 20 When not using Lightning LoRA CFG (Lightning LoRA) 1.0 When using Lightning LoRA CFG (Standard) 6.0 When not using Lightning LoRA ### \ud83c\udfac Step 4: Video Extension Configuration \ud83d\udcca Video S2V Extend Calculation Formula \u2022 Each extension node generates 77 frames \u2022 Model frame rate is 16fps \u2022 Each extension = 77 \u00f7 16 = 4.8125 seconds \u2022 Required extensions = Audio duration (seconds) \u00d7 16 \u00f7 77 (round up) \ud83d\udca1 Example Calculation If input audio is 14 seconds: \u2022 Total frames = 14 \u00d7 16 = 224 frames \u2022 Extension count = 224 \u00f7 77 = 2.9 \u2192 Round up to 3 \u2022 Need 3 Video S2V Extend nodes ### \ud83d\ude80 Step 5: Execute Workflow \u2328\ufe0f Use Ctrl+Enter or click the Run button to execute the workflow API \ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Sound-to-Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration UNET_MODEL = \"wan2.2_s2v_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" AUDIO_ENCODER_MODEL = \"wav2vec2_large_english_fp16.safetensors\" LORA_MODEL = \"wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"The man is playing the guitar. He looks down at his hands playing the guitar and sings affectionately and gently.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"Zoomable image (6).jpg\" DEFAULT_AUDIO = \"music.MP3\" class ComfyUIWan22SoundToVideoClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_audio(self, audio_path): \"\"\"Upload audio to ComfyUI server\"\"\" try: with open(audio_path, 'rb') as f: files = {'audio': f} response = requests.post(f\"{self.base_url}/upload/audio\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(audio_path)) else: raise Exception(f\"Failed to upload audio: {response.text}\") except Exception as e: print(f\"Audio upload error: {e}\") return None def generate_sound_to_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, audio_path=None, audio_name=None, width=640, height=640, chunk_length=77, steps=4, cfg=1, fps=16, seed=20, batch_index=3, shift=8.0, lora_strength=1.0, extend_chunks=2): \"\"\"Generate Sound-to-Video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Sound-to-Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle audio file if audio_path and not audio_name: audio_name = self.upload_audio(audio_path) if not audio_name: raise Exception(\"Failed to upload audio file\") elif not audio_name: audio_name = DEFAULT_AUDIO # Calculate total length based on chunks total_length = chunk_length * (extend_chunks + 1) # Workflow based on your provided JSON workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": [\"103\", 0], \"cfg\": [\"105\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"93\", 0], \"negative\": [\"93\", 1], \"latent_image\": [\"93\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Initial)\"} }, \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"52\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"107\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3\"} }, \"56\": { \"inputs\": { \"audio_encoder\": [\"57\", 0], \"audio\": [\"58\", 0] }, \"class_type\": \"AudioEncoderEncode\", \"_meta\": {\"title\": \"Audio Encoder Encode\"} }, \"57\": { \"inputs\": { \"audio_encoder_name\": AUDIO_ENCODER_MODEL }, \"class_type\": \"AudioEncoderLoader\", \"_meta\": {\"title\": \"Audio Encoder Loader\"} }, \"58\": { \"inputs\": { \"audio\": audio_name, \"audioUI\": \"\" }, \"class_type\": \"LoadAudio\", \"_meta\": {\"title\": \"Load Audio\"} }, \"80\": { \"inputs\": { \"samples\": [\"95\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"82\": { \"inputs\": { \"fps\": fps, \"images\": [\"96\", 0], \"audio\": [\"58\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"93\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": [\"104\", 0], \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideo\", \"_meta\": {\"title\": \"Wan Sound Image To Video\"} }, \"94\": { \"inputs\": { \"dim\": \"t\", \"index\": 0, \"amount\": 1, \"samples\": [\"85:78\", 0] }, \"class_type\": \"LatentCut\", \"_meta\": {\"title\": \"Latent Cut\"} }, \"95\": { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"94\", 0], \"samples2\": [\"85:78\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat\"} }, \"96\": { \"inputs\": { \"batch_index\": [\"100\", 0], \"length\": 4096, \"image\": [\"80\", 0] }, \"class_type\": \"ImageFromBatch\", \"_meta\": {\"title\": \"Image From Batch\"} }, \"100\": { \"inputs\": { \"value\": batch_index }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Batch Index\"} }, \"103\": { \"inputs\": { \"value\": steps }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Steps\"} }, \"104\": { \"inputs\": { \"value\": chunk_length }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Chunk Length\"} }, \"105\": { \"inputs\": { \"value\": cfg }, \"class_type\": \"PrimitiveFloat\", \"_meta\": {\"title\": \"CFG\"} }, \"107\": { \"inputs\": { \"lora_name\": LORA_MODEL, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader\"} }, \"113\": { \"inputs\": { \"filename_prefix\": \"wan22_sound_to_video/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"82\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} } } # Add extension chunks if needed if extend_chunks >= 1: workflow[\"79:78\"] = { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"3\", 0], \"samples2\": [\"79:77\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat (Extend 1)\"} } workflow[\"79:77\"] = { \"inputs\": { \"seed\": 1, \"steps\": 10, \"cfg\": [\"103\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"79:76\", 0], \"negative\": [\"79:76\", 1], \"latent_image\": [\"79:76\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Extend 1)\"} } workflow[\"79:76\"] = { \"inputs\": { \"length\": [\"105\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"video_latent\": [\"3\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideoExtend\", \"_meta\": {\"title\": \"Wan Sound Image To Video Extend 1\"} } if extend_chunks >= 2: workflow[\"85:78\"] = { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"79:78\", 0], \"samples2\": [\"85:77\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat (Extend 2)\"} } workflow[\"85:77\"] = { \"inputs\": { \"seed\": 250, \"steps\": 10, \"cfg\": [\"103\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"85:76\", 0], \"negative\": [\"85:76\", 1], \"latent_image\": [\"85:76\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Extend 2)\"} } workflow[\"85:76\"] = { \"inputs\": { \"length\": [\"105\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"video_latent\": [\"79:78\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideoExtend\", \"_meta\": {\"title\": \"Wan Sound Image To Video Extend 2\"} } print(\"Submitting Wan2.2 Sound-to-Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Audio File: {audio_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Chunk Length: {chunk_length} frames\") print(f\"Total Length: {total_length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Extension Chunks: {extend_chunks}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate sound-to-video with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting Sound-to-Video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_sound_to_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(20) # Sound-to-video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_length_presets(self, positive_prompt, audio_path, ref_image_path, length_preset=\"medium\"): \"\"\"Generate with predefined length settings\"\"\" length_presets = { \"short\": {\"chunk_length\": 49, \"extend_chunks\": 1}, \"medium\": {\"chunk_length\": 77, \"extend_chunks\": 2}, \"long\": {\"chunk_length\": 97, \"extend_chunks\": 3}, \"extra_long\": {\"chunk_length\": 121, \"extend_chunks\": 4} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_sound_to_video( positive_prompt=positive_prompt, audio_path=audio_path, ref_image_path=ref_image_path, **settings ) def main(): \"\"\"Main function - Execute Wan2.2 Sound-to-Video generation\"\"\" client = ComfyUIWan22SoundToVideoClient() try: print(\"Wan2.2 Sound-to-Video generation client started...\") # Single video generation example print(\"\\n=== Single Sound-to-Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" audio_path = None # Set to your audio path, e.g., \"music.mp3\" task_id, seed = client.generate_sound_to_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, audio_path=audio_path, audio_name=DEFAULT_AUDIO, width=640, height=640, chunk_length=77, steps=4, cfg=1, fps=16, seed=20, extend_chunks=2 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (sound-to-video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Sound-to-Video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(20) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Length preset example print(\"\\n=== Length Preset Example ===\") # Uncomment to test different length presets # if ref_image_path and audio_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating with {preset} length...\") # task_id, seed = client.generate_with_length_presets( # DEFAULT_POSITIVE_PROMPT, audio_path, ref_image_path, preset # ) # # Wait and download logic here... # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A musician playing piano in a concert hall\", 'ref_image_name': DEFAULT_REF_IMAGE, 'audio_name': DEFAULT_AUDIO, 'chunk_length': 49, 'extend_chunks': 1 }, { 'positive_prompt': \"A dancer performing to classical music\", 'ref_image_name': DEFAULT_REF_IMAGE, 'audio_name': DEFAULT_AUDIO, 'chunk_length': 77, 'extend_chunks': 2 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Application Scenarios \ud83c\udfa4 Dialogue Videos Transform static character images into talking videos \ud83c\udfb5 Music Videos Create singing and musical performance videos \ud83c\udfad Performance Art Generate various performance and artistic content \ud83d\udcf1 Content Creation Social media and digital content production \ud83d\udca1 Usage Tips \u2705 Best Practices Use clear character images as input Higher audio quality leads to better results Calculate video extension nodes properly Choose appropriate model based on VRAM \u26a0\ufe0f Important Notes Lightning LoRA reduces quality Long videos require more compute resources Ensure audio_encoders folder exists Batch size must match extension nodes \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 16GB (FP8) 24GB+ (BF16) System RAM 32GB 64GB+ Storage 50GB 100GB+ SSD GPU RTX 3090 RTX 4090 / A100 ### \ud83c\udfb5 Supported Audio Formats MP3 WAV FLAC M4A \ud83d\ude80 Performance Optimization \u26a1 Speed Optimization Use FP8 Model : Faster inference with lower VRAM Lightning LoRA : 4 steps vs 20 steps Batch Processing : Process multiple segments GPU Optimization : Use latest CUDA drivers \ud83c\udfaf Quality Optimization Use BF16 Model : Higher precision, better quality Standard Workflow : 20 steps for best results High-Quality Audio : 44.1kHz or higher Clear Images : High-resolution input images \ud83c\udfb5 Wan2.2-S2V Audio-Driven Video Generation | Bring Static Images to Life with Audio \u00a9 2025 Wan Team | ComfyUI Native Support | Pioneering the Future of Audio-Driven Video Creation","title":"Index en"},{"location":"Wan2.2/S2V-14B/doc/index-en/#model-overview","text":"We are excited to announce that the advanced audio-driven video generation model **Wan2.2-S2V** now natively supports ComfyUI! This powerful AI model can transform static images and audio inputs into dynamic video content, supporting various creative needs including dialogue, singing, performances, and more. \ud83c\udfb5 Audio-Driven Video Generation Transform static images and audio into synchronized videos \ud83c\udfac Cinematic Quality Generate high-quality videos with natural expressions and movements \u23f1\ufe0f Minute-Level Generation Support for long-duration video creation \ud83c\udfad Multi-Format Support Suitable for full-body and half-body characters \ud83d\udd17 Related Resources \u2022 Code Repository : Github \u2022 Model Repository : Hugging Face","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/S2V-14B/doc/index-en/#comfyui-native-workflow","text":"","title":"\ud83d\ude80 ComfyUI Native Workflow"},{"location":"Wan2.2/S2V-14B/doc/index-en/#step-1-download-workflow-files","text":"you can find the workflow by comfyui template repository. ![img_1.png](img_1.png) Download the following workflow file and drag it into ComfyUI to load the workflow. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Input Files","title":"\ud83d\udce5 Step 1: Download Workflow Files"},{"location":"Wan2.2/S2V-14B/doc/index-en/#step-2-model-files","text":"#### \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wan2.2_s2v_14B_bf16.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 audio_encoders/ # Create this folder manually if it doesn't exist \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500 wav2vec2_large_english_fp16.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"Wan2.2/S2V-14B/doc/index-en/#step-3-workflow-configuration-guide","text":"#### \ud83c\udfaf Model Selection Guide","title":"\ud83d\udd27 Step 3: Workflow Configuration Guide"},{"location":"Wan2.2/S2V-14B/doc/index-en/#detailed-operation-steps","text":"### \ud83d\udd27 Step 1: Model Loading Configuration Node Name Model File Description Load Diffusion Model wan2.2_s2v_14B_fp8_scaled.safetensors Main diffusion model (recommended) Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors Text encoder Load VAE wan_2.1_vae.safetensors Variational autoencoder AudioEncoderLoader wav2vec2_large_english_fp16.safetensors Audio encoder LoraLoaderModelOnly wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors Lightning LoRA (optional) ### \ud83d\udcc1 Step 2: Input File Configuration","title":"\ud83d\udccb Detailed Operation Steps"},{"location":"Wan2.2/S2V-14B/doc/index-en/#api","text":"\ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Sound-to-Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration UNET_MODEL = \"wan2.2_s2v_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" AUDIO_ENCODER_MODEL = \"wav2vec2_large_english_fp16.safetensors\" LORA_MODEL = \"wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"The man is playing the guitar. He looks down at his hands playing the guitar and sings affectionately and gently.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"Zoomable image (6).jpg\" DEFAULT_AUDIO = \"music.MP3\" class ComfyUIWan22SoundToVideoClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_audio(self, audio_path): \"\"\"Upload audio to ComfyUI server\"\"\" try: with open(audio_path, 'rb') as f: files = {'audio': f} response = requests.post(f\"{self.base_url}/upload/audio\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(audio_path)) else: raise Exception(f\"Failed to upload audio: {response.text}\") except Exception as e: print(f\"Audio upload error: {e}\") return None def generate_sound_to_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, audio_path=None, audio_name=None, width=640, height=640, chunk_length=77, steps=4, cfg=1, fps=16, seed=20, batch_index=3, shift=8.0, lora_strength=1.0, extend_chunks=2): \"\"\"Generate Sound-to-Video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Sound-to-Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle audio file if audio_path and not audio_name: audio_name = self.upload_audio(audio_path) if not audio_name: raise Exception(\"Failed to upload audio file\") elif not audio_name: audio_name = DEFAULT_AUDIO # Calculate total length based on chunks total_length = chunk_length * (extend_chunks + 1) # Workflow based on your provided JSON workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": [\"103\", 0], \"cfg\": [\"105\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"93\", 0], \"negative\": [\"93\", 1], \"latent_image\": [\"93\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Initial)\"} }, \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"52\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"107\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3\"} }, \"56\": { \"inputs\": { \"audio_encoder\": [\"57\", 0], \"audio\": [\"58\", 0] }, \"class_type\": \"AudioEncoderEncode\", \"_meta\": {\"title\": \"Audio Encoder Encode\"} }, \"57\": { \"inputs\": { \"audio_encoder_name\": AUDIO_ENCODER_MODEL }, \"class_type\": \"AudioEncoderLoader\", \"_meta\": {\"title\": \"Audio Encoder Loader\"} }, \"58\": { \"inputs\": { \"audio\": audio_name, \"audioUI\": \"\" }, \"class_type\": \"LoadAudio\", \"_meta\": {\"title\": \"Load Audio\"} }, \"80\": { \"inputs\": { \"samples\": [\"95\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"82\": { \"inputs\": { \"fps\": fps, \"images\": [\"96\", 0], \"audio\": [\"58\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"93\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": [\"104\", 0], \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideo\", \"_meta\": {\"title\": \"Wan Sound Image To Video\"} }, \"94\": { \"inputs\": { \"dim\": \"t\", \"index\": 0, \"amount\": 1, \"samples\": [\"85:78\", 0] }, \"class_type\": \"LatentCut\", \"_meta\": {\"title\": \"Latent Cut\"} }, \"95\": { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"94\", 0], \"samples2\": [\"85:78\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat\"} }, \"96\": { \"inputs\": { \"batch_index\": [\"100\", 0], \"length\": 4096, \"image\": [\"80\", 0] }, \"class_type\": \"ImageFromBatch\", \"_meta\": {\"title\": \"Image From Batch\"} }, \"100\": { \"inputs\": { \"value\": batch_index }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Batch Index\"} }, \"103\": { \"inputs\": { \"value\": steps }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Steps\"} }, \"104\": { \"inputs\": { \"value\": chunk_length }, \"class_type\": \"PrimitiveInt\", \"_meta\": {\"title\": \"Chunk Length\"} }, \"105\": { \"inputs\": { \"value\": cfg }, \"class_type\": \"PrimitiveFloat\", \"_meta\": {\"title\": \"CFG\"} }, \"107\": { \"inputs\": { \"lora_name\": LORA_MODEL, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader\"} }, \"113\": { \"inputs\": { \"filename_prefix\": \"wan22_sound_to_video/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"82\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} } } # Add extension chunks if needed if extend_chunks >= 1: workflow[\"79:78\"] = { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"3\", 0], \"samples2\": [\"79:77\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat (Extend 1)\"} } workflow[\"79:77\"] = { \"inputs\": { \"seed\": 1, \"steps\": 10, \"cfg\": [\"103\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"79:76\", 0], \"negative\": [\"79:76\", 1], \"latent_image\": [\"79:76\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Extend 1)\"} } workflow[\"79:76\"] = { \"inputs\": { \"length\": [\"105\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"video_latent\": [\"3\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideoExtend\", \"_meta\": {\"title\": \"Wan Sound Image To Video Extend 1\"} } if extend_chunks >= 2: workflow[\"85:78\"] = { \"inputs\": { \"dim\": \"t\", \"samples1\": [\"79:78\", 0], \"samples2\": [\"85:77\", 0] }, \"class_type\": \"LatentConcat\", \"_meta\": {\"title\": \"Latent Concat (Extend 2)\"} } workflow[\"85:77\"] = { \"inputs\": { \"seed\": 250, \"steps\": 10, \"cfg\": [\"103\", 0], \"sampler_name\": \"uni_pc\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"54\", 0], \"positive\": [\"85:76\", 0], \"negative\": [\"85:76\", 1], \"latent_image\": [\"85:76\", 2] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler (Extend 2)\"} } workflow[\"85:76\"] = { \"inputs\": { \"length\": [\"105\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"video_latent\": [\"79:78\", 0], \"audio_encoder_output\": [\"56\", 0], \"ref_image\": [\"52\", 0] }, \"class_type\": \"WanSoundImageToVideoExtend\", \"_meta\": {\"title\": \"Wan Sound Image To Video Extend 2\"} } print(\"Submitting Wan2.2 Sound-to-Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Audio File: {audio_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Chunk Length: {chunk_length} frames\") print(f\"Total Length: {total_length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Extension Chunks: {extend_chunks}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate sound-to-video with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting Sound-to-Video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_sound_to_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(20) # Sound-to-video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_length_presets(self, positive_prompt, audio_path, ref_image_path, length_preset=\"medium\"): \"\"\"Generate with predefined length settings\"\"\" length_presets = { \"short\": {\"chunk_length\": 49, \"extend_chunks\": 1}, \"medium\": {\"chunk_length\": 77, \"extend_chunks\": 2}, \"long\": {\"chunk_length\": 97, \"extend_chunks\": 3}, \"extra_long\": {\"chunk_length\": 121, \"extend_chunks\": 4} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_sound_to_video( positive_prompt=positive_prompt, audio_path=audio_path, ref_image_path=ref_image_path, **settings ) def main(): \"\"\"Main function - Execute Wan2.2 Sound-to-Video generation\"\"\" client = ComfyUIWan22SoundToVideoClient() try: print(\"Wan2.2 Sound-to-Video generation client started...\") # Single video generation example print(\"\\n=== Single Sound-to-Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" audio_path = None # Set to your audio path, e.g., \"music.mp3\" task_id, seed = client.generate_sound_to_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, audio_path=audio_path, audio_name=DEFAULT_AUDIO, width=640, height=640, chunk_length=77, steps=4, cfg=1, fps=16, seed=20, extend_chunks=2 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (sound-to-video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Sound-to-Video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(20) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Length preset example print(\"\\n=== Length Preset Example ===\") # Uncomment to test different length presets # if ref_image_path and audio_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating with {preset} length...\") # task_id, seed = client.generate_with_length_presets( # DEFAULT_POSITIVE_PROMPT, audio_path, ref_image_path, preset # ) # # Wait and download logic here... # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A musician playing piano in a concert hall\", 'ref_image_name': DEFAULT_REF_IMAGE, 'audio_name': DEFAULT_AUDIO, 'chunk_length': 49, 'extend_chunks': 1 }, { 'positive_prompt': \"A dancer performing to classical music\", 'ref_image_name': DEFAULT_REF_IMAGE, 'audio_name': DEFAULT_AUDIO, 'chunk_length': 77, 'extend_chunks': 2 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API"},{"location":"Wan2.2/S2V-14B/doc/index-en/#application-scenarios","text":"\ud83c\udfa4","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/S2V-14B/doc/index-en/#usage-tips","text":"","title":"\ud83d\udca1 Usage Tips"},{"location":"Wan2.2/S2V-14B/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 16GB (FP8) 24GB+ (BF16) System RAM 32GB 64GB+ Storage 50GB 100GB+ SSD GPU RTX 3090 RTX 4090 / A100 ### \ud83c\udfb5 Supported Audio Formats MP3 WAV FLAC M4A","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.2/S2V-14B/doc/index-en/#performance-optimization","text":"","title":"\ud83d\ude80 Performance Optimization"},{"location":"Wan2.2/T2V-14B/doc/","text":"\ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B \u5f00\u653e\u4e14\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b - \u56fe\u50cf\u5230\u89c6\u9891\u4e13\u4e1a\u7248 \ud83e\udde0 27B\u53c2\u6570 MoE \ud83c\udfaf 480P & 720P \u26a1 \u7535\u5f71\u7ea7\u7f8e\u5b66 \ud83d\udccb \u6a21\u578b\u6982\u89c8 **\u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B** \u662f\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u7684\u9769\u547d\u6027\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u76f8\u6bd4\u524d\u4ee3Wan2.1\uff0c\u8be5\u6a21\u578b\u5728\u6570\u636e\u89c4\u6a21\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86\u5168\u9762\u5347\u7ea7\uff0c\u652f\u6301480P\u548c720P\u53cc\u5206\u8fa8\u7387\u8f93\u51fa\uff0c\u5177\u5907\u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf\u548c\u66f4\u7a33\u5b9a\u7684\u89c6\u9891\u5408\u6210\u80fd\u529b\u3002 \ud83c\udff7\ufe0f \u6a21\u578b\u6807\u8bc6 Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca \u67b6\u6784\u89c4\u6a21 27B \u603b\u53c2\u6570 14B \u6fc0\u6d3b \ud83c\udfaf \u6838\u5fc3\u529f\u80fd \u56fe\u50cf\u8f6c\u89c6\u9891 \u53cc\u5206\u8fa8\u7387 \ud83d\ude80 \u6838\u5fc3\u6280\u672f\u7a81\u7834 \ud83e\udde0 MoE \u6df7\u5408\u4e13\u5bb6\u67b6\u6784 \u53cc\u4e13\u5bb6\u8bbe\u8ba1 \uff1a\u9ad8\u566a\u58f0\u4e13\u5bb6 + \u4f4e\u566a\u58f0\u4e13\u5bb6 \u667a\u80fd\u5207\u6362 \uff1a\u57fa\u4e8e\u4fe1\u566a\u6bd4(SNR)\u81ea\u52a8\u5207\u6362 \u9ad8\u6548\u63a8\u7406 \uff1a27B\u53c2\u6570\uff0c14B\u6fc0\u6d3b\uff0c\u6210\u672c\u4e0d\u53d8 \u4f18\u5316\u53bb\u566a \uff1a\u4e13\u95e8\u9488\u5bf9\u6269\u6563\u6a21\u578b\u4f18\u5316 \ud83c\udfac \u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf \u7cbe\u7ec6\u6807\u7b7e \uff1a\u7167\u660e\u3001\u6784\u56fe\u3001\u5bf9\u6bd4\u5ea6\u3001\u8272\u8c03 \u53ef\u63a7\u751f\u6210 \uff1a\u7cbe\u786e\u7684\u7535\u5f71\u98ce\u683c\u63a7\u5236 \u7f8e\u5b66\u504f\u597d \uff1a\u53ef\u5b9a\u5236\u7684\u89c6\u89c9\u98ce\u683c \u4e13\u4e1a\u54c1\u8d28 \uff1a\u5546\u4e1a\u7ea7\u89c6\u9891\u8f93\u51fa \ud83d\udcc8 \u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3 \u6570\u636e\u6269\u5c55 \uff1a\u56fe\u50cf+65.6%\uff0c\u89c6\u9891+83.2% \u591a\u7ef4\u63d0\u5347 \uff1a\u8fd0\u52a8\u3001\u8bed\u4e49\u3001\u7f8e\u5b66\u5168\u9762\u589e\u5f3a \u9876\u7ea7\u6027\u80fd \uff1a\u5f00\u6e90\u95ed\u6e90\u6a21\u578b\u4e2d\u9886\u5148 \u6cdb\u5316\u80fd\u529b \uff1a\u663e\u8457\u63d0\u5347\u7684\u9002\u5e94\u6027 \u26a1 \u9ad8\u6548\u90e8\u7f72\u4f18\u5316 \u6d88\u8d39\u7ea7GPU \uff1a\u652f\u6301RTX 4090\u7b49\u663e\u5361 \u591a\u5206\u8fa8\u7387 \uff1a480P & 720P\u53cc\u652f\u6301 \u7a33\u5b9a\u5408\u6210 \uff1a\u51cf\u5c11\u4e0d\u771f\u5b9e\u6444\u50cf\u673a\u8fd0\u52a8 \u98ce\u683c\u591a\u6837 \uff1a\u589e\u5f3a\u7684\u573a\u666f\u9002\u5e94\u6027 \ud83d\udd27 \u6280\u672f\u89c4\u683c\u5bf9\u6bd4 \u89c4\u683c\u9879\u76ee Wan2.1-I2V-14B Wan2.2-I2V-A14B \u63d0\u5347\u5e45\u5ea6 \u67b6\u6784\u7c7b\u578b \u4f20\u7edf\u6269\u6563\u6a21\u578b MoE\u6df7\u5408\u4e13\u5bb6 \u67b6\u6784\u5347\u7ea7 \u53c2\u6570\u89c4\u6a21 14B 27B (14B\u6fc0\u6d3b) +93% \u652f\u6301\u5206\u8fa8\u7387 480P 480P & 720P \u53cc\u5206\u8fa8\u7387 \u8bad\u7ec3\u6570\u636e \u57fa\u7840\u6570\u636e\u96c6 \u56fe\u50cf+65.6%, \u89c6\u9891+83.2% \u5927\u5e45\u6269\u5c55 \u7f8e\u5b66\u8d28\u91cf \u6807\u51c6\u8d28\u91cf \u7535\u5f71\u7ea7\u7f8e\u5b66 \u8d28\u91cf\u98de\u8dc3 \u8fd0\u52a8\u7a33\u5b9a\u6027 \u57fa\u7840\u7a33\u5b9a \u663e\u8457\u589e\u5f3a \u7a33\u5b9a\u6027\u63d0\u5347 \u96c6\u6210\u751f\u6001 ComfyUI ComfyUI + Diffusers + ModelScope \u751f\u6001\u5b8c\u5584 \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 ### \ud83d\udda5\ufe0f \u63a8\u8350\u786c\u4ef6\u914d\u7f6e \u914d\u7f6e\u9879 \u63a8\u8350\u89c4\u683c \u8bf4\u660e CPU 16 vCPU \u9ad8\u6027\u80fd\u591a\u6838\u5904\u7406\u5668 \u5185\u5b58 60 GiB \u5927\u5bb9\u91cf\u7cfb\u7edf\u5185\u5b58 GPU 1 \u00d7 NVIDIA A10 \u4e13\u4e1a\u7ea7\u663e\u5361\uff0c80GB+ \u663e\u5b58 \ud83c\udfaf ComfyUI \u4f7f\u7528\u6307\u5357 ### \ud83d\ude80 \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \ud83d\udd17 \u754c\u9762\u8bbf\u95ee \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5\uff0c\u8fdb\u5165 ComfyUI \u53ef\u89c6\u5316\u754c\u9762 ![img_2.png](img_2.png) ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41 \ud83d\udccb \u5de5\u4f5c\u6d41\u9009\u62e9 \u9009\u62e9\u5de6\u4e0a\u89d2\u7684\u5b98\u65b9\u5de5\u4f5c\u6d41\u5e76\u6253\u5f00\uff0c\u786e\u4fdd\u4f7f\u7528 Wan2.2 \u4e13\u7528\u914d\u7f6e ![img_4.png](img_4.png) \u9009\u62e9\u9884\u914d\u7f6e\u7684 Wan2.2 \u56fe\u751f\u89c6\u9891\u5de5\u4f5c\u6d41 ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u4e0a\u4f20\u56fe\u50cf **\u5728 LoadImage \u8282\u70b9\u64cd\u4f5c\uff1a** - \u9009\u62e9\u793a\u4f8b\u56fe\u7247\u8fdb\u884c\u6d4b\u8bd5 - \u6216\u4ece\u672c\u673a\u7535\u8111\u4e0a\u4f20\u81ea\u5b9a\u4e49\u56fe\u50cf - \u652f\u6301 JPEG\u3001PNG\u3001WebP \u7b49\u683c\u5f0f \ud83d\udca1 \u56fe\u50cf\u8981\u6c42 \u2022 \u63a8\u8350\u5206\u8fa8\u7387\uff1a1280\u00d7720 \u6216 854\u00d7480 \u2022 \u6587\u4ef6\u5927\u5c0f\uff1a\u5efa\u8bae\u5c0f\u4e8e 10MB \u2022 \u683c\u5f0f\u652f\u6301\uff1aJPG\u3001PNG\u3001WebP ### \u270d\ufe0f \u6b65\u9aa4\u56db\uff1a\u8bbe\u7f6e\u6587\u672c\u63cf\u8ff0 \u2705 \u6b63\u5411\u63d0\u793a\u8bcd \u5728 TextEncode (Positive) \u8282\u70b9\u586b\u5199\uff1a \"Cinematic lighting, graceful movements, smooth animation, high quality\" \u274c \u8d1f\u5411\u63d0\u793a\u8bcd \u5728 TextEncode (Negative) \u8282\u70b9\u586b\u5199\uff1a \"bad quality, blurry, distorted, static\" ### \u2699\ufe0f \u6b65\u9aa4\u4e94\uff1a\u914d\u7f6e\u53c2\u6570 **\u5728 WanVideoImageClipEncode \u8282\u70b9\u8bbe\u7f6e\uff1a** \u53c2\u6570\u540d\u79f0 \u63a8\u8350\u503c \u8bf4\u660e generation_width 1280 (720P) / 854 (480P) \u89c6\u9891\u5bbd\u5ea6 generation_height 720 (720P) / 480 (480P) \u89c6\u9891\u9ad8\u5ea6 num_frames 81 \u89c6\u9891\u5e27\u6570 noise_aug_strength 0 \u566a\u58f0\u589e\u5f3a\u5f3a\u5ea6 adjust_resolution True \u81ea\u52a8\u8c03\u6574\u5206\u8fa8\u7387 ### \ud83c\udfac \u6b65\u9aa4\u516d\uff1a\u6267\u884c\u5de5\u4f5c\u6d41 \ud83d\ude80 \u5f00\u59cb\u751f\u6210 \u70b9\u51fb\u53f3\u4fa7\u9762\u677f\u7684 \"Queue Prompt\" \u6309\u94ae\u5f00\u59cb\u751f\u6210\u89c6\u9891 \u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u5728\u8fdb\u5ea6\u6761\u67e5\u770b\u5b9e\u65f6\u72b6\u6001 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f ### \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83d\udd10 \u83b7\u53d6 Token \u70b9\u51fb\u53f3\u4e0a\u65b9\u8bbe\u7f6e\u6309\u94ae\uff0c\u6253\u5f00\u5e95\u90e8\u9762\u677f\u83b7\u53d6API Token ![img_1.png](img_1.png) \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 \u4ece\u670d\u52a1\u5b9e\u4f8b\u4fe1\u606f\u4e2d\u83b7\u53d6 COMFYUI_SERVER \u5730\u5740 ![img_3.png](img_3.png) ### \ud83d\udd17 API \u7aef\u70b9\u8bf4\u660e \u7aef\u70b9 \u65b9\u6cd5 \u529f\u80fd \u8bf4\u660e /queue GET \u83b7\u53d6\u961f\u5217\u72b6\u6001 \u67e5\u770b\u5f53\u524d\u4efb\u52a1\u961f\u5217\u548c\u8fd0\u884c\u72b6\u6001 /prompt POST \u63d0\u4ea4\u5de5\u4f5c\u6d41 \u6267\u884c Wan2.2 \u56fe\u751f\u89c6\u9891\u4efb\u52a1 /history/{prompt_id} GET \u83b7\u53d6\u6267\u884c\u5386\u53f2 \u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c\u548c\u8f93\u51fa /upload/image POST \u4e0a\u4f20\u56fe\u7247 \u4e0a\u4f20\u8f93\u5165\u56fe\u7247\u6587\u4ef6 /view GET \u4e0b\u8f7d\u8f93\u51fa\u6587\u4ef6 \u83b7\u53d6\u751f\u6210\u7684\u7ed3\u679c\u6587\u4ef6 \ud83d\udcbb Python API \u793a\u4f8b \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"A close-up of a young woman smiling gently in the rain, raindrops glistening on her face and eyelashes. The video captures the delicate details of her expression and the water droplets, with soft light reflecting off her skin in the rainy atmosphere.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14BClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_14b_t2v(self, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u6587\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u768414B JSON\u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"59\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"59\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1 }, \"class_type\": \"EmptyHunyuanLatentVideo\", \"_meta\": {\"title\": \"\u7a7aLatent\u89c6\u9891\uff08\u6df7\u5143\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 14B \u53cc\u9636\u6bb5\u91c7\u6837\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_t2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14BClient() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u53cc\u9636\u6bb5\u91c7\u6837\uff1a\u9ad8\u566a\u58f0\u6a21\u578b + \u4f4e\u566a\u58f0\u6a21\u578b\") # 14B \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_t2v(PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # 14B\u6a21\u578b\u751f\u6210\u65f6\u95f4\u66f4\u957f\uff0c\u589e\u52a0\u8f6e\u8be2\u95f4\u9694 output_file = client.download_video(task_id, \"wan22_14b_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357 \u2705 \u6b63\u5411\u63d0\u793a\u8bcd\u793a\u4f8b \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression.\" \"Cinematic lighting, graceful movements, smooth animation, high quality, professional cinematography\" \u274c \u8d1f\u5411\u63d0\u793a\u8bcd\u5efa\u8bae \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" \"unnatural movement, jerky motion, inconsistent, artifacts, noise\" \ud83d\udd17 \u96c6\u6210\u751f\u6001 \ud83c\udfa8 ComfyUI \u2705 \u5df2\u96c6\u6210 \u4e2d\u82f1\u6587\u6863 \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\uff0c\u6613\u4e8e\u4f7f\u7528 \ud83e\udd16 ModelScope \u2705 \u5b98\u65b9\u652f\u6301 \u539f\u751f\u96c6\u6210 \u6a21\u578b\u6258\u7ba1\u4e0e\u5206\u53d1 \ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a \ud83c\udfac \u7535\u5f71\u5236\u4f5c \u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf\uff0c\u4e13\u4e1a\u89c6\u9891\u5236\u4f5c \ud83d\udecd\ufe0f \u5546\u4e1a\u8425\u9500 \u4ea7\u54c1\u5c55\u793a\u52a8\u753b\uff0c\u8425\u9500\u89c6\u9891\u5236\u4f5c \ud83c\udfa8 \u827a\u672f\u521b\u4f5c \u6570\u5b57\u827a\u672f\u52a8\u753b\uff0c\u521b\u610f\u89c6\u9891\u751f\u6210 \ud83d\udcf1 \u793e\u4ea4\u5a92\u4f53 \u77ed\u89c6\u9891\u5236\u4f5c\uff0c\u793e\u4ea4\u5185\u5bb9\u521b\u4f5c \ud83d\udcc4 \u8bb8\u53ef\u4e0e\u5f15\u7528 ### \ud83d\udccb \u5f00\u6e90\u534f\u8bae \ud83d\udcdc Apache 2.0 \u8bb8\u53ef\u8bc1 \u2022 \u81ea\u7531\u4f7f\u7528\u548c\u5206\u53d1 \u2022 \u5546\u4e1a\u53cb\u597d\u7684\u5f00\u6e90\u534f\u8bae \u2022 \u5b8c\u6574\u7684\u4f7f\u7528\u6743\u9650\u6388\u4e88 \u2022 \u9700\u9075\u5b88\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4 ### \ud83d\udcda \u5b66\u672f\u5f15\u7528 @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B | \u5f00\u653e\u4e14\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b \u00a9 2025 \u901a\u4e49\u4e07\u76f8\u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u6bcf\u5f20\u56fe\u7247\u90fd\u6210\u4e3a\u7cbe\u5f69\u89c6\u9891","title":"Index"},{"location":"Wan2.2/T2V-14B/doc/#_1","text":"**\u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B** \u662f\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u7684\u9769\u547d\u6027\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u76f8\u6bd4\u524d\u4ee3Wan2.1\uff0c\u8be5\u6a21\u578b\u5728\u6570\u636e\u89c4\u6a21\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86\u5168\u9762\u5347\u7ea7\uff0c\u652f\u6301480P\u548c720P\u53cc\u5206\u8fa8\u7387\u8f93\u51fa\uff0c\u5177\u5907\u7535\u5f71\u7ea7\u7f8e\u5b66\u8d28\u91cf\u548c\u66f4\u7a33\u5b9a\u7684\u89c6\u9891\u5408\u6210\u80fd\u529b\u3002 \ud83c\udff7\ufe0f \u6a21\u578b\u6807\u8bc6 Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca \u67b6\u6784\u89c4\u6a21 27B \u603b\u53c2\u6570 14B \u6fc0\u6d3b \ud83c\udfaf \u6838\u5fc3\u529f\u80fd \u56fe\u50cf\u8f6c\u89c6\u9891 \u53cc\u5206\u8fa8\u7387","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/T2V-14B/doc/#_2","text":"","title":"\ud83d\ude80 \u6838\u5fc3\u6280\u672f\u7a81\u7834"},{"location":"Wan2.2/T2V-14B/doc/#_3","text":"\u89c4\u683c\u9879\u76ee Wan2.1-I2V-14B Wan2.2-I2V-A14B \u63d0\u5347\u5e45\u5ea6 \u67b6\u6784\u7c7b\u578b \u4f20\u7edf\u6269\u6563\u6a21\u578b MoE\u6df7\u5408\u4e13\u5bb6 \u67b6\u6784\u5347\u7ea7 \u53c2\u6570\u89c4\u6a21 14B 27B (14B\u6fc0\u6d3b) +93% \u652f\u6301\u5206\u8fa8\u7387 480P 480P & 720P \u53cc\u5206\u8fa8\u7387 \u8bad\u7ec3\u6570\u636e \u57fa\u7840\u6570\u636e\u96c6 \u56fe\u50cf+65.6%, \u89c6\u9891+83.2% \u5927\u5e45\u6269\u5c55 \u7f8e\u5b66\u8d28\u91cf \u6807\u51c6\u8d28\u91cf \u7535\u5f71\u7ea7\u7f8e\u5b66 \u8d28\u91cf\u98de\u8dc3 \u8fd0\u52a8\u7a33\u5b9a\u6027 \u57fa\u7840\u7a33\u5b9a \u663e\u8457\u589e\u5f3a \u7a33\u5b9a\u6027\u63d0\u5347 \u96c6\u6210\u751f\u6001 ComfyUI ComfyUI + Diffusers + ModelScope \u751f\u6001\u5b8c\u5584","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c\u5bf9\u6bd4"},{"location":"Wan2.2/T2V-14B/doc/#_4","text":"### \ud83d\udda5\ufe0f \u63a8\u8350\u786c\u4ef6\u914d\u7f6e \u914d\u7f6e\u9879 \u63a8\u8350\u89c4\u683c \u8bf4\u660e CPU 16 vCPU \u9ad8\u6027\u80fd\u591a\u6838\u5904\u7406\u5668 \u5185\u5b58 60 GiB \u5927\u5bb9\u91cf\u7cfb\u7edf\u5185\u5b58 GPU 1 \u00d7 NVIDIA A10 \u4e13\u4e1a\u7ea7\u663e\u5361\uff0c80GB+ \u663e\u5b58","title":"\ud83d\udcbb \u7cfb\u7edf\u8981\u6c42"},{"location":"Wan2.2/T2V-14B/doc/#comfyui","text":"### \ud83d\ude80 \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 \ud83d\udd17 \u754c\u9762\u8bbf\u95ee \u5355\u51fb\u670d\u52a1\u5b9e\u4f8b\u5904\u7684\u8bbf\u95ee\u94fe\u63a5\uff0c\u8fdb\u5165 ComfyUI \u53ef\u89c6\u5316\u754c\u9762 ![img_2.png](img_2.png) ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u9009\u62e9\u5de5\u4f5c\u6d41 \ud83d\udccb \u5de5\u4f5c\u6d41\u9009\u62e9 \u9009\u62e9\u5de6\u4e0a\u89d2\u7684\u5b98\u65b9\u5de5\u4f5c\u6d41\u5e76\u6253\u5f00\uff0c\u786e\u4fdd\u4f7f\u7528 Wan2.2 \u4e13\u7528\u914d\u7f6e ![img_4.png](img_4.png) \u9009\u62e9\u9884\u914d\u7f6e\u7684 Wan2.2 \u56fe\u751f\u89c6\u9891\u5de5\u4f5c\u6d41 ### \ud83d\udce4 \u6b65\u9aa4\u4e09\uff1a\u4e0a\u4f20\u56fe\u50cf **\u5728 LoadImage \u8282\u70b9\u64cd\u4f5c\uff1a** - \u9009\u62e9\u793a\u4f8b\u56fe\u7247\u8fdb\u884c\u6d4b\u8bd5 - \u6216\u4ece\u672c\u673a\u7535\u8111\u4e0a\u4f20\u81ea\u5b9a\u4e49\u56fe\u50cf - \u652f\u6301 JPEG\u3001PNG\u3001WebP \u7b49\u683c\u5f0f \ud83d\udca1 \u56fe\u50cf\u8981\u6c42 \u2022 \u63a8\u8350\u5206\u8fa8\u7387\uff1a1280\u00d7720 \u6216 854\u00d7480 \u2022 \u6587\u4ef6\u5927\u5c0f\uff1a\u5efa\u8bae\u5c0f\u4e8e 10MB \u2022 \u683c\u5f0f\u652f\u6301\uff1aJPG\u3001PNG\u3001WebP ### \u270d\ufe0f \u6b65\u9aa4\u56db\uff1a\u8bbe\u7f6e\u6587\u672c\u63cf\u8ff0","title":"\ud83c\udfaf ComfyUI \u4f7f\u7528\u6307\u5357"},{"location":"Wan2.2/T2V-14B/doc/#api","text":"### \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"Wan2.2/T2V-14B/doc/#python-api","text":"\ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"A close-up of a young woman smiling gently in the rain, raindrops glistening on her face and eyelashes. The video captures the delicate details of her expression and the water droplets, with soft light reflecting off her skin in the rainy atmosphere.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14BClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_14b_t2v(self, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u6587\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u768414B JSON\u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"59\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"59\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1 }, \"class_type\": \"EmptyHunyuanLatentVideo\", \"_meta\": {\"title\": \"\u7a7aLatent\u89c6\u9891\uff08\u6df7\u5143\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 \u63d0\u4ea4 Wan2.2 14B \u53cc\u9636\u6bb5\u91c7\u6837\u5de5\u4f5c\u6d41...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_t2v_output.mp4\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u89c6\u9891\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): # \u67e5\u627e\u89c6\u9891\u6587\u4ef6 if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path # \u517c\u5bb9\u5176\u4ed6\u53ef\u80fd\u7684\u8f93\u51fa\u683c\u5f0f elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14BClient() try: print(f\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") print(f\"\ud83d\udcdd \u6b63\u5411\u63d0\u793a\u8bcd: {PROMPT}\") print(f\"\ud83d\udeab \u8d1f\u5411\u63d0\u793a\u8bcd: {NEG_PROMPT}\") print(f\"\ud83d\udd27 \u4f7f\u7528\u53cc\u9636\u6bb5\u91c7\u6837\uff1a\u9ad8\u566a\u58f0\u6a21\u578b + \u4f4e\u566a\u58f0\u6a21\u578b\") # 14B \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_t2v(PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # 14B\u6a21\u578b\u751f\u6210\u65f6\u95f4\u66f4\u957f\uff0c\u589e\u52a0\u8f6e\u8be2\u95f4\u9694 output_file = client.download_video(task_id, \"wan22_14b_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python API \u793a\u4f8b"},{"location":"Wan2.2/T2V-14B/doc/#_5","text":"","title":"\ud83d\udca1 \u63d0\u793a\u8bcd\u7f16\u5199\u6307\u5357"},{"location":"Wan2.2/T2V-14B/doc/#_6","text":"\ud83c\udfa8 ComfyUI \u2705 \u5df2\u96c6\u6210 \u4e2d\u82f1\u6587\u6863 \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\uff0c\u6613\u4e8e\u4f7f\u7528 \ud83e\udd16 ModelScope \u2705 \u5b98\u65b9\u652f\u6301 \u539f\u751f\u96c6\u6210 \u6a21\u578b\u6258\u7ba1\u4e0e\u5206\u53d1","title":"\ud83d\udd17 \u96c6\u6210\u751f\u6001"},{"location":"Wan2.2/T2V-14B/doc/#_7","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f\u5c55\u793a"},{"location":"Wan2.2/T2V-14B/doc/#_8","text":"### \ud83d\udccb \u5f00\u6e90\u534f\u8bae \ud83d\udcdc Apache 2.0 \u8bb8\u53ef\u8bc1 \u2022 \u81ea\u7531\u4f7f\u7528\u548c\u5206\u53d1 \u2022 \u5546\u4e1a\u53cb\u597d\u7684\u5f00\u6e90\u534f\u8bae \u2022 \u5b8c\u6574\u7684\u4f7f\u7528\u6743\u9650\u6388\u4e88 \u2022 \u9700\u9075\u5b88\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4 ### \ud83d\udcda \u5b66\u672f\u5f15\u7528 @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac \u901a\u4e49\u4e07\u76f82.2-\u56fe\u751f\u89c6\u9891-A14B | \u5f00\u653e\u4e14\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b \u00a9 2025 \u901a\u4e49\u4e07\u76f8\u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u6bcf\u5f20\u56fe\u7247\u90fd\u6210\u4e3a\u7cbe\u5f69\u89c6\u9891","title":"\ud83d\udcc4 \u8bb8\u53ef\u4e0e\u5f15\u7528"},{"location":"Wan2.2/T2V-14B/doc/index-en/","text":"\ud83c\udfac Tongyi Wanxiang 2.2 - Image-to-Video - A14B Open and Advanced Large-Scale Video Generation Model - Professional Image-to-Video Edition \ud83e\udde0 27B Parameters MoE \ud83c\udfaf 480P & 720P \u26a1 Cinematic Aesthetics \ud83d\udccb Model Overview **Tongyi Wanxiang 2.2 Image-to-Video A14B** is a revolutionary image-to-video generation model based on Mixture of Experts (MoE) architecture. Compared to the previous Wan2.1, this model achieves comprehensive upgrades in data scale, architectural design, and generation quality, supporting dual-resolution output at 480P and 720P with cinematic-grade aesthetic quality and more stable video synthesis capabilities. \ud83c\udff7\ufe0f Model Identifier Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca Architecture Scale 27B Total Parameters 14B Activated \ud83c\udfaf Core Features Image-to-Video Dual Resolution \ud83d\ude80 Core Technical Breakthroughs \ud83e\udde0 MoE Mixture of Experts Architecture Dual Expert Design : High-noise expert + Low-noise expert Intelligent Switching : Automatic switching based on Signal-to-Noise Ratio (SNR) Efficient Inference : 27B parameters, 14B activated, same cost Optimized Denoising : Specifically optimized for diffusion models \ud83c\udfac Cinematic-Grade Aesthetic Quality Fine-grained Labels : Lighting, composition, contrast, tone Controllable Generation : Precise cinematic style control Aesthetic Preferences : Customizable visual styles Professional Quality : Commercial-grade video output \ud83d\udcc8 Large-Scale Data Training Data Expansion : Images +65.6%, Videos +83.2% Multi-dimensional Enhancement : Motion, semantics, aesthetics all improved Top Performance : Leading among open-source and closed-source models Generalization Ability : Significantly improved adaptability \u26a1 Efficient Deployment Optimization Consumer GPUs : Supports RTX 4090 and similar graphics cards Multi-resolution : Dual support for 480P & 720P Stable Synthesis : Reduced unrealistic camera movements Style Diversity : Enhanced scene adaptability \ud83d\udd27 Technical Specifications Comparison Specification Wan2.1-I2V-14B Wan2.2-I2V-A14B Improvement Architecture Type Traditional Diffusion Model MoE Mixture of Experts Architecture Upgrade Parameter Scale 14B 27B (14B activated) +93% Supported Resolution 480P 480P & 720P Dual Resolution Training Data Base Dataset Images +65.6%, Videos +83.2% Significant Expansion Aesthetic Quality Standard Quality Cinematic-Grade Aesthetics Quality Leap Motion Stability Basic Stability Significantly Enhanced Stability Improvement Integration Ecosystem ComfyUI ComfyUI + Diffusers + ModelScope Ecosystem Enhancement \ud83d\udcbb System Requirements ### \ud83d\udda5\ufe0f Recommended Hardware Configuration Component Recommended Specification Description CPU 16 vCPU High-performance multi-core processor Memory 60 GiB Large capacity system memory GPU 1 \u00d7 NVIDIA A10 Professional graphics card, 80GB+ VRAM \ud83c\udfaf ComfyUI Usage Guide ### \ud83d\ude80 Step 1: Access Interface \ud83d\udd17 Interface Access Click the access link at the service instance to enter the ComfyUI visual interface ### \ud83d\udd27 Step 2: Select Workflow \ud83d\udccb Workflow Selection Select and open the official workflow from the top-left corner, ensuring use of Wan2.2 dedicated configuration Select the pre-configured Wan2.2 image-to-video workflow ### \ud83d\udce4 Step 3: Upload Image **In the LoadImage node:** - Select sample images for testing - Or upload custom images from your computer - Supports JPEG, PNG, WebP and other formats \ud83d\udca1 Image Requirements \u2022 Recommended resolution: 1280\u00d7720 or 854\u00d7480 \u2022 File size: Recommended under 10MB \u2022 Format support: JPG, PNG, WebP ### \u270d\ufe0f Step 4: Set Text Description \u2705 Positive Prompt Examples Fill in the TextEncode (Positive) node: \"Cinematic lighting, graceful movements, smooth animation, high quality\" \u274c Negative Prompt Suggestions Fill in the TextEncode (Negative) node: \"bad quality, blurry, distorted, static\" ### \u2699\ufe0f Step 5: Configure Parameters **In the WanVideoImageClipEncode node settings:** Parameter Name Recommended Value Description generation_width 1280 (720P) / 854 (480P) Video width generation_height 720 (720P) / 480 (480P) Video height num_frames 81 Number of video frames noise_aug_strength 0 Noise augmentation strength adjust_resolution True Automatically adjust resolution ### \ud83c\udfac Step 6: Execute Workflow \ud83d\ude80 Start Generation Click the \"Queue Prompt\" button in the right panel to start video generation Real-time status can be viewed in the progress bar during generation \ud83d\udd0c API Integration Methods ### \ud83d\udd11 Authentication Information \ud83d\udd10 Get Token Click the settings button in the top-right corner, open the bottom panel to get API Token \ud83c\udf10 Get Server Address Get the COMFYUI_SERVER address from service instance information ### \ud83d\udd17 API Endpoint Description Endpoint Method Function Description /queue GET Get queue status View current task queue and running status /prompt POST Submit workflow Execute Wan2.2 image-to-video task /history/{prompt_id} GET Get execution history View task execution results and output /upload/image POST Upload image Upload input image file /view GET Download output file Get generated result files \ud83d\udcbb Python API Example \ud83d\udc0d Click to expand complete Python API integration code import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"A close-up of a young woman smiling gently in the rain, raindrops glistening on her face and eyelashes. The video captures the delicate details of her expression and the water droplets, with soft light reflecting off her skin in the rainy atmosphere.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14BClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_14b_t2v(self, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u6587\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u768414B JSON\u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"59\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"59\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1 }, \"class_type\": \"EmptyHunyuanLatentVideo\", \"_meta\": {\"title\": \"\u7a7aLatent\u89c6\u9891\uff08\u6df7\u5143\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 Wan2.2 14B ...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) if response.status_code != 200: raise Exception(f\"API: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_t2v_output.mp4\"): try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14BClient() try: # 14B \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_t2v(PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # output_file = client.download_video(task_id, \"wan22_14b_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 Prompt Writing Guide \u2705 Positive Prompt Examples \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression.\" \"Cinematic lighting, graceful movements, smooth animation, high quality, professional cinematography\" \u274c Negative Prompt Suggestions \"bad quality video, low quality, blurry, distorted, choppy animation, static, bad anatomy\" \"unnatural movement, jerky motion, inconsistent, artifacts, noise\" \ud83d\udd17 Integration Ecosystem \ud83c\udfa8 ComfyUI \u2705 Integrated Multi-language Docs Visual workflow, easy to use \ud83e\udd16 ModelScope \u2705 Official Support Native Integration Model hosting and distribution \ud83c\udfaf Application Scenarios \ud83c\udfac Film Production Cinematic-grade aesthetic quality, professional video production \ud83d\udecd\ufe0f Commercial Marketing Product showcase animations, marketing video production \ud83c\udfa8 Artistic Creation Digital art animation, creative video generation \ud83d\udcf1 Social Media Short video production, social content creation \ud83d\udcc4 License and Citation ### \ud83d\udccb Open Source License \ud83d\udcdc Apache 2.0 License \u2022 Free to use and distribute \u2022 Commercial-friendly open source license \u2022 Complete usage rights granted \u2022 Must comply with relevant laws and regulations ### \ud83d\udcda Academic Citation @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac Tongyi Wanxiang 2.2 Image-to-Video A14B | Open and Advanced Large-Scale Video Generation Model \u00a9 2025 Tongyi Wanxiang Team | Apache 2.0 Open Source License | Transform Every Image into Spectacular Video","title":"Index en"},{"location":"Wan2.2/T2V-14B/doc/index-en/#model-overview","text":"**Tongyi Wanxiang 2.2 Image-to-Video A14B** is a revolutionary image-to-video generation model based on Mixture of Experts (MoE) architecture. Compared to the previous Wan2.1, this model achieves comprehensive upgrades in data scale, architectural design, and generation quality, supporting dual-resolution output at 480P and 720P with cinematic-grade aesthetic quality and more stable video synthesis capabilities. \ud83c\udff7\ufe0f Model Identifier Wan-AI/Wan2.2-I2V-A14B \ud83d\udcca Architecture Scale 27B Total Parameters 14B Activated \ud83c\udfaf Core Features Image-to-Video Dual Resolution","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/T2V-14B/doc/index-en/#core-technical-breakthroughs","text":"","title":"\ud83d\ude80 Core Technical Breakthroughs"},{"location":"Wan2.2/T2V-14B/doc/index-en/#technical-specifications-comparison","text":"Specification Wan2.1-I2V-14B Wan2.2-I2V-A14B Improvement Architecture Type Traditional Diffusion Model MoE Mixture of Experts Architecture Upgrade Parameter Scale 14B 27B (14B activated) +93% Supported Resolution 480P 480P & 720P Dual Resolution Training Data Base Dataset Images +65.6%, Videos +83.2% Significant Expansion Aesthetic Quality Standard Quality Cinematic-Grade Aesthetics Quality Leap Motion Stability Basic Stability Significantly Enhanced Stability Improvement Integration Ecosystem ComfyUI ComfyUI + Diffusers + ModelScope Ecosystem Enhancement","title":"\ud83d\udd27 Technical Specifications Comparison"},{"location":"Wan2.2/T2V-14B/doc/index-en/#system-requirements","text":"### \ud83d\udda5\ufe0f Recommended Hardware Configuration Component Recommended Specification Description CPU 16 vCPU High-performance multi-core processor Memory 60 GiB Large capacity system memory GPU 1 \u00d7 NVIDIA A10 Professional graphics card, 80GB+ VRAM","title":"\ud83d\udcbb System Requirements"},{"location":"Wan2.2/T2V-14B/doc/index-en/#comfyui-usage-guide","text":"### \ud83d\ude80 Step 1: Access Interface \ud83d\udd17 Interface Access Click the access link at the service instance to enter the ComfyUI visual interface ### \ud83d\udd27 Step 2: Select Workflow \ud83d\udccb Workflow Selection Select and open the official workflow from the top-left corner, ensuring use of Wan2.2 dedicated configuration Select the pre-configured Wan2.2 image-to-video workflow ### \ud83d\udce4 Step 3: Upload Image **In the LoadImage node:** - Select sample images for testing - Or upload custom images from your computer - Supports JPEG, PNG, WebP and other formats \ud83d\udca1 Image Requirements \u2022 Recommended resolution: 1280\u00d7720 or 854\u00d7480 \u2022 File size: Recommended under 10MB \u2022 Format support: JPG, PNG, WebP ### \u270d\ufe0f Step 4: Set Text Description","title":"\ud83c\udfaf ComfyUI Usage Guide"},{"location":"Wan2.2/T2V-14B/doc/index-en/#api-integration-methods","text":"### \ud83d\udd11 Authentication Information","title":"\ud83d\udd0c API Integration Methods"},{"location":"Wan2.2/T2V-14B/doc/index-en/#python-api-example","text":"\ud83d\udc0d Click to expand complete Python API integration code import requests, json, uuid, time, random, os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 - Wan2.2 14B \u4e13\u7528 COMFYUI_SERVER = \"127.0.0.1:8188\" # \u672c\u5730\u670d\u52a1\u5668 COMFYUI_TOKEN = \"\" # \u672c\u5730\u901a\u5e38\u4e0d\u9700\u8981token UNET_HIGH_NOISE_MODEL = \"wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors\" UNET_LOW_NOISE_MODEL = \"wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" # \ud83c\udfaf \u9884\u8bbe\u53c2\u6570 PROMPT = \"A close-up of a young woman smiling gently in the rain, raindrops glistening on her face and eyelashes. The video captures the delicate details of her expression and the water droplets, with soft light reflecting off her skin in the rainy atmosphere.\" NEG_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" class ComfyUIWan22_14BClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def generate_wan22_14b_t2v(self, prompt, neg_prompt, steps=20, cfg=3.5, width=1280, height=704, frames=81, fps=16): \"\"\"\ud83c\udfac Wan2.2 14B \u6587\u751f\u89c6\u9891\u751f\u6210 - \u53cc\u9636\u6bb5\u91c7\u6837\"\"\" print(\"\ud83c\udfac \u5f00\u59cb Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1...\") # \u5b8c\u5168\u57fa\u4e8e\u4f60\u63d0\u4f9b\u768414B JSON\u5de5\u4f5c\u6d41 workflow = { \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": neg_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE\u89e3\u7801\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_HIGH_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dCLIP\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"\u52a0\u8f7dVAE\"} }, \"54\": { \"inputs\": { \"shift\": 8.000000000000002, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"55\": { \"inputs\": { \"shift\": 8, \"model\": [\"56\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"\u91c7\u6837\u7b97\u6cd5\uff08SD3\uff09\"} }, \"56\": { \"inputs\": { \"unet_name\": UNET_LOW_NOISE_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet\u52a0\u8f7d\u5668\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": random.randint(1, 1000000000000000), \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 10, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"59\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 10, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K\u91c7\u6837\u5668\uff08\u9ad8\u7ea7\uff09\"} }, \"59\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": frames, \"batch_size\": 1 }, \"class_type\": \"EmptyHunyuanLatentVideo\", \"_meta\": {\"title\": \"\u7a7aLatent\u89c6\u9891\uff08\u6df7\u5143\uff09\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"\u521b\u5efa\u89c6\u9891\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"video/ComfyUI_14B\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"\u4fdd\u5b58\u89c6\u9891\"} } } print(\"\ud83d\udce4 Wan2.2 14B ...\") response = requests.post(f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id}) if response.status_code != 200: raise Exception(f\"API: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): try: queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) return \"completed\" if history_response.status_code == 200 and task_id in history_response.json() else \"processing\" except: return \"processing\" def download_video(self, task_id, output_path=\"wan22_14b_t2v_output.mp4\"): try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: for output in history[task_id]['outputs'].values(): if 'videos' in output: filename = output['videos'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path elif 'gifs' in output: filename = output['gifs'][0]['filename'] video_response = requests.get(f\"{self.base_url}/view?filename={filename}\", headers=self.headers) with open(output_path, \"wb\") as f: f.write(video_response.content) return output_path except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Wan2.2 14B \u6587\u751f\u89c6\u9891\u4efb\u52a1\"\"\" client = ComfyUIWan22_14BClient() try: # 14B \u6587\u751f\u89c6\u9891\u751f\u6210 task_id = client.generate_wan22_14b_t2v(PROMPT, NEG_PROMPT, 20, 3.5, 1280, 704, 81, 16) print(f\"\ud83c\udd94 Task ID: {task_id}\") while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Wan2.2 14B Video ready!\"); break elif status == \"failed\": print(\"\u274c Generation failed!\"); exit(1) time.sleep(15) # output_file = client.download_video(task_id, \"wan22_14b_t2v_output.mp4\") print(\"\ud83c\udf89 Video downloaded successfully!\" if output_file else \"\u274c Failed to download video\") if output_file: print(f\"\ud83d\udcc1 Saved as: {output_file}\") except Exception as e: print(f\"\u274c Error: {e}\") if __name__ == \"__main__\": main()","title":"\ud83d\udcbb Python API Example"},{"location":"Wan2.2/T2V-14B/doc/index-en/#prompt-writing-guide","text":"","title":"\ud83d\udca1 Prompt Writing Guide"},{"location":"Wan2.2/T2V-14B/doc/index-en/#integration-ecosystem","text":"\ud83c\udfa8 ComfyUI \u2705 Integrated Multi-language Docs Visual workflow, easy to use \ud83e\udd16 ModelScope \u2705 Official Support Native Integration Model hosting and distribution","title":"\ud83d\udd17 Integration Ecosystem"},{"location":"Wan2.2/T2V-14B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/T2V-14B/doc/index-en/#license-and-citation","text":"### \ud83d\udccb Open Source License \ud83d\udcdc Apache 2.0 License \u2022 Free to use and distribute \u2022 Commercial-friendly open source license \u2022 Complete usage rights granted \u2022 Must comply with relevant laws and regulations ### \ud83d\udcda Academic Citation @article{wan2025, title={Wan: Open and Advanced Large-Scale Video Generative Models}, author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu}, journal = {arXiv preprint arXiv:2503.20314}, year={2025} } \ud83c\udfac Tongyi Wanxiang 2.2 Image-to-Video A14B | Open and Advanced Large-Scale Video Generation Model \u00a9 2025 Tongyi Wanxiang Team | Apache 2.0 Open Source License | Transform Every Image into Spectacular Video","title":"\ud83d\udcc4 License and Citation"},{"location":"Wan2.2/fun-camera-14B/doc/","text":"\ud83d\udcf9 Wan2.2-Fun-Camera-Control \u76f8\u673a\u63a7\u5236\u89c6\u9891\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u4e13\u4e1a\u7ea7\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u7684\u89c6\u9891\u751f\u6210 \ud83d\udcf9 \u76f8\u673a\u8fd0\u52a8\u63a7\u5236 \ud83c\udfac \u5f71\u89c6\u7ea7\u8d28\u91cf \ud83c\udfaf \u7ec4\u5408\u8fd0\u52a8 \ud83d\udccb \u6a21\u578b\u6982\u89c8 **Wan2.2-Fun-Camera-Control** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u65b0\u4e00\u4ee3\u89c6\u9891\u751f\u6210\u4e0e\u76f8\u673a\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u6027\u7684\u76f8\u673a\u63a7\u5236\u4ee3\u7801\uff08Camera Control Codes\uff09\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u9884\u8bbe\u76f8\u673a\u8fd0\u52a8\u6761\u4ef6\u7684\u89c6\u9891\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83d\udcf9 \u76f8\u673a\u8fd0\u52a8\u63a7\u5236 \u652f\u6301 Pan Up/Down\u3001Pan Left/Right\u3001Zoom In/Out \u7b49\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83c\udfaf \u7ec4\u5408\u8fd0\u52a8\u63a7\u5236 \u652f\u6301\u591a\u79cd\u76f8\u673a\u8fd0\u52a8\u7684\u7ec4\u5408\u63a7\u5236 \u2699\ufe0f \u7cbe\u786e\u53c2\u6570\u63a7\u5236 \u53ef\u8c03\u8282\u8fd0\u52a8\u901f\u5ea6\u3001\u5f3a\u5ea6\u7b49\u7cbe\u7ec6\u53c2\u6570 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-A14B-Control-Camera \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun \ud83d\ude80 Wan2.2 Fun Camera Control \u5de5\u4f5c\u6d41\u793a\u4f8b \ud83d\udd27 \u5de5\u4f5c\u6d41\u7248\u672c\u8bf4\u660e \u4f60\u53ef\u4ee5\u76f4\u63a5\u4eceComfyui\u7684\u6a21\u7248\u4ed3\u5e93\u4e2d\u6253\u5f00\uff1a ![img.png](img.png) \u63d0\u4f9b\u4e24\u4e2a\u7248\u672c\u7684\u5de5\u4f5c\u6d41\u4f9b\u9009\u62e9\uff1a \u26a1 Lightning \u52a0\u901f\u7248 \u4f7f\u7528 Wan2.2-Lightning 4\u6b65 LoRA \u2705 \u901f\u5ea6\u66f4\u5feb \u26a0\ufe0f \u52a8\u6001\u635f\u5931 \ud83c\udfaf \u6807\u51c6\u8d28\u91cf\u7248 \u4f7f\u7528 fp8_scaled \u7248\u672c\uff0c\u65e0\u52a0\u901f LoRA \u2705 \u8d28\u91cf\u66f4\u9ad8 \u23f1\ufe0f \u8017\u65f6\u66f4\u957f #### \ud83d\udcca \u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5 \ud83e\uddea \u6d4b\u8bd5\u73af\u5883 \uff1aRTX 4090D 24GB \u663e\u5b58\uff0c640\u00d7640 \u5206\u8fa8\u7387\uff0c81 \u5e27\u957f\u5ea6 \u6a21\u578b\u7c7b\u578b \u5206\u8fa8\u7387 \u663e\u5b58\u5360\u7528 \u9996\u6b21\u751f\u6210 \u7b2c\u4e8c\u6b21\u751f\u6210 fp8_scaled 640\u00d7640 84% \u2248 536\u79d2 \u2248 513\u79d2 fp8_scaled + 4\u6b65LoRA 640\u00d7640 89% \u2248 108\u79d2 \u2248 71\u79d2 \ud83d\udca1 \u7248\u672c\u5207\u6362\u8bf4\u660e \u7531\u4e8e 4 \u6b65 LoRA \u5bf9\u521d\u6b21\u4f7f\u7528\u5de5\u4f5c\u6d41\u7684\u7528\u6237\u4f53\u9a8c\u8f83\u597d\uff0c\u9ed8\u8ba4\u542f\u7528\u52a0\u901f\u7248\u672c\u3002\u5982\u9700\u5207\u6362\u5230\u6807\u51c6\u7248\u672c\uff0c\u6846\u9009\u5bf9\u5e94\u5de5\u4f5c\u6d41\u540e\u4f7f\u7528 Ctrl+B \u5373\u53ef\u542f\u7528\u3002 \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u53ca\u7d20\u6750\u4e0b\u8f7d \u76f4\u63a5\u901a\u8fc7comfyui \u6a21\u7248\u4ed3\u5e93\u6253\u5f00\u5de5\u4f5c\u6d41 ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u89c6\u9891\u6216 JSON \u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u5de5\u4f5c\u6d41\uff0c\u5de5\u4f5c\u6d41\u4f1a\u63d0\u793a\u4e0b\u8f7d\u6a21\u578b\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d \ud83d\uddbc\ufe0f \u8f93\u5165\u8d77\u59cb\u56fe\u7247 \u6b64\u56fe\u7247\u5c06\u4f5c\u4e3a\u89c6\u9891\u751f\u6210\u7684\u8d77\u59cb\u5e27 \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6\u8bf4\u660e #### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c \u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0chigh noise \u548c low noise \u7684\u6a21\u578b\u548c LoRA \u9700\u8981\u5bf9\u5e94\u4f7f\u7528\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 High Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf \u57fa\u7840\u6a21\u578b\u914d\u7f6e \u8282\u70b9\u540d\u79f0 \u6a21\u578b\u6587\u4ef6 \u8bf4\u660e Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors \u6587\u672c\u7f16\u7801\u5668 Load VAE wan_2.1_vae.safetensors \u53d8\u5206\u81ea\u7f16\u7801\u5668 #### \ud83d\udcc1 \u8f93\u5165\u914d\u7f6e\u6b65\u9aa4 \ud83d\uddbc\ufe0f \u8d77\u59cb\u5e27\u4e0a\u4f20 \u5728 Load Image \u8282\u70b9\u4e0a\u4f20\u8d77\u59cb\u5e27\u56fe\u7247 \ud83d\udcdd \u63d0\u793a\u8bcd\u4fee\u6539 \u4fee\u6539 Prompt\uff0c\u652f\u6301\u4e2d\u82f1\u6587\u8f93\u5165 #### \ud83d\udcf9 \u76f8\u673a\u63a7\u5236\u53c2\u6570\u914d\u7f6e \ud83c\udfac WanCameraEmbedding \u8282\u70b9\u914d\u7f6e \u5728\u6b64\u8282\u70b9\u4e2d\u8bbe\u7f6e\u76f8\u673a\u63a7\u5236\u7684\u5404\u9879\u53c2\u6570\uff1a \u53c2\u6570\u540d\u79f0 \u53ef\u9009\u503c \u8bf4\u660e Camera Motion Zoom In, Zoom Out, Pan Up, Pan Down, Pan Left, Pan Right, Static \u9009\u62e9\u76f8\u673a\u8fd0\u52a8\u7c7b\u578b Width/Height 640\u00d7640\uff08\u9ed8\u8ba4\uff09 \u8bbe\u7f6e\u89c6\u9891\u5206\u8fa8\u7387 Length 81\uff08\u9ed8\u8ba4\uff09 \u8bbe\u7f6e\u89c6\u9891\u5e27\u6570 Speed 1.0\uff08\u9ed8\u8ba4\uff09 \u8bbe\u7f6e\u89c6\u9891\u901f\u5ea6 #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Run \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u6267\u884c\u89c6\u9891\u751f\u6210 \ud83d\udcf9 \u76f8\u673a\u8fd0\u52a8\u7c7b\u578b\u8be6\u89e3 \ud83d\udd0d Zoom In\uff08\u653e\u5927\uff09 \u76f8\u673a\u5411\u524d\u63a8\u8fdb\uff0c\u753b\u9762\u9010\u6e10\u653e\u5927 \ud83d\udd0e Zoom Out\uff08\u7f29\u5c0f\uff09 \u76f8\u673a\u5411\u540e\u62c9\u8fdc\uff0c\u753b\u9762\u9010\u6e10\u7f29\u5c0f \u2b06\ufe0f Pan Up\uff08\u5411\u4e0a\u5e73\u79fb\uff09 \u76f8\u673a\u5411\u4e0a\u79fb\u52a8\uff0c\u5c55\u73b0\u4e0a\u65b9\u5185\u5bb9 \u2b07\ufe0f Pan Down\uff08\u5411\u4e0b\u5e73\u79fb\uff09 \u76f8\u673a\u5411\u4e0b\u79fb\u52a8\uff0c\u5c55\u73b0\u4e0b\u65b9\u5185\u5bb9 \u2b05\ufe0f Pan Left\uff08\u5411\u5de6\u5e73\u79fb\uff09 \u76f8\u673a\u5411\u5de6\u79fb\u52a8\uff0c\u5c55\u73b0\u5de6\u4fa7\u5185\u5bb9 \u27a1\ufe0f Pan Right\uff08\u5411\u53f3\u5e73\u79fb\uff09 \u76f8\u673a\u5411\u53f3\u79fb\u52a8\uff0c\u5c55\u73b0\u53f3\u4fa7\u5185\u5bb9 \ud83c\udfaf \u5e94\u7528\u573a\u666f \ud83c\udfac \u7535\u5f71\u5236\u4f5c \u4e13\u4e1a\u7ea7\u76f8\u673a\u8fd0\u52a8\uff0c\u521b\u9020\u7535\u5f71\u7ea7\u89c6\u89c9\u6548\u679c \ud83d\udcf1 \u77ed\u89c6\u9891\u521b\u4f5c \u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u7684\u52a8\u6001\u89c6\u89c9\u5448\u73b0 \ud83c\udfe2 \u4ea7\u54c1\u5c55\u793a \u4ea7\u54c1\u5ba3\u4f20\u89c6\u9891\u7684\u4e13\u4e1a\u76f8\u673a\u8fd0\u52a8 \ud83c\udfa8 \u827a\u672f\u521b\u4f5c \u827a\u672f\u4f5c\u54c1\u7684\u52a8\u6001\u5c55\u793a\u548c\u8868\u73b0 \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u9009\u62e9\u5408\u9002\u7684\u76f8\u673a\u8fd0\u52a8\u5339\u914d\u5185\u5bb9\u4e3b\u9898 \u8c03\u6574\u8fd0\u52a8\u901f\u5ea6\u4ee5\u83b7\u5f97\u6700\u4f73\u89c6\u89c9\u6548\u679c \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u5206\u8fa8\u7387 \u63d0\u793a\u8bcd\u8981\u4e0e\u76f8\u673a\u8fd0\u52a8\u4fdd\u6301\u534f\u8c03 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd High/Low Noise \u6a21\u578b\u4e0e LoRA \u5339\u914d \u8fc7\u5feb\u7684\u76f8\u673a\u8fd0\u52a8\u53ef\u80fd\u5bfc\u81f4\u753b\u9762\u4e0d\u7a33\u5b9a \u590d\u6742\u573a\u666f\u5efa\u8bae\u4f7f\u7528\u8f83\u6162\u7684\u8fd0\u52a8\u901f\u5ea6 \u957f\u65f6\u95f4\u751f\u6210\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcf9 \u76f8\u673a\u63a7\u5236\u7cbe\u5ea6 \ud83c\udfaf \u8fd0\u52a8\u7cbe\u5ea6 \u5e73\u79fb\u63a7\u5236 : \u50cf\u7d20\u7ea7\u7cbe\u5ea6 \u7f29\u653e\u63a7\u5236 : \u5e73\u6ed1\u6e10\u8fdb\u5f0f \u901f\u5ea6\u63a7\u5236 : 0.1-2.0 \u500d\u901f \u26a1 \u6027\u80fd\u4f18\u5316 Lightning LoRA : 5\u500d\u901f\u5ea6\u63d0\u5347 FP8 \u91cf\u5316 : \u663e\u5b58\u4f18\u5316 \u667a\u80fd\u7f13\u5b58 : \u51cf\u5c11\u91cd\u590d\u8ba1\u7b97 ### \ud83c\udfac \u9ad8\u7ea7\u76f8\u673a\u529f\u80fd \ud83c\udfaf \u7ec4\u5408\u8fd0\u52a8 \u652f\u6301\u591a\u79cd\u76f8\u673a\u8fd0\u52a8\u7684\u7ec4\u5408\u63a7\u5236 \u23f1\ufe0f \u65f6\u5e8f\u63a7\u5236 \u7cbe\u786e\u63a7\u5236\u8fd0\u52a8\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u95f4 \ud83d\udcd0 \u8fd0\u52a8\u8f68\u8ff9 \u81ea\u5b9a\u4e49\u76f8\u673a\u8fd0\u52a8\u7684\u8f68\u8ff9\u8def\u5f84 \ud83d\udcf9 Wan2.2-Fun-Camera-Control \u76f8\u673a\u63a7\u5236\u89c6\u9891\u751f\u6210 | \u4e13\u4e1a\u7ea7\u76f8\u673a\u8fd0\u52a8\u7684\u89c6\u9891\u521b\u4f5c \u00a9 2025 Alibaba PAI \u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u6bcf\u4e2a\u955c\u5934\u90fd\u5145\u6ee1\u7535\u5f71\u611f","title":"Index"},{"location":"Wan2.2/fun-camera-14B/doc/#_1","text":"**Wan2.2-Fun-Camera-Control** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u65b0\u4e00\u4ee3\u89c6\u9891\u751f\u6210\u4e0e\u76f8\u673a\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u6027\u7684\u76f8\u673a\u63a7\u5236\u4ee3\u7801\uff08Camera Control Codes\uff09\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u9884\u8bbe\u76f8\u673a\u8fd0\u52a8\u6761\u4ef6\u7684\u89c6\u9891\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83d\udcf9 \u76f8\u673a\u8fd0\u52a8\u63a7\u5236 \u652f\u6301 Pan Up/Down\u3001Pan Left/Right\u3001Zoom In/Out \u7b49\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83c\udfaf \u7ec4\u5408\u8fd0\u52a8\u63a7\u5236 \u652f\u6301\u591a\u79cd\u76f8\u673a\u8fd0\u52a8\u7684\u7ec4\u5408\u63a7\u5236 \u2699\ufe0f \u7cbe\u786e\u53c2\u6570\u63a7\u5236 \u53ef\u8c03\u8282\u8fd0\u52a8\u901f\u5ea6\u3001\u5f3a\u5ea6\u7b49\u7cbe\u7ec6\u53c2\u6570 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-A14B-Control-Camera \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/fun-camera-14B/doc/#wan22-fun-camera-control","text":"","title":"\ud83d\ude80 Wan2.2 Fun Camera Control \u5de5\u4f5c\u6d41\u793a\u4f8b"},{"location":"Wan2.2/fun-camera-14B/doc/#_2","text":"\u4f60\u53ef\u4ee5\u76f4\u63a5\u4eceComfyui\u7684\u6a21\u7248\u4ed3\u5e93\u4e2d\u6253\u5f00\uff1a ![img.png](img.png) \u63d0\u4f9b\u4e24\u4e2a\u7248\u672c\u7684\u5de5\u4f5c\u6d41\u4f9b\u9009\u62e9\uff1a","title":"\ud83d\udd27 \u5de5\u4f5c\u6d41\u7248\u672c\u8bf4\u660e"},{"location":"Wan2.2/fun-camera-14B/doc/#_3","text":"\u76f4\u63a5\u901a\u8fc7comfyui \u6a21\u7248\u4ed3\u5e93\u6253\u5f00\u5de5\u4f5c\u6d41 ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u89c6\u9891\u6216 JSON \u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u5de5\u4f5c\u6d41\uff0c\u5de5\u4f5c\u6d41\u4f1a\u63d0\u793a\u4e0b\u8f7d\u6a21\u578b\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u53ca\u7d20\u6750\u4e0b\u8f7d"},{"location":"Wan2.2/fun-camera-14B/doc/#_4","text":"#### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6\u8bf4\u660e"},{"location":"Wan2.2/fun-camera-14B/doc/#_5","text":"\u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0chigh noise \u548c low noise \u7684\u6a21\u578b\u548c LoRA \u9700\u8981\u5bf9\u5e94\u4f7f\u7528\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c"},{"location":"Wan2.2/fun-camera-14B/doc/#_6","text":"\ud83d\udd0d","title":"\ud83d\udcf9 \u76f8\u673a\u8fd0\u52a8\u7c7b\u578b\u8be6\u89e3"},{"location":"Wan2.2/fun-camera-14B/doc/#_7","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f"},{"location":"Wan2.2/fun-camera-14B/doc/#_8","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"Wan2.2/fun-camera-14B/doc/#_9","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcf9 \u76f8\u673a\u63a7\u5236\u7cbe\u5ea6","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/","text":"\ud83d\udcf9 Wan2.2-Fun-Camera-Control Video Generation ComfyUI Native Workflow - Professional Camera Motion Control for Video Generation \ud83d\udcf9 Camera Motion Control \ud83c\udfac Cinematic Quality \ud83c\udfaf Combined Motion \ud83d\udccb Model Overview **Wan2.2-Fun-Camera-Control** is a next-generation video generation and camera control model developed by the Alibaba PAI team. By introducing innovative Camera Control Codes mechanisms combined with deep learning and multi-modal conditional inputs, it can generate high-quality videos that conform to preset camera motion conditions. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83d\udcf9 Camera Motion Control Supports Pan Up/Down, Pan Left/Right, Zoom In/Out and other motion modes \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83c\udfaf Combined Motion Control Supports combination control of multiple camera motions \u2699\ufe0f Precise Parameter Control Adjustable motion speed, intensity and other fine parameters \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-A14B-Control-Camera \u2022 Code Repository : VideoX-Fun \ud83d\ude80 Wan2.2 Fun Camera Control Workflow Example \ud83d\udd27 Workflow Version Description Two workflow versions are provided for selection: \u26a1 Lightning Accelerated Version Uses Wan2.2-Lightning 4-step LoRA \u2705 Faster Speed \u26a0\ufe0f Dynamic Loss \ud83c\udfaf Standard Quality Version Uses fp8_scaled version without acceleration LoRA \u2705 Higher Quality \u23f1\ufe0f Longer Time #### \ud83d\udcca Performance Comparison Test \ud83e\uddea Test Environment : RTX 4090D 24GB VRAM, 640\u00d7640 resolution, 81 frames length Model Type Resolution VRAM Usage First Generation Second Generation fp8_scaled 640\u00d7640 84% \u2248 536s \u2248 513s fp8_scaled + 4-step LoRA 640\u00d7640 89% \u2248 108s \u2248 71s \ud83d\udca1 Version Switching Instructions Since the 4-step LoRA provides better user experience for first-time workflow users, the accelerated version is enabled by default. To switch to the standard version, select the corresponding workflow and use Ctrl+B to enable it. \ud83d\udce5 Step 1: Download Workflow and Materials you can find open the template from comfyui repository. ![img.png](img.png) Download the following video or JSON file and drag it into ComfyUI to load the corresponding workflow. The workflow will prompt to download models. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download \ud83d\uddbc\ufe0f Input Starting Image This image will serve as the starting frame for video generation \ud83d\udd17 Step 2: Model Files \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 Step 3: Workflow Configuration Operations \u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent, with high noise and low noise models and LoRAs used correspondingly. #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 High Noise Model Configuration Load Diffusion Model : wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise Model Configuration Load Diffusion Model : wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf Base Model Configuration Node Name Model File Description Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors Text encoder Load VAE wan_2.1_vae.safetensors Variational autoencoder #### \ud83d\udcc1 Input Configuration Steps \ud83d\uddbc\ufe0f Starting Frame Upload Upload the starting frame image in the Load Image node \ud83d\udcdd Modify Prompts Modify prompts, supports Chinese and English input #### \ud83d\udcf9 Camera Control Parameter Configuration \ud83c\udfac WanCameraEmbedding Node Configuration Set various camera control parameters in this node: Parameter Name Available Values Description Camera Motion Zoom In, Zoom Out, Pan Up, Pan Down, Pan Left, Pan Right, Static Select camera motion type Width/Height 640\u00d7640 (default) Set video resolution Length 81 (default) Set video frame count Speed 1.0 (default) Set video speed #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click the Run button or use shortcut Ctrl(Cmd) + Enter to execute video generation \ud83d\udcf9 Camera Motion Types Explained \ud83d\udd0d Zoom In Camera moves forward, gradually enlarging the frame \ud83d\udd0e Zoom Out Camera moves backward, gradually reducing the frame \u2b06\ufe0f Pan Up Camera moves upward, revealing upper content \u2b07\ufe0f Pan Down Camera moves downward, revealing lower content \u2b05\ufe0f Pan Left Camera moves left, revealing left-side content \u27a1\ufe0f Pan Right Camera moves right, revealing right-side content \ud83c\udfaf Application Scenarios \ud83c\udfac Film Production Professional camera movements for cinematic visual effects \ud83d\udcf1 Short Video Creation Dynamic visual presentation for social media content \ud83c\udfe2 Product Showcase Professional camera movements for product promotional videos \ud83c\udfa8 Artistic Creation Dynamic display and expression of artistic works \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Choose appropriate camera motion to match content theme Adjust motion speed for optimal visual effects Select appropriate resolution based on VRAM capacity Keep prompts coordinated with camera motion \u26a0\ufe0f Important Notes Ensure High/Low Noise models match with LoRA Too fast camera motion may cause frame instability Complex scenes recommend using slower motion speeds Long-duration generation requires more compute resources \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcf9 Camera Control Precision \ud83c\udfaf Motion Precision Pan Control : Pixel-level precision Zoom Control : Smooth progressive Speed Control : 0.1-2.0x speed \u26a1 Performance Optimization Lightning LoRA : 5x speed improvement FP8 Quantization : VRAM optimization Smart Caching : Reduce redundant computation ### \ud83c\udfac Advanced Camera Features \ud83c\udfaf Combined Motion Support combination control of multiple camera motions \u23f1\ufe0f Temporal Control Precise control of motion start and end times \ud83d\udcd0 Motion Trajectory Customize camera motion trajectory paths ### \ud83c\udfa8 Creative Applications \ud83c\udfac Cinematic Shots Create professional film-style camera movements \ud83d\udcf1 Social Media Engaging camera movements for viral content \ud83c\udfe2 Commercial Use Professional product demonstrations and marketing \ud83d\udcf9 Wan2.2-Fun-Camera-Control Video Generation | Professional Camera Motion for Video Creation \u00a9 2025 Alibaba PAI Team | Apache 2.0 Open Source License | Making Every Shot Cinematic","title":"Index en"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#model-overview","text":"**Wan2.2-Fun-Camera-Control** is a next-generation video generation and camera control model developed by the Alibaba PAI team. By introducing innovative Camera Control Codes mechanisms combined with deep learning and multi-modal conditional inputs, it can generate high-quality videos that conform to preset camera motion conditions. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83d\udcf9 Camera Motion Control Supports Pan Up/Down, Pan Left/Right, Zoom In/Out and other motion modes \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83c\udfaf Combined Motion Control Supports combination control of multiple camera motions \u2699\ufe0f Precise Parameter Control Adjustable motion speed, intensity and other fine parameters \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-A14B-Control-Camera \u2022 Code Repository : VideoX-Fun","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#wan22-fun-camera-control-workflow-example","text":"","title":"\ud83d\ude80 Wan2.2 Fun Camera Control Workflow Example"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#workflow-version-description","text":"Two workflow versions are provided for selection:","title":"\ud83d\udd27 Workflow Version Description"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#step-1-download-workflow-and-materials","text":"you can find open the template from comfyui repository. ![img.png](img.png) Download the following video or JSON file and drag it into ComfyUI to load the corresponding workflow. The workflow will prompt to download models. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download","title":"\ud83d\udce5 Step 1: Download Workflow and Materials"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#step-2-model-files","text":"","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#model-file-structure","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 Model File Structure"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#step-3-workflow-configuration-operations","text":"\u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent, with high noise and low noise models and LoRAs used correspondingly. #### \ud83d\udccb Detailed Configuration Steps","title":"\ud83d\udd27 Step 3: Workflow Configuration Operations"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#camera-motion-types-explained","text":"\ud83d\udd0d","title":"\ud83d\udcf9 Camera Motion Types Explained"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"Wan2.2/fun-camera-14B/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcf9 Camera Control Precision","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.2/fun-control-14B/doc/","text":"\ud83c\udfae Wan2.2-Fun-Control \u89c6\u9891\u63a7\u5236\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u591a\u6a21\u6001\u63a7\u5236\u6761\u4ef6\u7684\u7cbe\u51c6\u89c6\u9891\u751f\u6210 \ud83c\udfaf \u591a\u6a21\u6001\u63a7\u5236 \ud83c\udfac \u5f71\u89c6\u7ea7\u8d28\u91cf \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \ud83d\udccb \u6a21\u578b\u6982\u89c8 **Wan2.2-Fun-Control** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u65b0\u4e00\u4ee3\u89c6\u9891\u751f\u6210\u4e0e\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u6027\u7684\u63a7\u5236\u4ee3\u7801\uff08Control Codes\uff09\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u9884\u8bbe\u63a7\u5236\u6761\u4ef6\u7684\u89c6\u9891\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u591a\u6a21\u6001\u63a7\u5236 \u652f\u6301 Canny\u3001Depth\u3001OpenPose\u3001MLSD \u7b49\u591a\u79cd\u63a7\u5236\u6761\u4ef6 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u82f1\u6587\u7b49\u591a\u8bed\u8a00\u63d0\u793a\u8bcd\u8f93\u5165 \ud83c\udfae \u8f68\u8ff9\u63a7\u5236 \u652f\u6301\u7cbe\u786e\u7684\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\u529f\u80fd \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun \ud83d\ude80 Wan2.2 Fun Control \u5de5\u4f5c\u6d41\u793a\u4f8b \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u53ca\u7d20\u6750\u4e0b\u8f7d \u53ef\u76f4\u63a5\u901a\u8fc7Comfyui\u6a21\u7248\u4ed3\u5e93\u6253\u5f00 ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u89c6\u9891\u6216 JSON \u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u5de5\u4f5c\u6d41\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d \ud83d\uddbc\ufe0f \u8f93\u5165\u8d77\u59cb\u56fe\u7247 \ud83c\udfac \u63a7\u5236\u89c6\u9891 \u6b64\u89c6\u9891\u5df2\u7ecf\u8fc7\u9884\u5904\u7406\uff0c\u53ef\u76f4\u63a5\u7528\u4e8e\u63a7\u5236\u89c6\u9891\u751f\u6210 \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c \u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0chigh noise \u548c low noise \u7684\u6a21\u578b\u548c LoRA \u9700\u8981\u5bf9\u5e94\u4f7f\u7528\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 High Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf \u57fa\u7840\u6a21\u578b\u914d\u7f6e \u8282\u70b9\u540d\u79f0 \u6a21\u578b\u6587\u4ef6 \u8bf4\u660e Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors \u6587\u672c\u7f16\u7801\u5668 Load VAE wan_2.1_vae.safetensors \u53d8\u5206\u81ea\u7f16\u7801\u5668 #### \ud83d\udcc1 \u8f93\u5165\u914d\u7f6e\u6b65\u9aa4 \ud83d\uddbc\ufe0f \u8d77\u59cb\u5e27\u4e0a\u4f20 \u5728 Load Image \u8282\u70b9\u4e0a\u4f20\u8d77\u59cb\u5e27\u56fe\u7247 \ud83c\udfac \u63a7\u5236\u89c6\u9891\u4e0a\u4f20 \u5728\u7b2c\u4e8c\u4e2a Load Video \u8282\u70b9\u4e0a\u4f20\u63a7\u5236\u89c6\u9891\u7684 pose \u89c6\u9891 \ud83d\udd27 \u9884\u5904\u7406\u8282\u70b9\u7981\u7528 \u7531\u4e8e\u63d0\u4f9b\u7684\u89c6\u9891\u5df2\u9884\u5904\u7406\uff0c\u9700\u8981\u7981\u7528\u5bf9\u5e94\u7684\u9884\u5904\u7406\u8282\u70b9\uff08\u9009\u4e2d\u540e\u4f7f\u7528 Ctrl+B \uff09 \ud83d\udcdd \u63d0\u793a\u8bcd\u4fee\u6539 \u4fee\u6539 Prompt\uff0c\u652f\u6301\u4e2d\u82f1\u6587\u8f93\u5165 #### \u2699\ufe0f \u53c2\u6570\u8c03\u6574 \ud83c\udfac Wan22FunControlToVideo \u8282\u70b9\u914d\u7f6e \u2022 \u4fee\u6539\u5bf9\u5e94\u89c6\u9891\u7684\u5c3a\u5bf8\u53c2\u6570 \u2022 \u9ed8\u8ba4\u8bbe\u7f6e\u4e86 640\u00d7640 \u7684\u5206\u8fa8\u7387\uff0c\u907f\u514d\u4f4e\u663e\u5b58\u7528\u6237\u4f7f\u7528\u65f6\u8fc7\u4e8e\u8017\u65f6 \u2022 \u53ef\u6839\u636e\u9700\u8981\u8c03\u6574\u4e3a\u66f4\u9ad8\u5206\u8fa8\u7387 #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Run \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u6267\u884c\u89c6\u9891\u751f\u6210 API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf \u63a7\u5236\u7c7b\u578b\u8be6\u89e3 \u270f\ufe0f Canny\uff08\u7ebf\u7a3f\uff09 \u8fb9\u7f18\u68c0\u6d4b\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u8f6e\u5ed3\u548c\u7ebf\u6761\u63a7\u5236 \ud83c\udfd4\ufe0f Depth\uff08\u6df1\u5ea6\uff09 \u6df1\u5ea6\u4fe1\u606f\u63a7\u5236\uff0c\u4fdd\u6301\u573a\u666f\u7684\u7a7a\u95f4\u7ed3\u6784 \ud83e\udd38 OpenPose\uff08\u4eba\u4f53\u59ff\u52bf\uff09 \u4eba\u4f53\u9aa8\u9abc\u70b9\u63a7\u5236\uff0c\u7cbe\u786e\u63a7\u5236\u4eba\u7269\u52a8\u4f5c \ud83d\udcd0 MLSD\uff08\u51e0\u4f55\u8fb9\u7f18\uff09 \u51e0\u4f55\u7ebf\u6761\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u5efa\u7b51\u548c\u7ed3\u6784\u63a7\u5236 \ud83d\udca1 \u8865\u5145\u8bf4\u660e\u4e0e\u6269\u5c55 ### \ud83d\udd27 \u9884\u5904\u7406\u5668\u6269\u5c55 \ud83d\udce6 ComfyUI-comfyui_controlnet_aux \u7531\u4e8e ComfyUI \u81ea\u5e26\u7684\u8282\u70b9\u4e2d\uff0c\u9884\u5904\u7406\u5668\u8282\u70b9\u53ea\u6709 Canny \u7684\u9884\u5904\u7406\u5668\uff0c\u53ef\u4ee5\u4f7f\u7528 ComfyUI-comfyui_controlnet_aux \u6765\u5b9e\u73b0\u5176\u4ed6\u7c7b\u578b\u7684\u56fe\u50cf\u9884\u5904\u7406\u3002 ### \ud83c\udfa8 \u652f\u6301\u7684\u9884\u5904\u7406\u7c7b\u578b Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation \ud83c\udfaf \u5e94\u7528\u573a\u666f \ud83c\udfac \u52a8\u753b\u5236\u4f5c \u7cbe\u786e\u63a7\u5236\u89d2\u8272\u52a8\u4f5c\u548c\u573a\u666f\u53d8\u5316 \ud83c\udfa8 \u827a\u672f\u521b\u4f5c \u5c06\u9759\u6001\u827a\u672f\u4f5c\u54c1\u8f6c\u6362\u4e3a\u52a8\u6001\u8868\u73b0 \ud83c\udfc3 \u8fd0\u52a8\u5206\u6790 \u4f53\u80b2\u52a8\u4f5c\u5206\u6790\u548c\u8bad\u7ec3\u89c6\u9891\u5236\u4f5c \ud83c\udfd7\ufe0f \u5efa\u7b51\u53ef\u89c6\u5316 \u5efa\u7b51\u8bbe\u8ba1\u7684\u52a8\u6001\u5c55\u793a\u548c\u6f2b\u6e38 \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u9009\u62e9\u5408\u9002\u7684\u63a7\u5236\u7c7b\u578b\u5339\u914d\u5185\u5bb9\u9700\u6c42 \u786e\u4fdd\u63a7\u5236\u89c6\u9891\u8d28\u91cf\u6e05\u6670\u7a33\u5b9a \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u5206\u8fa8\u7387 \u63d0\u793a\u8bcd\u8981\u4e0e\u63a7\u5236\u5185\u5bb9\u4fdd\u6301\u4e00\u81f4 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd High/Low Noise \u6a21\u578b\u4e0e LoRA \u5339\u914d \u9884\u5904\u7406\u89c6\u9891\u65f6\u6ce8\u610f\u7981\u7528\u5bf9\u5e94\u8282\u70b9 \u63a7\u5236\u89c6\u9891\u5e27\u7387\u8981\u4e0e\u76ee\u6807\u89c6\u9891\u5339\u914d \u590d\u6742\u63a7\u5236\u53ef\u80fd\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac \u652f\u6301\u7684\u63a7\u5236\u7cbe\u5ea6 \ud83c\udfaf \u7cbe\u786e\u63a7\u5236 OpenPose : \u5173\u8282\u70b9\u7ea7\u522b\u7cbe\u5ea6 Depth : \u50cf\u7d20\u7ea7\u6df1\u5ea6\u63a7\u5236 Canny : \u8fb9\u7f18\u8f6e\u5ed3\u63a7\u5236 \u26a1 \u6027\u80fd\u4f18\u5316 Lightning LoRA : 4\u500d\u901f\u5ea6\u63d0\u5347 FP8 \u91cf\u5316 : \u663e\u5b58\u4f18\u5316 \u6279\u5904\u7406 : \u591a\u89c6\u9891\u5e76\u884c \ud83c\udfae Wan2.2-Fun-Control \u89c6\u9891\u63a7\u5236\u751f\u6210 | \u591a\u6a21\u6001\u63a7\u5236\u6761\u4ef6\u7684\u7cbe\u51c6\u89c6\u9891\u521b\u4f5c \u00a9 2025 Alibaba PAI \u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u6bcf\u4e00\u5e27\u90fd\u5728\u7cbe\u786e\u63a7\u5236\u4e4b\u4e0b","title":"Index"},{"location":"Wan2.2/fun-control-14B/doc/#_1","text":"**Wan2.2-Fun-Control** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u65b0\u4e00\u4ee3\u89c6\u9891\u751f\u6210\u4e0e\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u6027\u7684\u63a7\u5236\u4ee3\u7801\uff08Control Codes\uff09\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u9884\u8bbe\u63a7\u5236\u6761\u4ef6\u7684\u89c6\u9891\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u591a\u6a21\u6001\u63a7\u5236 \u652f\u6301 Canny\u3001Depth\u3001OpenPose\u3001MLSD \u7b49\u591a\u79cd\u63a7\u5236\u6761\u4ef6 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u82f1\u6587\u7b49\u591a\u8bed\u8a00\u63d0\u793a\u8bcd\u8f93\u5165 \ud83c\udfae \u8f68\u8ff9\u63a7\u5236 \u652f\u6301\u7cbe\u786e\u7684\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\u529f\u80fd \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/fun-control-14B/doc/#wan22-fun-control","text":"","title":"\ud83d\ude80 Wan2.2 Fun Control \u5de5\u4f5c\u6d41\u793a\u4f8b"},{"location":"Wan2.2/fun-control-14B/doc/#_2","text":"\u53ef\u76f4\u63a5\u901a\u8fc7Comfyui\u6a21\u7248\u4ed3\u5e93\u6253\u5f00 ![img.png](img.png) \u6216\u4e0b\u8f7d\u4ee5\u4e0b\u89c6\u9891\u6216 JSON \u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u5de5\u4f5c\u6d41\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u53ca\u7d20\u6750\u4e0b\u8f7d"},{"location":"Wan2.2/fun-control-14B/doc/#_3","text":"","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"Wan2.2/fun-control-14B/doc/#_4","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784"},{"location":"Wan2.2/fun-control-14B/doc/#_5","text":"\u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0chigh noise \u548c low noise \u7684\u6a21\u578b\u548c LoRA \u9700\u8981\u5bf9\u5e94\u4f7f\u7528\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c"},{"location":"Wan2.2/fun-control-14B/doc/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"Wan2.2/fun-control-14B/doc/#_6","text":"\u270f\ufe0f","title":"\ud83c\udfaf \u63a7\u5236\u7c7b\u578b\u8be6\u89e3"},{"location":"Wan2.2/fun-control-14B/doc/#_7","text":"### \ud83d\udd27 \u9884\u5904\u7406\u5668\u6269\u5c55 \ud83d\udce6 ComfyUI-comfyui_controlnet_aux \u7531\u4e8e ComfyUI \u81ea\u5e26\u7684\u8282\u70b9\u4e2d\uff0c\u9884\u5904\u7406\u5668\u8282\u70b9\u53ea\u6709 Canny \u7684\u9884\u5904\u7406\u5668\uff0c\u53ef\u4ee5\u4f7f\u7528 ComfyUI-comfyui_controlnet_aux \u6765\u5b9e\u73b0\u5176\u4ed6\u7c7b\u578b\u7684\u56fe\u50cf\u9884\u5904\u7406\u3002 ### \ud83c\udfa8 \u652f\u6301\u7684\u9884\u5904\u7406\u7c7b\u578b Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation","title":"\ud83d\udca1 \u8865\u5145\u8bf4\u660e\u4e0e\u6269\u5c55"},{"location":"Wan2.2/fun-control-14B/doc/#_8","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f"},{"location":"Wan2.2/fun-control-14B/doc/#_9","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"Wan2.2/fun-control-14B/doc/#_10","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac \u652f\u6301\u7684\u63a7\u5236\u7cbe\u5ea6","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Wan2.2/fun-control-14B/doc/index-en/","text":"\ud83c\udfae Wan2.2-Fun-Control Video Control Generation ComfyUI Native Workflow - Precise Video Generation with Multi-Modal Control Conditions \ud83c\udfaf Multi-Modal Control \ud83c\udfac Cinematic Quality \ud83c\udf10 Multi-Language \ud83d\udccb Model Overview **Wan2.2-Fun-Control** is a next-generation video generation and control model developed by the Alibaba PAI team. By introducing innovative Control Codes mechanisms combined with deep learning and multi-modal conditional inputs, it can generate high-quality videos that conform to preset control conditions. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf Multi-Modal Control Supports Canny, Depth, OpenPose, MLSD and other control conditions \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83c\udf10 Multi-Language Support Supports Chinese, English and other multi-language prompt inputs \ud83c\udfae Trajectory Control Supports precise motion trajectory control functionality \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 Code Repository : VideoX-Fun \ud83d\ude80 Wan2.2 Fun Control Workflow Example \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Workflow Templates \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83d\udd27 Workflow Version Description Two workflow versions are provided for selection: \u26a1 Lightning Accelerated Version Uses Wan2.2-Lightning 4-step LoRA \u2705 Faster Speed \u26a0\ufe0f Dynamic Loss \ud83c\udfaf Standard Quality Version Uses fp8_scaled version without acceleration LoRA \u2705 Higher Quality \u23f1\ufe0f Longer Time #### \ud83d\udcca Performance Comparison Test \ud83e\uddea Test Environment : RTX 4090D 24GB VRAM, 640\u00d7640 resolution, 81 frames length Model Type Resolution VRAM Usage First Generation Second Generation fp8_scaled 640\u00d7640 83% \u2248 524s \u2248 520s fp8_scaled + 4-step LoRA 640\u00d7640 89% \u2248 138s \u2248 79s \ud83d\udca1 Version Switching Instructions Since the 4-step LoRA provides better user experience for first-time workflow users, the accelerated version is enabled by default. To switch to the standard version, select the corresponding workflow and use Ctrl+B to enable it. \ud83d\udce5 Step 1: Download Workflow and Materials you can find the workflow file in the following figure ![img_1.png](img_1.png) Download the following video or JSON file and drag it into ComfyUI to load the corresponding workflow. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download \ud83d\uddbc\ufe0f Input Starting Image \ud83c\udfac Control Video This video has been pre-processed and can be used directly for control video generation \ud83d\udd17 Step 2: Model Files \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 Step 3: Workflow Configuration Operations \u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent, with high noise and low noise models and LoRAs used correspondingly. #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 High Noise Model Configuration Load Diffusion Model : wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise Model Configuration Load Diffusion Model : wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf Base Model Configuration Node Name Model File Description Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors Text encoder Load VAE wan_2.1_vae.safetensors Variational autoencoder #### \ud83d\udcc1 Input Configuration Steps \ud83d\uddbc\ufe0f Starting Frame Upload Upload the starting frame image in the Load Image node \ud83c\udfac Control Video Upload Upload the control video's pose video in the second Load Video node \ud83d\udd27 Disable Preprocessing Nodes Since the provided video is pre-processed, disable corresponding preprocessing nodes (select and use Ctrl+B ) \ud83d\udcdd Modify Prompts Modify prompts, supports Chinese and English input #### \u2699\ufe0f Parameter Adjustment \ud83c\udfac Wan22FunControlToVideo Node Configuration \u2022 Modify the corresponding video dimension parameters \u2022 Default set to 640\u00d7640 resolution to avoid excessive time consumption for low-VRAM users \u2022 Can be adjusted to higher resolution as needed #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click the Run button or use shortcut Ctrl(Cmd) + Enter to execute video generation API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Control Types Explained \u270f\ufe0f Canny (Edge Detection) Edge detection control, suitable for contour and line control \ud83c\udfd4\ufe0f Depth (Depth Map) Depth information control, maintains spatial structure of scenes \ud83e\udd38 OpenPose (Human Pose) Human skeleton point control, precise control of character actions \ud83d\udcd0 MLSD (Geometric Lines) Geometric line detection, suitable for architectural and structural control \ud83d\udca1 Additional Notes and Extensions ### \ud83d\udd27 Preprocessor Extensions \ud83d\udce6 ComfyUI-comfyui_controlnet_aux Since ComfyUI's built-in nodes only include Canny preprocessor, you can use ComfyUI-comfyui_controlnet_aux to implement other types of image preprocessing. ### \ud83c\udfa8 Supported Preprocessing Types Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation \ud83c\udfaf Application Scenarios \ud83c\udfac Animation Production Precise control of character actions and scene changes \ud83c\udfa8 Artistic Creation Convert static artworks into dynamic expressions \ud83c\udfc3 Motion Analysis Sports action analysis and training video production \ud83c\udfd7\ufe0f Architectural Visualization Dynamic display and walkthrough of architectural designs \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Choose appropriate control type to match content requirements Ensure control video quality is clear and stable Select appropriate resolution based on VRAM capacity Keep prompts consistent with control content \u26a0\ufe0f Important Notes Ensure High/Low Noise models match with LoRA Disable corresponding nodes when preprocessing videos Control video frame rate should match target video Complex controls may require more compute resources \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac Supported Control Precision \ud83c\udfaf Precise Control OpenPose : Joint-level precision Depth : Pixel-level depth control Canny : Edge contour control \u26a1 Performance Optimization Lightning LoRA : 4x speed improvement FP8 Quantization : VRAM optimization Batch Processing : Multi-video parallel ### \ud83c\udfa8 Advanced Control Features \ud83c\udfaf Multi-Condition Control Combine multiple control types for complex scene generation \ud83d\udd04 Temporal Consistency Maintain consistent control across video frames \u2699\ufe0f Adaptive Processing Automatically adjust control strength based on content \ud83c\udfae Wan2.2-Fun-Control Video Control Generation | Precise Video Creation with Multi-Modal Control Conditions \u00a9 2025 Alibaba PAI Team | Apache 2.0 Open Source License | Every Frame Under Precise Control","title":"Index en"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#model-overview","text":"**Wan2.2-Fun-Control** is a next-generation video generation and control model developed by the Alibaba PAI team. By introducing innovative Control Codes mechanisms combined with deep learning and multi-modal conditional inputs, it can generate high-quality videos that conform to preset control conditions. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf Multi-Modal Control Supports Canny, Depth, OpenPose, MLSD and other control conditions \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83c\udf10 Multi-Language Support Supports Chinese, English and other multi-language prompt inputs \ud83c\udfae Trajectory Control Supports precise motion trajectory control functionality \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 Code Repository : VideoX-Fun","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#wan22-fun-control-workflow-example","text":"","title":"\ud83d\ude80 Wan2.2 Fun Control Workflow Example"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status","title":"\u26a0\ufe0f Environment Requirements"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#workflow-version-description","text":"Two workflow versions are provided for selection:","title":"\ud83d\udd27 Workflow Version Description"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#step-1-download-workflow-and-materials","text":"you can find the workflow file in the following figure ![img_1.png](img_1.png) Download the following video or JSON file and drag it into ComfyUI to load the corresponding workflow. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download","title":"\ud83d\udce5 Step 1: Download Workflow and Materials"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#step-2-model-files","text":"","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#model-file-structure","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 Model File Structure"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#step-3-workflow-configuration-operations","text":"\u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent, with high noise and low noise models and LoRAs used correspondingly. #### \ud83d\udccb Detailed Configuration Steps","title":"\ud83d\udd27 Step 3: Workflow Configuration Operations"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#control-types-explained","text":"\u270f\ufe0f","title":"\ud83c\udfaf Control Types Explained"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#additional-notes-and-extensions","text":"### \ud83d\udd27 Preprocessor Extensions \ud83d\udce6 ComfyUI-comfyui_controlnet_aux Since ComfyUI's built-in nodes only include Canny preprocessor, you can use ComfyUI-comfyui_controlnet_aux to implement other types of image preprocessing. ### \ud83c\udfa8 Supported Preprocessing Types Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation","title":"\ud83d\udca1 Additional Notes and Extensions"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"Wan2.2/fun-control-14B/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac Supported Control Precision","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.2/fun-control-5B/doc/","text":"\ud83c\udfae Wan2.2-Fun-Control \u89c6\u9891\u63a7\u5236\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u591a\u6a21\u6001\u63a7\u5236\u6761\u4ef6\u7684\u7cbe\u51c6\u89c6\u9891\u751f\u6210 \ud83c\udfaf \u591a\u6a21\u6001\u63a7\u5236 \ud83c\udfac \u5f71\u89c6\u7ea7\u8d28\u91cf \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \ud83d\udccb \u6a21\u578b\u6982\u89c8 **Wan2.2-Fun-Control** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u65b0\u4e00\u4ee3\u89c6\u9891\u751f\u6210\u4e0e\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u6027\u7684\u63a7\u5236\u4ee3\u7801\uff08Control Codes\uff09\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u9884\u8bbe\u63a7\u5236\u6761\u4ef6\u7684\u89c6\u9891\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u591a\u6a21\u6001\u63a7\u5236 \u652f\u6301 Canny\u3001Depth\u3001OpenPose\u3001MLSD \u7b49\u591a\u79cd\u63a7\u5236\u6761\u4ef6 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u82f1\u6587\u7b49\u591a\u8bed\u8a00\u63d0\u793a\u8bcd\u8f93\u5165 \ud83c\udfae \u8f68\u8ff9\u63a7\u5236 \u652f\u6301\u7cbe\u786e\u7684\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\u529f\u80fd \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun \ud83d\ude80 Wan2.2 Fun Control \u5de5\u4f5c\u6d41\u793a\u4f8b \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u53ca\u7d20\u6750\u4e0b\u8f7d \u76f4\u63a5\u901a\u8fc7Comfyui\u6a21\u7248\u4ed3\u5e93\u6253\u5f00 ![img.png](img.png) \u4e0b\u8f7d\u4ee5\u4e0b\u89c6\u9891\u6216 \u6216JSON \u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u5de5\u4f5c\u6d41\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d \ud83d\uddbc\ufe0f \u8f93\u5165\u8d77\u59cb\u56fe\u7247 \ud83c\udfac \u63a7\u5236\u89c6\u9891 \u6b64\u89c6\u9891\u5df2\u7ecf\u8fc7\u9884\u5904\u7406\uff0c\u53ef\u76f4\u63a5\u7528\u4e8e\u63a7\u5236\u89c6\u9891\u751f\u6210 \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c \u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0chigh noise \u548c low noise \u7684\u6a21\u578b\u548c LoRA \u9700\u8981\u5bf9\u5e94\u4f7f\u7528\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 High Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf \u57fa\u7840\u6a21\u578b\u914d\u7f6e \u8282\u70b9\u540d\u79f0 \u6a21\u578b\u6587\u4ef6 \u8bf4\u660e Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors \u6587\u672c\u7f16\u7801\u5668 Load VAE wan_2.1_vae.safetensors \u53d8\u5206\u81ea\u7f16\u7801\u5668 #### \ud83d\udcc1 \u8f93\u5165\u914d\u7f6e\u6b65\u9aa4 \ud83d\uddbc\ufe0f \u8d77\u59cb\u5e27\u4e0a\u4f20 \u5728 Load Image \u8282\u70b9\u4e0a\u4f20\u8d77\u59cb\u5e27\u56fe\u7247 \ud83c\udfac \u63a7\u5236\u89c6\u9891\u4e0a\u4f20 \u5728\u7b2c\u4e8c\u4e2a Load Video \u8282\u70b9\u4e0a\u4f20\u63a7\u5236\u89c6\u9891\u7684 pose \u89c6\u9891 \ud83d\udd27 \u9884\u5904\u7406\u8282\u70b9\u7981\u7528 \u7531\u4e8e\u63d0\u4f9b\u7684\u89c6\u9891\u5df2\u9884\u5904\u7406\uff0c\u9700\u8981\u7981\u7528\u5bf9\u5e94\u7684\u9884\u5904\u7406\u8282\u70b9\uff08\u9009\u4e2d\u540e\u4f7f\u7528 Ctrl+B \uff09 \ud83d\udcdd \u63d0\u793a\u8bcd\u4fee\u6539 \u4fee\u6539 Prompt\uff0c\u652f\u6301\u4e2d\u82f1\u6587\u8f93\u5165 #### \u2699\ufe0f \u53c2\u6570\u8c03\u6574 \ud83c\udfac Wan22FunControlToVideo \u8282\u70b9\u914d\u7f6e \u2022 \u4fee\u6539\u5bf9\u5e94\u89c6\u9891\u7684\u5c3a\u5bf8\u53c2\u6570 \u2022 \u9ed8\u8ba4\u8bbe\u7f6e\u4e86 640\u00d7640 \u7684\u5206\u8fa8\u7387\uff0c\u907f\u514d\u4f4e\u663e\u5b58\u7528\u6237\u4f7f\u7528\u65f6\u8fc7\u4e8e\u8017\u65f6 \u2022 \u53ef\u6839\u636e\u9700\u8981\u8c03\u6574\u4e3a\u66f4\u9ad8\u5206\u8fa8\u7387 #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Run \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u6267\u884c\u89c6\u9891\u751f\u6210 API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf \u63a7\u5236\u7c7b\u578b\u8be6\u89e3 \u270f\ufe0f Canny\uff08\u7ebf\u7a3f\uff09 \u8fb9\u7f18\u68c0\u6d4b\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u8f6e\u5ed3\u548c\u7ebf\u6761\u63a7\u5236 \ud83c\udfd4\ufe0f Depth\uff08\u6df1\u5ea6\uff09 \u6df1\u5ea6\u4fe1\u606f\u63a7\u5236\uff0c\u4fdd\u6301\u573a\u666f\u7684\u7a7a\u95f4\u7ed3\u6784 \ud83e\udd38 OpenPose\uff08\u4eba\u4f53\u59ff\u52bf\uff09 \u4eba\u4f53\u9aa8\u9abc\u70b9\u63a7\u5236\uff0c\u7cbe\u786e\u63a7\u5236\u4eba\u7269\u52a8\u4f5c \ud83d\udcd0 MLSD\uff08\u51e0\u4f55\u8fb9\u7f18\uff09 \u51e0\u4f55\u7ebf\u6761\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u5efa\u7b51\u548c\u7ed3\u6784\u63a7\u5236 \ud83d\udca1 \u8865\u5145\u8bf4\u660e\u4e0e\u6269\u5c55 ### \ud83d\udd27 \u9884\u5904\u7406\u5668\u6269\u5c55 \ud83d\udce6 ComfyUI-comfyui_controlnet_aux \u7531\u4e8e ComfyUI \u81ea\u5e26\u7684\u8282\u70b9\u4e2d\uff0c\u9884\u5904\u7406\u5668\u8282\u70b9\u53ea\u6709 Canny \u7684\u9884\u5904\u7406\u5668\uff0c\u53ef\u4ee5\u4f7f\u7528 ComfyUI-comfyui_controlnet_aux \u6765\u5b9e\u73b0\u5176\u4ed6\u7c7b\u578b\u7684\u56fe\u50cf\u9884\u5904\u7406\u3002 ### \ud83c\udfa8 \u652f\u6301\u7684\u9884\u5904\u7406\u7c7b\u578b Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation \ud83c\udfaf \u5e94\u7528\u573a\u666f \ud83c\udfac \u52a8\u753b\u5236\u4f5c \u7cbe\u786e\u63a7\u5236\u89d2\u8272\u52a8\u4f5c\u548c\u573a\u666f\u53d8\u5316 \ud83c\udfa8 \u827a\u672f\u521b\u4f5c \u5c06\u9759\u6001\u827a\u672f\u4f5c\u54c1\u8f6c\u6362\u4e3a\u52a8\u6001\u8868\u73b0 \ud83c\udfc3 \u8fd0\u52a8\u5206\u6790 \u4f53\u80b2\u52a8\u4f5c\u5206\u6790\u548c\u8bad\u7ec3\u89c6\u9891\u5236\u4f5c \ud83c\udfd7\ufe0f \u5efa\u7b51\u53ef\u89c6\u5316 \u5efa\u7b51\u8bbe\u8ba1\u7684\u52a8\u6001\u5c55\u793a\u548c\u6f2b\u6e38 \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u9009\u62e9\u5408\u9002\u7684\u63a7\u5236\u7c7b\u578b\u5339\u914d\u5185\u5bb9\u9700\u6c42 \u786e\u4fdd\u63a7\u5236\u89c6\u9891\u8d28\u91cf\u6e05\u6670\u7a33\u5b9a \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u5206\u8fa8\u7387 \u63d0\u793a\u8bcd\u8981\u4e0e\u63a7\u5236\u5185\u5bb9\u4fdd\u6301\u4e00\u81f4 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd High/Low Noise \u6a21\u578b\u4e0e LoRA \u5339\u914d \u9884\u5904\u7406\u89c6\u9891\u65f6\u6ce8\u610f\u7981\u7528\u5bf9\u5e94\u8282\u70b9 \u63a7\u5236\u89c6\u9891\u5e27\u7387\u8981\u4e0e\u76ee\u6807\u89c6\u9891\u5339\u914d \u590d\u6742\u63a7\u5236\u53ef\u80fd\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac \u652f\u6301\u7684\u63a7\u5236\u7cbe\u5ea6 \ud83c\udfaf \u7cbe\u786e\u63a7\u5236 OpenPose : \u5173\u8282\u70b9\u7ea7\u522b\u7cbe\u5ea6 Depth : \u50cf\u7d20\u7ea7\u6df1\u5ea6\u63a7\u5236 Canny : \u8fb9\u7f18\u8f6e\u5ed3\u63a7\u5236 \u26a1 \u6027\u80fd\u4f18\u5316 Lightning LoRA : 4\u500d\u901f\u5ea6\u63d0\u5347 FP8 \u91cf\u5316 : \u663e\u5b58\u4f18\u5316 \u6279\u5904\u7406 : \u591a\u89c6\u9891\u5e76\u884c \ud83c\udfae Wan2.2-Fun-Control \u89c6\u9891\u63a7\u5236\u751f\u6210 | \u591a\u6a21\u6001\u63a7\u5236\u6761\u4ef6\u7684\u7cbe\u51c6\u89c6\u9891\u521b\u4f5c \u00a9 2025 Alibaba PAI \u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u6bcf\u4e00\u5e27\u90fd\u5728\u7cbe\u786e\u63a7\u5236\u4e4b\u4e0b","title":"Index"},{"location":"Wan2.2/fun-control-5B/doc/#_1","text":"**Wan2.2-Fun-Control** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u65b0\u4e00\u4ee3\u89c6\u9891\u751f\u6210\u4e0e\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u6027\u7684\u63a7\u5236\u4ee3\u7801\uff08Control Codes\uff09\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6761\u4ef6\u8f93\u5165\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u9884\u8bbe\u63a7\u5236\u6761\u4ef6\u7684\u89c6\u9891\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u591a\u6a21\u6001\u63a7\u5236 \u652f\u6301 Canny\u3001Depth\u3001OpenPose\u3001MLSD \u7b49\u591a\u79cd\u63a7\u5236\u6761\u4ef6 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u82f1\u6587\u7b49\u591a\u8bed\u8a00\u63d0\u793a\u8bcd\u8f93\u5165 \ud83c\udfae \u8f68\u8ff9\u63a7\u5236 \u652f\u6301\u7cbe\u786e\u7684\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\u529f\u80fd \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/fun-control-5B/doc/#wan22-fun-control","text":"","title":"\ud83d\ude80 Wan2.2 Fun Control \u5de5\u4f5c\u6d41\u793a\u4f8b"},{"location":"Wan2.2/fun-control-5B/doc/#_2","text":"\u76f4\u63a5\u901a\u8fc7Comfyui\u6a21\u7248\u4ed3\u5e93\u6253\u5f00 ![img.png](img.png) \u4e0b\u8f7d\u4ee5\u4e0b\u89c6\u9891\u6216 \u6216JSON \u6587\u4ef6\u5e76\u62d6\u5165 ComfyUI \u4e2d\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u5de5\u4f5c\u6d41\u3002 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u53ca\u7d20\u6750\u4e0b\u8f7d"},{"location":"Wan2.2/fun-control-5B/doc/#_3","text":"","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"Wan2.2/fun-control-5B/doc/#_4","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784"},{"location":"Wan2.2/fun-control-5B/doc/#_5","text":"\u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0chigh noise \u548c low noise \u7684\u6a21\u578b\u548c LoRA \u9700\u8981\u5bf9\u5e94\u4f7f\u7528\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c"},{"location":"Wan2.2/fun-control-5B/doc/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"Wan2.2/fun-control-5B/doc/#_6","text":"\u270f\ufe0f","title":"\ud83c\udfaf \u63a7\u5236\u7c7b\u578b\u8be6\u89e3"},{"location":"Wan2.2/fun-control-5B/doc/#_7","text":"### \ud83d\udd27 \u9884\u5904\u7406\u5668\u6269\u5c55 \ud83d\udce6 ComfyUI-comfyui_controlnet_aux \u7531\u4e8e ComfyUI \u81ea\u5e26\u7684\u8282\u70b9\u4e2d\uff0c\u9884\u5904\u7406\u5668\u8282\u70b9\u53ea\u6709 Canny \u7684\u9884\u5904\u7406\u5668\uff0c\u53ef\u4ee5\u4f7f\u7528 ComfyUI-comfyui_controlnet_aux \u6765\u5b9e\u73b0\u5176\u4ed6\u7c7b\u578b\u7684\u56fe\u50cf\u9884\u5904\u7406\u3002 ### \ud83c\udfa8 \u652f\u6301\u7684\u9884\u5904\u7406\u7c7b\u578b Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation","title":"\ud83d\udca1 \u8865\u5145\u8bf4\u660e\u4e0e\u6269\u5c55"},{"location":"Wan2.2/fun-control-5B/doc/#_8","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f"},{"location":"Wan2.2/fun-control-5B/doc/#_9","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"Wan2.2/fun-control-5B/doc/#_10","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac \u652f\u6301\u7684\u63a7\u5236\u7cbe\u5ea6","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Wan2.2/fun-control-5B/doc/index-en/","text":"\ud83c\udfae Wan2.2-Fun-Control Video Control Generation ComfyUI Native Workflow - Precise Video Generation with Multi-Modal Control Conditions \ud83c\udfaf Multi-Modal Control \ud83c\udfac Cinematic Quality \ud83c\udf10 Multi-Language \ud83d\udccb Model Overview **Wan2.2-Fun-Control** is a next-generation video generation and control model developed by the Alibaba PAI team. By introducing innovative Control Codes mechanisms combined with deep learning and multi-modal conditional inputs, it can generate high-quality videos that conform to preset control conditions. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf Multi-Modal Control Supports Canny, Depth, OpenPose, MLSD and other control conditions \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83c\udf10 Multi-Language Support Supports Chinese, English and other multi-language prompt inputs \ud83c\udfae Trajectory Control Supports precise motion trajectory control functionality \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 Code Repository : VideoX-Fun \ud83c\udfa5 ComfyOrg Wan2.2 Fun Control Live Stream Replay A dedicated live demonstration has been conducted for ComfyUI Wan2.2 usage. You can learn detailed usage methods and control techniques through the following replay. \ud83d\ude80 Wan2.2 Fun Control Workflow Example \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Workflow Templates \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83d\udd27 Workflow Version Description Two workflow versions are provided for selection: \u26a1 Lightning Accelerated Version Uses Wan2.2-Lightning 4-step LoRA \u2705 Faster Speed \u26a0\ufe0f Dynamic Loss \ud83c\udfaf Standard Quality Version Uses fp8_scaled version without acceleration LoRA \u2705 Higher Quality \u23f1\ufe0f Longer Time #### \ud83d\udcca Performance Comparison Test \ud83e\uddea Test Environment : RTX 4090D 24GB VRAM, 640\u00d7640 resolution, 81 frames length Model Type Resolution VRAM Usage First Generation Second Generation fp8_scaled 640\u00d7640 83% \u2248 524s \u2248 520s fp8_scaled + 4-step LoRA 640\u00d7640 89% \u2248 138s \u2248 79s \ud83d\udca1 Version Switching Instructions Since the 4-step LoRA provides better user experience for first-time workflow users, the accelerated version is enabled by default. To switch to the standard version, select the corresponding workflow and use Ctrl+B to enable it. \ud83d\udce5 Step 1: Download Workflow and Materials Download the following video or JSON file and drag it into ComfyUI to load the corresponding workflow. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download \ud83d\uddbc\ufe0f Input Starting Image \ud83c\udfac Control Video This video has been pre-processed and can be used directly for control video generation \ud83d\udd17 Step 2: Model Files \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 Step 3: Workflow Configuration Operations you can find the workflow steps in the following figure ![img.png](img.png) \u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent, with high noise and low noise models and LoRAs used correspondingly. #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 High Noise Model Configuration Load Diffusion Model : wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise Model Configuration Load Diffusion Model : wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf Base Model Configuration Node Name Model File Description Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors Text encoder Load VAE wan_2.1_vae.safetensors Variational autoencoder #### \ud83d\udcc1 Input Configuration Steps \ud83d\uddbc\ufe0f Starting Frame Upload Upload the starting frame image in the Load Image node \ud83c\udfac Control Video Upload Upload the control video's pose video in the second Load Video node \ud83d\udd27 Disable Preprocessing Nodes Since the provided video is pre-processed, disable corresponding preprocessing nodes (select and use Ctrl+B ) \ud83d\udcdd Modify Prompts Modify prompts, supports Chinese and English input #### \u2699\ufe0f Parameter Adjustment \ud83c\udfac Wan22FunControlToVideo Node Configuration \u2022 Modify the corresponding video dimension parameters \u2022 Default set to 640\u00d7640 resolution to avoid excessive time consumption for low-VRAM users \u2022 Can be adjusted to higher resolution as needed #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click the Run button or use shortcut Ctrl(Cmd) + Enter to execute video generation API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Control Types Explained \u270f\ufe0f Canny (Edge Detection) Edge detection control, suitable for contour and line control \ud83c\udfd4\ufe0f Depth (Depth Map) Depth information control, maintains spatial structure of scenes \ud83e\udd38 OpenPose (Human Pose) Human skeleton point control, precise control of character actions \ud83d\udcd0 MLSD (Geometric Lines) Geometric line detection, suitable for architectural and structural control \ud83d\udca1 Additional Notes and Extensions ### \ud83d\udd27 Preprocessor Extensions \ud83d\udce6 ComfyUI-comfyui_controlnet_aux Since ComfyUI's built-in nodes only include Canny preprocessor, you can use ComfyUI-comfyui_controlnet_aux to implement other types of image preprocessing. ### \ud83c\udfa8 Supported Preprocessing Types Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation \ud83c\udfaf Application Scenarios \ud83c\udfac Animation Production Precise control of character actions and scene changes \ud83c\udfa8 Artistic Creation Convert static artworks into dynamic expressions \ud83c\udfc3 Motion Analysis Sports action analysis and training video production \ud83c\udfd7\ufe0f Architectural Visualization Dynamic display and walkthrough of architectural designs \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Choose appropriate control type to match content requirements Ensure control video quality is clear and stable Select appropriate resolution based on VRAM capacity Keep prompts consistent with control content \u26a0\ufe0f Important Notes Ensure High/Low Noise models match with LoRA Disable corresponding nodes when preprocessing videos Control video frame rate should match target video Complex controls may require more compute resources \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac Supported Control Precision \ud83c\udfaf Precise Control OpenPose : Joint-level precision Depth : Pixel-level depth control Canny : Edge contour control \u26a1 Performance Optimization Lightning LoRA : 4x speed improvement FP8 Quantization : VRAM optimization Batch Processing : Multi-video parallel ### \ud83c\udfa8 Advanced Control Features \ud83c\udfaf Multi-Condition Control Combine multiple control types for complex scene generation \ud83d\udd04 Temporal Consistency Maintain consistent control across video frames \u2699\ufe0f Adaptive Processing Automatically adjust control strength based on content \ud83c\udfae Wan2.2-Fun-Control Video Control Generation | Precise Video Creation with Multi-Modal Control Conditions \u00a9 2025 Alibaba PAI Team | Apache 2.0 Open Source License | Every Frame Under Precise Control","title":"Index en"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#model-overview","text":"**Wan2.2-Fun-Control** is a next-generation video generation and control model developed by the Alibaba PAI team. By introducing innovative Control Codes mechanisms combined with deep learning and multi-modal conditional inputs, it can generate high-quality videos that conform to preset control conditions. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf Multi-Modal Control Supports Canny, Depth, OpenPose, MLSD and other control conditions \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83c\udf10 Multi-Language Support Supports Chinese, English and other multi-language prompt inputs \ud83c\udfae Trajectory Control Supports precise motion trajectory control functionality \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-A14B-Control \u2022 Code Repository : VideoX-Fun","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#comfyorg-wan22-fun-control-live-stream-replay","text":"A dedicated live demonstration has been conducted for ComfyUI Wan2.2 usage. You can learn detailed usage methods and control techniques through the following replay.","title":"\ud83c\udfa5 ComfyOrg Wan2.2 Fun Control Live Stream Replay"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#wan22-fun-control-workflow-example","text":"","title":"\ud83d\ude80 Wan2.2 Fun Control Workflow Example"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status","title":"\u26a0\ufe0f Environment Requirements"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#workflow-version-description","text":"Two workflow versions are provided for selection:","title":"\ud83d\udd27 Workflow Version Description"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#step-1-download-workflow-and-materials","text":"Download the following video or JSON file and drag it into ComfyUI to load the corresponding workflow. \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download","title":"\ud83d\udce5 Step 1: Download Workflow and Materials"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#step-2-model-files","text":"","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#model-file-structure","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 Model File Structure"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#step-3-workflow-configuration-operations","text":"you can find the workflow steps in the following figure ![img.png](img.png) \u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent, with high noise and low noise models and LoRAs used correspondingly. #### \ud83d\udccb Detailed Configuration Steps","title":"\ud83d\udd27 Step 3: Workflow Configuration Operations"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 Fun Control Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"On a sunny summer day, there are marshmallow - like clouds, and the sunlight is bright and warm. A girl with white curly double - ponytails is wearing unique sunglasses, distinctive clothes and shoes. Her posture is natural and full of dynamic tension. The background is the scene of the Leaning Tower of Pisa in Italy, emphasizing the realistic contrast of details in reality. The whole picture is in a realistic 3D style, rich in details and with a relaxed atmosphere. She is dancing slowly, waving her hands.\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_REF_IMAGE = \"fun_control.jpg\" DEFAULT_CONTROL_VIDEO = \"control_video.mp4\" class ComfyUIWan22FunControlClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def upload_video(self, video_path): \"\"\"Upload video to ComfyUI server\"\"\" try: with open(video_path, 'rb') as f: files = {'video': f} response = requests.post(f\"{self.base_url}/upload/video\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(video_path)) else: raise Exception(f\"Failed to upload video: {response.text}\") except Exception as e: print(f\"Video upload error: {e}\") return None def generate_fun_control_video(self, positive_prompt, negative_prompt=None, ref_image_path=None, ref_image_name=None, control_video_path=None, control_video_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, canny_low_threshold=0.1, canny_high_threshold=0.6, shift=8.0, lora_strength=1.0): \"\"\"Generate Fun Control video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 Fun Control video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle reference image if ref_image_path and not ref_image_name: ref_image_name = self.upload_image(ref_image_path) if not ref_image_name: raise Exception(\"Failed to upload reference image\") elif not ref_image_name: ref_image_name = DEFAULT_REF_IMAGE # Handle control video if control_video_path and not control_video_name: control_video_name = self.upload_video(control_video_path) if not control_video_name: raise Exception(\"Failed to upload control video\") elif not control_video_name: control_video_name = DEFAULT_CONTROL_VIDEO # Workflow based on your provided JSON workflow = { \"162\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"163\": { \"inputs\": { \"fps\": fps, \"images\": [\"164\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"164\": { \"inputs\": { \"samples\": [\"175\", 0], \"vae\": [\"168\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"165\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"166\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"167\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"168\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"169\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"176\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"180\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise)\"} }, \"170\": { \"inputs\": { \"filename_prefix\": \"wan22_fun_control/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"163\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"171\": { \"inputs\": { \"video\": [\"174\", 0] }, \"class_type\": \"GetVideoComponents\", \"_meta\": {\"title\": \"Get Video Components\"} }, \"172\": { \"inputs\": { \"images\": [\"173\", 0] }, \"class_type\": \"PreviewImage\", \"_meta\": {\"title\": \"Preview Image\"} }, \"173\": { \"inputs\": { \"low_threshold\": canny_low_threshold, \"high_threshold\": canny_high_threshold, \"image\": [\"171\", 0] }, \"class_type\": \"Canny\", \"_meta\": {\"title\": \"Canny Edge Detection\"} }, \"174\": { \"inputs\": { \"file\": control_video_name, \"video-preview\": \"\" }, \"class_type\": \"LoadVideo\", \"_meta\": {\"title\": \"Load Video\"} }, \"175\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 4, \"return_with_leftover_noise\": \"disable\", \"model\": [\"177\", 0], \"positive\": [\"180\", 0], \"negative\": [\"180\", 1], \"latent_image\": [\"169\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise)\"} }, \"176\": { \"inputs\": { \"shift\": shift, \"model\": [\"181\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"177\": { \"inputs\": { \"shift\": shift, \"model\": [\"182\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"178\": { \"inputs\": { \"image\": ref_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Reference Image\"} }, \"179\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"167\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"180\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"179\", 0], \"negative\": [\"162\", 0], \"vae\": [\"168\", 0], \"ref_image\": [\"178\", 0], \"control_video\": [\"173\", 0] }, \"class_type\": \"Wan22FunControlToVideo\", \"_meta\": {\"title\": \"Wan22 Fun Control To Video\"} }, \"181\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"165\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"182\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"166\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 Fun Control video generation workflow...\") print(f\"Positive Prompt: {positive_prompt}\") print(f\"Reference Image: {ref_image_name}\") print(f\"Control Video: {control_video_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Random Seed: {seed}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video and preview files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, prompt_configs, **kwargs): \"\"\"Batch generate videos with different configurations\"\"\" results = [] for i, config in enumerate(prompt_configs): print(f\"\\nStarting Fun Control video generation task {i+1}/{len(prompt_configs)}...\") try: task_id, seed = self.generate_fun_control_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 Fun Control video generation\"\"\" client = ComfyUIWan22FunControlClient() try: print(\"Wan2.2 Fun Control video generation client started...\") # Single video generation example print(\"\\n=== Single Fun Control Video Generation ===\") # You can provide local file paths or use existing file names ref_image_path = None # Set to your image path, e.g., \"reference.jpg\" control_video_path = None # Set to your video path, e.g., \"control.mp4\" task_id, seed = client.generate_fun_control_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, ref_image_path=ref_image_path, ref_image_name=DEFAULT_REF_IMAGE, control_video_path=control_video_path, control_video_name=DEFAULT_CONTROL_VIDEO, width=640, height=640, length=81, fps=16, steps=4, cfg=1 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (video generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Fun Control video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video and preview files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A dancer performing ballet in a beautiful garden with flowers\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 49 }, { 'positive_prompt': \"A person walking through a bustling city street at sunset\", 'ref_image_name': DEFAULT_REF_IMAGE, 'control_video_name': DEFAULT_CONTROL_VIDEO, 'width': 640, 'height': 640, 'length': 65 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, fps=16, steps=4) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#control-types-explained","text":"\u270f\ufe0f","title":"\ud83c\udfaf Control Types Explained"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#additional-notes-and-extensions","text":"### \ud83d\udd27 Preprocessor Extensions \ud83d\udce6 ComfyUI-comfyui_controlnet_aux Since ComfyUI's built-in nodes only include Canny preprocessor, you can use ComfyUI-comfyui_controlnet_aux to implement other types of image preprocessing. ### \ud83c\udfa8 Supported Preprocessing Types Canny Edge Depth Map OpenPose MLSD Lines Normal Map Segmentation","title":"\ud83d\udca1 Additional Notes and Extensions"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"Wan2.2/fun-control-5B/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83c\udfac Supported Control Precision","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.2/fun-inpaint-14B/doc/","text":"\ud83c\udfac Wan2.2-Fun-Inp \u9996\u5c3e\u5e27\u89c6\u9891\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u7cbe\u786e\u63a7\u5236\u89c6\u9891\u9996\u5c3e\u5e27\u7684\u521b\u610f\u751f\u6210 \ud83c\udfaf \u9996\u5c3e\u5e27\u63a7\u5236 \ud83c\udfac \u5f71\u89c6\u7ea7\u8d28\u91cf \ud83d\udcd0 \u591a\u5206\u8fa8\u7387 \ud83d\udccb \u6a21\u578b\u6982\u89c8 **Wan2.2-Fun-Inp** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u9996\u5c3e\u5e27\u63a7\u5236\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u8f93\u5165**\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf**\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891\uff0c\u4e3a\u521b\u4f5c\u8005\u5e26\u6765\u66f4\u5f3a\u7684\u521b\u610f\u63a7\u5236\u529b\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u9996\u5c3e\u5e27\u63a7\u5236 \u652f\u6301\u8f93\u5165\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83d\udcd0 \u591a\u5206\u8fa8\u7387\u652f\u6301 \u652f\u6301 512\u00d7512\u3001768\u00d7768\u30011024\u00d71024 \u7b49\u5206\u8fa8\u7387 \u26a1 14B \u9ad8\u6027\u80fd\u7248 \u6a21\u578b\u4f53\u79ef\u8fbe 32GB+\uff0c\u6548\u679c\u66f4\u4f18\u4f46\u9700\u9ad8\u663e\u5b58\u652f\u6301 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d \u4f60\u53ef\u4ee5\u76f4\u63a5\u4eceComfyui\u7684\u6a21\u7248\u6253\u5f00 ![img_1.png](img_1.png) \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d \ud83d\uddbc\ufe0f \u9996\u5e27\u56fe\u50cf \ud83d\uddbc\ufe0f \u5c3e\u5e27\u56fe\u50cf \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c \u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff08\u9ad8\u566a\u58f0\u5bf9\u9ad8\u566a\u58f0\uff0c\u4f4e\u566a\u58f0\u5bf9\u4f4e\u566a\u58f0\uff09\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 High Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf \u57fa\u7840\u6a21\u578b\u914d\u7f6e \u8282\u70b9\u540d\u79f0 \u6a21\u578b\u6587\u4ef6 \u8bf4\u660e Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors \u6587\u672c\u7f16\u7801\u5668 Load VAE wan_2.1_vae.safetensors \u53d8\u5206\u81ea\u7f16\u7801\u5668 #### \ud83d\udcc1 \u8f93\u5165\u914d\u7f6e \ud83d\uddbc\ufe0f \u9996\u5c3e\u5e27\u56fe\u7247\u4e0a\u4f20 \u5206\u522b\u4e0a\u4f20\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u7247\u7d20\u6750\u5230\u5bf9\u5e94\u7684 Load Image \u8282\u70b9 \ud83d\udcdd \u63d0\u793a\u8bcd\u8f93\u5165 \u5728 Prompt \u7ec4\u4e2d\u8f93\u5165\u63cf\u8ff0\u89c6\u9891\u5185\u5bb9\u7684\u63d0\u793a\u8bcd #### \u2699\ufe0f \u53c2\u6570\u8c03\u6574 \ud83c\udfac WanFunInpaintToVideo \u8282\u70b9\u914d\u7f6e \u2022 width & height \uff1a\u8c03\u6574\u89c6\u9891\u5c3a\u5bf8\uff0c\u9ed8\u8ba4\u4e3a 640\u00d7640 \u2022 length \uff1a\u8bbe\u7f6e\u89c6\u9891\u603b\u5e27\u6570\uff0c\u5f53\u524d\u5de5\u4f5c\u6d41 fps \u4e3a 16 \u2022 \u8ba1\u7b97\u793a\u4f8b \uff1a\u751f\u6210 5 \u79d2\u89c6\u9891\u9700\u8981\u8bbe\u7f6e 5 \u00d7 16 = 80 \u5e27 #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Run \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u6267\u884c\u89c6\u9891\u751f\u6210 API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf \u5e94\u7528\u573a\u666f \ud83c\udfac \u5f71\u89c6\u5236\u4f5c \u7cbe\u786e\u63a7\u5236\u573a\u666f\u8f6c\u6362\u548c\u955c\u5934\u8fc7\u6e21 \ud83c\udfa8 \u521b\u610f\u52a8\u753b \u827a\u672f\u521b\u4f5c\u548c\u6982\u5ff5\u8bbe\u8ba1\u53ef\u89c6\u5316 \ud83d\udcf1 \u793e\u4ea4\u5a92\u4f53 \u77ed\u89c6\u9891\u5185\u5bb9\u548c\u8425\u9500\u7d20\u6750\u5236\u4f5c \ud83c\udfae \u6e38\u620f\u5f00\u53d1 \u6e38\u620f\u52a8\u753b\u548c\u8fc7\u573a\u52a8\u753b\u5236\u4f5c \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u9996\u5c3e\u5e27\u56fe\u50cf\u4fdd\u6301\u98ce\u683c\u4e00\u81f4\u6027 \u9009\u62e9\u5408\u9002\u7684\u5206\u8fa8\u7387\u5e73\u8861\u8d28\u91cf\u4e0e\u6027\u80fd \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u7248\u672c \u63d0\u793a\u8bcd\u8981\u51c6\u786e\u63cf\u8ff0\u8fc7\u6e21\u8fc7\u7a0b \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd High/Low Noise \u6a21\u578b\u4e0e LoRA \u5339\u914d Lightning LoRA \u4f1a\u727a\u7272\u90e8\u5206\u52a8\u6001\u6548\u679c \u957f\u89c6\u9891\u751f\u6210\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 \u9996\u5c3e\u5e27\u5dee\u5f02\u8fc7\u5927\u53ef\u80fd\u5f71\u54cd\u8fc7\u6e21\u6548\u679c \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 \u652f\u6301\u5206\u8fa8\u7387 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 \ud83c\udfac Wan2.2-Fun-Inp \u9996\u5c3e\u5e27\u89c6\u9891\u751f\u6210 | \u7cbe\u786e\u63a7\u5236\u6bcf\u4e00\u5e27\u7684\u521b\u610f\u8868\u8fbe \u00a9 2025 Alibaba PAI \u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u521b\u610f\u5728\u9996\u5c3e\u4e4b\u95f4\u81ea\u7531\u6d41\u6dcc","title":"Index"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_1","text":"**Wan2.2-Fun-Inp** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u9996\u5c3e\u5e27\u63a7\u5236\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u8f93\u5165**\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf**\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891\uff0c\u4e3a\u521b\u4f5c\u8005\u5e26\u6765\u66f4\u5f3a\u7684\u521b\u610f\u63a7\u5236\u529b\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u9996\u5c3e\u5e27\u63a7\u5236 \u652f\u6301\u8f93\u5165\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83d\udcd0 \u591a\u5206\u8fa8\u7387\u652f\u6301 \u652f\u6301 512\u00d7512\u3001768\u00d7768\u30011024\u00d71024 \u7b49\u5206\u8fa8\u7387 \u26a1 14B \u9ad8\u6027\u80fd\u7248 \u6a21\u578b\u4f53\u79ef\u8fbe 32GB+\uff0c\u6548\u679c\u66f4\u4f18\u4f46\u9700\u9ad8\u663e\u5b58\u652f\u6301 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_2","text":"\u4f60\u53ef\u4ee5\u76f4\u63a5\u4eceComfyui\u7684\u6a21\u7248\u6253\u5f00 ![img_1.png](img_1.png) \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_3","text":"","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_4","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_5","text":"\u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff08\u9ad8\u566a\u58f0\u5bf9\u9ad8\u566a\u58f0\uff0c\u4f4e\u566a\u58f0\u5bf9\u4f4e\u566a\u58f0\uff09\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c"},{"location":"Wan2.2/fun-inpaint-14B/doc/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_6","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_7","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"Wan2.2/fun-inpaint-14B/doc/#_8","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 \u652f\u6301\u5206\u8fa8\u7387 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 \ud83c\udfac Wan2.2-Fun-Inp \u9996\u5c3e\u5e27\u89c6\u9891\u751f\u6210 | \u7cbe\u786e\u63a7\u5236\u6bcf\u4e00\u5e27\u7684\u521b\u610f\u8868\u8fbe \u00a9 2025 Alibaba PAI \u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u521b\u610f\u5728\u9996\u5c3e\u4e4b\u95f4\u81ea\u7531\u6d41\u6dcc","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/","text":"\ud83c\udfac Wan2.2-Fun-Inp First & Last Frame Video Generation ComfyUI Native Workflow - Precise Control of Video First and Last Frames \ud83c\udfaf Frame Control \ud83c\udfac Cinematic Quality \ud83d\udcd0 Multi-Resolution \ud83d\udccb Model Overview **Wan2.2-Fun-Inp** is a first and last frame controlled video generation model developed by the Alibaba PAI team. It supports inputting **first and last frame images** to generate intermediate transition videos, providing creators with enhanced creative control. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf First & Last Frame Control Support input of first and last frame images to generate intermediate transition videos \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83d\udcd0 Multi-Resolution Support Supports 512\u00d7512, 768\u00d7768, 1024\u00d71024 and other resolutions \u26a1 14B High-Performance Version Model size reaches 32GB+, better results but requires high VRAM \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 Code Repository : VideoX-Fun \ud83d\udcca Performance Comparison Test \ud83e\uddea Test Environment : RTX 4090D 24GB VRAM, 640\u00d7640 resolution, 81 frames length Model Type Resolution VRAM Usage First Generation Second Generation fp8_scaled 640\u00d7640 83% \u2248 524s \u2248 520s fp8_scaled + 4-step LoRA 640\u00d7640 89% \u2248 138s \u2248 79s \ud83d\udca1 Version Switching Instructions Due to the significant speed improvement of the accelerated LoRA version and its friendliness to low-VRAM users, the accelerated version is enabled by default. To switch to the standard version, select the corresponding workflow and use Ctrl+B to enable it. \ud83d\udce5 Step 1: Download Workflow Files you can use the template from comfyui. ![img.png](img.png) \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download \ud83d\uddbc\ufe0f First Frame Image \ud83d\uddbc\ufe0f Last Frame Image \ud83d\udd17 Step 2: Model Files \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 Step 3: Workflow Configuration Operations \u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent (high-noise with high-noise, low-noise with low-noise). #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 High Noise Model Configuration Load Diffusion Model : wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise Model Configuration Load Diffusion Model : wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf Base Model Configuration Node Name Model File Description Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors Text encoder Load VAE wan_2.1_vae.safetensors Variational autoencoder #### \ud83d\udcc1 Input Configuration \ud83d\uddbc\ufe0f First & Last Frame Image Upload Upload first and last frame image materials to corresponding Load Image nodes \ud83d\udcdd Prompt Input Enter prompts describing video content in the Prompt group #### \u2699\ufe0f Parameter Adjustment \ud83c\udfac WanFunInpaintToVideo Node Configuration \u2022 width & height : Adjust video dimensions, default is 640\u00d7640 \u2022 length : Set total video frames, current workflow fps is 16 \u2022 Calculation Example : To generate a 5-second video, set 5 \u00d7 16 = 80 frames #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click the Run button or use shortcut Ctrl(Cmd) + Enter to execute video generation API \ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Application Scenarios \ud83c\udfac Film Production Precise control of scene transitions and shot transitions \ud83c\udfa8 Creative Animation Artistic creation and concept design visualization \ud83d\udcf1 Social Media Short video content and marketing material production \ud83c\udfae Game Development Game animation and cutscene production \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Maintain style consistency between first and last frame images Choose appropriate resolution to balance quality and performance Select suitable model version based on VRAM capacity Prompts should accurately describe the transition process \u26a0\ufe0f Important Notes Ensure High/Low Noise models match with LoRA Lightning LoRA sacrifices some dynamic effects Long video generation requires more compute resources Excessive differences between first and last frames may affect transition \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 Supported Resolutions 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 ### \ud83d\ude80 Performance Optimization Tips \u26a1 Speed Optimization Use Lightning LoRA : 4 steps vs 20 steps Lower Resolution : Start with 640\u00d7640 for testing Batch Processing : Process multiple videos efficiently GPU Optimization : Use latest CUDA drivers \ud83c\udfaf Quality Optimization Standard Workflow : 20 steps for best results Higher Resolution : Use 1024\u00d71024 for final output Consistent Frames : Ensure style consistency Clear Prompts : Detailed transition descriptions \ud83c\udfac Advanced Features ### \ud83c\udfaf Frame Interpolation Control \ud83c\udfac Precise Transition Control The model excels at creating smooth transitions between first and last frames, maintaining visual coherence while allowing for creative transformations. This makes it ideal for storytelling and narrative video creation. ### \ud83c\udfa8 Creative Applications \ud83c\udf1f Morphing Effects Create seamless object or character transformations \ud83c\udf05 Time-lapse Simulation Generate time progression effects and environmental changes \ud83c\udfad Emotion Transitions Capture subtle facial expression and mood changes \ud83c\udfac Wan2.2-Fun-Inp First & Last Frame Video Generation | Precise Control of Every Frame's Creative Expression \u00a9 2025 Alibaba PAI Team | Apache 2.0 Open Source License | Let Creativity Flow Freely Between First and Last Frames","title":"Index en"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#model-overview","text":"**Wan2.2-Fun-Inp** is a first and last frame controlled video generation model developed by the Alibaba PAI team. It supports inputting **first and last frame images** to generate intermediate transition videos, providing creators with enhanced creative control. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf First & Last Frame Control Support input of first and last frame images to generate intermediate transition videos \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83d\udcd0 Multi-Resolution Support Supports 512\u00d7512, 768\u00d7768, 1024\u00d71024 and other resolutions \u26a1 14B High-Performance Version Model size reaches 32GB+, better results but requires high VRAM \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 Code Repository : VideoX-Fun","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#performance-comparison-test","text":"\ud83e\uddea Test Environment : RTX 4090D 24GB VRAM, 640\u00d7640 resolution, 81 frames length Model Type Resolution VRAM Usage First Generation Second Generation fp8_scaled 640\u00d7640 83% \u2248 524s \u2248 520s fp8_scaled + 4-step LoRA 640\u00d7640 89% \u2248 138s \u2248 79s \ud83d\udca1 Version Switching Instructions Due to the significant speed improvement of the accelerated LoRA version and its friendliness to low-VRAM users, the accelerated version is enabled by default. To switch to the standard version, select the corresponding workflow and use Ctrl+B to enable it.","title":"\ud83d\udcca Performance Comparison Test"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#step-1-download-workflow-files","text":"you can use the template from comfyui. ![img.png](img.png) \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download","title":"\ud83d\udce5 Step 1: Download Workflow Files"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#step-2-model-files","text":"","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#model-file-structure","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 Model File Structure"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#step-3-workflow-configuration-operations","text":"\u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent (high-noise with high-noise, low-noise with low-noise). #### \ud83d\udccb Detailed Configuration Steps","title":"\ud83d\udd27 Step 3: Workflow Configuration Operations"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#api","text":"\ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 Supported Resolutions 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 ### \ud83d\ude80 Performance Optimization Tips","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.2/fun-inpaint-14B/doc/index-en/#advanced-features","text":"### \ud83c\udfaf Frame Interpolation Control \ud83c\udfac Precise Transition Control The model excels at creating smooth transitions between first and last frames, maintaining visual coherence while allowing for creative transformations. This makes it ideal for storytelling and narrative video creation. ### \ud83c\udfa8 Creative Applications \ud83c\udf1f Morphing Effects Create seamless object or character transformations \ud83c\udf05 Time-lapse Simulation Generate time progression effects and environmental changes \ud83c\udfad Emotion Transitions Capture subtle facial expression and mood changes \ud83c\udfac Wan2.2-Fun-Inp First & Last Frame Video Generation | Precise Control of Every Frame's Creative Expression \u00a9 2025 Alibaba PAI Team | Apache 2.0 Open Source License | Let Creativity Flow Freely Between First and Last Frames","title":"\ud83c\udfac Advanced Features"},{"location":"Wan2.2/fun-inpaint-5B/doc/","text":"\ud83c\udfac Wan2.2-Fun-Inp \u9996\u5c3e\u5e27\u89c6\u9891\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u7cbe\u786e\u63a7\u5236\u89c6\u9891\u9996\u5c3e\u5e27\u7684\u521b\u610f\u751f\u6210 \ud83c\udfaf \u9996\u5c3e\u5e27\u63a7\u5236 \ud83c\udfac \u5f71\u89c6\u7ea7\u8d28\u91cf \ud83d\udcd0 \u591a\u5206\u8fa8\u7387 \ud83d\udccb \u6a21\u578b\u6982\u89c8 **Wan2.2-Fun-Inp** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u9996\u5c3e\u5e27\u63a7\u5236\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u8f93\u5165**\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf**\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891\uff0c\u4e3a\u521b\u4f5c\u8005\u5e26\u6765\u66f4\u5f3a\u7684\u521b\u610f\u63a7\u5236\u529b\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u9996\u5c3e\u5e27\u63a7\u5236 \u652f\u6301\u8f93\u5165\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83d\udcd0 \u591a\u5206\u8fa8\u7387\u652f\u6301 \u652f\u6301 512\u00d7512\u3001768\u00d7768\u30011024\u00d71024 \u7b49\u5206\u8fa8\u7387 \u26a1 14B \u9ad8\u6027\u80fd\u7248 \u6a21\u578b\u4f53\u79ef\u8fbe 32GB+\uff0c\u6548\u679c\u66f4\u4f18\u4f46\u9700\u9ad8\u663e\u5b58\u652f\u6301 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun \ud83d\ude80 Wan2.2 Fun Inp \u5de5\u4f5c\u6d41\u793a\u4f8b \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d \u6216\u8005\u4f60\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7Comfyui\u81ea\u5e26\u7684\u6a21\u7248\u4ed3\u5e93\u6253\u5f00\uff1a ![img_1.png](img_1.png) \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d \ud83d\uddbc\ufe0f \u9996\u5e27\u56fe\u50cf \ud83d\uddbc\ufe0f \u5c3e\u5e27\u56fe\u50cf \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c \u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff08\u9ad8\u566a\u58f0\u5bf9\u9ad8\u566a\u58f0\uff0c\u4f4e\u566a\u58f0\u5bf9\u4f4e\u566a\u58f0\uff09\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 High Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise \u6a21\u578b\u914d\u7f6e Load Diffusion Model \uff1a wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly \uff1a wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf \u57fa\u7840\u6a21\u578b\u914d\u7f6e \u8282\u70b9\u540d\u79f0 \u6a21\u578b\u6587\u4ef6 \u8bf4\u660e Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors \u6587\u672c\u7f16\u7801\u5668 Load VAE wan_2.1_vae.safetensors \u53d8\u5206\u81ea\u7f16\u7801\u5668 #### \ud83d\udcc1 \u8f93\u5165\u914d\u7f6e \ud83d\uddbc\ufe0f \u9996\u5c3e\u5e27\u56fe\u7247\u4e0a\u4f20 \u5206\u522b\u4e0a\u4f20\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u7247\u7d20\u6750\u5230\u5bf9\u5e94\u7684 Load Image \u8282\u70b9 \ud83d\udcdd \u63d0\u793a\u8bcd\u8f93\u5165 \u5728 Prompt \u7ec4\u4e2d\u8f93\u5165\u63cf\u8ff0\u89c6\u9891\u5185\u5bb9\u7684\u63d0\u793a\u8bcd #### \u2699\ufe0f \u53c2\u6570\u8c03\u6574 \ud83c\udfac WanFunInpaintToVideo \u8282\u70b9\u914d\u7f6e \u2022 width & height \uff1a\u8c03\u6574\u89c6\u9891\u5c3a\u5bf8\uff0c\u9ed8\u8ba4\u4e3a 640\u00d7640 \u2022 length \uff1a\u8bbe\u7f6e\u89c6\u9891\u603b\u5e27\u6570\uff0c\u5f53\u524d\u5de5\u4f5c\u6d41 fps \u4e3a 16 \u2022 \u8ba1\u7b97\u793a\u4f8b \uff1a\u751f\u6210 5 \u79d2\u89c6\u9891\u9700\u8981\u8bbe\u7f6e 5 \u00d7 16 = 80 \u5e27 #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Run \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u6267\u884c\u89c6\u9891\u751f\u6210 API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf \u5e94\u7528\u573a\u666f \ud83c\udfac \u5f71\u89c6\u5236\u4f5c \u7cbe\u786e\u63a7\u5236\u573a\u666f\u8f6c\u6362\u548c\u955c\u5934\u8fc7\u6e21 \ud83c\udfa8 \u521b\u610f\u52a8\u753b \u827a\u672f\u521b\u4f5c\u548c\u6982\u5ff5\u8bbe\u8ba1\u53ef\u89c6\u5316 \ud83d\udcf1 \u793e\u4ea4\u5a92\u4f53 \u77ed\u89c6\u9891\u5185\u5bb9\u548c\u8425\u9500\u7d20\u6750\u5236\u4f5c \ud83c\udfae \u6e38\u620f\u5f00\u53d1 \u6e38\u620f\u52a8\u753b\u548c\u8fc7\u573a\u52a8\u753b\u5236\u4f5c \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u9996\u5c3e\u5e27\u56fe\u50cf\u4fdd\u6301\u98ce\u683c\u4e00\u81f4\u6027 \u9009\u62e9\u5408\u9002\u7684\u5206\u8fa8\u7387\u5e73\u8861\u8d28\u91cf\u4e0e\u6027\u80fd \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u7248\u672c \u63d0\u793a\u8bcd\u8981\u51c6\u786e\u63cf\u8ff0\u8fc7\u6e21\u8fc7\u7a0b \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd High/Low Noise \u6a21\u578b\u4e0e LoRA \u5339\u914d Lightning LoRA \u4f1a\u727a\u7272\u90e8\u5206\u52a8\u6001\u6548\u679c \u957f\u89c6\u9891\u751f\u6210\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 \u9996\u5c3e\u5e27\u5dee\u5f02\u8fc7\u5927\u53ef\u80fd\u5f71\u54cd\u8fc7\u6e21\u6548\u679c \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 \u652f\u6301\u5206\u8fa8\u7387 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 \ud83c\udfac Wan2.2-Fun-Inp \u9996\u5c3e\u5e27\u89c6\u9891\u751f\u6210 | \u7cbe\u786e\u63a7\u5236\u6bcf\u4e00\u5e27\u7684\u521b\u610f\u8868\u8fbe \u00a9 2025 Alibaba PAI \u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u521b\u610f\u5728\u9996\u5c3e\u4e4b\u95f4\u81ea\u7531\u6d41\u6dcc","title":"Index"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_1","text":"**Wan2.2-Fun-Inp** \u662f Alibaba PAI \u56e2\u961f\u63a8\u51fa\u7684\u9996\u5c3e\u5e27\u63a7\u5236\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u8f93\u5165**\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf**\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891\uff0c\u4e3a\u521b\u4f5c\u8005\u5e26\u6765\u66f4\u5f3a\u7684\u521b\u610f\u63a7\u5236\u529b\u3002\u8be5\u6a21\u578b\u91c7\u7528 **Apache 2.0 \u8bb8\u53ef\u534f\u8bae**\u53d1\u5e03\uff0c\u652f\u6301\u5546\u4e1a\u4f7f\u7528\u3002 \ud83c\udfaf \u9996\u5c3e\u5e27\u63a7\u5236 \u652f\u6301\u8f93\u5165\u9996\u5e27\u548c\u5c3e\u5e27\u56fe\u50cf\uff0c\u751f\u6210\u4e2d\u95f4\u8fc7\u6e21\u89c6\u9891 \ud83c\udfac \u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210 \u57fa\u4e8e Wan2.2 \u67b6\u6784\uff0c\u8f93\u51fa\u5f71\u89c6\u7ea7\u8d28\u91cf\u89c6\u9891 \ud83d\udcd0 \u591a\u5206\u8fa8\u7387\u652f\u6301 \u652f\u6301 512\u00d7512\u3001768\u00d7768\u30011024\u00d71024 \u7b49\u5206\u8fa8\u7387 \u26a1 14B \u9ad8\u6027\u80fd\u7248 \u6a21\u578b\u4f53\u79ef\u8fbe 32GB+\uff0c\u6548\u679c\u66f4\u4f18\u4f46\u9700\u9ad8\u663e\u5b58\u652f\u6301 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 \u4ee3\u7801\u4ed3\u5e93 \uff1a VideoX-Fun","title":"\ud83d\udccb \u6a21\u578b\u6982\u89c8"},{"location":"Wan2.2/fun-inpaint-5B/doc/#wan22-fun-inp","text":"","title":"\ud83d\ude80 Wan2.2 Fun Inp \u5de5\u4f5c\u6d41\u793a\u4f8b"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_2","text":"\u6216\u8005\u4f60\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7Comfyui\u81ea\u5e26\u7684\u6a21\u7248\u4ed3\u5e93\u6253\u5f00\uff1a ![img_1.png](img_1.png) \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u7d20\u6750\u4e0b\u8f7d","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_3","text":"","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_4","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_5","text":"\u26a0\ufe0f \u91cd\u8981\u63d0\u9192 \u6b64\u5de5\u4f5c\u6d41\u4f7f\u7528\u4e86 LoRA \u52a0\u901f\u7248\u672c\uff0c\u8bf7\u786e\u4fdd\u5bf9\u5e94\u7684 Diffusion Model \u548c LoRA \u6587\u4ef6\u4fdd\u6301\u4e00\u81f4\uff08\u9ad8\u566a\u58f0\u5bf9\u9ad8\u566a\u58f0\uff0c\u4f4e\u566a\u58f0\u5bf9\u4f4e\u566a\u58f0\uff09\u3002 #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c"},{"location":"Wan2.2/fun-inpaint-5B/doc/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_6","text":"\ud83c\udfac","title":"\ud83c\udfaf \u5e94\u7528\u573a\u666f"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_7","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"Wan2.2/fun-inpaint-5B/doc/#_8","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 20GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 \u652f\u6301\u5206\u8fa8\u7387 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 \ud83c\udfac Wan2.2-Fun-Inp \u9996\u5c3e\u5e27\u89c6\u9891\u751f\u6210 | \u7cbe\u786e\u63a7\u5236\u6bcf\u4e00\u5e27\u7684\u521b\u610f\u8868\u8fbe \u00a9 2025 Alibaba PAI \u56e2\u961f | Apache 2.0 \u5f00\u6e90\u534f\u8bae | \u8ba9\u521b\u610f\u5728\u9996\u5c3e\u4e4b\u95f4\u81ea\u7531\u6d41\u6dcc","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/","text":"\ud83c\udfac Wan2.2-Fun-Inp First & Last Frame Video Generation ComfyUI Native Workflow - Precise Control of Video First and Last Frames \ud83c\udfaf Frame Control \ud83c\udfac Cinematic Quality \ud83d\udcd0 Multi-Resolution \ud83d\udccb Model Overview **Wan2.2-Fun-Inp** is a first and last frame controlled video generation model developed by the Alibaba PAI team. It supports inputting **first and last frame images** to generate intermediate transition videos, providing creators with enhanced creative control. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf First & Last Frame Control Support input of first and last frame images to generate intermediate transition videos \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83d\udcd0 Multi-Resolution Support Supports 512\u00d7512, 768\u00d7768, 1024\u00d71024 and other resolutions \u26a1 14B High-Performance Version Model size reaches 32GB+, better results but requires high VRAM \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 Code Repository : VideoX-Fun \ud83c\udfa5 ComfyOrg Wan2.2 Fun InP Live Stream Replay A dedicated live demonstration has been conducted for ComfyUI Wan2.2 usage. You can learn detailed usage methods and techniques through the following replay. \ud83d\ude80 Wan2.2 Fun Inp Workflow Example \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Workflow Templates \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83d\udd27 Workflow Version Description Two workflow versions are provided for selection: \u26a1 Lightning Accelerated Version Uses Wan2.2-Lightning 4-step LoRA \u2705 Faster Speed \u26a0\ufe0f Dynamic Loss \ud83c\udfaf Standard Quality Version Uses fp8_scaled version without acceleration LoRA \u2705 Higher Quality \u23f1\ufe0f Longer Time #### \ud83d\udcca Performance Comparison Test \ud83e\uddea Test Environment : RTX 4090D 24GB VRAM, 640\u00d7640 resolution, 81 frames length Model Type Resolution VRAM Usage First Generation Second Generation fp8_scaled 640\u00d7640 83% \u2248 524s \u2248 520s fp8_scaled + 4-step LoRA 640\u00d7640 89% \u2248 138s \u2248 79s \ud83d\udca1 Version Switching Instructions Due to the significant speed improvement of the accelerated LoRA version and its friendliness to low-VRAM users, the accelerated version is enabled by default. To switch to the standard version, select the corresponding workflow and use Ctrl+B to enable it. \ud83d\udce5 Step 1: Download Workflow Files you can find the workflow from comfyui template repository. ![img.png](img.png) \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download \ud83d\uddbc\ufe0f First Frame Image \ud83d\uddbc\ufe0f Last Frame Image \ud83d\udd17 Step 2: Model Files \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors \ud83d\udd27 Step 3: Workflow Configuration Operations \u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent (high-noise with high-noise, low-noise with low-noise). #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 High Noise Model Configuration Load Diffusion Model : wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \ud83d\udd27 Low Noise Model Configuration Load Diffusion Model : wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors LoraLoaderModelOnly : wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors #### \ud83c\udfaf Base Model Configuration Node Name Model File Description Load CLIP umt5_xxl_fp8_e4m3fn_scaled.safetensors Text encoder Load VAE wan_2.1_vae.safetensors Variational autoencoder #### \ud83d\udcc1 Input Configuration \ud83d\uddbc\ufe0f First & Last Frame Image Upload Upload first and last frame image materials to corresponding Load Image nodes \ud83d\udcdd Prompt Input Enter prompts describing video content in the Prompt group #### \u2699\ufe0f Parameter Adjustment \ud83c\udfac WanFunInpaintToVideo Node Configuration \u2022 width & height : Adjust video dimensions, default is 640\u00d7640 \u2022 length : Set total video frames, current workflow fps is 16 \u2022 Calculation Example : To generate a 5-second video, set 5 \u00d7 16 = 80 frames #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click the Run button or use shortcut Ctrl(Cmd) + Enter to execute video generation API \ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf Application Scenarios \ud83c\udfac Film Production Precise control of scene transitions and shot transitions \ud83c\udfa8 Creative Animation Artistic creation and concept design visualization \ud83d\udcf1 Social Media Short video content and marketing material production \ud83c\udfae Game Development Game animation and cutscene production \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Maintain style consistency between first and last frame images Choose appropriate resolution to balance quality and performance Select suitable model version based on VRAM capacity Prompts should accurately describe the transition process \u26a0\ufe0f Important Notes Ensure High/Low Noise models match with LoRA Lightning LoRA sacrifices some dynamic effects Long video generation requires more compute resources Excessive differences between first and last frames may affect transition \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 Supported Resolutions 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 ### \ud83d\ude80 Performance Optimization Tips \u26a1 Speed Optimization Use Lightning LoRA : 4 steps vs 20 steps Lower Resolution : Start with 640\u00d7640 for testing Batch Processing : Process multiple videos efficiently GPU Optimization : Use latest CUDA drivers \ud83c\udfaf Quality Optimization Standard Workflow : 20 steps for best results Higher Resolution : Use 1024\u00d71024 for final output Consistent Frames : Ensure style consistency Clear Prompts : Detailed transition descriptions \ud83c\udfac Advanced Features ### \ud83c\udfaf Frame Interpolation Control \ud83c\udfac Precise Transition Control The model excels at creating smooth transitions between first and last frames, maintaining visual coherence while allowing for creative transformations. This makes it ideal for storytelling and narrative video creation. ### \ud83c\udfa8 Creative Applications \ud83c\udf1f Morphing Effects Create seamless object or character transformations \ud83c\udf05 Time-lapse Simulation Generate time progression effects and environmental changes \ud83c\udfad Emotion Transitions Capture subtle facial expression and mood changes \ud83c\udfac Wan2.2-Fun-Inp First & Last Frame Video Generation | Precise Control of Every Frame's Creative Expression \u00a9 2025 Alibaba PAI Team | Apache 2.0 Open Source License | Let Creativity Flow Freely Between First and Last Frames","title":"Index en"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#model-overview","text":"**Wan2.2-Fun-Inp** is a first and last frame controlled video generation model developed by the Alibaba PAI team. It supports inputting **first and last frame images** to generate intermediate transition videos, providing creators with enhanced creative control. The model is released under the **Apache 2.0 license** and supports commercial use. \ud83c\udfaf First & Last Frame Control Support input of first and last frame images to generate intermediate transition videos \ud83c\udfac High-Quality Video Generation Based on Wan2.2 architecture, outputs cinematic-quality videos \ud83d\udcd0 Multi-Resolution Support Supports 512\u00d7512, 768\u00d7768, 1024\u00d71024 and other resolutions \u26a1 14B High-Performance Version Model size reaches 32GB+, better results but requires high VRAM \ud83d\udd17 Related Resources \u2022 Model Repository : \ud83e\udd17 Wan2.2-Fun-Inp-14B \u2022 Code Repository : VideoX-Fun","title":"\ud83d\udccb Model Overview"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#comfyorg-wan22-fun-inp-live-stream-replay","text":"A dedicated live demonstration has been conducted for ComfyUI Wan2.2 usage. You can learn detailed usage methods and techniques through the following replay.","title":"\ud83c\udfa5 ComfyOrg Wan2.2 Fun InP Live Stream Replay"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#wan22-fun-inp-workflow-example","text":"","title":"\ud83d\ude80 Wan2.2 Fun Inp Workflow Example"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status","title":"\u26a0\ufe0f Environment Requirements"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#workflow-version-description","text":"Two workflow versions are provided for selection:","title":"\ud83d\udd27 Workflow Version Description"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#step-1-download-workflow-files","text":"you can find the workflow from comfyui template repository. ![img.png](img.png) \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Material Download","title":"\ud83d\udce5 Step 1: Download Workflow Files"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#step-2-model-files","text":"","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#model-file-structure","text":"ComfyUI/ \u251c\u2500\u2500\u2500\ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 loras/ \u2502 \u2502 \u251c\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors \u2502 \u2502 \u2514\u2500\u2500\u2500 wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors \u2502 \u251c\u2500\u2500\u2500\ud83d\udcc2 text_encoders/ \u2502 \u2502 \u2514\u2500\u2500\u2500 umt5_xxl_fp8_e4m3fn_scaled.safetensors \u2502 \u2514\u2500\u2500\u2500\ud83d\udcc2 vae/ \u2502 \u2514\u2500\u2500 wan_2.1_vae.safetensors","title":"\ud83d\udcc2 Model File Structure"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#step-3-workflow-configuration-operations","text":"\u26a0\ufe0f Important Reminder This workflow uses the LoRA accelerated version. Please ensure that the corresponding Diffusion Model and LoRA files are consistent (high-noise with high-noise, low-noise with low-noise). #### \ud83d\udccb Detailed Configuration Steps","title":"\ud83d\udd27 Step 3: Workflow Configuration Operations"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#api","text":"\ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Wan2.2 First-Last Frame to Video Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration HIGH_NOISE_UNET = \"wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors\" LOW_NOISE_UNET = \"wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors\" CLIP_MODEL = \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\" VAE_MODEL = \"wan_2.1_vae.safetensors\" HIGH_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" LOW_NOISE_LORA = \"wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" # Default Parameters DEFAULT_POSITIVE_PROMPT = \"\"\"A bearded man with red facial hair wearing a yellow straw hat and dark coat in Van Gogh's self-portrait style, slowly and continuously transforms into a space astronaut. The transformation flows like liquid paint - his beard fades away strand by strand, the yellow hat melts and reforms smoothly into a silver space helmet, dark coat gradually lightens and restructures into a white spacesuit. The background swirling brushstrokes slowly organize and clarify into realistic stars and space, with Earth appearing gradually in the distance. Every change happens in seamless waves, maintaining visual continuity throughout the metamorphosis. Consistent soft lighting throughout, medium close-up maintaining same framing, central composition stays fixed, gentle color temperature shift from warm to cool, gradual contrast increase, smooth style transition from painterly to photorealistic. Static camera with subtle slow zoom, emphasizing the flowing transformation process without abrupt changes.\"\"\" DEFAULT_NEGATIVE_PROMPT = \"\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70\" DEFAULT_START_IMAGE = \"ComfyUI_00592_.png\" DEFAULT_END_IMAGE = \"Qwen-Image_00002_.png\" class ComfyUIWan22FirstLastFrameClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_first_last_frame_video(self, positive_prompt, negative_prompt=None, start_image_path=None, start_image_name=None, end_image_path=None, end_image_name=None, width=640, height=640, length=81, fps=16, steps=4, cfg=1, seed=None, shift=5.0, lora_strength=1.0): \"\"\"Generate First-Last Frame video using Wan2.2 based on original JSON workflow\"\"\" print(\"Starting Wan2.2 First-Last Frame to Video generation...\") # Use default negative prompt if not provided if negative_prompt is None: negative_prompt = DEFAULT_NEGATIVE_PROMPT # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle start image if start_image_path and not start_image_name: start_image_name = self.upload_image(start_image_path) if not start_image_name: raise Exception(\"Failed to upload start image\") elif not start_image_name: start_image_name = DEFAULT_START_IMAGE # Handle end image if end_image_path and not end_image_name: end_image_name = self.upload_image(end_image_path) if not end_image_name: raise Exception(\"Failed to upload end image\") elif not end_image_name: end_image_name = DEFAULT_END_IMAGE # Workflow based on your provided JSON workflow = { \"6\": { \"inputs\": { \"text\": positive_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Positive Prompt)\"} }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": {\"title\": \"CLIP Text Encode (Negative Prompt)\"} }, \"8\": { \"inputs\": { \"samples\": [\"58\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": HIGH_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (High Noise)\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"wan\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"54\": { \"inputs\": { \"shift\": shift, \"model\": [\"91\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (High Noise)\"} }, \"55\": { \"inputs\": { \"shift\": shift, \"model\": [\"92\", 0] }, \"class_type\": \"ModelSamplingSD3\", \"_meta\": {\"title\": \"Model Sampling SD3 (Low Noise)\"} }, \"56\": { \"inputs\": { \"unet_name\": LOW_NOISE_UNET, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader (Low Noise)\"} }, \"57\": { \"inputs\": { \"add_noise\": \"enable\", \"noise_seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 0, \"end_at_step\": 2, \"return_with_leftover_noise\": \"enable\", \"model\": [\"54\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"67\", 2] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (High Noise Stage)\"} }, \"58\": { \"inputs\": { \"add_noise\": \"disable\", \"noise_seed\": 0, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"start_at_step\": 2, \"end_at_step\": 10000, \"return_with_leftover_noise\": \"disable\", \"model\": [\"55\", 0], \"positive\": [\"67\", 0], \"negative\": [\"67\", 1], \"latent_image\": [\"57\", 0] }, \"class_type\": \"KSamplerAdvanced\", \"_meta\": {\"title\": \"K Sampler Advanced (Low Noise Stage)\"} }, \"60\": { \"inputs\": { \"fps\": fps, \"images\": [\"8\", 0] }, \"class_type\": \"CreateVideo\", \"_meta\": {\"title\": \"Create Video\"} }, \"61\": { \"inputs\": { \"filename_prefix\": \"wan22_first_last_frame/video\", \"format\": \"auto\", \"codec\": \"auto\", \"video\": [\"60\", 0] }, \"class_type\": \"SaveVideo\", \"_meta\": {\"title\": \"Save Video\"} }, \"62\": { \"inputs\": { \"image\": end_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load End Image\"} }, \"67\": { \"inputs\": { \"width\": width, \"height\": height, \"length\": length, \"batch_size\": 1, \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"39\", 0], \"start_image\": [\"68\", 0], \"end_image\": [\"62\", 0] }, \"class_type\": \"WanFirstLastFrameToVideo\", \"_meta\": {\"title\": \"Wan First Last Frame To Video\"} }, \"68\": { \"inputs\": { \"image\": start_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Start Image\"} }, \"91\": { \"inputs\": { \"lora_name\": HIGH_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"37\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (High Noise)\"} }, \"92\": { \"inputs\": { \"lora_name\": LOW_NOISE_LORA, \"strength_model\": lora_strength, \"model\": [\"56\", 0] }, \"class_type\": \"LoraLoaderModelOnly\", \"_meta\": {\"title\": \"LoRA Loader (Low Noise)\"} } } print(\"Submitting Wan2.2 First-Last Frame to Video generation workflow...\") print(f\"Positive Prompt: {positive_prompt[:100]}...\") print(f\"Start Image: {start_image_name}\") print(f\"End Image: {end_image_name}\") print(f\"Video Dimensions: {width}x{height}\") print(f\"Video Length: {length} frames\") print(f\"FPS: {fps}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Shift: {shift}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_video(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated video files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for video files if 'videos' in output: for video_info in output['videos']: filename = video_info['filename'] # Download video video_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if video_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(video_response.content) downloaded_files.append(output_path) print(f\"Video saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, configs, **kwargs): \"\"\"Batch generate first-last frame videos with different configurations\"\"\" results = [] for i, config in enumerate(configs): print(f\"\\nStarting First-Last Frame video generation task {i+1}/{len(configs)}...\") try: task_id, seed = self.generate_first_last_frame_video(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_video(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(15) # Video generation takes time except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_transformation_sequence(self, start_image_path, end_image_path, transformation_prompt, length_preset=\"medium\"): \"\"\"Generate transformation video with predefined length settings\"\"\" length_presets = { \"short\": {\"length\": 49, \"fps\": 12}, \"medium\": {\"length\": 81, \"fps\": 16}, \"long\": {\"length\": 121, \"fps\": 20}, \"extra_long\": {\"length\": 161, \"fps\": 24} } settings = length_presets.get(length_preset, length_presets[\"medium\"]) return self.generate_first_last_frame_video( positive_prompt=transformation_prompt, start_image_path=start_image_path, end_image_path=end_image_path, **settings ) def generate_morphing_video(self, image_pairs, base_prompt_template): \"\"\"Generate multiple morphing videos from image pairs\"\"\" results = [] for i, (start_img, end_img, description) in enumerate(image_pairs): prompt = base_prompt_template.format(description=description) print(f\"\\nGenerating morphing video {i+1}: {description}\") try: task_id, seed = self.generate_first_last_frame_video( positive_prompt=prompt, start_image_path=start_img, end_image_path=end_img ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_video(task_id) results.append({ 'description': description, 'files': files, 'start_image': start_img, 'end_image': end_img }) break elif status == \"failed\": print(f\"Morphing video {i+1} failed\") break time.sleep(15) except Exception as e: print(f\"Morphing video {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Wan2.2 First-Last Frame to Video generation\"\"\" client = ComfyUIWan22FirstLastFrameClient() try: print(\"Wan2.2 First-Last Frame to Video generation client started...\") # Single transformation video generation example print(\"\\n=== Single Transformation Video Generation ===\") # You can provide local file paths or use existing file names start_image_path = None # Set to your start image path, e.g., \"start.jpg\" end_image_path = None # Set to your end image path, e.g., \"end.jpg\" task_id, seed = client.generate_first_last_frame_video( positive_prompt=DEFAULT_POSITIVE_PROMPT, negative_prompt=DEFAULT_NEGATIVE_PROMPT, start_image_path=start_image_path, start_image_name=DEFAULT_START_IMAGE, end_image_path=end_image_path, end_image_name=DEFAULT_END_IMAGE, width=640, height=640, length=81, fps=16, steps=4, cfg=1, shift=5.0 ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"First-Last Frame video generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(15) # Download video files downloaded_files = client.download_video(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Transformation sequence example print(\"\\n=== Transformation Sequence Example ===\") # Uncomment to test different transformation lengths # if start_image_path and end_image_path: # for preset in [\"short\", \"medium\", \"long\"]: # print(f\"Generating {preset} transformation...\") # task_id, seed = client.generate_transformation_sequence( # start_image_path, end_image_path, # \"Smooth transformation between two states\", preset # ) # # Wait and download logic here... # Morphing video example print(\"\\n=== Morphing Video Example ===\") # image_pairs = [ # (\"portrait1.jpg\", \"portrait2.jpg\", \"person transforms into another person\"), # (\"cat.jpg\", \"dog.jpg\", \"cat slowly morphs into a dog\"), # (\"day.jpg\", \"night.jpg\", \"day scene transitions to night scene\") # ] # # prompt_template = \"Smooth and seamless transformation where {description}. \" \\ # \"The change happens gradually with flowing motion, maintaining \" \\ # \"visual continuity throughout the metamorphosis.\" # # morphing_results = client.generate_morphing_video(image_pairs, prompt_template) # print(f\"Generated {len(morphing_results)} morphing videos\") # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_configs = [ { 'positive_prompt': \"A flower blooming from bud to full bloom, time-lapse style transformation\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 49, 'fps': 12 }, { 'positive_prompt': \"Sunset to sunrise transformation, smooth color transition in the sky\", 'start_image_name': DEFAULT_START_IMAGE, 'end_image_name': DEFAULT_END_IMAGE, 'length': 81, 'fps': 16 } ] # Uncomment to run batch generation # batch_results = client.generate_batch(batch_configs, steps=4, cfg=1) # print(f\"Batch generation completed, generated {len(batch_results)} videos\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#application-scenarios","text":"\ud83c\udfac","title":"\ud83c\udfaf Application Scenarios"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 20GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4090 RTX 4090 / A100 ### \ud83d\udcd0 Supported Resolutions 512\u00d7512 640\u00d7640 768\u00d7768 1024\u00d71024 ### \ud83d\ude80 Performance Optimization Tips","title":"\ud83d\udd27 Technical Specifications"},{"location":"Wan2.2/fun-inpaint-5B/doc/index-en/#advanced-features","text":"### \ud83c\udfaf Frame Interpolation Control \ud83c\udfac Precise Transition Control The model excels at creating smooth transitions between first and last frames, maintaining visual coherence while allowing for creative transformations. This makes it ideal for storytelling and narrative video creation. ### \ud83c\udfa8 Creative Applications \ud83c\udf1f Morphing Effects Create seamless object or character transformations \ud83c\udf05 Time-lapse Simulation Generate time progression effects and environmental changes \ud83c\udfad Emotion Transitions Capture subtle facial expression and mood changes \ud83c\udfac Wan2.2-Fun-Inp First & Last Frame Video Generation | Precise Control of Every Frame's Creative Expression \u00a9 2025 Alibaba PAI Team | Apache 2.0 Open Source License | Let Creativity Flow Freely Between First and Last Frames","title":"\ud83c\udfac Advanced Features"},{"location":"cosmos/index-cosmos-reason1-en/","text":"\ud83c\udf0c Cosmos-Reason1 Multimodal Reasoning Model Fully Customizable Multimodal AI Reasoning Expert - Understanding Motion and Spatiotemporal Relationships \ud83c\udfaf Product Overview Cosmos-Reason1 is a fully customizable multimodal AI reasoning model specifically built for understanding motion, object interactions, and spatiotemporal relationships. Based on Chain-of-thought (CoT) reasoning, the Cosmos-Reason1 model can interpret visual inputs, predict outcomes based on given prompts, and reward optimal decisions. \ud83d\udd2c Core Innovation This model implements reasoning based on real-world physical laws, generating clear and context-aware natural language responses, setting new benchmarks for multimodal AI reasoning. \u2728 Core Features \ud83c\udfac Motion Understanding Expert Specifically built for understanding motion, object interactions, and spatiotemporal relationships, capable of precisely analyzing complex physical phenomena in dynamic scenes. \ud83e\udde0 Chain-of-Thought Reasoning Based on Chain-of-thought (CoT) reasoning, can interpret visual inputs, predict outcomes based on given prompts, and reward optimal decisions. \u2696\ufe0f Physics-Based Reasoning Implements reasoning based on real-world physical laws, generating clear and context-aware natural language responses. \ud83d\udd27 Fully Customizable Fully customizable architecture design supporting deep optimization and personalized adjustments for specific application scenarios. \ud83d\udcca Data Enhancement Capabilities Can enhance synthetic data curation capabilities by acting as a discriminator or annotating massive visual data. \ud83d\ude80 Application Scenarios \ud83e\udd16 Robot Control \u2022 Embodied intelligence navigation \u2022 Object manipulation planning \u2022 Environmental interaction understanding \u2022 Action sequence optimization \ud83c\udfae Game AI \u2022 Physics engine optimization \u2022 Intelligent NPC behavior \u2022 Scene interaction design \u2022 Dynamic event prediction \ud83c\udfac Video Analysis \u2022 Action recognition and classification \u2022 Event detection and localization \u2022 Behavior prediction analysis \u2022 Content understanding annotation \ud83d\udd2c Scientific Research \u2022 Physics experiment analysis \u2022 Phenomenon mechanism explanation \u2022 Data pattern discovery \u2022 Hypothesis validation support \ud83d\udcd6 User Guide \ud83d\udca1 After Deployment After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses (available after enabling public network access), and Api_Key. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name \ud83d\uddbc\ufe0f Image Format Support The image_url parameter supports two formats: \u2022 HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 Encoding : data:image/jpeg;base64,<base64 encoded image format> Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image format>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code that supports image and video processing: import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Answer in Chinese, what number should be in the box in the image?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Please describe the video content\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83c\udf0c Cosmos-Reason1 | Multimodal Reasoning Expert, AI Pioneer in Understanding Motion and Spacetime","title":"Index cosmos reason1 en"},{"location":"cosmos/index-cosmos-reason1-en/#product-overview","text":"Cosmos-Reason1 is a fully customizable multimodal AI reasoning model specifically built for understanding motion, object interactions, and spatiotemporal relationships. Based on Chain-of-thought (CoT) reasoning, the Cosmos-Reason1 model can interpret visual inputs, predict outcomes based on given prompts, and reward optimal decisions. \ud83d\udd2c Core Innovation This model implements reasoning based on real-world physical laws, generating clear and context-aware natural language responses, setting new benchmarks for multimodal AI reasoning.","title":"\ud83c\udfaf Product Overview"},{"location":"cosmos/index-cosmos-reason1-en/#core-features","text":"\ud83c\udfac Motion Understanding Expert Specifically built for understanding motion, object interactions, and spatiotemporal relationships, capable of precisely analyzing complex physical phenomena in dynamic scenes. \ud83e\udde0 Chain-of-Thought Reasoning Based on Chain-of-thought (CoT) reasoning, can interpret visual inputs, predict outcomes based on given prompts, and reward optimal decisions. \u2696\ufe0f Physics-Based Reasoning Implements reasoning based on real-world physical laws, generating clear and context-aware natural language responses. \ud83d\udd27 Fully Customizable Fully customizable architecture design supporting deep optimization and personalized adjustments for specific application scenarios. \ud83d\udcca Data Enhancement Capabilities Can enhance synthetic data curation capabilities by acting as a discriminator or annotating massive visual data.","title":"\u2728 Core Features"},{"location":"cosmos/index-cosmos-reason1-en/#application-scenarios","text":"\ud83e\udd16 Robot Control \u2022 Embodied intelligence navigation \u2022 Object manipulation planning \u2022 Environmental interaction understanding \u2022 Action sequence optimization \ud83c\udfae Game AI \u2022 Physics engine optimization \u2022 Intelligent NPC behavior \u2022 Scene interaction design \u2022 Dynamic event prediction \ud83c\udfac Video Analysis \u2022 Action recognition and classification \u2022 Event detection and localization \u2022 Behavior prediction analysis \u2022 Content understanding annotation \ud83d\udd2c Scientific Research \u2022 Physics experiment analysis \u2022 Phenomenon mechanism explanation \u2022 Data pattern discovery \u2022 Hypothesis validation support","title":"\ud83d\ude80 Application Scenarios"},{"location":"cosmos/index-cosmos-reason1-en/#user-guide","text":"\ud83d\udca1 After Deployment After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses (available after enabling public network access), and Api_Key.","title":"\ud83d\udcd6 User Guide"},{"location":"cosmos/index-cosmos-reason1-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"cosmos/index-cosmos-reason1-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name \ud83d\uddbc\ufe0f Image Format Support The image_url parameter supports two formats: \u2022 HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 Encoding : data:image/jpeg;base64,<base64 encoded image format> Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image format>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"cosmos/index-cosmos-reason1-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code that supports image and video processing: import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Answer in Chinese, what number should be in the box in the image?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Please describe the video content\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video()","title":"\ud83d\udc0d Python SDK Call"},{"location":"cosmos/index-cosmos-reason1-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"cosmos/index-cosmos-reason1/","text":"\ud83c\udf0c Cosmos-Reason1 \u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b \u53ef\u5b8c\u5168\u5b9a\u5236\u7684\u591a\u6a21\u6001 AI \u63a8\u7406\u4e13\u5bb6 - \u7406\u89e3\u8fd0\u52a8\u4e0e\u65f6\u7a7a\u5173\u7cfb \ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb Cosmos-Reason1 \u662f\u4e00\u6b3e\u53ef\u5b8c\u5168\u5b9a\u5236\u7684\u591a\u6a21\u6001 AI \u63a8\u7406\u6a21\u578b\uff0c\u5b83\u4e13\u95e8\u4e3a\u7406\u89e3\u8fd0\u52a8\u3001\u7269\u4f53\u4ea4\u4e92\u4ee5\u53ca\u65f6\u7a7a\u5173\u7cfb\u800c\u6784\u5efa\u3002\u57fa\u4e8e\u601d\u7ef4\u94fe\uff08Chain-of-thought, CoT\uff09\u63a8\u7406\uff0cCosmos-Reason1 \u6a21\u578b\u53ef\u4ee5\u89e3\u8bfb\u89c6\u89c9\u8f93\u5165\u3001\u6839\u636e\u7ed9\u5b9a\u7684\u63d0\u793a\u8bcd\u9884\u6d4b\u7ed3\u679c\u3001\u5e76\u5956\u52b1\u6700\u4f73\u51b3\u7b56\u3002 \ud83d\udd2c \u6838\u5fc3\u521b\u65b0 \u8be5\u6a21\u578b\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u7269\u7406\u89c4\u5f8b\u5b9e\u73b0\u63a8\u7406\uff0c\u4ece\u800c\u751f\u6210\u6e05\u6670\u4e14\u80fd\u591f\u611f\u77e5\u4e0a\u4e0b\u6587\u73af\u5883\u7684\u81ea\u7136\u8bed\u8a00\u56de\u590d\uff0c\u4e3a\u591a\u6a21\u6001 AI \u63a8\u7406\u6811\u7acb\u4e86\u65b0\u6807\u6746\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \ud83c\udfac \u8fd0\u52a8\u7406\u89e3\u4e13\u5bb6 \u4e13\u95e8\u4e3a\u7406\u89e3\u8fd0\u52a8\u3001\u7269\u4f53\u4ea4\u4e92\u4ee5\u53ca\u65f6\u7a7a\u5173\u7cfb\u800c\u6784\u5efa\uff0c\u80fd\u591f\u7cbe\u786e\u5206\u6790\u52a8\u6001\u573a\u666f\u4e2d\u7684\u590d\u6742\u7269\u7406\u73b0\u8c61\u3002 \ud83e\udde0 \u601d\u7ef4\u94fe\u63a8\u7406 \u57fa\u4e8e\u601d\u7ef4\u94fe\uff08Chain-of-thought, CoT\uff09\u63a8\u7406\uff0c\u53ef\u4ee5\u89e3\u8bfb\u89c6\u89c9\u8f93\u5165\u3001\u6839\u636e\u7ed9\u5b9a\u7684\u63d0\u793a\u8bcd\u9884\u6d4b\u7ed3\u679c\u3001\u5e76\u5956\u52b1\u6700\u4f73\u51b3\u7b56\u3002 \u2696\ufe0f \u7269\u7406\u89c4\u5f8b\u63a8\u7406 \u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u7269\u7406\u89c4\u5f8b\u5b9e\u73b0\u63a8\u7406\uff0c\u751f\u6210\u6e05\u6670\u4e14\u80fd\u591f\u611f\u77e5\u4e0a\u4e0b\u6587\u73af\u5883\u7684\u81ea\u7136\u8bed\u8a00\u56de\u590d\u3002 \ud83d\udd27 \u5b8c\u5168\u53ef\u5b9a\u5236 \u53ef\u5b8c\u5168\u5b9a\u5236\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u652f\u6301\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u573a\u666f\u8fdb\u884c\u6df1\u5ea6\u4f18\u5316\u548c\u4e2a\u6027\u5316\u8c03\u6574\u3002 \ud83d\udcca \u6570\u636e\u589e\u5f3a\u80fd\u529b \u80fd\u591f\u901a\u8fc7\u5145\u5f53\u5224\u522b\u5668\u6216\u5bf9\u6d77\u91cf\u89c6\u89c9\u6570\u636e\u8fdb\u884c\u6807\u6ce8\uff0c\u4ece\u800c\u589e\u5f3a\u5408\u6210\u6570\u636e\u7b56\u7ba1\u80fd\u529b\u3002 \ud83d\ude80 \u5e94\u7528\u573a\u666f \ud83e\udd16 \u673a\u5668\u4eba\u63a7\u5236 \u2022 \u5177\u8eab\u667a\u80fd\u5bfc\u822a \u2022 \u7269\u4f53\u64cd\u4f5c\u89c4\u5212 \u2022 \u73af\u5883\u4ea4\u4e92\u7406\u89e3 \u2022 \u52a8\u4f5c\u5e8f\u5217\u4f18\u5316 \ud83c\udfae \u6e38\u620f AI \u2022 \u7269\u7406\u5f15\u64ce\u4f18\u5316 \u2022 \u667a\u80fd NPC \u884c\u4e3a \u2022 \u573a\u666f\u4ea4\u4e92\u8bbe\u8ba1 \u2022 \u52a8\u6001\u4e8b\u4ef6\u9884\u6d4b \ud83c\udfac \u89c6\u9891\u5206\u6790 \u2022 \u52a8\u4f5c\u8bc6\u522b\u5206\u7c7b \u2022 \u4e8b\u4ef6\u68c0\u6d4b\u5b9a\u4f4d \u2022 \u884c\u4e3a\u9884\u6d4b\u5206\u6790 \u2022 \u5185\u5bb9\u7406\u89e3\u6807\u6ce8 \ud83d\udd2c \u79d1\u5b66\u7814\u7a76 \u2022 \u7269\u7406\u5b9e\u9a8c\u5206\u6790 \u2022 \u73b0\u8c61\u673a\u7406\u89e3\u91ca \u2022 \u6570\u636e\u6a21\u5f0f\u53d1\u73b0 \u2022 \u5047\u8bbe\u9a8c\u8bc1\u652f\u6301 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u90e8\u7f72\u5b8c\u6210\u540e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\uff08\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u540e\u4f1a\u6709\uff09\u548c Api_Key\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \ud83d\uddbc\ufe0f \u56fe\u7247\u683c\u5f0f\u652f\u6301 image_url \u53c2\u6570\u652f\u6301\u4e24\u79cd\u683c\u5f0f\uff1a \u2022 HTTP URL \uff1a\u5982 https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f> Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff1a import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f7f\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u56fe\u4e2d\u65b9\u6846\u5904\u5e94\u8be5\u662f\u6570\u5b57\u591a\u5c11?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"\u8bf7\u63cf\u8ff0\u4e0b\u89c6\u9891\u5185\u5bb9\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\uff0c\u5bf9\u4e8e\u591a\u6a21\u6001\u9700\u8981\u8f93\u5165\u56fe\u7247\u7684\u573a\u666f\uff0c\u53ef\u4ee5\u5728\u8f93\u5165\u6846\u53f3\u4e0b\u89d2\u9009\u62e9\u56fe\u7247\u8fdb\u884c\u4e0a\u4f20\u3002 \ud83c\udf0c Cosmos-Reason1 | \u591a\u6a21\u6001\u63a8\u7406\u4e13\u5bb6\uff0c\u7406\u89e3\u8fd0\u52a8\u4e0e\u65f6\u7a7a\u7684 AI \u5148\u950b","title":"Index cosmos reason1"},{"location":"cosmos/index-cosmos-reason1/#_1","text":"Cosmos-Reason1 \u662f\u4e00\u6b3e\u53ef\u5b8c\u5168\u5b9a\u5236\u7684\u591a\u6a21\u6001 AI \u63a8\u7406\u6a21\u578b\uff0c\u5b83\u4e13\u95e8\u4e3a\u7406\u89e3\u8fd0\u52a8\u3001\u7269\u4f53\u4ea4\u4e92\u4ee5\u53ca\u65f6\u7a7a\u5173\u7cfb\u800c\u6784\u5efa\u3002\u57fa\u4e8e\u601d\u7ef4\u94fe\uff08Chain-of-thought, CoT\uff09\u63a8\u7406\uff0cCosmos-Reason1 \u6a21\u578b\u53ef\u4ee5\u89e3\u8bfb\u89c6\u89c9\u8f93\u5165\u3001\u6839\u636e\u7ed9\u5b9a\u7684\u63d0\u793a\u8bcd\u9884\u6d4b\u7ed3\u679c\u3001\u5e76\u5956\u52b1\u6700\u4f73\u51b3\u7b56\u3002 \ud83d\udd2c \u6838\u5fc3\u521b\u65b0 \u8be5\u6a21\u578b\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u7269\u7406\u89c4\u5f8b\u5b9e\u73b0\u63a8\u7406\uff0c\u4ece\u800c\u751f\u6210\u6e05\u6670\u4e14\u80fd\u591f\u611f\u77e5\u4e0a\u4e0b\u6587\u73af\u5883\u7684\u81ea\u7136\u8bed\u8a00\u56de\u590d\uff0c\u4e3a\u591a\u6a21\u6001 AI \u63a8\u7406\u6811\u7acb\u4e86\u65b0\u6807\u6746\u3002","title":"\ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb"},{"location":"cosmos/index-cosmos-reason1/#_2","text":"\ud83c\udfac \u8fd0\u52a8\u7406\u89e3\u4e13\u5bb6 \u4e13\u95e8\u4e3a\u7406\u89e3\u8fd0\u52a8\u3001\u7269\u4f53\u4ea4\u4e92\u4ee5\u53ca\u65f6\u7a7a\u5173\u7cfb\u800c\u6784\u5efa\uff0c\u80fd\u591f\u7cbe\u786e\u5206\u6790\u52a8\u6001\u573a\u666f\u4e2d\u7684\u590d\u6742\u7269\u7406\u73b0\u8c61\u3002 \ud83e\udde0 \u601d\u7ef4\u94fe\u63a8\u7406 \u57fa\u4e8e\u601d\u7ef4\u94fe\uff08Chain-of-thought, CoT\uff09\u63a8\u7406\uff0c\u53ef\u4ee5\u89e3\u8bfb\u89c6\u89c9\u8f93\u5165\u3001\u6839\u636e\u7ed9\u5b9a\u7684\u63d0\u793a\u8bcd\u9884\u6d4b\u7ed3\u679c\u3001\u5e76\u5956\u52b1\u6700\u4f73\u51b3\u7b56\u3002 \u2696\ufe0f \u7269\u7406\u89c4\u5f8b\u63a8\u7406 \u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u7269\u7406\u89c4\u5f8b\u5b9e\u73b0\u63a8\u7406\uff0c\u751f\u6210\u6e05\u6670\u4e14\u80fd\u591f\u611f\u77e5\u4e0a\u4e0b\u6587\u73af\u5883\u7684\u81ea\u7136\u8bed\u8a00\u56de\u590d\u3002 \ud83d\udd27 \u5b8c\u5168\u53ef\u5b9a\u5236 \u53ef\u5b8c\u5168\u5b9a\u5236\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u652f\u6301\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u573a\u666f\u8fdb\u884c\u6df1\u5ea6\u4f18\u5316\u548c\u4e2a\u6027\u5316\u8c03\u6574\u3002 \ud83d\udcca \u6570\u636e\u589e\u5f3a\u80fd\u529b \u80fd\u591f\u901a\u8fc7\u5145\u5f53\u5224\u522b\u5668\u6216\u5bf9\u6d77\u91cf\u89c6\u89c9\u6570\u636e\u8fdb\u884c\u6807\u6ce8\uff0c\u4ece\u800c\u589e\u5f3a\u5408\u6210\u6570\u636e\u7b56\u7ba1\u80fd\u529b\u3002","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"cosmos/index-cosmos-reason1/#_3","text":"\ud83e\udd16 \u673a\u5668\u4eba\u63a7\u5236 \u2022 \u5177\u8eab\u667a\u80fd\u5bfc\u822a \u2022 \u7269\u4f53\u64cd\u4f5c\u89c4\u5212 \u2022 \u73af\u5883\u4ea4\u4e92\u7406\u89e3 \u2022 \u52a8\u4f5c\u5e8f\u5217\u4f18\u5316 \ud83c\udfae \u6e38\u620f AI \u2022 \u7269\u7406\u5f15\u64ce\u4f18\u5316 \u2022 \u667a\u80fd NPC \u884c\u4e3a \u2022 \u573a\u666f\u4ea4\u4e92\u8bbe\u8ba1 \u2022 \u52a8\u6001\u4e8b\u4ef6\u9884\u6d4b \ud83c\udfac \u89c6\u9891\u5206\u6790 \u2022 \u52a8\u4f5c\u8bc6\u522b\u5206\u7c7b \u2022 \u4e8b\u4ef6\u68c0\u6d4b\u5b9a\u4f4d \u2022 \u884c\u4e3a\u9884\u6d4b\u5206\u6790 \u2022 \u5185\u5bb9\u7406\u89e3\u6807\u6ce8 \ud83d\udd2c \u79d1\u5b66\u7814\u7a76 \u2022 \u7269\u7406\u5b9e\u9a8c\u5206\u6790 \u2022 \u73b0\u8c61\u673a\u7406\u89e3\u91ca \u2022 \u6570\u636e\u6a21\u5f0f\u53d1\u73b0 \u2022 \u5047\u8bbe\u9a8c\u8bc1\u652f\u6301","title":"\ud83d\ude80 \u5e94\u7528\u573a\u666f"},{"location":"cosmos/index-cosmos-reason1/#_4","text":"\ud83d\udca1 \u90e8\u7f72\u5b8c\u6210\u540e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\uff08\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u540e\u4f1a\u6709\uff09\u548c Api_Key\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"cosmos/index-cosmos-reason1/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"cosmos/index-cosmos-reason1/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \ud83d\uddbc\ufe0f \u56fe\u7247\u683c\u5f0f\u652f\u6301 image_url \u53c2\u6570\u652f\u6301\u4e24\u79cd\u683c\u5f0f\uff1a \u2022 HTTP URL \uff1a\u5982 https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f> Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"cosmos/index-cosmos-reason1/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff1a import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f7f\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u56fe\u4e2d\u65b9\u6846\u5904\u5e94\u8be5\u662f\u6570\u5b57\u591a\u5c11?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"\u8bf7\u63cf\u8ff0\u4e0b\u89c6\u9891\u5185\u5bb9\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"cosmos/index-cosmos-reason1/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"cosyvoice/","text":"\ud83c\udfa4 CosyVoice \u8bed\u97f3\u5408\u6210\u670d\u52a1 \u963f\u91cc\u4e91\u63a8\u51fa\u7684\u5148\u8fdb\u8bed\u97f3\u5408\u6210\u6280\u672f CosyVoice\u662f\u963f\u91cc\u4e91\u63a8\u51fa\u7684\u4e00\u6b3e\u8bed\u97f3\u5408\u6210\u670d\u52a1\uff0c\u5b83\u80fd\u591f\u5c06\u6587\u672c\u8f6c\u6362\u6210\u81ea\u7136\u6d41\u7545\u7684\u8bed\u97f3\u3002\u8fd9\u9879\u670d\u52a1\u652f\u6301\u591a\u79cd\u8bed\u8a00\u548c\u65b9\u8a00\uff0c\u53ef\u4ee5\u6ee1\u8db3\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9700\u6c42\uff0c\u5982\u65b0\u95fb\u64ad\u62a5\u3001\u6709\u58f0\u8bfb\u7269\u5236\u4f5c\u3001\u667a\u80fd\u5ba2\u670d\u7b49\u3002\u901a\u8fc7\u4f7f\u7528\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0cCosyVoice\u80fd\u591f\u751f\u6210\u63a5\u8fd1\u771f\u4eba\u53d1\u58f0\u6548\u679c\u7684\u58f0\u97f3\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u52a0\u4e30\u5bcc\u548c\u4eba\u6027\u5316\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002 \ud83c\udf0d \u591a\u8bed\u8a00\u652f\u6301 \u2022 \u652f\u6301\u7684\u8bed\u8a00: \u4e2d\u6587\u3001\u82f1\u6587\u3001\u65e5\u6587\u3001\u97e9\u6587\u3001\u4e2d\u6587\u65b9\u8a00\uff08\u7ca4\u8bed\u3001\u56db\u5ddd\u8bdd\u3001\u4e0a\u6d77\u8bdd\u3001\u5929\u6d25\u8bdd\u3001\u6b66\u6c49\u8bdd\u7b49\uff09 \u2022 \u8de8\u8bed\u8a00\u53ca\u6df7\u5408\u8bed\u8a00\uff1a\u652f\u6301\u96f6\u6837\u672c\u7684\u8de8\u8bed\u8a00\u548c\u4ee3\u7801\u8f6c\u6362\u573a\u666f\u7684\u8bed\u97f3\u514b\u9686\u3002 \u26a1 \u8d85\u4f4e\u5ef6\u8fdf \u2022 \u53cc\u5411\u6d41\u652f\u6301: CosyVoice 2.0 \u96c6\u6210\u4e86\u79bb\u7ebf\u548c\u6d41\u5f0f\u5efa\u6a21\u6280\u672f\u3002 \u2022 \u5feb\u901f\u9996\u5305\u5408\u6210: \u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u97f3\u9891\u8f93\u51fa\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f4e\u81f3150\u6beb\u79d2\u7684\u5ef6\u8fdf\u3002 \ud83c\udfaf \u9ad8\u7cbe\u5ea6 \u2022 \u6539\u8fdb\u53d1\u97f3: \u4e0eCosyVoice 1.0\u76f8\u6bd4\uff0c\u51cf\u5c11\u4e8630%\u523050%\u7684\u53d1\u97f3\u9519\u8bef\u3002 \u2022 \u57fa\u51c6\u6d4b\u8bd5\u6210\u5c31: \u5728Seed-TTS\u8bc4\u4f30\u96c6\u7684\u56f0\u96be\u6d4b\u8bd5\u96c6\u4e2d\u8fbe\u5230\u4e86\u6700\u4f4e\u5b57\u7b26\u9519\u8bef\u7387\u3002 \ud83d\udee1\ufe0f \u5f3a\u7a33\u5b9a\u6027 \u2022 \u97f3\u8272\u4e00\u81f4\u6027: \u786e\u4fdd\u4e86\u5728\u96f6\u6837\u672c\u548c\u8de8\u8bed\u8a00\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u53ef\u9760\u97f3\u8272\u4e00\u81f4\u6027\u3002 \u2022 \u8de8\u8bed\u8a00\u5408\u6210: \u76f8\u6bd41.0\u7248\u672c\u6709\u4e86\u663e\u8457\u63d0\u5347\u3002 \ud83c\udfb5 \u81ea\u7136\u4f53\u9a8c \u2022 \u589e\u5f3a\u97f5\u5f8b\u548c\u97f3\u8d28: \u6539\u5584\u4e86\u5408\u6210\u97f3\u9891\u7684\u4e00\u81f4\u6027\uff0c\u5c06MOS\u8bc4\u5206\u4ece5.4\u63d0\u9ad8\u5230\u4e865.53\u3002 \u2022 \u60c5\u611f\u548c\u65b9\u8a00\u7075\u6d3b\u6027: \u73b0\u5728\u652f\u6301\u66f4\u591a\u7ec6\u7c92\u5ea6\u7684\u60c5\u611f\u63a7\u5236\u548c\u53e3\u97f3\u8c03\u6574\u3002 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86Api\u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548cApiKey\uff0c\u4e0b\u9762\u4f1a\u5206\u522b\u4ecb\u7ecd\u5982\u4f55\u8bbf\u95ee\u4f7f\u7528\u3002 1.png \ud83d\udd39 API\u8c03\u7528 \ud83d\udc0d Python \u8c03\u7528\u793a\u4f8b \u4ee5\u4e0b\u4e3a\u5b8c\u6574\u7684 Python \u8c03\u7528\u793a\u4f8b\u4ee3\u7801\uff1a \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a \u5176\u4e2d${ApiKey}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684ApiKey\uff1b${ServerUrl}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\u3002 import argparse import logging import os import requests import torch import torchaudio import numpy as np from contextlib import closing # \u914d\u7f6e\u65e5\u5fd7 logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') def build_url(args): \"\"\"\u6784\u5efaAPI\u8bf7\u6c42URL\"\"\" return f\"http://{args.host}:{args.port}/inference_{args.mode}\" def create_payload(args): \"\"\"\u521b\u5efa\u8bf7\u6c42\u8d1f\u8f7d\"\"\" payload = {'tts_text': args.tts_text} if args.mode == 'sft': payload['spk_id'] = args.spk_id elif args.mode == 'instruct': payload.update({ 'spk_id': args.spk_id, 'instruct_text': args.instruct_text }) return payload def create_files(args): \"\"\"\u521b\u5efa\u6587\u4ef6\u4e0a\u4f20\u53c2\u6570\"\"\" if args.mode in ['zero_shot', 'cross_lingual']: return [('prompt_wav', ('prompt_wav', open(args.prompt_wav, 'rb'), 'application/octet-stream'))] return None def main(): try: # \u83b7\u53d6\u53c2\u6570 args = get_args() # \u6784\u5efa\u8bf7\u6c42\u53c2\u6570 url = build_url(args) headers = { \"X-API-TOKEN\": os.getenv(\"TTS_API_KEY\", \"${ApiKey}\"), # \u4ece\u73af\u5883\u53d8\u91cf\u83b7\u53d6\u5bc6\u94a5 \"User-Agent\": \"TTS Client/1.0\" } # \u521b\u5efa\u8bf7\u6c42\u53c2\u6570 payload = create_payload(args) files = create_files(args) # \u53d1\u8d77\u8bf7\u6c42 with closing(requests.get( url, params=payload, files=files, headers=headers, stream=True, timeout=30 )) as response: response.raise_for_status() # \u5904\u7406\u97f3\u9891\u6570\u636e audio_data = b'' for chunk in response.iter_content(chunk_size=16000): if chunk: audio_data += chunk # \u8f6c\u6362\u97f3\u9891\u683c\u5f0f audio_tensor = torch.from_numpy( np.frombuffer(audio_data, dtype=np.int16) ).unsqueeze(0) # \u4fdd\u5b58\u97f3\u9891\u6587\u4ef6 torchaudio.save(args.tts_wav, audio_tensor, args.target_sr) logging.info(f\"\u97f3\u9891\u5df2\u4fdd\u5b58\u5230: {args.tts_wav}\") except requests.exceptions.RequestException as e: logging.error(f\"\u8bf7\u6c42\u5931\u8d25: {e}\") except Exception as e: logging.error(f\"\u53d1\u751f\u9519\u8bef: {e}\", exc_info=True) def get_args(): \"\"\"\u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570\"\"\" parser = argparse.ArgumentParser(description='TTS\u5ba2\u6237\u7aef') # \u670d\u52a1\u5668\u914d\u7f6e parser.add_argument('--host', type=str, default=os.getenv(\"TTS_HOST\", \"localhost\")) parser.add_argument('--port', type=int, default=80) parser.add_argument('--target-sr', type=int, default=22050, help='\u76ee\u6807\u91c7\u6837\u7387') # \u6a21\u5f0f\u914d\u7f6e parser.add_argument('--mode', default='sft', choices=['sft', 'zero_shot', 'cross_lingual', 'instruct'], help='\u8bf7\u6c42\u6a21\u5f0f') # \u6587\u672c\u53c2\u6570 parser.add_argument('--tts_text', type=str, default='\u4f60\u597d\uff0c\u6211\u662f\u901a\u4e49\u5343\u95ee\u8bed\u97f3\u5408\u6210\u5927\u6a21\u578b\uff0c\u8bf7\u95ee\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u60a8\u7684\u5417\uff1f') # \u6a21\u5f0f\u76f8\u5173\u53c2\u6570 parser.add_argument('--spk_id', type=str, default='\u4e2d\u6587\u5973') parser.add_argument('--prompt_text', type=str, default='\u5e0c\u671b\u4f60\u4ee5\u540e\u80fd\u591f\u505a\u7684\u6bd4\u6211\u8fd8\u597d\u5466\u3002') parser.add_argument('--prompt_wav', type=str, default='../../../asset/zero_shot_prompt.wav') parser.add_argument('--instruct_text', type=str, default='Theo \\'Crimson\\', is a fiery, passionate rebel leader. ' 'Fights with fervor for justice, but struggles with impulsiveness.') # \u8f93\u51fa\u53c2\u6570 parser.add_argument('--tts_wav', type=str, default='demo.wav') args = parser.parse_args() return args if __name__ == \"__main__\": main() \ud83d\udd39 Web\u5e94\u7528 \u70b9\u51fb\u5b89\u5168\u4ee3\u7406\u8bbf\u95ee\uff0c\u8df3\u8f6c\u5230\u5bf9\u5e94\u7684\u9875\u9762\u5c31\u53ef\u4ee5\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1\u5728\u7ebf\u8bbf\u95ee\u4e86\u3002 \ud83c\udfaf \u5feb\u901f\u5f00\u59cb 1\ufe0f\u20e3 \u83b7\u53d6\u8bbf\u95ee\u4fe1\u606f \u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u83b7\u53d6 ApiKey \u548c\u8bbf\u95ee\u5730\u5740 2\ufe0f\u20e3 \u9009\u62e9\u8c03\u7528\u65b9\u5f0f \u652f\u6301 Python API \u8c03\u7528\u548c Web \u5e94\u7528\u76f4\u63a5\u8bbf\u95ee 3\ufe0f\u20e3 \u5f00\u59cb\u4f7f\u7528 \u914d\u7f6e\u53c2\u6570\u540e\u5373\u53ef\u4eab\u53d7\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u5408\u6210\u670d\u52a1 \ud83c\udfa4 CosyVoice | \u8ba9\u6587\u5b57\u62e5\u6709\u751f\u547d\u529b\u7684\u58f0\u97f3","title":"Index"},{"location":"cosyvoice/index-en/","text":"\ud83d\udccb Introduction CosyVoice is a speech synthesis service launched by Alibaba Cloud, capable of converting text into natural and smooth speech. This service supports multiple languages and dialects, meeting the needs of various scenarios such as news broadcasting, audiobook production, and intelligent customer service. By using advanced deep learning technology, CosyVoice can generate sounds that closely mimic human voices, providing users with a richer and more humanized interactive experience. Multilingual Support - Supported Languages: Chinese, English, Japanese, Korean, Chinese Dialects (Cantonese, Sichuanese, Shanghainese, Tianjin dialect, Wuhan dialect, etc.) - Cross-language and Mixed-language: Supports zero-shot cross-language and code-switching scenarios for voice cloning. Ultra-low Latency Bidirectional Stream Support: CosyVoice 2.0 integrates offline and streaming modeling technologies. Fast First Packet Synthesis: Achieves a latency as low as 150 milliseconds while maintaining high-quality audio output. High Accuracy - Improved Pronunciation: Compared to CosyVoice 1.0, it reduces pronunciation errors by 30% to 50%. - Benchmark Achievements: Achieved the lowest character error rate on the Seed-TTS evaluation set's challenging test set. Strong Stability Voice Consistency: Ensures reliable voice consistency in zero-shot and cross-language speech synthesis. Cross-language Synthesis: Significantly improved compared to version 1.0. Natural Experience - Enhanced Prosody and Sound Quality: Improves the consistency of synthesized audio, raising the MOS score from 5.4 to 5.53. - Emotional and Dialect Flexibility: Now supports more fine-grained emotional control and accent adjustments. \ud83d\udccb Usage Instructions After completing the model deployment, you can see the usage method of the model on the service instance overview page of the Compute Nest, which provides API call examples, intranet access addresses, public network access addresses, and APIKey. The following will introduce how to access and use it. 1.png \ud83d\udd39 API Call Python Call The following is a Python example code: where ${ApiKey} needs to be filled with the APIKey on the page; ${ServerUrl} needs to be filled with the public or intranet address on the page. import argparse import logging import os import requests import torch import torchaudio import numpy as np from contextlib import closing # \u914d\u7f6e\u65e5\u5fd7 logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') def build_url(args): \"\"\"\u6784\u5efaAPI\u8bf7\u6c42URL\"\"\" return f\"http://{args.host}:{args.port}/inference_{args.mode}\" def create_payload(args): \"\"\"\u521b\u5efa\u8bf7\u6c42\u8d1f\u8f7d\"\"\" payload = {'tts_text': args.tts_text} if args.mode == 'sft': payload['spk_id'] = args.spk_id elif args.mode == 'instruct': payload.update({ 'spk_id': args.spk_id, 'instruct_text': args.instruct_text }) return payload def create_files(args): \"\"\"\u521b\u5efa\u6587\u4ef6\u4e0a\u4f20\u53c2\u6570\"\"\" if args.mode in ['zero_shot', 'cross_lingual']: return [('prompt_wav', ('prompt_wav', open(args.prompt_wav, 'rb'), 'application/octet-stream'))] return None def main(): try: # \u83b7\u53d6\u53c2\u6570 args = get_args() # \u6784\u5efa\u8bf7\u6c42\u53c2\u6570 url = build_url(args) headers = { \"X-API-TOKEN\": os.getenv(\"TTS_API_KEY\", \"${ApiKey}\"), # \u4ece\u73af\u5883\u53d8\u91cf\u83b7\u53d6\u5bc6\u94a5 \"User-Agent\": \"TTS Client/1.0\" } # \u521b\u5efa\u8bf7\u6c42\u53c2\u6570 payload = create_payload(args) files = create_files(args) # \u53d1\u8d77\u8bf7\u6c42 with closing(requests.get( url, params=payload, files=files, headers=headers, stream=True, timeout=30 )) as response: response.raise_for_status() # \u5904\u7406\u97f3\u9891\u6570\u636e audio_data = b'' for chunk in response.iter_content(chunk_size=16000): if chunk: audio_data += chunk # \u8f6c\u6362\u97f3\u9891\u683c\u5f0f audio_tensor = torch.from_numpy( np.frombuffer(audio_data, dtype=np.int16) ).unsqueeze(0) # \u4fdd\u5b58\u97f3\u9891\u6587\u4ef6 torchaudio.save(args.tts_wav, audio_tensor, args.target_sr) logging.info(f\"\u97f3\u9891\u5df2\u4fdd\u5b58\u5230: {args.tts_wav}\") except requests.exceptions.RequestException as e: logging.error(f\"\u8bf7\u6c42\u5931\u8d25: {e}\") except Exception as e: logging.error(f\"\u53d1\u751f\u9519\u8bef: {e}\", exc_info=True) def get_args(): \"\"\"\u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570\"\"\" parser = argparse.ArgumentParser(description='TTS\u5ba2\u6237\u7aef') # \u670d\u52a1\u5668\u914d\u7f6e parser.add_argument('--host', type=str, default=os.getenv(\"TTS_HOST\", \"localhost\")) parser.add_argument('--port', type=int, default=80) parser.add_argument('--target-sr', type=int, default=22050, help='\u76ee\u6807\u91c7\u6837\u7387') # \u6a21\u5f0f\u914d\u7f6e parser.add_argument('--mode', default='sft', choices=['sft', 'zero_shot', 'cross_lingual', 'instruct'], help='\u8bf7\u6c42\u6a21\u5f0f') # \u6587\u672c\u53c2\u6570 parser.add_argument('--tts_text', type=str, default='\u4f60\u597d\uff0c\u6211\u662f\u901a\u4e49\u5343\u95ee\u8bed\u97f3\u5408\u6210\u5927\u6a21\u578b\uff0c\u8bf7\u95ee\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u60a8\u7684\u5417\uff1f') # \u6a21\u5f0f\u76f8\u5173\u53c2\u6570 parser.add_argument('--spk_id', type=str, default='\u4e2d\u6587\u5973') parser.add_argument('--prompt_text', type=str, default='\u5e0c\u671b\u4f60\u4ee5\u540e\u80fd\u591f\u505a\u7684\u6bd4\u6211\u8fd8\u597d\u5466\u3002') parser.add_argument('--prompt_wav', type=str, default='../../../asset/zero_shot_prompt.wav') parser.add_argument('--instruct_text', type=str, default='Theo \\'Crimson\\', is a fiery, passionate rebel leader. ' 'Fights with fervor for justice, but struggles with impulsiveness.') # \u8f93\u51fa\u53c2\u6570 parser.add_argument('--tts_wav', type=str, default='demo.wav') args = parser.parse_args() return args if __name__ == \"__main__\": main() \ud83d\udd39 Web Application Click on the secure proxy access, and jump to the corresponding page to directly access the online model service. \ud83c\udfaf \u5feb\u901f\u5f00\u59cb 1\ufe0f\u20e3 \u83b7\u53d6\u8bbf\u95ee\u4fe1\u606f \u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u83b7\u53d6 ApiKey \u548c\u8bbf\u95ee\u5730\u5740 2\ufe0f\u20e3 \u9009\u62e9\u8c03\u7528\u65b9\u5f0f \u652f\u6301 Python API \u8c03\u7528\u548c Web \u5e94\u7528\u76f4\u63a5\u8bbf\u95ee 3\ufe0f\u20e3 \u5f00\u59cb\u4f7f\u7528 \u914d\u7f6e\u53c2\u6570\u540e\u5373\u53ef\u4eab\u53d7\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u5408\u6210\u670d\u52a1 \ud83c\udfa4 CosyVoice | \u8ba9\u6587\u5b57\u62e5\u6709\u751f\u547d\u529b\u7684\u58f0\u97f3","title":"Index en"},{"location":"cosyvoice/index-en/#python-call","text":"The following is a Python example code: where ${ApiKey} needs to be filled with the APIKey on the page; ${ServerUrl} needs to be filled with the public or intranet address on the page. import argparse import logging import os import requests import torch import torchaudio import numpy as np from contextlib import closing # \u914d\u7f6e\u65e5\u5fd7 logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') def build_url(args): \"\"\"\u6784\u5efaAPI\u8bf7\u6c42URL\"\"\" return f\"http://{args.host}:{args.port}/inference_{args.mode}\" def create_payload(args): \"\"\"\u521b\u5efa\u8bf7\u6c42\u8d1f\u8f7d\"\"\" payload = {'tts_text': args.tts_text} if args.mode == 'sft': payload['spk_id'] = args.spk_id elif args.mode == 'instruct': payload.update({ 'spk_id': args.spk_id, 'instruct_text': args.instruct_text }) return payload def create_files(args): \"\"\"\u521b\u5efa\u6587\u4ef6\u4e0a\u4f20\u53c2\u6570\"\"\" if args.mode in ['zero_shot', 'cross_lingual']: return [('prompt_wav', ('prompt_wav', open(args.prompt_wav, 'rb'), 'application/octet-stream'))] return None def main(): try: # \u83b7\u53d6\u53c2\u6570 args = get_args() # \u6784\u5efa\u8bf7\u6c42\u53c2\u6570 url = build_url(args) headers = { \"X-API-TOKEN\": os.getenv(\"TTS_API_KEY\", \"${ApiKey}\"), # \u4ece\u73af\u5883\u53d8\u91cf\u83b7\u53d6\u5bc6\u94a5 \"User-Agent\": \"TTS Client/1.0\" } # \u521b\u5efa\u8bf7\u6c42\u53c2\u6570 payload = create_payload(args) files = create_files(args) # \u53d1\u8d77\u8bf7\u6c42 with closing(requests.get( url, params=payload, files=files, headers=headers, stream=True, timeout=30 )) as response: response.raise_for_status() # \u5904\u7406\u97f3\u9891\u6570\u636e audio_data = b'' for chunk in response.iter_content(chunk_size=16000): if chunk: audio_data += chunk # \u8f6c\u6362\u97f3\u9891\u683c\u5f0f audio_tensor = torch.from_numpy( np.frombuffer(audio_data, dtype=np.int16) ).unsqueeze(0) # \u4fdd\u5b58\u97f3\u9891\u6587\u4ef6 torchaudio.save(args.tts_wav, audio_tensor, args.target_sr) logging.info(f\"\u97f3\u9891\u5df2\u4fdd\u5b58\u5230: {args.tts_wav}\") except requests.exceptions.RequestException as e: logging.error(f\"\u8bf7\u6c42\u5931\u8d25: {e}\") except Exception as e: logging.error(f\"\u53d1\u751f\u9519\u8bef: {e}\", exc_info=True) def get_args(): \"\"\"\u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570\"\"\" parser = argparse.ArgumentParser(description='TTS\u5ba2\u6237\u7aef') # \u670d\u52a1\u5668\u914d\u7f6e parser.add_argument('--host', type=str, default=os.getenv(\"TTS_HOST\", \"localhost\")) parser.add_argument('--port', type=int, default=80) parser.add_argument('--target-sr', type=int, default=22050, help='\u76ee\u6807\u91c7\u6837\u7387') # \u6a21\u5f0f\u914d\u7f6e parser.add_argument('--mode', default='sft', choices=['sft', 'zero_shot', 'cross_lingual', 'instruct'], help='\u8bf7\u6c42\u6a21\u5f0f') # \u6587\u672c\u53c2\u6570 parser.add_argument('--tts_text', type=str, default='\u4f60\u597d\uff0c\u6211\u662f\u901a\u4e49\u5343\u95ee\u8bed\u97f3\u5408\u6210\u5927\u6a21\u578b\uff0c\u8bf7\u95ee\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u60a8\u7684\u5417\uff1f') # \u6a21\u5f0f\u76f8\u5173\u53c2\u6570 parser.add_argument('--spk_id', type=str, default='\u4e2d\u6587\u5973') parser.add_argument('--prompt_text', type=str, default='\u5e0c\u671b\u4f60\u4ee5\u540e\u80fd\u591f\u505a\u7684\u6bd4\u6211\u8fd8\u597d\u5466\u3002') parser.add_argument('--prompt_wav', type=str, default='../../../asset/zero_shot_prompt.wav') parser.add_argument('--instruct_text', type=str, default='Theo \\'Crimson\\', is a fiery, passionate rebel leader. ' 'Fights with fervor for justice, but struggles with impulsiveness.') # \u8f93\u51fa\u53c2\u6570 parser.add_argument('--tts_wav', type=str, default='demo.wav') args = parser.parse_args() return args if __name__ == \"__main__\": main()","title":"Python Call"},{"location":"deepseek/index-deepseek-ocr-en/","text":"Introduction DeepSeek-OCR is an advanced Optical Character Recognition (OCR) model developed by DeepSeek, specifically designed for extracting and recognizing text content from images. Based on deep learning technology, the model supports multiple languages and text formats, and can accurately recognize text in images and convert it to editable text format. Official Links ComputeNest Deployment : https://computenest.console.aliyun.com/ai-lab/model/cn-hangzhou/DeepSeek-OCR GitHub Repository : https://github.com/deepseek-ai/DeepSeek-OCR Official Website : https://deepseek-ocr.io/ Model Download : https://huggingface.co/deepseek-ai/DeepSeek-OCR Technical Documentation : https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html Paper Link : https://arxiv.org/html/2510.18234v1 Usage Instructions After completing model deployment, you can see the model usage methods in the ComputeNest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey information. API Calling Methods Curl Command The basic structure of API calls is as follows: Parameter Description: - ${ServerIP} : IP address from internal or public network address - ${ApiKey} : ApiKey provided on the page - ${ModelName} : Model name Image Format Support: - HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png - Base64 Encoding : data:image/jpeg;base64,<base64 encoded image> curl -X POST http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image>\" } }, { \"type\": \"text\", \"text\": \"Please recognize the text content in the image\" } ] } ] }' Python SDK Configuration Instructions: - ${ApiKey} : Fill in the ApiKey from the page - ${ServerUrl} : Fill in the public or internal network address from the page, need to add /v1 import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_image_to_base64(image_url: str) -> str: \"\"\"Convert image URL to base64 encoding\"\"\" response = requests.get(image_url) return base64.b64encode(response.content).decode('utf-8') def ocr_image(image_url: str, prompt: str = \"Please recognize the text content in the image\"): \"\"\"OCR image recognition\"\"\" image_base64 = encode_image_to_base64(image_url) completion = client.chat.completions.create( model=model, messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": prompt }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{image_base64}\" } } ] } ], max_tokens=1000 ) return completion.choices[0].message.content # Usage example if __name__ == \"__main__\": image_url = \"https://example.com/document.jpg\" result = ocr_image(image_url) print(\"Recognition result:\", result) Web Application Access\u3010Coming Soon\u3011 Get Access Link : Click on the Web application link in the service instance overview page Start Using : Upload images in the model service Web page, and the system will automatically recognize text content in the images Get Results : After recognition is complete, you can copy or download the recognition results Usage Recommendations Image Quality : Recommend using clear, well-lit images for optimal recognition results Image Format : Supports common formats like JPG, PNG, PDF Text Size : Recommend moderate text size, avoid too small or too large text Background Interference : Use images with simple backgrounds to avoid complex background interference with recognition","title":"Index deepseek ocr en"},{"location":"deepseek/index-deepseek-ocr-en/#introduction","text":"DeepSeek-OCR is an advanced Optical Character Recognition (OCR) model developed by DeepSeek, specifically designed for extracting and recognizing text content from images. Based on deep learning technology, the model supports multiple languages and text formats, and can accurately recognize text in images and convert it to editable text format.","title":"Introduction"},{"location":"deepseek/index-deepseek-ocr-en/#official-links","text":"ComputeNest Deployment : https://computenest.console.aliyun.com/ai-lab/model/cn-hangzhou/DeepSeek-OCR GitHub Repository : https://github.com/deepseek-ai/DeepSeek-OCR Official Website : https://deepseek-ocr.io/ Model Download : https://huggingface.co/deepseek-ai/DeepSeek-OCR Technical Documentation : https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html Paper Link : https://arxiv.org/html/2510.18234v1","title":"Official Links"},{"location":"deepseek/index-deepseek-ocr-en/#usage-instructions","text":"After completing model deployment, you can see the model usage methods in the ComputeNest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey information.","title":"Usage Instructions"},{"location":"deepseek/index-deepseek-ocr-en/#api-calling-methods","text":"","title":"API Calling Methods"},{"location":"deepseek/index-deepseek-ocr-en/#curl-command","text":"The basic structure of API calls is as follows: Parameter Description: - ${ServerIP} : IP address from internal or public network address - ${ApiKey} : ApiKey provided on the page - ${ModelName} : Model name Image Format Support: - HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png - Base64 Encoding : data:image/jpeg;base64,<base64 encoded image> curl -X POST http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image>\" } }, { \"type\": \"text\", \"text\": \"Please recognize the text content in the image\" } ] } ] }'","title":"Curl Command"},{"location":"deepseek/index-deepseek-ocr-en/#python-sdk","text":"Configuration Instructions: - ${ApiKey} : Fill in the ApiKey from the page - ${ServerUrl} : Fill in the public or internal network address from the page, need to add /v1 import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_image_to_base64(image_url: str) -> str: \"\"\"Convert image URL to base64 encoding\"\"\" response = requests.get(image_url) return base64.b64encode(response.content).decode('utf-8') def ocr_image(image_url: str, prompt: str = \"Please recognize the text content in the image\"): \"\"\"OCR image recognition\"\"\" image_base64 = encode_image_to_base64(image_url) completion = client.chat.completions.create( model=model, messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": prompt }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{image_base64}\" } } ] } ], max_tokens=1000 ) return completion.choices[0].message.content # Usage example if __name__ == \"__main__\": image_url = \"https://example.com/document.jpg\" result = ocr_image(image_url) print(\"Recognition result:\", result)","title":"Python SDK"},{"location":"deepseek/index-deepseek-ocr-en/#web-application-accesscoming-soon","text":"Get Access Link : Click on the Web application link in the service instance overview page Start Using : Upload images in the model service Web page, and the system will automatically recognize text content in the images Get Results : After recognition is complete, you can copy or download the recognition results","title":"Web Application Access\u3010Coming Soon\u3011"},{"location":"deepseek/index-deepseek-ocr-en/#usage-recommendations","text":"Image Quality : Recommend using clear, well-lit images for optimal recognition results Image Format : Supports common formats like JPG, PNG, PDF Text Size : Recommend moderate text size, avoid too small or too large text Background Interference : Use images with simple backgrounds to avoid complex background interference with recognition","title":"Usage Recommendations"},{"location":"deepseek/index-deepseek-ocr/","text":"\u7b80\u4ecb DeepSeek-OCR \u662f\u7531 DeepSeek \u516c\u53f8\u5f00\u53d1\u7684\u5148\u8fdb\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u548c\u8bc6\u522b\u6587\u672c\u5185\u5bb9\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u652f\u6301\u591a\u79cd\u8bed\u8a00\u548c\u6587\u672c\u683c\u5f0f\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5e76\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u7684\u6587\u672c\u683c\u5f0f\u3002 \u5b98\u65b9\u94fe\u63a5 GitHub\u4ed3\u5e93 : https://github.com/deepseek-ai/DeepSeek-OCR \u5b98\u65b9\u7f51\u7ad9 : https://deepseek-ocr.io/ \u8ba1\u7b97\u5de2\u90e8\u7f72 : https://computenest.console.aliyun.com/ai-lab/model/cn-hangzhou/DeepSeek-OCR \u6a21\u578b\u4e0b\u8f7d : https://huggingface.co/deepseek-ai/DeepSeek-OCR \u6280\u672f\u6587\u6863 : https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html \u8bba\u6587\u94fe\u63a5 : https://arxiv.org/html/2510.18234v1 \u4f7f\u7528\u8bf4\u660e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548c ApiKey\u3002 API \u8c03\u7528\u65b9\u5f0f Curl \u547d\u4ee4\u8c03\u7528 API \u8c03\u7528\u7684\u57fa\u672c\u7ed3\u6784\u5982\u4e0b\uff1a \u53c2\u6570\u8bf4\u660e\uff1a - ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 - ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey - ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \u56fe\u7247\u683c\u5f0f\u652f\u6301\uff1a - HTTP URL \uff1a\u5982 https://example.com/image.jpg - Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684base64\u7f16\u7801> curl -X POST http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684base64\u7f16\u7801>\" } }, { \"type\": \"text\", \"text\": \"\u8bf7\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5185\u5bb9\" } ] } ] }' Python SDK \u8c03\u7528 \u914d\u7f6e\u8bf4\u660e\uff1a - ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey - ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_image_to_base64(image_url: str) -> str: \"\"\"\u5c06\u56fe\u7247URL\u8f6c\u6362\u4e3abase64\u7f16\u7801\"\"\" response = requests.get(image_url) return base64.b64encode(response.content).decode('utf-8') def ocr_image(image_url: str, prompt: str = \"\u8bf7\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5185\u5bb9\"): \"\"\"OCR\u56fe\u7247\u8bc6\u522b\"\"\" image_base64 = encode_image_to_base64(image_url) completion = client.chat.completions.create( model=model, messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": prompt }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{image_base64}\" } } ] } ], max_tokens=1000 ) return completion.choices[0].message.content # \u4f7f\u7528\u793a\u4f8b if __name__ == \"__main__\": image_url = \"https://example.com/document.jpg\" result = ocr_image(image_url) print(\"\u8bc6\u522b\u7ed3\u679c:\", result) Web \u5e94\u7528\u8bbf\u95ee \u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \uff1a\u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5 \u5f00\u59cb\u4f7f\u7528 \uff1a\u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u4e2d\u4e0a\u4f20\u56fe\u7247\uff0c\u7cfb\u7edf\u5c06\u81ea\u52a8\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5185\u5bb9 \u7ed3\u679c\u83b7\u53d6 \uff1a\u8bc6\u522b\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u590d\u5236\u6216\u4e0b\u8f7d\u8bc6\u522b\u7ed3\u679c \u4f7f\u7528\u5efa\u8bae \u56fe\u7247\u8d28\u91cf \uff1a\u5efa\u8bae\u4f7f\u7528\u6e05\u6670\u3001\u5149\u7167\u826f\u597d\u7684\u56fe\u7247\u4ee5\u83b7\u5f97\u6700\u4f73\u8bc6\u522b\u6548\u679c \u56fe\u7247\u683c\u5f0f \uff1a\u652f\u6301 JPG\u3001PNG\u3001PDF \u7b49\u5e38\u89c1\u683c\u5f0f \u6587\u5b57\u5927\u5c0f \uff1a\u5efa\u8bae\u6587\u5b57\u5927\u5c0f\u9002\u4e2d\uff0c\u907f\u514d\u8fc7\u5c0f\u6216\u8fc7\u5927\u7684\u6587\u5b57 \u80cc\u666f\u5e72\u6270 \uff1a\u5c3d\u91cf\u4f7f\u7528\u80cc\u666f\u7b80\u6d01\u7684\u56fe\u7247\uff0c\u907f\u514d\u590d\u6742\u80cc\u666f\u5e72\u6270\u8bc6\u522b","title":"Index deepseek ocr"},{"location":"deepseek/index-deepseek-ocr/#_1","text":"DeepSeek-OCR \u662f\u7531 DeepSeek \u516c\u53f8\u5f00\u53d1\u7684\u5148\u8fdb\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u548c\u8bc6\u522b\u6587\u672c\u5185\u5bb9\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u652f\u6301\u591a\u79cd\u8bed\u8a00\u548c\u6587\u672c\u683c\u5f0f\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5e76\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u7684\u6587\u672c\u683c\u5f0f\u3002","title":"\u7b80\u4ecb"},{"location":"deepseek/index-deepseek-ocr/#_2","text":"GitHub\u4ed3\u5e93 : https://github.com/deepseek-ai/DeepSeek-OCR \u5b98\u65b9\u7f51\u7ad9 : https://deepseek-ocr.io/ \u8ba1\u7b97\u5de2\u90e8\u7f72 : https://computenest.console.aliyun.com/ai-lab/model/cn-hangzhou/DeepSeek-OCR \u6a21\u578b\u4e0b\u8f7d : https://huggingface.co/deepseek-ai/DeepSeek-OCR \u6280\u672f\u6587\u6863 : https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html \u8bba\u6587\u94fe\u63a5 : https://arxiv.org/html/2510.18234v1","title":"\u5b98\u65b9\u94fe\u63a5"},{"location":"deepseek/index-deepseek-ocr/#_3","text":"\u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548c ApiKey\u3002","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"deepseek/index-deepseek-ocr/#api","text":"","title":"API \u8c03\u7528\u65b9\u5f0f"},{"location":"deepseek/index-deepseek-ocr/#curl","text":"API \u8c03\u7528\u7684\u57fa\u672c\u7ed3\u6784\u5982\u4e0b\uff1a \u53c2\u6570\u8bf4\u660e\uff1a - ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 - ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey - ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \u56fe\u7247\u683c\u5f0f\u652f\u6301\uff1a - HTTP URL \uff1a\u5982 https://example.com/image.jpg - Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684base64\u7f16\u7801> curl -X POST http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684base64\u7f16\u7801>\" } }, { \"type\": \"text\", \"text\": \"\u8bf7\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5185\u5bb9\" } ] } ] }'","title":"Curl \u547d\u4ee4\u8c03\u7528"},{"location":"deepseek/index-deepseek-ocr/#python-sdk","text":"\u914d\u7f6e\u8bf4\u660e\uff1a - ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey - ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_image_to_base64(image_url: str) -> str: \"\"\"\u5c06\u56fe\u7247URL\u8f6c\u6362\u4e3abase64\u7f16\u7801\"\"\" response = requests.get(image_url) return base64.b64encode(response.content).decode('utf-8') def ocr_image(image_url: str, prompt: str = \"\u8bf7\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5185\u5bb9\"): \"\"\"OCR\u56fe\u7247\u8bc6\u522b\"\"\" image_base64 = encode_image_to_base64(image_url) completion = client.chat.completions.create( model=model, messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": prompt }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{image_base64}\" } } ] } ], max_tokens=1000 ) return completion.choices[0].message.content # \u4f7f\u7528\u793a\u4f8b if __name__ == \"__main__\": image_url = \"https://example.com/document.jpg\" result = ocr_image(image_url) print(\"\u8bc6\u522b\u7ed3\u679c:\", result)","title":"Python SDK \u8c03\u7528"},{"location":"deepseek/index-deepseek-ocr/#web","text":"\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \uff1a\u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5 \u5f00\u59cb\u4f7f\u7528 \uff1a\u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u4e2d\u4e0a\u4f20\u56fe\u7247\uff0c\u7cfb\u7edf\u5c06\u81ea\u52a8\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u5185\u5bb9 \u7ed3\u679c\u83b7\u53d6 \uff1a\u8bc6\u522b\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u590d\u5236\u6216\u4e0b\u8f7d\u8bc6\u522b\u7ed3\u679c","title":"Web \u5e94\u7528\u8bbf\u95ee"},{"location":"deepseek/index-deepseek-ocr/#_4","text":"\u56fe\u7247\u8d28\u91cf \uff1a\u5efa\u8bae\u4f7f\u7528\u6e05\u6670\u3001\u5149\u7167\u826f\u597d\u7684\u56fe\u7247\u4ee5\u83b7\u5f97\u6700\u4f73\u8bc6\u522b\u6548\u679c \u56fe\u7247\u683c\u5f0f \uff1a\u652f\u6301 JPG\u3001PNG\u3001PDF \u7b49\u5e38\u89c1\u683c\u5f0f \u6587\u5b57\u5927\u5c0f \uff1a\u5efa\u8bae\u6587\u5b57\u5927\u5c0f\u9002\u4e2d\uff0c\u907f\u514d\u8fc7\u5c0f\u6216\u8fc7\u5927\u7684\u6587\u5b57 \u80cc\u666f\u5e72\u6270 \uff1a\u5c3d\u91cf\u4f7f\u7528\u80cc\u666f\u7b80\u6d01\u7684\u56fe\u7247\uff0c\u907f\u514d\u590d\u6742\u80cc\u666f\u5e72\u6270\u8bc6\u522b","title":"\u4f7f\u7528\u5efa\u8bae"},{"location":"deepseek/index-deepseek-r1-en/","text":"\ud83e\udde0 DeepSeek-R1 Reasoning Model 671 Billion Parameter Reasoning Expert - Open Source Challenger to OpenAI o1 \ud83c\udfaf Product Overview DeepSeek-R1 is a large language model (LLM) developed by Hangzhou DeepSeek Company, specifically optimized for tasks such as mathematics, coding, and logical reasoning. It adopts advanced technologies including Mixture of Experts (MoE) and Multi-Head Latent Attention (MLA), with **671 billion parameters** and support for input contexts up to **128,000 tokens**. \ud83c\udfaf Core Objective DeepSeek-R1 aims to match or exceed the performance of OpenAI's o1 model in reasoning tasks, providing world-class reasoning capabilities to the open source community. \u2728 Core Features \ud83d\ude80 Powerful Reasoning Capabilities Outstanding performance in mathematics, code generation, and natural language reasoning tasks, even surpassing similar models, demonstrating excellent logical thinking abilities. \u26a1 MoE Architecture Advantages Adopts mixture of experts model, using numerous experts in each layer to handle different inputs, significantly improving reasoning capabilities and processing efficiency. \ud83d\udcda Long Context Support Can handle longer input contexts (128,000 tokens), which is crucial for complex reasoning tasks and long document analysis. \ud83d\udd13 Fully Open Source DeepSeek Company has completely open-sourced DeepSeek-R1's training techniques and model weights, enabling developers to conduct further exploration and research. \ud83c\udf93 Open Source Distilled Models Through distillation techniques, generated 6 smaller models (such as Qwen2.5 and Llama3.1) for community use, lowering deployment barriers. \ud83c\udf93 Distilled Model Ecosystem \ud83d\udce6 Open Source Distilled Model Series \ud83c\udfaf Distillation Strategy Through advanced distillation techniques, DeepSeek-R1's reasoning capabilities are transferred to smaller models, generating 6 lightweight versions for community use. Qwen2.5 Series High-performance distilled version Llama3.1 Series Compatibility optimized version Lightweight Models Edge deployment friendly Customized Versions Specific scenario optimization \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83e\udde0 DeepSeek-R1 | 671 Billion Parameter Reasoning Expert, Open Source Challenger to OpenAI o1","title":"Index deepseek r1 en"},{"location":"deepseek/index-deepseek-r1-en/#product-overview","text":"DeepSeek-R1 is a large language model (LLM) developed by Hangzhou DeepSeek Company, specifically optimized for tasks such as mathematics, coding, and logical reasoning. It adopts advanced technologies including Mixture of Experts (MoE) and Multi-Head Latent Attention (MLA), with **671 billion parameters** and support for input contexts up to **128,000 tokens**. \ud83c\udfaf Core Objective DeepSeek-R1 aims to match or exceed the performance of OpenAI's o1 model in reasoning tasks, providing world-class reasoning capabilities to the open source community.","title":"\ud83c\udfaf Product Overview"},{"location":"deepseek/index-deepseek-r1-en/#core-features","text":"\ud83d\ude80 Powerful Reasoning Capabilities Outstanding performance in mathematics, code generation, and natural language reasoning tasks, even surpassing similar models, demonstrating excellent logical thinking abilities. \u26a1 MoE Architecture Advantages Adopts mixture of experts model, using numerous experts in each layer to handle different inputs, significantly improving reasoning capabilities and processing efficiency. \ud83d\udcda Long Context Support Can handle longer input contexts (128,000 tokens), which is crucial for complex reasoning tasks and long document analysis. \ud83d\udd13 Fully Open Source DeepSeek Company has completely open-sourced DeepSeek-R1's training techniques and model weights, enabling developers to conduct further exploration and research. \ud83c\udf93 Open Source Distilled Models Through distillation techniques, generated 6 smaller models (such as Qwen2.5 and Llama3.1) for community use, lowering deployment barriers.","title":"\u2728 Core Features"},{"location":"deepseek/index-deepseek-r1-en/#distilled-model-ecosystem","text":"","title":"\ud83c\udf93 Distilled Model Ecosystem"},{"location":"deepseek/index-deepseek-r1-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"deepseek/index-deepseek-r1-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"deepseek/index-deepseek-r1-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"deepseek/index-deepseek-r1-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"deepseek/index-deepseek-r1-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"deepseek/index-deepseek-r1/","text":"\ud83e\udde0 DeepSeek-R1 \u63a8\u7406\u6a21\u578b 6710 \u4ebf\u53c2\u6570\u63a8\u7406\u4e13\u5bb6 - \u6311\u6218 OpenAI o1 \u7684\u5f00\u6e90\u529b\u4f5c \ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb DeepSeek-R1 \u662f\u4e00\u6b3e\u7531\u676d\u5dde\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u7814\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4e13\u95e8\u9488\u5bf9\u6570\u5b66\u3001\u4ee3\u7801\u548c\u903b\u8f91\u63a8\u7406\u7b49\u4efb\u52a1\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5b83\u91c7\u7528\u4e86\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\u548c\u591a\u5934\u6f5c\u6ce8\u610f\u529b\uff08MLA\uff09\u7b49\u5148\u8fdb\u6280\u672f\uff0c\u62e5\u6709 **6710 \u4ebf\u53c2\u6570**\uff0c\u5e76\u652f\u6301\u957f\u8fbe **128,000 token** \u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u3002 \ud83c\udfaf \u6838\u5fc3\u76ee\u6807 DeepSeek-R1 \u7684\u76ee\u6807\u662f\u8fbe\u5230\u6216\u8d85\u8d8a OpenAI \u7684 o1 \u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u4e16\u754c\u7ea7\u7684\u63a8\u7406\u80fd\u529b\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \ud83d\ude80 \u63a8\u7406\u80fd\u529b\u5f3a\u52b2 \u5728\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u540c\u7c7b\u578b\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u903b\u8f91\u601d\u7ef4\u80fd\u529b\u3002 \u26a1 MoE \u67b6\u6784\u4f18\u52bf \u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u6bcf\u5c42\u4e2d\u4f7f\u7528\u5927\u91cf\u7684\u4e13\u5bb6\u6765\u5904\u7406\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u548c\u5904\u7406\u6548\u7387\u3002 \ud83d\udcda \u957f\u4e0a\u4e0b\u6587\u652f\u6301 \u53ef\u4ee5\u5904\u7406\u66f4\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\uff08128,000 token\uff09\uff0c\u8fd9\u5bf9\u4e8e\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u548c\u957f\u6587\u6863\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002 \ud83d\udd13 \u5b8c\u5168\u5f00\u6e90 \u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u5c06 DeepSeek-R1 \u7684\u8bad\u7ec3\u6280\u672f\u548c\u6a21\u578b\u6743\u91cd\u5b8c\u5168\u5f00\u6e90\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u63a2\u7d22\u548c\u7814\u7a76\u3002 \ud83c\udf93 \u5f00\u6e90\u84b8\u998f\u6a21\u578b \u901a\u8fc7\u84b8\u998f\u6280\u672f\uff0c\u751f\u6210\u4e86 6 \u4e2a\u5c0f\u6a21\u578b\uff08\u5982 Qwen2.5 \u548c Llama3.1\uff09\u4f9b\u793e\u533a\u4f7f\u7528\uff0c\u964d\u4f4e\u90e8\u7f72\u95e8\u69db\u3002 \ud83c\udf93 \u84b8\u998f\u6a21\u578b\u751f\u6001 \ud83d\udce6 \u5f00\u6e90\u84b8\u998f\u6a21\u578b\u7cfb\u5217 \ud83c\udfaf \u84b8\u998f\u7b56\u7565 \u901a\u8fc7\u5148\u8fdb\u7684\u84b8\u998f\u6280\u672f\uff0c\u5c06 DeepSeek-R1 \u7684\u63a8\u7406\u80fd\u529b\u4f20\u9012\u7ed9\u66f4\u5c0f\u7684\u6a21\u578b\uff0c\u751f\u6210\u4e86 6 \u4e2a\u8f7b\u91cf\u7ea7\u7248\u672c\u4f9b\u793e\u533a\u4f7f\u7528\u3002 Qwen2.5 \u7cfb\u5217 \u9ad8\u6027\u80fd\u84b8\u998f\u7248\u672c Llama3.1 \u7cfb\u5217 \u517c\u5bb9\u6027\u4f18\u5316\u7248\u672c \u8f7b\u91cf\u7ea7\u6a21\u578b \u8fb9\u7f18\u90e8\u7f72\u53cb\u597d \u5b9a\u5236\u5316\u7248\u672c \u7279\u5b9a\u573a\u666f\u4f18\u5316 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83e\udde0 DeepSeek-R1 | 6710 \u4ebf\u53c2\u6570\u63a8\u7406\u4e13\u5bb6\uff0c\u6311\u6218 OpenAI o1 \u7684\u5f00\u6e90\u529b\u4f5c","title":"Index deepseek r1"},{"location":"deepseek/index-deepseek-r1/#_1","text":"DeepSeek-R1 \u662f\u4e00\u6b3e\u7531\u676d\u5dde\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u7814\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4e13\u95e8\u9488\u5bf9\u6570\u5b66\u3001\u4ee3\u7801\u548c\u903b\u8f91\u63a8\u7406\u7b49\u4efb\u52a1\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5b83\u91c7\u7528\u4e86\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\u548c\u591a\u5934\u6f5c\u6ce8\u610f\u529b\uff08MLA\uff09\u7b49\u5148\u8fdb\u6280\u672f\uff0c\u62e5\u6709 **6710 \u4ebf\u53c2\u6570**\uff0c\u5e76\u652f\u6301\u957f\u8fbe **128,000 token** \u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u3002 \ud83c\udfaf \u6838\u5fc3\u76ee\u6807 DeepSeek-R1 \u7684\u76ee\u6807\u662f\u8fbe\u5230\u6216\u8d85\u8d8a OpenAI \u7684 o1 \u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u4e16\u754c\u7ea7\u7684\u63a8\u7406\u80fd\u529b\u3002","title":"\ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb"},{"location":"deepseek/index-deepseek-r1/#_2","text":"\ud83d\ude80 \u63a8\u7406\u80fd\u529b\u5f3a\u52b2 \u5728\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u540c\u7c7b\u578b\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u903b\u8f91\u601d\u7ef4\u80fd\u529b\u3002 \u26a1 MoE \u67b6\u6784\u4f18\u52bf \u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u6bcf\u5c42\u4e2d\u4f7f\u7528\u5927\u91cf\u7684\u4e13\u5bb6\u6765\u5904\u7406\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u548c\u5904\u7406\u6548\u7387\u3002 \ud83d\udcda \u957f\u4e0a\u4e0b\u6587\u652f\u6301 \u53ef\u4ee5\u5904\u7406\u66f4\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\uff08128,000 token\uff09\uff0c\u8fd9\u5bf9\u4e8e\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u548c\u957f\u6587\u6863\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002 \ud83d\udd13 \u5b8c\u5168\u5f00\u6e90 \u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u5c06 DeepSeek-R1 \u7684\u8bad\u7ec3\u6280\u672f\u548c\u6a21\u578b\u6743\u91cd\u5b8c\u5168\u5f00\u6e90\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u63a2\u7d22\u548c\u7814\u7a76\u3002 \ud83c\udf93 \u5f00\u6e90\u84b8\u998f\u6a21\u578b \u901a\u8fc7\u84b8\u998f\u6280\u672f\uff0c\u751f\u6210\u4e86 6 \u4e2a\u5c0f\u6a21\u578b\uff08\u5982 Qwen2.5 \u548c Llama3.1\uff09\u4f9b\u793e\u533a\u4f7f\u7528\uff0c\u964d\u4f4e\u90e8\u7f72\u95e8\u69db\u3002","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"deepseek/index-deepseek-r1/#_3","text":"","title":"\ud83c\udf93 \u84b8\u998f\u6a21\u578b\u751f\u6001"},{"location":"deepseek/index-deepseek-r1/#_4","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"deepseek/index-deepseek-r1/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"deepseek/index-deepseek-r1/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"deepseek/index-deepseek-r1/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"deepseek/index-deepseek-r1/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"deepseek/index-deepseek-v3-en/","text":"\ud83d\ude80 DeepSeek-V3 Large Language Model 671 Billion Parameter Mixture of Experts Model - New Milestone for Open Source AI \ud83c\udfaf Product Overview DeepSeek-V3 is a powerful Mixture of Experts (MoE) language model with a total of **671 billion parameters**, of which **37 billion parameters** are activated per token. To achieve efficient inference and economical training costs, DeepSeek-V3 adopts Multi-Head Latent Attention (MLA) and DeepSeekMoE architectures, which have been thoroughly validated in DeepSeek-V2. \ud83c\udfc6 Breakthrough Achievement DeepSeek-V3 outperforms other open source models and achieves performance comparable to leading closed source models, marking a major breakthrough in open source AI technology. \ud83d\udcca Core Parameters Parameter Type Value Description Total Parameters 671 Billion Ultra-large scale parameters Activated Parameters 37 Billion/token Efficient inference design Training Data 14.8 Trillion tokens Diverse and high-quality data Training Cost 2.788M H800 hours Cost-effective training \ud83c\udfd7\ufe0f Technical Architecture \ud83e\udde0 Multi-Head Latent Attention (MLA) Adopts advanced MLA architecture, thoroughly validated in DeepSeek-V2, providing efficient attention mechanisms and excellent performance. \u26a1 DeepSeekMoE Architecture Mixture of experts model architecture that achieves efficient parameter utilization while maintaining powerful capabilities and significantly reducing inference costs. \u2696\ufe0f Auxiliary-Loss-Free Load Balancing Pioneering innovative load balancing strategy that achieves balanced load distribution among experts without auxiliary losses. \ud83c\udfaf Multi-Token Prediction Training Sets multi-token prediction training objectives, significantly improving model prediction accuracy and generation quality. \ud83d\udcc8 Performance Results \ud83c\udfc6 Competitive Comparison \ud83e\udd47 Leading Open Source Models Best performance among all open source large language models, setting new performance benchmarks \u26a1 Matching Closed Source Models Performance comparable to leading closed source models, bridging the performance gap between open and closed source \ud83d\udca1 Comprehensive Evaluation Validation Excellent performance across multiple dimensions through comprehensive benchmark evaluations \ud83d\udd27 High Inference Efficiency MoE architecture ensures efficient inference, significantly reducing actual deployment costs \ud83c\udfaf Application Value \u2705 Core Advantages Excellent Performance : Matches top-tier closed source models Controllable Costs : Reasonable training and inference costs Advanced Architecture : MoE + MLA dual optimization Stable Training : Zero-failure training completion \ud83d\ude80 Application Scenarios Enterprise AI Services : High-performance intelligent applications Research Institutions : Cutting-edge AI research platforms Developer Community : Open source AI infrastructure Commercial Deployment : Optimal cost-benefit solutions \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83d\ude80 DeepSeek-V3 | 671 Billion Parameters, New Milestone for Open Source AI","title":"Index deepseek v3 en"},{"location":"deepseek/index-deepseek-v3-en/#product-overview","text":"DeepSeek-V3 is a powerful Mixture of Experts (MoE) language model with a total of **671 billion parameters**, of which **37 billion parameters** are activated per token. To achieve efficient inference and economical training costs, DeepSeek-V3 adopts Multi-Head Latent Attention (MLA) and DeepSeekMoE architectures, which have been thoroughly validated in DeepSeek-V2. \ud83c\udfc6 Breakthrough Achievement DeepSeek-V3 outperforms other open source models and achieves performance comparable to leading closed source models, marking a major breakthrough in open source AI technology.","title":"\ud83c\udfaf Product Overview"},{"location":"deepseek/index-deepseek-v3-en/#core-parameters","text":"Parameter Type Value Description Total Parameters 671 Billion Ultra-large scale parameters Activated Parameters 37 Billion/token Efficient inference design Training Data 14.8 Trillion tokens Diverse and high-quality data Training Cost 2.788M H800 hours Cost-effective training","title":"\ud83d\udcca Core Parameters"},{"location":"deepseek/index-deepseek-v3-en/#technical-architecture","text":"\ud83e\udde0 Multi-Head Latent Attention (MLA) Adopts advanced MLA architecture, thoroughly validated in DeepSeek-V2, providing efficient attention mechanisms and excellent performance. \u26a1 DeepSeekMoE Architecture Mixture of experts model architecture that achieves efficient parameter utilization while maintaining powerful capabilities and significantly reducing inference costs. \u2696\ufe0f Auxiliary-Loss-Free Load Balancing Pioneering innovative load balancing strategy that achieves balanced load distribution among experts without auxiliary losses. \ud83c\udfaf Multi-Token Prediction Training Sets multi-token prediction training objectives, significantly improving model prediction accuracy and generation quality.","title":"\ud83c\udfd7\ufe0f Technical Architecture"},{"location":"deepseek/index-deepseek-v3-en/#performance-results","text":"","title":"\ud83d\udcc8 Performance Results"},{"location":"deepseek/index-deepseek-v3-en/#application-value","text":"","title":"\ud83c\udfaf Application Value"},{"location":"deepseek/index-deepseek-v3-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"deepseek/index-deepseek-v3-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"deepseek/index-deepseek-v3-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"deepseek/index-deepseek-v3-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"deepseek/index-deepseek-v3-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"deepseek/index-deepseek-v3/","text":"\ud83d\ude80 DeepSeek-V3 \u5927\u8bed\u8a00\u6a21\u578b 6710 \u4ebf\u53c2\u6570\u4e13\u5bb6\u6df7\u5408\u6a21\u578b - \u5f00\u6e90 AI \u7684\u65b0\u91cc\u7a0b\u7891 \ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb DeepSeek-V3 \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u8bed\u8a00\u6a21\u578b\uff0c\u603b\u53c2\u6570\u4e3a **6710 \u4ebf**\uff0c\u5176\u4e2d\u6bcf\u4e2a token \u6fc0\u6d3b **370 \u4ebf\u4e2a\u53c2\u6570**\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u548c\u7ecf\u6d4e\u7684\u8bad\u7ec3\u6210\u672c\uff0cDeepSeek-V3 \u91c7\u7528\u4e86\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u548c DeepSeekMoE \u67b6\u6784\uff0c\u8fd9\u4e9b\u67b6\u6784\u5728 DeepSeek-V2 \u4e2d\u5df2\u7ecf\u7ecf\u8fc7\u5f7b\u5e95\u9a8c\u8bc1\u3002 \ud83c\udfc6 \u7a81\u7834\u6027\u6210\u5c31 DeepSeek-V3 \u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u9886\u5148\u7684\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u6807\u5fd7\u7740\u5f00\u6e90 AI \u6280\u672f\u7684\u91cd\u5927\u7a81\u7834\u3002 \ud83d\udcca \u6838\u5fc3\u53c2\u6570 \u53c2\u6570\u7c7b\u578b \u6570\u503c \u8bf4\u660e \u603b\u53c2\u6570\u91cf 6710 \u4ebf \u8d85\u5927\u89c4\u6a21\u53c2\u6570\u91cf \u6fc0\u6d3b\u53c2\u6570 370 \u4ebf/token \u9ad8\u6548\u63a8\u7406\u8bbe\u8ba1 \u8bad\u7ec3\u6570\u636e 14.8 \u4e07\u4ebf tokens \u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u6570\u636e \u8bad\u7ec3\u6210\u672c 2.788M H800 \u5c0f\u65f6 \u7ecf\u6d4e\u9ad8\u6548\u7684\u8bad\u7ec3 \ud83c\udfd7\ufe0f \u6280\u672f\u67b6\u6784 \ud83e\udde0 \u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09 \u91c7\u7528\u5148\u8fdb\u7684 MLA \u67b6\u6784\uff0c\u5728 DeepSeek-V2 \u4e2d\u5df2\u7ecf\u8fc7\u5f7b\u5e95\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002 \u26a1 DeepSeekMoE \u67b6\u6784 \u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u5229\u7528\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002 \u2696\ufe0f \u65e0\u8f85\u52a9\u635f\u5931\u8d1f\u8f7d\u5e73\u8861 \u7387\u5148\u91c7\u7528\u521b\u65b0\u7684\u8d1f\u8f7d\u5e73\u8861\u7b56\u7565\uff0c\u65e0\u9700\u8f85\u52a9\u635f\u5931\u5373\u53ef\u5b9e\u73b0\u4e13\u5bb6\u95f4\u7684\u5747\u8861\u8d1f\u8f7d\u5206\u914d\u3002 \ud83c\udfaf \u591a Token \u9884\u6d4b\u8bad\u7ec3 \u8bbe\u5b9a\u591a token \u9884\u6d4b\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002 \ud83d\udcc8 \u6027\u80fd\u8868\u73b0 \ud83c\udfc6 \u7ade\u4e89\u529b\u5bf9\u6bd4 \ud83e\udd47 \u5f00\u6e90\u6a21\u578b\u9886\u5148 \u5728\u6240\u6709\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u6811\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746 \u26a1 \u95ed\u6e90\u6a21\u578b\u5339\u654c \u4e0e\u9886\u5148\u7684\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u6253\u7834\u5f00\u6e90\u4e0e\u95ed\u6e90\u7684\u6027\u80fd\u5dee\u8ddd \ud83d\udca1 \u5168\u9762\u8bc4\u4f30\u9a8c\u8bc1 \u901a\u8fc7\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u5747\u8868\u73b0\u5353\u8d8a \ud83d\udd27 \u63a8\u7406\u6548\u7387\u9ad8 MoE \u67b6\u6784\u786e\u4fdd\u9ad8\u6548\u63a8\u7406\uff0c\u5927\u5e45\u964d\u4f4e\u5b9e\u9645\u90e8\u7f72\u6210\u672c \ud83c\udfaf \u5e94\u7528\u4ef7\u503c \u2705 \u6838\u5fc3\u4f18\u52bf \u6027\u80fd\u5353\u8d8a \uff1a\u5339\u654c\u9876\u7ea7\u95ed\u6e90\u6a21\u578b \u6210\u672c\u53ef\u63a7 \uff1a\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u5408\u7406 \u67b6\u6784\u5148\u8fdb \uff1aMoE + MLA \u53cc\u91cd\u4f18\u5316 \u8bad\u7ec3\u7a33\u5b9a \uff1a\u96f6\u6545\u969c\u5b8c\u6210\u8bad\u7ec3 \ud83d\ude80 \u5e94\u7528\u573a\u666f \u4f01\u4e1a\u7ea7 AI \u670d\u52a1 \uff1a\u9ad8\u6027\u80fd\u667a\u80fd\u5e94\u7528 \u79d1\u7814\u673a\u6784 \uff1a\u524d\u6cbf AI \u7814\u7a76\u5e73\u53f0 \u5f00\u53d1\u8005\u793e\u533a \uff1a\u5f00\u6e90 AI \u57fa\u7840\u8bbe\u65bd \u5546\u4e1a\u5316\u90e8\u7f72 \uff1a\u6210\u672c\u6548\u76ca\u6700\u4f18\u89e3 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83d\ude80 DeepSeek-V3 | 6710 \u4ebf\u53c2\u6570\uff0c\u5f00\u6e90 AI \u7684\u65b0\u91cc\u7a0b\u7891","title":"Index deepseek v3"},{"location":"deepseek/index-deepseek-v3/#_1","text":"DeepSeek-V3 \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u8bed\u8a00\u6a21\u578b\uff0c\u603b\u53c2\u6570\u4e3a **6710 \u4ebf**\uff0c\u5176\u4e2d\u6bcf\u4e2a token \u6fc0\u6d3b **370 \u4ebf\u4e2a\u53c2\u6570**\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u548c\u7ecf\u6d4e\u7684\u8bad\u7ec3\u6210\u672c\uff0cDeepSeek-V3 \u91c7\u7528\u4e86\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u548c DeepSeekMoE \u67b6\u6784\uff0c\u8fd9\u4e9b\u67b6\u6784\u5728 DeepSeek-V2 \u4e2d\u5df2\u7ecf\u7ecf\u8fc7\u5f7b\u5e95\u9a8c\u8bc1\u3002 \ud83c\udfc6 \u7a81\u7834\u6027\u6210\u5c31 DeepSeek-V3 \u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u9886\u5148\u7684\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u6807\u5fd7\u7740\u5f00\u6e90 AI \u6280\u672f\u7684\u91cd\u5927\u7a81\u7834\u3002","title":"\ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb"},{"location":"deepseek/index-deepseek-v3/#_2","text":"\u53c2\u6570\u7c7b\u578b \u6570\u503c \u8bf4\u660e \u603b\u53c2\u6570\u91cf 6710 \u4ebf \u8d85\u5927\u89c4\u6a21\u53c2\u6570\u91cf \u6fc0\u6d3b\u53c2\u6570 370 \u4ebf/token \u9ad8\u6548\u63a8\u7406\u8bbe\u8ba1 \u8bad\u7ec3\u6570\u636e 14.8 \u4e07\u4ebf tokens \u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u6570\u636e \u8bad\u7ec3\u6210\u672c 2.788M H800 \u5c0f\u65f6 \u7ecf\u6d4e\u9ad8\u6548\u7684\u8bad\u7ec3","title":"\ud83d\udcca \u6838\u5fc3\u53c2\u6570"},{"location":"deepseek/index-deepseek-v3/#_3","text":"\ud83e\udde0 \u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09 \u91c7\u7528\u5148\u8fdb\u7684 MLA \u67b6\u6784\uff0c\u5728 DeepSeek-V2 \u4e2d\u5df2\u7ecf\u8fc7\u5f7b\u5e95\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002 \u26a1 DeepSeekMoE \u67b6\u6784 \u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u5229\u7528\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002 \u2696\ufe0f \u65e0\u8f85\u52a9\u635f\u5931\u8d1f\u8f7d\u5e73\u8861 \u7387\u5148\u91c7\u7528\u521b\u65b0\u7684\u8d1f\u8f7d\u5e73\u8861\u7b56\u7565\uff0c\u65e0\u9700\u8f85\u52a9\u635f\u5931\u5373\u53ef\u5b9e\u73b0\u4e13\u5bb6\u95f4\u7684\u5747\u8861\u8d1f\u8f7d\u5206\u914d\u3002 \ud83c\udfaf \u591a Token \u9884\u6d4b\u8bad\u7ec3 \u8bbe\u5b9a\u591a token \u9884\u6d4b\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002","title":"\ud83c\udfd7\ufe0f \u6280\u672f\u67b6\u6784"},{"location":"deepseek/index-deepseek-v3/#_4","text":"","title":"\ud83d\udcc8 \u6027\u80fd\u8868\u73b0"},{"location":"deepseek/index-deepseek-v3/#_5","text":"","title":"\ud83c\udfaf \u5e94\u7528\u4ef7\u503c"},{"location":"deepseek/index-deepseek-v3/#_6","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"deepseek/index-deepseek-v3/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"deepseek/index-deepseek-v3/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"deepseek/index-deepseek-v3/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"deepseek/index-deepseek-v3/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"deepseek-ocr/index-deepseek-ocr-en/","text":"Introduction DeepSeek-OCR is an advanced Optical Character Recognition (OCR) model developed by DeepSeek, specifically designed for extracting and recognizing text content from images. The model is based on deep learning technology, supports multiple languages and text formats, and can accurately recognize text in images and convert it to editable text format. Official Links ComputeNest Deployment : https://computenest.console.aliyun.com/ai-lab/model/cn-hangzhou/DeepSeek-OCR GitHub Repository : https://github.com/deepseek-ai/DeepSeek-OCR Official Website : https://deepseek-ocr.io/ Model Download : https://huggingface.co/deepseek-ai/DeepSeek-OCR Technical Documentation : https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html Paper Link : https://arxiv.org/html/2510.18234v1 Usage Instructions After completing model deployment, you can see the model usage methods in the ComputeNest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey information. API Calling Methods Curl Command The basic structure of API calls is as follows: Parameter Description: - ${ServerIP} : IP address from internal or public network address - ${ApiKey} : ApiKey provided on the page - ${ModelName} : Model name Image Format Support: - HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png - Base64 Encoding : data:image/jpeg;base64,<base64 encoded image> curl -X POST http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image>\" } }, { \"type\": \"text\", \"text\": \"Please recognize the text content in the image\" } ] } ] }' Python SDK Configuration Instructions: - ${ApiKey} : Fill in the ApiKey from the page - ${ServerUrl} : Fill in the public or internal network address from the page, need to add /v1 import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_image_to_base64(image_url: str) -> str: \"\"\"Convert image URL to base64 encoding\"\"\" response = requests.get(image_url) return base64.b64encode(response.content).decode('utf-8') def ocr_image(image_url: str, prompt: str = \"Please recognize the text content in the image\"): \"\"\"OCR image recognition\"\"\" image_base64 = encode_image_to_base64(image_url) completion = client.chat.completions.create( model=model, messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": prompt }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{image_base64}\" } } ] } ], max_tokens=1000 ) return completion.choices[0].message.content # Usage example if __name__ == \"__main__\": image_url = \"https://example.com/document.jpg\" result = ocr_image(image_url) print(\"Recognition result:\", result) Web Application Access\u3010Coming Soon\u3011 Get Access Link : Click on the Web application link in the service instance overview page Start Using : Upload images in the model service Web page, and the system will automatically recognize text content in the images Get Results : After recognition is complete, you can copy or download the recognition results Usage Recommendations Image Quality : Recommend using clear, well-lit images for optimal recognition results Image Format : Supports common formats like JPG, PNG, PDF Text Size : Recommend moderate text size, avoid too small or too large text Background Interference : Use images with simple backgrounds to avoid complex background interference with recognition","title":"Index deepseek ocr en"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#introduction","text":"DeepSeek-OCR is an advanced Optical Character Recognition (OCR) model developed by DeepSeek, specifically designed for extracting and recognizing text content from images. The model is based on deep learning technology, supports multiple languages and text formats, and can accurately recognize text in images and convert it to editable text format.","title":"Introduction"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#official-links","text":"ComputeNest Deployment : https://computenest.console.aliyun.com/ai-lab/model/cn-hangzhou/DeepSeek-OCR GitHub Repository : https://github.com/deepseek-ai/DeepSeek-OCR Official Website : https://deepseek-ocr.io/ Model Download : https://huggingface.co/deepseek-ai/DeepSeek-OCR Technical Documentation : https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html Paper Link : https://arxiv.org/html/2510.18234v1","title":"Official Links"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#usage-instructions","text":"After completing model deployment, you can see the model usage methods in the ComputeNest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey information.","title":"Usage Instructions"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#api-calling-methods","text":"","title":"API Calling Methods"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#curl-command","text":"The basic structure of API calls is as follows: Parameter Description: - ${ServerIP} : IP address from internal or public network address - ${ApiKey} : ApiKey provided on the page - ${ModelName} : Model name Image Format Support: - HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png - Base64 Encoding : data:image/jpeg;base64,<base64 encoded image> curl -X POST http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image>\" } }, { \"type\": \"text\", \"text\": \"Please recognize the text content in the image\" } ] } ] }'","title":"Curl Command"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#python-sdk","text":"Configuration Instructions: - ${ApiKey} : Fill in the ApiKey from the page - ${ServerUrl} : Fill in the public or internal network address from the page, need to add /v1 import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_image_to_base64(image_url: str) -> str: \"\"\"Convert image URL to base64 encoding\"\"\" response = requests.get(image_url) return base64.b64encode(response.content).decode('utf-8') def ocr_image(image_url: str, prompt: str = \"Please recognize the text content in the image\"): \"\"\"OCR image recognition\"\"\" image_base64 = encode_image_to_base64(image_url) completion = client.chat.completions.create( model=model, messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": prompt }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{image_base64}\" } } ] } ], max_tokens=1000 ) return completion.choices[0].message.content # Usage example if __name__ == \"__main__\": image_url = \"https://example.com/document.jpg\" result = ocr_image(image_url) print(\"Recognition result:\", result)","title":"Python SDK"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#web-application-accesscoming-soon","text":"Get Access Link : Click on the Web application link in the service instance overview page Start Using : Upload images in the model service Web page, and the system will automatically recognize text content in the images Get Results : After recognition is complete, you can copy or download the recognition results","title":"Web Application Access\u3010Coming Soon\u3011"},{"location":"deepseek-ocr/index-deepseek-ocr-en/#usage-recommendations","text":"Image Quality : Recommend using clear, well-lit images for optimal recognition results Image Format : Supports common formats like JPG, PNG, PDF Text Size : Recommend moderate text size, avoid too small or too large text Background Interference : Use images with simple backgrounds to avoid complex background interference with recognition","title":"Usage Recommendations"},{"location":"gpt/index-gpt-en/","text":"\ud83d\ude80 GPT-OSS Open Source Language Models High-performance lightweight AI models with open innovation under Apache 2.0 license \ud83c\udfaf Core Highlights \ud83c\udfc6 Exceptional Performance GPT-OSS-120B : Matches OpenAI o4-mini performance GPT-OSS-20B : Achieves OpenAI o3-mini level Reasoning Advantage : Best performance among open models of similar scale Tool Integration : Powerful tool usage capabilities \ud83d\udcbb Hardware Friendly 120B Model : Runs on a single 80GB GPU 20B Model : Requires only 16GB memory edge devices Consumer Hardware : Optimized deployment costs Edge Computing : Local inference without cloud dependency \ud83d\udd2c Technical Features \ud83e\udde0 Advanced Training Technology Trained using reinforcement learning combined with techniques inspired by OpenAI's most advanced internal models (including o3 and other cutting-edge systems), ensuring exceptional performance in reasoning and tool usage. ### \ud83d\udcca Performance Benchmark Comparison Model Benchmark Model Hardware Requirements License GPT-OSS-120B OpenAI o4-mini Single 80GB GPU Apache 2.0 GPT-OSS-20B OpenAI o3-mini 16GB Memory Device Apache 2.0 \ud83c\udfaf Application Scenarios \ud83d\udcf1 On-Device Applications Mobile AI assistants Offline intelligent applications Edge computing scenarios \ud83d\udd27 Development & Research Rapid prototyping Local inference testing Low-cost experimentation \ud83c\udfe2 Enterprise Deployment Private deployment Data security protection Cost optimization \ud83c\udfc5 Benchmark Performance ### \ud83c\udfaf Core Capability Assessment \u2705 Tool Usage \u2705 Few-shot Function Calling \u2705 CoT Reasoning \u2705 Agent Evaluation \ud83c\udfc6 Outstanding Performance Strong performance in Tau-Bench agent evaluation suite and HealthBench testing, even surpassing proprietary models like OpenAI o1 and GPT-4o. \ud83d\udccb Technical Specifications \ud83d\udd27 GPT-OSS-120B Parameter Scale : 120 billion parameters Recommended Hardware : 80GB GPU Performance Benchmark : OpenAI o4-mini Use Cases : High-performance reasoning tasks \u26a1 GPT-OSS-20B Parameter Scale : 20 billion parameters Minimum Configuration : 16GB memory Performance Benchmark : OpenAI o3-mini Use Cases : Edge device deployment \ud83d\ude80 Getting Started \ud83d\udce6 Quick Deployment \ud83d\udcc4 License Advantages Apache 2.0 license provides maximum usage flexibility, supporting commercial applications, modifications, and distribution without license restrictions. \ud83d\udd13 Open Source & Free \ud83d\udcbc Commercial Friendly \ud83d\udd27 Customizable \ud83c\udf10 Community Support \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83d\ude80 GPT-OSS Series | The Future of Open AI, Powerful Performance Within Reach","title":"Index gpt en"},{"location":"gpt/index-gpt-en/#core-highlights","text":"","title":"\ud83c\udfaf Core Highlights"},{"location":"gpt/index-gpt-en/#technical-features","text":"\ud83e\udde0 Advanced Training Technology Trained using reinforcement learning combined with techniques inspired by OpenAI's most advanced internal models (including o3 and other cutting-edge systems), ensuring exceptional performance in reasoning and tool usage. ### \ud83d\udcca Performance Benchmark Comparison Model Benchmark Model Hardware Requirements License GPT-OSS-120B OpenAI o4-mini Single 80GB GPU Apache 2.0 GPT-OSS-20B OpenAI o3-mini 16GB Memory Device Apache 2.0","title":"\ud83d\udd2c Technical Features"},{"location":"gpt/index-gpt-en/#application-scenarios","text":"\ud83d\udcf1 On-Device Applications Mobile AI assistants Offline intelligent applications Edge computing scenarios \ud83d\udd27 Development & Research Rapid prototyping Local inference testing Low-cost experimentation \ud83c\udfe2 Enterprise Deployment Private deployment Data security protection Cost optimization","title":"\ud83c\udfaf Application Scenarios"},{"location":"gpt/index-gpt-en/#benchmark-performance","text":"### \ud83c\udfaf Core Capability Assessment \u2705 Tool Usage \u2705 Few-shot Function Calling \u2705 CoT Reasoning \u2705 Agent Evaluation \ud83c\udfc6 Outstanding Performance Strong performance in Tau-Bench agent evaluation suite and HealthBench testing, even surpassing proprietary models like OpenAI o1 and GPT-4o.","title":"\ud83c\udfc5 Benchmark Performance"},{"location":"gpt/index-gpt-en/#technical-specifications","text":"","title":"\ud83d\udccb Technical Specifications"},{"location":"gpt/index-gpt-en/#getting-started","text":"","title":"\ud83d\ude80 Getting Started"},{"location":"gpt/index-gpt-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"gpt/index-gpt-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"gpt/index-gpt-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"gpt/index-gpt-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"gpt/index-gpt-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"gpt/index-gpt-original/","text":"\u7b80\u4ecb \u6211\u4eec\u53d1\u5e03\u4e86 gpt-oss-120b \u548c gpt-oss-20b\u2014\u2014\u4e24\u6b3e\u6027\u80fd\u5353\u8d8a\u7684\u5f00\u653e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u5728\u4f4e\u6210\u672c\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u5b9e\u9645\u5e94\u7528\u6027\u80fd\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u7075\u6d3b\u7684 Apache 2.0 \u8bb8\u53ef\u8bc1\u4e0b\u63d0\u4f9b\uff0c\u4e0e\u540c\u7b49\u89c4\u6a21\u7684\u5f00\u653e\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5e76\u9488\u5bf9\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u9ad8\u6548\u90e8\u7f72\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5b83\u4eec\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e OpenAI \u6700\u5148\u8fdb\u5185\u90e8\u6a21\u578b\uff08\u5305\u62ec o3 \u53ca\u5176\u4ed6\u524d\u6cbf\u7cfb\u7edf\uff09\u6240\u542f\u53d1\u7684\u6280\u672f\u76f8\u7ed3\u5408\u8fdb\u884c\u8bad\u7ec3\u3002 Gpt-oss-120b \u6a21\u578b\u5728\u6838\u5fc3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e OpenAI o4-mini \u6a21\u578b\u51e0\u4e4e\u6301\u5e73\uff0c\u540c\u65f6\u80fd\u5728\u5355\u4e2a 80GB GPU \u4e0a\u9ad8\u6548\u8fd0\u884c\u3002Gpt-oss-20b \u6a21\u578b\u5728\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e OpenAI o3\u2011mini \u6a21\u578b\u53d6\u5f97\u7c7b\u4f3c\u7ed3\u679c\uff0c\u4e14\u53ef\u5728\u4ec5\u914d\u5907 16GB \u5185\u5b58\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u4f7f\u5176\u6210\u4e3a\u8bbe\u5907\u7aef\u5e94\u7528\u3001\u672c\u5730\u63a8\u7406\u6216\u65e0\u9700\u6602\u8d35\u57fa\u7840\u8bbe\u65bd\u7684\u5feb\u901f\u8fed\u4ee3\u7684\u7406\u60f3\u9009\u62e9\u3002\u8fd9\u4e24\u4e2a\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u3001\u5c11\u6837\u672c\u51fd\u6570\u8c03\u7528\u3001CoT\u63a8\u7406\uff08\u5982\u5728 Tau-Bench \u667a\u80fd\u4f53\u8bc4\u4f30\u5957\u4ef6\u4e2d\u7684\u7ed3\u679c\u6240\u793a\uff09\u4ee5\u53ca HealthBench \u6d4b\u8bd5\u4e2d\u8868\u73b0\u5f3a\u52b2\uff08\u751a\u81f3\u8d85\u8d8a\u4e86 OpenAI o1 \u548c GPT\u20114o \u7b49\u4e13\u6709\u6a21\u578b\uff09\u3002","title":"\u7b80\u4ecb"},{"location":"gpt/index-gpt-original/#_1","text":"\u6211\u4eec\u53d1\u5e03\u4e86 gpt-oss-120b \u548c gpt-oss-20b\u2014\u2014\u4e24\u6b3e\u6027\u80fd\u5353\u8d8a\u7684\u5f00\u653e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u5728\u4f4e\u6210\u672c\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u5b9e\u9645\u5e94\u7528\u6027\u80fd\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u7075\u6d3b\u7684 Apache 2.0 \u8bb8\u53ef\u8bc1\u4e0b\u63d0\u4f9b\uff0c\u4e0e\u540c\u7b49\u89c4\u6a21\u7684\u5f00\u653e\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5e76\u9488\u5bf9\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u9ad8\u6548\u90e8\u7f72\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5b83\u4eec\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e OpenAI \u6700\u5148\u8fdb\u5185\u90e8\u6a21\u578b\uff08\u5305\u62ec o3 \u53ca\u5176\u4ed6\u524d\u6cbf\u7cfb\u7edf\uff09\u6240\u542f\u53d1\u7684\u6280\u672f\u76f8\u7ed3\u5408\u8fdb\u884c\u8bad\u7ec3\u3002 Gpt-oss-120b \u6a21\u578b\u5728\u6838\u5fc3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e OpenAI o4-mini \u6a21\u578b\u51e0\u4e4e\u6301\u5e73\uff0c\u540c\u65f6\u80fd\u5728\u5355\u4e2a 80GB GPU \u4e0a\u9ad8\u6548\u8fd0\u884c\u3002Gpt-oss-20b \u6a21\u578b\u5728\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e OpenAI o3\u2011mini \u6a21\u578b\u53d6\u5f97\u7c7b\u4f3c\u7ed3\u679c\uff0c\u4e14\u53ef\u5728\u4ec5\u914d\u5907 16GB \u5185\u5b58\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u4f7f\u5176\u6210\u4e3a\u8bbe\u5907\u7aef\u5e94\u7528\u3001\u672c\u5730\u63a8\u7406\u6216\u65e0\u9700\u6602\u8d35\u57fa\u7840\u8bbe\u65bd\u7684\u5feb\u901f\u8fed\u4ee3\u7684\u7406\u60f3\u9009\u62e9\u3002\u8fd9\u4e24\u4e2a\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u3001\u5c11\u6837\u672c\u51fd\u6570\u8c03\u7528\u3001CoT\u63a8\u7406\uff08\u5982\u5728 Tau-Bench \u667a\u80fd\u4f53\u8bc4\u4f30\u5957\u4ef6\u4e2d\u7684\u7ed3\u679c\u6240\u793a\uff09\u4ee5\u53ca HealthBench \u6d4b\u8bd5\u4e2d\u8868\u73b0\u5f3a\u52b2\uff08\u751a\u81f3\u8d85\u8d8a\u4e86 OpenAI o1 \u548c GPT\u20114o \u7b49\u4e13\u6709\u6a21\u578b\uff09\u3002","title":"\u7b80\u4ecb"},{"location":"gpt/index-gpt/","text":"\ud83d\ude80 GPT-OSS \u5f00\u6e90\u8bed\u8a00\u6a21\u578b \u9ad8\u6027\u80fd\u8f7b\u91cf\u7ea7AI\u6a21\u578b\uff0cApache 2.0\u8bb8\u53ef\u8bc1\u4e0b\u7684\u5f00\u653e\u521b\u65b0 \ud83c\udfaf \u6838\u5fc3\u4eae\u70b9 \ud83c\udfc6 \u5353\u8d8a\u6027\u80fd GPT-OSS-120B \uff1a\u4e0e OpenAI o4-mini \u6027\u80fd\u6301\u5e73 GPT-OSS-20B \uff1a\u8fbe\u5230 OpenAI o3-mini \u6c34\u51c6 \u63a8\u7406\u4f18\u52bf \uff1a\u540c\u7b49\u89c4\u6a21\u5f00\u653e\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18 \u5de5\u5177\u96c6\u6210 \uff1a\u5f3a\u5927\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b \ud83d\udcbb \u786c\u4ef6\u53cb\u597d 120B\u6a21\u578b \uff1a\u5355\u4e2a80GB GPU\u5373\u53ef\u8fd0\u884c 20B\u6a21\u578b \uff1a\u4ec5\u970016GB\u5185\u5b58\u8fb9\u7f18\u8bbe\u5907 \u6d88\u8d39\u7ea7\u786c\u4ef6 \uff1a\u4f18\u5316\u90e8\u7f72\u6210\u672c \u8fb9\u7f18\u8ba1\u7b97 \uff1a\u672c\u5730\u63a8\u7406\u65e0\u9700\u4e91\u7aef \ud83d\udd2c \u6280\u672f\u7279\u8272 \ud83e\udde0 \u5148\u8fdb\u8bad\u7ec3\u6280\u672f \u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408OpenAI\u6700\u5148\u8fdb\u5185\u90e8\u6a21\u578b\uff08\u5305\u62eco3\u53ca\u5176\u4ed6\u524d\u6cbf\u7cfb\u7edf\uff09\u542f\u53d1\u7684\u6280\u672f\u8fdb\u884c\u8bad\u7ec3\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u7684\u5353\u8d8a\u8868\u73b0\u3002 ### \ud83d\udcca \u6027\u80fd\u57fa\u51c6\u5bf9\u6bd4 \u6a21\u578b \u5bf9\u6807\u6a21\u578b \u786c\u4ef6\u9700\u6c42 \u8bb8\u53ef\u8bc1 GPT-OSS-120B OpenAI o4-mini \u5355\u4e2a80GB GPU Apache 2.0 GPT-OSS-20B OpenAI o3-mini 16GB\u5185\u5b58\u8bbe\u5907 Apache 2.0 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83d\ude80 GPT-OSS\u7cfb\u5217 | \u5f00\u653eAI\u7684\u672a\u6765\uff0c\u89e6\u624b\u53ef\u53ca\u7684\u5f3a\u5927\u6027\u80fd","title":"Index gpt"},{"location":"gpt/index-gpt/#_1","text":"","title":"\ud83c\udfaf \u6838\u5fc3\u4eae\u70b9"},{"location":"gpt/index-gpt/#_2","text":"\ud83e\udde0 \u5148\u8fdb\u8bad\u7ec3\u6280\u672f \u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408OpenAI\u6700\u5148\u8fdb\u5185\u90e8\u6a21\u578b\uff08\u5305\u62eco3\u53ca\u5176\u4ed6\u524d\u6cbf\u7cfb\u7edf\uff09\u542f\u53d1\u7684\u6280\u672f\u8fdb\u884c\u8bad\u7ec3\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u7684\u5353\u8d8a\u8868\u73b0\u3002 ### \ud83d\udcca \u6027\u80fd\u57fa\u51c6\u5bf9\u6bd4 \u6a21\u578b \u5bf9\u6807\u6a21\u578b \u786c\u4ef6\u9700\u6c42 \u8bb8\u53ef\u8bc1 GPT-OSS-120B OpenAI o4-mini \u5355\u4e2a80GB GPU Apache 2.0 GPT-OSS-20B OpenAI o3-mini 16GB\u5185\u5b58\u8bbe\u5907 Apache 2.0","title":"\ud83d\udd2c \u6280\u672f\u7279\u8272"},{"location":"gpt/index-gpt/#_3","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"gpt/index-gpt/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"gpt/index-gpt/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"gpt/index-gpt/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"gpt/index-gpt/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"hunyuan-video/doc/","text":"\ud83c\udfac ComfyUI \u6df7\u5143\u89c6\u9891\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u6587\u751f\u89c6\u9891\u4e0e\u56fe\u751f\u89c6\u9891\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848 \ud83d\udcdd \u6587\u751f\u89c6\u9891 \ud83d\uddbc\ufe0f \u56fe\u751f\u89c6\u9891 \u26a1 \u9ad8\u8d28\u91cf\u8f93\u51fa \ud83d\udccb \u6df7\u5143\u89c6\u9891\u6a21\u578b\u6982\u89c8 \u6df7\u5143\u89c6\u9891\uff08Hunyuan Video\uff09\u7cfb\u5217\u662f[\u817e\u8baf](https://huggingface.co/tencent)\u7814\u53d1\u5e76\u5f00\u6e90\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ee5\u6df7\u5408\u67b6\u6784\u4e3a\u6838\u5fc3\uff0c\u652f\u6301[\u6587\u672c\u751f\u6210\u89c6\u9891](https://github.com/Tencent/HunyuanVideo)\u548c[\u56fe\u751f\u6210\u89c6\u9891](https://github.com/Tencent/HunyuanVideo-I2V)\uff0c\u53c2\u6570\u89c4\u6a21\u8fbe 13B\u3002 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6587\u751f\u89c6\u9891 \uff1a GitHub - HunyuanVideo \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f \u6280\u672f\u7279\u70b9 \ud83e\udde0 \u6838\u5fc3\u67b6\u6784 \u91c7\u7528\u7c7b\u4f3cSora\u7684DiT\uff08Diffusion Transformer\uff09\u67b6\u6784\uff0c\u6709\u6548\u878d\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u52a8\u4f5c\u4fe1\u606f \ud83c\udfaf 3D VAE \u5c06\u89c6\u9891\u538b\u7f29\u5230\u7d27\u51d1\u7684\u6f5c\u7a7a\u95f4\uff0c\u4f7f\u56fe\u751f\u89c6\u9891\u7684\u751f\u6210\u66f4\u52a0\u9ad8\u6548 \ud83c\udfa8 \u5353\u8d8a\u5bf9\u9f50 \u4f7f\u7528MLLM\u6587\u672c\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u56fe\u50cf-\u89c6\u9891-\u6587\u672c\u7684\u7cbe\u51c6\u5bf9\u9f50 ### \u2728 \u4e3b\u8981\u4f18\u52bf \ud83c\udfaf \u9ad8\u8d28\u91cf\u751f\u6210 \u63d0\u9ad8\u751f\u6210\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3001\u8d28\u91cf\u548c\u5bf9\u9f50\u5ea6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5168\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u591a\u89c6\u89d2\u955c\u5934\u5207\u6362 \ud83c\udf10 \u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301\u4e2d\u82f1\u6587\u8f93\u5165\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9075\u5faa\u6587\u672c\u6307\u4ee4\uff0c\u6355\u6349\u7ec6\u8282\uff0c\u5e76\u8fdb\u884c\u590d\u6742\u63a8\u7406 \u26a1 \u9ad8\u6548\u751f\u6210 \u652f\u6301\u751f\u62105\u79d2\u9ad8\u8d28\u91cf\u77ed\u89c6\u9891\uff0c\u786e\u4fdd\u4e3b\u4f53\u4e00\u81f4\u6027\u548c\u6d41\u7545\u7684\u52a8\u4f5c\u8868\u73b0 \u26a0\ufe0f \u73af\u5883\u8981\u6c42 \ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c \u2022 \u5de5\u4f5c\u6d41\u56fe\u7247\u7684 Metadata \u4e2d\u5305\u542b\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41 JSON \u2022 \u672c\u6307\u5357\u7684\u5de5\u4f5c\u6d41\u53ef\u5728 ComfyUI \u7684\u5de5\u4f5c\u6d41\u6a21\u677f\u4e2d\u627e\u5230 \u2022 \u5982\u679c\u52a0\u8f7d\u5de5\u4f5c\u6d41\u65f6\u6709\u8282\u70b9\u7f3a\u5931\uff0c\u8bf7\u68c0\u67e5 ComfyUI \u7248\u672c\u6216\u8282\u70b9\u5bfc\u5165\u72b6\u6001 \ud83d\udce5 \u4e0b\u8f7d\u94fe\u63a5 ComfyUI \u4e0b\u8f7d ComfyUI \u66f4\u65b0\u6559\u7a0b \u5de5\u4f5c\u6d41\u6a21\u677f \ud83d\udd27 \u5e38\u89c1\u95ee\u9898 \u8282\u70b9\u7f3a\u5931\uff1a\u7248\u672c\u8fc7\u65e7\u6216\u5bfc\u5165\u5931\u8d25 \u529f\u80fd\u4e0d\u5168\uff1a\u4f7f\u7528\u7a33\u5b9a\u7248\u800c\u975e\u5f00\u53d1\u7248 \u52a0\u8f7d\u5931\u8d25\uff1a\u542f\u52a8\u65f6\u8282\u70b9\u5bfc\u5165\u5f02\u5e38 \ud83d\udca1 \u81ea\u52a8\u4e0b\u8f7d\u63d0\u793a \u5de5\u4f5c\u6d41\u56fe\u7247\u7684 Metadata \u4e2d\u5df2\u5305\u542b\u5bf9\u5e94\u6a21\u578b\u4e0b\u8f7d\u4fe1\u606f\uff0c\u76f4\u63a5\u62d6\u5165 ComfyUI \u4f1a\u63d0\u793a\u5b8c\u6210\u5bf9\u5e94\u7684\u6a21\u578b\u4e0b\u8f7d\u3002\u5982\u679c\u81ea\u52a8\u4e0b\u8f7d\u65e0\u6cd5\u5b8c\u6210\uff0c\u6240\u6709\u6a21\u578b\u53ef\u5728 \u8fd9\u91cc \u624b\u52a8\u4e0b\u8f7d\u3002 \ud83d\udcdd \u6df7\u5143\u6587\u751f\u89c6\u9891\u5de5\u4f5c\u6d41 \u6df7\u5143\u6587\u751f\u89c6\u9891\u5f00\u6e90\u4e8e 2024 \u5e74 12 \u6708\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210 5 \u79d2\u7684\u77ed\u89c6\u9891\uff0c\u652f\u6301\u4e2d\u82f1\u6587\u8f93\u5165\u3002 ### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41 \u6216\u8005\u76f4\u63a5\u901a\u8fc7\u5b98\u65b9\u6a21\u7248\u6253\u5f00 ![img_1.png](img_1.png) \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e #### \ud83d\udccb \u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 \u6587\u672c\u7f16\u7801\u5668 \u786e\u4fdd DualCLIPLoader \u4e2d\u52a0\u8f7d\u4e86\uff1a clip_name1: clip_l.safetensors clip_name2: llava_llama3_fp8_scaled.safetensors \ud83e\udde0 \u6269\u6563\u6a21\u578b \u786e\u4fdd Load Diffusion Model \u52a0\u8f7d\u4e86 hunyuan_video_t2v_720p_bf16.safetensors \ud83c\udfa8 VAE \u6a21\u578b \u786e\u4fdd Load VAE \u4e2d\u52a0\u8f7d\u4e86 hunyuan_video_vae_bf16.safetensors #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 \ud83d\udca1 \u751f\u6210\u63d0\u793a EmptyHunyuanLatentVideo \u8282\u70b9\u7684 length \u8bbe\u7f6e\u4e3a 1 \u65f6\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u9759\u6001\u56fe\u50cf\u3002 \ud83d\uddbc\ufe0f \u6df7\u5143\u56fe\u751f\u89c6\u9891\u5de5\u4f5c\u6d41 \u6df7\u5143\u56fe\u751f\u89c6\u9891\u6a21\u578b\u5f00\u6e90\u4e8e2025\u5e743\u67086\u65e5\uff0c\u57fa\u4e8e HunyuanVideo \u6846\u67b6\uff0c\u652f\u6301\u5c06\u9759\u6001\u56fe\u50cf\u8f6c\u5316\u4e3a\u6d41\u7545\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u540c\u65f6\u5f00\u653e\u4e86 LoRA \u8bad\u7ec3\u4ee3\u7801\uff0c\u652f\u6301\u5b9a\u5236\u7279\u6b8a\u89c6\u9891\u6548\u679c\u3002 ### \ud83c\udd9a \u7248\u672c\u5bf9\u6bd4 \u76ee\u524d\u6df7\u5143\u56fe\u751f\u89c6\u9891\u6a21\u578b\u5206\u4e3a\u4e24\u4e2a\u7248\u672c\uff1a v1 \"concat\" \u89c6\u9891\u7684\u8fd0\u52a8\u6d41\u7545\u6027\u8f83\u597d\uff0c\u4f46\u6bd4\u8f83\u5c11\u9075\u5faa\u56fe\u50cf\u5f15\u5bfc v2 \"replace\" \u56fe\u50cf\u7684\u5f15\u5bfc\u6027\u8f83\u597d\uff0c\u4f46\u76f8\u5bf9\u4e8e V1 \u7248\u672c\u4f3c\u4e4e\u4e0d\u90a3\u4e48\u6709\u6d3b\u529b ### \ud83c\udfaf v1 \"concat\" \u56fe\u751f\u89c6\u9891\u5de5\u4f5c\u6d41 #### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41\u548c\u7d20\u6750 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\u5de5\u4f5c\u6d41 \ud83d\uddbc\ufe0f \u8d77\u59cb\u5e27\u7d20\u6750 \u70b9\u51fb\u4e0b\u8f7d\u8d77\u59cb\u5e27\u56fe\u7247 #### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e #### \ud83d\udccb \u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 \u6587\u672c\u7f16\u7801\u5668 \u786e\u4fdd DualCLIPLoader \u4e2d\u52a0\u8f7d\u4e86\uff1a clip_name1: clip_l.safetensors clip_name2: llava_llama3_fp8_scaled.safetensors \ud83d\udc41\ufe0f \u89c6\u89c9\u7f16\u7801\u5668 \u786e\u4fdd Load CLIP Vision \u52a0\u8f7d\u4e86 llava_llama3_vision.safetensors \ud83e\udde0 \u6269\u6563\u6a21\u578b \u786e\u4fdd Load Diffusion Model \u52a0\u8f7d\u4e86 hunyuan_video_image_to_video_720p_bf16.safetensors ### \u26a1 v2 \"replace\" \u56fe\u751f\u89c6\u9891\u5de5\u4f5c\u6d41 #### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41\u548c\u7d20\u6750 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\u5de5\u4f5c\u6d41 \ud83d\uddbc\ufe0f \u8d77\u59cb\u5e27\u7d20\u6750 \u70b9\u51fb\u4e0b\u8f7d\u8d77\u59cb\u5e27\u56fe\u7247 #### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e #### \ud83d\udccb \u914d\u7f6e\u6b65\u9aa4 v2 \u7248\u672c\u7684\u914d\u7f6e\u4e0e v1 \u7248\u672c\u57fa\u672c\u76f8\u540c\uff0c\u4e3b\u8981\u533a\u522b\u5728\u4e8e\u6269\u6563\u6a21\u578b\uff1a \ud83e\udde0 \u6269\u6563\u6a21\u578b\u5dee\u5f02 \u786e\u4fdd Load Diffusion Model \u52a0\u8f7d\u4e86 hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors \ud83c\udfa8 \u521b\u610f\u793a\u4f8b\u4e0e\u63d0\u793a\u8bcd \u4ee5\u4e0b\u662f\u4e00\u4e9b\u793a\u4f8b\u56fe\u7247\u548c\u5bf9\u5e94\u7684\u63d0\u793a\u8bcd\uff0c\u4f60\u53ef\u4ee5\u57fa\u4e8e\u8fd9\u4e9b\u5185\u5bb9\u8fdb\u884c\u4fee\u6539\uff0c\u521b\u4f5c\u51fa\u5c5e\u4e8e\u4f60\u81ea\u5df1\u7684\u89c6\u9891\u3002 \ud83e\udd16 \u672a\u6765\u673a\u5668\u4eba Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene \u2694\ufe0f \u6b66\u58eb\u6218\u6597 Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic \ud83d\ude97 \u98de\u884c\u6c7d\u8f66 flying car fastly moving and flying through the city \ud83c\udfce\ufe0f \u8d5b\u535a\u670b\u514b\u8d5b\u8f66 cyberpunk car race in night city, dynamic, super fast, fast shot \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u4f7f\u7528\u6e05\u6670\u3001\u5177\u4f53\u7684\u52a8\u4f5c\u63cf\u8ff0\u8bcd \u56fe\u751f\u89c6\u9891\u65f6\u9009\u62e9\u4e3b\u4f53\u660e\u786e\u7684\u8d77\u59cb\u56fe\u50cf \u5408\u7406\u4f7f\u7528\u52a8\u6001\u5173\u952e\u8bcd\u5982\"fast motion\"\u3001\"dynamic\" \u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u7248\u672c\uff08v1\u6d41\u7545\u6027\u597d\uff0cv2\u5f15\u5bfc\u6027\u5f3a\uff09 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd\u6709\u8db3\u591f\u7684\u663e\u5b58\u8fd0\u884c\u6a21\u578b\uff08\u63a8\u835016GB+\uff09 \u751f\u6210\u65f6\u95f4\u8f83\u957f\uff0c\u8bf7\u8010\u5fc3\u7b49\u5f85 \u590d\u6742\u573a\u666f\u53ef\u80fd\u9700\u8981\u591a\u6b21\u5c1d\u8bd5\u4f18\u5316\u63d0\u793a\u8bcd \u9996\u6b21\u8fd0\u884c\u9700\u8981\u4e0b\u8f7d\u5927\u91cf\u6a21\u578b\u6587\u4ef6 \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 16GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4080 RTX 4090 / A100 ### \ud83d\udcf9 \u8f93\u51fa\u89c4\u683c 720p \u5206\u8fa8\u7387 5\u79d2\u89c6\u9891 \u4e2d\u82f1\u6587\u652f\u6301 \u9ad8\u5e27\u7387\u6d41\u7545 \ud83c\udfac ComfyUI \u6df7\u5143\u89c6\u9891\u751f\u6210 | \u6587\u751f\u89c6\u9891\u4e0e\u56fe\u751f\u89c6\u9891\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848","title":"Index"},{"location":"hunyuan-video/doc/#_1","text":"\u6df7\u5143\u89c6\u9891\uff08Hunyuan Video\uff09\u7cfb\u5217\u662f[\u817e\u8baf](https://huggingface.co/tencent)\u7814\u53d1\u5e76\u5f00\u6e90\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ee5\u6df7\u5408\u67b6\u6784\u4e3a\u6838\u5fc3\uff0c\u652f\u6301[\u6587\u672c\u751f\u6210\u89c6\u9891](https://github.com/Tencent/HunyuanVideo)\u548c[\u56fe\u751f\u6210\u89c6\u9891](https://github.com/Tencent/HunyuanVideo-I2V)\uff0c\u53c2\u6570\u89c4\u6a21\u8fbe 13B\u3002 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u6587\u751f\u89c6\u9891 \uff1a GitHub - HunyuanVideo \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f \u6280\u672f\u7279\u70b9 \ud83e\udde0","title":"\ud83d\udccb \u6df7\u5143\u89c6\u9891\u6a21\u578b\u6982\u89c8"},{"location":"hunyuan-video/doc/#_2","text":"\ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c \u2022 \u5de5\u4f5c\u6d41\u56fe\u7247\u7684 Metadata \u4e2d\u5305\u542b\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41 JSON \u2022 \u672c\u6307\u5357\u7684\u5de5\u4f5c\u6d41\u53ef\u5728 ComfyUI \u7684\u5de5\u4f5c\u6d41\u6a21\u677f\u4e2d\u627e\u5230 \u2022 \u5982\u679c\u52a0\u8f7d\u5de5\u4f5c\u6d41\u65f6\u6709\u8282\u70b9\u7f3a\u5931\uff0c\u8bf7\u68c0\u67e5 ComfyUI \u7248\u672c\u6216\u8282\u70b9\u5bfc\u5165\u72b6\u6001","title":"\u26a0\ufe0f \u73af\u5883\u8981\u6c42"},{"location":"hunyuan-video/doc/#_3","text":"\u6df7\u5143\u6587\u751f\u89c6\u9891\u5f00\u6e90\u4e8e 2024 \u5e74 12 \u6708\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210 5 \u79d2\u7684\u77ed\u89c6\u9891\uff0c\u652f\u6301\u4e2d\u82f1\u6587\u8f93\u5165\u3002 ### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41 \u6216\u8005\u76f4\u63a5\u901a\u8fc7\u5b98\u65b9\u6a21\u7248\u6253\u5f00 ![img_1.png](img_1.png) \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e #### \ud83d\udccb \u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udcdd \u6df7\u5143\u6587\u751f\u89c6\u9891\u5de5\u4f5c\u6d41"},{"location":"hunyuan-video/doc/#_4","text":"\u6df7\u5143\u56fe\u751f\u89c6\u9891\u6a21\u578b\u5f00\u6e90\u4e8e2025\u5e743\u67086\u65e5\uff0c\u57fa\u4e8e HunyuanVideo \u6846\u67b6\uff0c\u652f\u6301\u5c06\u9759\u6001\u56fe\u50cf\u8f6c\u5316\u4e3a\u6d41\u7545\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u540c\u65f6\u5f00\u653e\u4e86 LoRA \u8bad\u7ec3\u4ee3\u7801\uff0c\u652f\u6301\u5b9a\u5236\u7279\u6b8a\u89c6\u9891\u6548\u679c\u3002 ### \ud83c\udd9a \u7248\u672c\u5bf9\u6bd4 \u76ee\u524d\u6df7\u5143\u56fe\u751f\u89c6\u9891\u6a21\u578b\u5206\u4e3a\u4e24\u4e2a\u7248\u672c\uff1a","title":"\ud83d\uddbc\ufe0f \u6df7\u5143\u56fe\u751f\u89c6\u9891\u5de5\u4f5c\u6d41"},{"location":"hunyuan-video/doc/#_5","text":"\u4ee5\u4e0b\u662f\u4e00\u4e9b\u793a\u4f8b\u56fe\u7247\u548c\u5bf9\u5e94\u7684\u63d0\u793a\u8bcd\uff0c\u4f60\u53ef\u4ee5\u57fa\u4e8e\u8fd9\u4e9b\u5185\u5bb9\u8fdb\u884c\u4fee\u6539\uff0c\u521b\u4f5c\u51fa\u5c5e\u4e8e\u4f60\u81ea\u5df1\u7684\u89c6\u9891\u3002","title":"\ud83c\udfa8 \u521b\u610f\u793a\u4f8b\u4e0e\u63d0\u793a\u8bcd"},{"location":"hunyuan-video/doc/#_6","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"hunyuan-video/doc/#_7","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 16GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4080 RTX 4090 / A100 ### \ud83d\udcf9 \u8f93\u51fa\u89c4\u683c 720p \u5206\u8fa8\u7387 5\u79d2\u89c6\u9891 \u4e2d\u82f1\u6587\u652f\u6301 \u9ad8\u5e27\u7387\u6d41\u7545 \ud83c\udfac ComfyUI \u6df7\u5143\u89c6\u9891\u751f\u6210 | \u6587\u751f\u89c6\u9891\u4e0e\u56fe\u751f\u89c6\u9891\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"hunyuan-video/doc/index-en/","text":"\ud83c\udfac ComfyUI Hunyuan Video Generation ComfyUI Native Workflow - Complete Solution for Text-to-Video and Image-to-Video \ud83d\udcdd Text-to-Video \ud83d\uddbc\ufe0f Image-to-Video \u26a1 High-Quality Output \ud83d\udccb Hunyuan Video Model Overview The Hunyuan Video series is a video generation model developed and open-sourced by [Tencent](https://huggingface.co/tencent). Built on a hybrid architecture core, it supports both [text-to-video](https://github.com/Tencent/HunyuanVideo) and [image-to-video](https://github.com/Tencent/HunyuanVideo-I2V) generation with 13B parameters. \ud83d\udd17 Related Resources \u2022 Text-to-Video : GitHub - HunyuanVideo \u2022 Image-to-Video : GitHub - HunyuanVideo-I2V \u2022 Model Repository : \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f Technical Features \ud83e\udde0 Core Architecture Adopts Sora-like DiT (Diffusion Transformer) architecture, effectively fusing text, image, and motion information \ud83c\udfaf 3D VAE Compresses videos into compact latent space, making image-to-video generation more efficient \ud83c\udfa8 Excellent Alignment Uses MLLM text encoder for precise image-video-text alignment ### \u2728 Key Advantages \ud83c\udfaf High-Quality Generation Improves consistency, quality, and alignment between generated video frames through unified full attention mechanism for multi-view shot transitions \ud83c\udf10 Multi-Language Support Supports Chinese and English input, better follows text instructions, captures details, and performs complex reasoning \u26a1 Efficient Generation Supports generating 5-second high-quality short videos, ensuring subject consistency and smooth motion performance \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Workflow image metadata contains complete workflow JSON \u2022 Workflows in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading workflows, check ComfyUI version or node import status \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Workflow Templates \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83d\udca1 Automatic Download Tip Workflow image metadata contains corresponding model download information. Dragging directly into ComfyUI will prompt to complete model downloads. If automatic download fails, all models can be manually downloaded here . \ud83d\udcdd Hunyuan Text-to-Video Workflow Hunyuan text-to-video was open-sourced in December 2024, supporting generation of 5-second short videos through natural language descriptions, with Chinese and English input support. ### \ud83d\udce5 Step 1: Download Workflow you can open the template from the ComfyUI workflow templates. ![img.png](img.png) Click image to download, drag into ComfyUI to load workflow ### \ud83d\udd27 Step 2: Workflow Configuration #### \ud83d\udccb Configuration Steps \ud83d\udd27 Text Encoders Ensure DCLIPLoader has loaded: clip_name1: clip_l.safetensors clip_name2: llava_llama3_fp8_scaled.safetensors \ud83e\udde0 Diffusion Model Ensure Load Diffusion Model has loaded hunyuan_video_t2v_720p_bf16.safetensors \ud83c\udfa8 VAE Model Ensure Load VAE has loaded hunyuan_video_vae_bf16.safetensors #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click Queue button or use shortcut Ctrl(Cmd) + Enter to run workflow \ud83d\udca1 Generation Tip When the length in EmptyHunyuanLatentVideo node is set to 1, the model can generate static images. \ud83d\uddbc\ufe0f Hunyuan Image-to-Video Workflow The Hunyuan image-to-video model was open-sourced on March 6, 2025. Based on the HunyuanVideo framework, it supports converting static images into smooth, high-quality videos, and also provides LoRA training code for customizing special video effects. ### \ud83c\udd9a Version Comparison Currently, the Hunyuan image-to-video model has two versions: v1 \"concat\" Better video motion fluidity, but less adherence to image guidance v2 \"replace\" Better image guidance, but seems less dynamic compared to V1 version ### \ud83c\udfaf v1 \"concat\" Image-to-Video Workflow #### \ud83d\udce5 Step 1: Download Workflow and Materials Click image to download workflow \ud83d\uddbc\ufe0f Starting Frame Material Click to download starting frame image #### \ud83d\udd27 Step 2: Workflow Configuration #### \ud83d\udccb Configuration Steps \ud83d\udd27 Text Encoders Ensure DualCLIPLoader has loaded: clip_name1: clip_l.safetensors clip_name2: llava_llama3_fp8_scaled.safetensors \ud83d\udc41\ufe0f Vision Encoder Ensure Load CLIP Vision has loaded llava_llama3_vision.safetensors \ud83e\udde0 Diffusion Model Ensure Load Diffusion Model has loaded hunyuan_video_image_to_video_720p_bf16.safetensors ### \u26a1 v2 \"replace\" Image-to-Video Workflow #### \ud83d\udce5 Step 1: Download Workflow and Materials Click image to download workflow \ud83d\uddbc\ufe0f Starting Frame Material Click to download starting frame image #### \ud83d\udd27 Step 2: Workflow Configuration #### \ud83d\udccb Configuration Steps The v2 version configuration is basically the same as v1, with the main difference being the diffusion model: \ud83e\udde0 Diffusion Model Difference Ensure Load Diffusion Model has loaded hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors \ud83c\udfa8 Creative Examples and Prompts Here are some example images and corresponding prompts. You can modify these to create your own unique videos. \ud83e\udd16 Futuristic Robot Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene \u2694\ufe0f Samurai Battle Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic \ud83d\ude97 Flying Car flying car fastly moving and flying through the city \ud83c\udfce\ufe0f Cyberpunk Racing cyberpunk car race in night city, dynamic, super fast, fast shot \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Use clear, specific action description words Choose starting images with clear subjects for image-to-video Properly use dynamic keywords like \"fast motion\", \"dynamic\" Choose appropriate version based on needs (v1 for fluidity, v2 for guidance) \u26a0\ufe0f Important Notes Ensure sufficient VRAM to run models (recommended 16GB+) Generation takes time, please be patient Complex scenes may require multiple attempts to optimize prompts First run requires downloading large model files \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 16GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4080 RTX 4090 / A100 ### \ud83d\udcf9 Output Specifications 720p Resolution 5-Second Videos Chinese & English High Frame Rate \ud83c\udfac ComfyUI Hunyuan Video Generation | Complete Solution for Text-to-Video and Image-to-Video","title":"Index en"},{"location":"hunyuan-video/doc/index-en/#hunyuan-video-model-overview","text":"The Hunyuan Video series is a video generation model developed and open-sourced by [Tencent](https://huggingface.co/tencent). Built on a hybrid architecture core, it supports both [text-to-video](https://github.com/Tencent/HunyuanVideo) and [image-to-video](https://github.com/Tencent/HunyuanVideo-I2V) generation with 13B parameters. \ud83d\udd17 Related Resources \u2022 Text-to-Video : GitHub - HunyuanVideo \u2022 Image-to-Video : GitHub - HunyuanVideo-I2V \u2022 Model Repository : \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f Technical Features \ud83e\udde0","title":"\ud83d\udccb Hunyuan Video Model Overview"},{"location":"hunyuan-video/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Workflow image metadata contains complete workflow JSON \u2022 Workflows in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading workflows, check ComfyUI version or node import status","title":"\u26a0\ufe0f Environment Requirements"},{"location":"hunyuan-video/doc/index-en/#hunyuan-text-to-video-workflow","text":"Hunyuan text-to-video was open-sourced in December 2024, supporting generation of 5-second short videos through natural language descriptions, with Chinese and English input support. ### \ud83d\udce5 Step 1: Download Workflow you can open the template from the ComfyUI workflow templates. ![img.png](img.png) Click image to download, drag into ComfyUI to load workflow ### \ud83d\udd27 Step 2: Workflow Configuration #### \ud83d\udccb Configuration Steps","title":"\ud83d\udcdd Hunyuan Text-to-Video Workflow"},{"location":"hunyuan-video/doc/index-en/#hunyuan-image-to-video-workflow","text":"The Hunyuan image-to-video model was open-sourced on March 6, 2025. Based on the HunyuanVideo framework, it supports converting static images into smooth, high-quality videos, and also provides LoRA training code for customizing special video effects. ### \ud83c\udd9a Version Comparison Currently, the Hunyuan image-to-video model has two versions:","title":"\ud83d\uddbc\ufe0f Hunyuan Image-to-Video Workflow"},{"location":"hunyuan-video/doc/index-en/#creative-examples-and-prompts","text":"Here are some example images and corresponding prompts. You can modify these to create your own unique videos.","title":"\ud83c\udfa8 Creative Examples and Prompts"},{"location":"hunyuan-video/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"hunyuan-video/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 16GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4080 RTX 4090 / A100 ### \ud83d\udcf9 Output Specifications 720p Resolution 5-Second Videos Chinese & English High Frame Rate \ud83c\udfac ComfyUI Hunyuan Video Generation | Complete Solution for Text-to-Video and Image-to-Video","title":"\ud83d\udd27 Technical Specifications"},{"location":"hunyuan3d/2.0/doc/","text":"\ud83c\udfaf ComfyUI Hunyuan3D-2 3D \u8d44\u4ea7\u751f\u6210 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u4ece\u56fe\u50cf\u5230\u9ad8\u8d28\u91cf 3D \u6a21\u578b\u7684\u5b8c\u6574\u751f\u6210\u6d41\u7a0b \ud83c\udfa8 \u591a\u89c6\u89d2\u751f\u6210 \ud83c\udfac \u9ad8\u4fdd\u771f\u7eb9\u7406 \u26a1 \u8f7b\u91cf\u5316\u90e8\u7f72 \ud83d\udccb \u6df7\u51433D 2.0 \u6a21\u578b\u6982\u89c8 [\u6df7\u51433D 2.0](https://github.com/Tencent/Hunyuan3D-2) \u662f\u817e\u8baf\u63a8\u51fa\u7684\u5f00\u6e90 3D \u8d44\u4ea7\u751f\u6210\u6a21\u578b\uff0c\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u3001\u56fe\u50cf\u6216\u8349\u56fe\u751f\u6210\u5e26\u6709\u9ad8\u5206\u8fa8\u7387\u7eb9\u7406\u8d34\u56fe\u7684\u9ad8\u4fdd\u771f 3D \u6a21\u578b\u3002 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u9879\u76ee\u4ed3\u5e93 \uff1a GitHub - Hunyuan3D-2 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f \u4e24\u9636\u6bb5\u751f\u6210\u67b6\u6784 \u6df7\u51433D 2.0 \u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\uff0c\u6709\u6548\u5206\u79bb\u4e86\u5f62\u72b6\u548c\u7eb9\u7406\u751f\u6210\u7684\u590d\u6742\u6027\uff1a \ud83c\udfaf \u51e0\u4f55\u751f\u6210\u6a21\u578b\uff08Hunyuan3D-DiT\uff09 \u57fa\u4e8e\u6d41\u6269\u6563\u7684 Transformer \u67b6\u6784\uff0c\u751f\u6210\u65e0\u7eb9\u7406\u7684\u51e0\u4f55\u6a21\u578b\uff0c\u53ef\u7cbe\u51c6\u5339\u914d\u8f93\u5165\u6761\u4ef6 \ud83c\udfa8 \u7eb9\u7406\u751f\u6210\u6a21\u578b\uff08Hunyuan3D-Paint\uff09 \u7ed3\u5408\u51e0\u4f55\u6761\u4ef6\u548c\u591a\u89c6\u56fe\u6269\u6563\u6280\u672f\uff0c\u4e3a\u6a21\u578b\u6dfb\u52a0\u9ad8\u5206\u8fa8\u7387\u7eb9\u7406\uff0c\u652f\u6301 PBR \u6750\u8d28 ### \u2728 \u4e3b\u8981\u4f18\u52bf \ud83c\udfaf \u9ad8\u7cbe\u5ea6\u751f\u6210 \u51e0\u4f55\u7ed3\u6784\u9510\u5229\uff0c\u7eb9\u7406\u8272\u5f69\u4e30\u5bcc\uff0c\u652f\u6301 PBR \u6750\u8d28\u751f\u6210\uff0c\u5b9e\u73b0\u63a5\u8fd1\u771f\u5b9e\u7684\u5149\u5f71\u6548\u679c \ud83d\udee0\ufe0f \u591a\u6837\u5316\u4f7f\u7528\u65b9\u5f0f \u63d0\u4f9b\u4ee3\u7801\u8c03\u7528\u3001Blender \u63d2\u4ef6\u3001Gradio \u5e94\u7528\u53ca\u5b98\u7f51\u5728\u7ebf\u4f53\u9a8c\uff0c\u9002\u5408\u4e0d\u540c\u7528\u6237\u9700\u6c42 \u26a1 \u8f7b\u91cf\u5316\u4e0e\u517c\u5bb9\u6027 Mini \u6a21\u578b\u4ec5\u9700 5GB \u663e\u5b58\uff0c\u6807\u51c6\u7248\u672c\u5f62\u72b6\u751f\u6210\u9700 6GB \u663e\u5b58\uff0c\u5b8c\u6574\u6d41\u7a0b\u4ec5\u9700 12GB \u663e\u5b58 ### \ud83c\udd95 \u6700\u65b0\u66f4\u65b0 \ud83d\udcc5 2025\u5e743\u670818\u65e5\u66f4\u65b0 \u6df7\u51433D 2.0 \u65b0\u589e\u591a\u89c6\u89d2\u5f62\u72b6\u751f\u6210\u6a21\u578b\uff08Hunyuan3D-2mv\uff09\uff0c\u652f\u6301\u4ece\u4e0d\u540c\u89c6\u89d2\u8f93\u5165\u751f\u6210\u66f4\u7cbe\u7ec6\u7684\u51e0\u4f55\u7ed3\u6784\u3002 \ud83d\ude80 \u5de5\u4f5c\u6d41\u793a\u4f8b\u6982\u89c8 \u672c\u793a\u4f8b\u5305\u542b\u4e09\u4e2a\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41\uff0c\u6db5\u76d6\u4e0d\u540c\u7684\u4f7f\u7528\u573a\u666f\uff1a \u5747\u53ef\u4ece\u6a21\u7248\u627e\u5230 ![img.png](img.png) \ud83c\udfaf Hunyuan3D-2mv \u591a\u89c6\u89d2\u8f93\u5165\u751f\u6210\u9ad8\u7cbe\u5ea6 3D \u6a21\u578b \u26a1 Hunyuan3D-2mv-turbo \u5feb\u901f\u591a\u89c6\u89d2\u751f\u6210\uff0c\u5206\u6b65\u84b8\u998f\u4f18\u5316 \ud83d\uddbc\ufe0f Hunyuan3D-2 \u5355\u89c6\u56fe\u8f93\u5165\u7684\u6807\u51c6 3D \u751f\u6210 \u26a0\ufe0f \u73af\u5883\u8981\u6c42 \ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c\uff0c\u539f\u751f\u652f\u6301 Hunyuan3D-2mv \u2022 \u76ee\u524d\u6682\u672a\u652f\u6301\u7eb9\u7406\u548c\u6750\u8d28\u7684\u751f\u6210\u529f\u80fd \u2022 \u5de5\u4f5c\u6d41\u56fe\u7247\u7684 Metadata \u4e2d\u5305\u542b\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41 JSON \u2022 \u751f\u6210\u7684 .glb \u683c\u5f0f\u6a21\u578b\u5c06\u8f93\u51fa\u81f3 ComfyUI/output/mesh \u6587\u4ef6\u5939 \ud83d\udd27 \u5e38\u89c1\u95ee\u9898 \u8282\u70b9\u7f3a\u5931\uff1a\u7248\u672c\u8fc7\u65e7\u6216\u5bfc\u5165\u5931\u8d25 \u529f\u80fd\u4e0d\u5168\uff1a\u4f7f\u7528\u7a33\u5b9a\u7248\u800c\u975e\u5f00\u53d1\u7248 \u52a0\u8f7d\u5931\u8d25\uff1a\u542f\u52a8\u65f6\u8282\u70b9\u5bfc\u5165\u5f02\u5e38 \ud83c\udfaf \u5de5\u4f5c\u6d41\u4e00\uff1aHunyuan3D-2mv \u591a\u89c6\u89d2\u751f\u6210 Hunyuan3D-2mv \u5de5\u4f5c\u6d41\u652f\u6301\u4f7f\u7528\u591a\u89c6\u89d2\u56fe\u7247\u751f\u6210 3D \u6a21\u578b\u3002\u591a\u4e2a\u89c6\u89d2\u7684\u56fe\u7247\u5e76\u975e\u5fc5\u987b\uff0c\u4e5f\u53ef\u4ee5\u4ec5\u8f93\u5165 `front` \u89c6\u89d2\u7684\u56fe\u7247\u6765\u751f\u6210 3D \u6a21\u578b\u3002 ### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41\u548c\u7d20\u6750 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u591a\u89c6\u89d2\u8f93\u5165\u7d20\u6750 \ud83d\uddbc\ufe0f Front View \ud83d\uddbc\ufe0f Left View \ud83d\uddbc\ufe0f Back View \ud83d\udca1 \u7d20\u6750\u5904\u7406\u63d0\u793a \u793a\u4f8b\u4e2d\u7684\u8f93\u5165\u56fe\u7247\u5df2\u9884\u5904\u7406\u53bb\u9664\u80cc\u666f\u3002\u5b9e\u9645\u4f7f\u7528\u4e2d\uff0c\u53ef\u501f\u52a9 ComfyUI_essentials \u7b49\u81ea\u5b9a\u4e49\u8282\u70b9\u81ea\u52a8\u53bb\u9664\u80cc\u666f\u3002 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e #### \ud83d\udccb \u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 \u6a21\u578b\u52a0\u8f7d \u786e\u4fdd Image Only Checkpoint Loader \u8282\u70b9\u52a0\u8f7d\u4e86\u91cd\u547d\u540d\u540e\u7684 hunyuan3d-dit-v2-mv.safetensors \u6a21\u578b \ud83d\udcc1 \u56fe\u7247\u52a0\u8f7d \u5728\u5404\u4e2a Load Image \u8282\u70b9\u4e2d\u52a0\u8f7d\u5bf9\u5e94\u89c6\u89d2\u7684\u56fe\u7247 \ud83d\ude80 \u6267\u884c\u751f\u6210 \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl+Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 \ud83d\udca1 \u6269\u5c55\u89c6\u89d2 \u5982\u9700\u589e\u52a0\u66f4\u591a\u89c6\u89d2\uff0c\u8bf7\u786e\u4fdd Hunyuan3Dv2ConditioningMultiView \u8282\u70b9\u548c\u5bf9\u5e94\u7684 Load Image \u8282\u70b9\u90fd\u52a0\u8f7d\u4e86\u76f8\u5e94\u89c6\u89d2\u7684\u56fe\u7247\u3002 \u26a1 \u5de5\u4f5c\u6d41\u4e8c\uff1aHunyuan3D-2mv-turbo \u5feb\u901f\u751f\u6210 Hunyuan3D-2mv-turbo \u662f Hunyuan3D-2mv \u7684\u5206\u6b65\u84b8\u998f\uff08Step Distillation\uff09\u7248\u672c\uff0c\u53ef\u4ee5\u66f4\u5feb\u5730\u751f\u6210 3D \u6a21\u578b\u3002\u5728\u6b64\u7248\u672c\u4e2d\uff0c\u8bbe\u7f6e `cfg` \u4e3a 1.0 \u5e76\u6dfb\u52a0 `flux guidance` \u8282\u70b9\u6765\u63a7\u5236 `distilled cfg` \u7684\u751f\u6210\u3002 ### \ud83d\udcc1 \u591a\u89c6\u89d2\u8f93\u5165\u7d20\u6750 \ud83d\uddbc\ufe0f Front View \ud83d\uddbc\ufe0f Right View ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e #### \ud83d\udccb \u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 \u6a21\u578b\u52a0\u8f7d \u786e\u4fdd Image Only Checkpoint Loader \u8282\u70b9\u52a0\u8f7d\u4e86 hunyuan3d-dit-v2-mv-turbo.safetensors \u6a21\u578b \ud83d\udcc1 \u56fe\u7247\u52a0\u8f7d \u5728\u5404\u4e2a Load Image \u8282\u70b9\u4e2d\u52a0\u8f7d\u5bf9\u5e94\u89c6\u89d2\u7684\u56fe\u7247 \ud83d\ude80 \u6267\u884c\u751f\u6210 \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl+Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 \ud83d\uddbc\ufe0f \u5de5\u4f5c\u6d41\u4e09\uff1aHunyuan3D-2 \u5355\u89c6\u56fe\u751f\u6210 Hunyuan3D-2 \u5de5\u4f5c\u6d41\u4f7f\u7528\u5355\u89c6\u56fe\u8f93\u5165\u751f\u6210 3D \u6a21\u578b\u3002\u5728\u6b64\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f7f\u7528 `Hunyuan3Dv2Conditioning` \u8282\u70b9\u66ff\u6362 `Hunyuan3Dv2ConditioningMultiView` \u8282\u70b9\u3002 ### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41\u548c\u7d20\u6750 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u5355\u89c6\u56fe\u8f93\u5165\u7d20\u6750 \ud83d\uddbc\ufe0f \u8f93\u5165\u56fe\u50cf ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e #### \ud83d\udccb \u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 \u6a21\u578b\u52a0\u8f7d \u786e\u4fdd Image Only Checkpoint Loader \u8282\u70b9\u52a0\u8f7d\u4e86 hunyuan3d-dit-v2.safetensors \u6a21\u578b \ud83d\udcc1 \u56fe\u7247\u52a0\u8f7d \u5728 Load Image \u8282\u70b9\u4e2d\u52a0\u8f7d\u8f93\u5165\u56fe\u7247 \ud83d\ude80 \u6267\u884c\u751f\u6210 \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl+Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 \ud83d\udee0\ufe0f \u793e\u533a\u8d44\u6e90 \u4ee5\u4e0b\u662f Hunyuan3D-2 \u76f8\u5173\u7684 ComfyUI \u793e\u533a\u8d44\u6e90\uff1a \ud83d\udd27 ComfyUI-Hunyuan3DWrapper GitHub \u4ed3\u5e93 - \u5b8c\u6574\u7684 Hunyuan3D \u5305\u88c5\u5668 \ud83d\udce6 \u9884\u5904\u7406\u6a21\u578b Kijai/Hunyuan3D-2_safetensors - \u9884\u5904\u7406\u7684 safetensors \u683c\u5f0f\u6a21\u578b \ud83c\udfaf ComfyUI-3D-Pack GitHub \u4ed3\u5e93 - \u7efc\u5408 3D \u751f\u6210\u5de5\u5177\u5305 \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u4f7f\u7528\u53bb\u80cc\u666f\u7684\u9ad8\u8d28\u91cf\u8f93\u5165\u56fe\u50cf \u591a\u89c6\u89d2\u8f93\u5165\u65f6\u786e\u4fdd\u89c6\u89d2\u4e00\u81f4\u6027 \u6839\u636e\u663e\u5b58\u60c5\u51b5\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u7248\u672c \u4f18\u5148\u4f7f\u7528 turbo \u7248\u672c\u8fdb\u884c\u5feb\u901f\u9884\u89c8 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd ComfyUI \u7248\u672c\u4e3a\u6700\u65b0\u5f00\u53d1\u7248 \u8f93\u5165\u56fe\u50cf\u9700\u8981\u6e05\u6670\u7684\u4e3b\u4f53\u8f6e\u5ed3 \u591a\u89c6\u89d2\u56fe\u50cf\u5e94\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u6027 \u751f\u6210\u8fc7\u7a0b\u9700\u8981\u8db3\u591f\u7684\u663e\u5b58\u652f\u6301 \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u6a21\u578b\u7248\u672c \u663e\u5b58\u8981\u6c42 \u63a8\u8350\u914d\u7f6e Hunyuan3D-2mini 5GB RTX 3060 12GB Hunyuan3D-2 (\u5f62\u72b6\u751f\u6210) 6GB RTX 4060 Ti 16GB Hunyuan3D-2 (\u5b8c\u6574\u6d41\u7a0b) 12GB RTX 4070 Ti / RTX 4080 ### \ud83d\udcd0 \u652f\u6301\u7684\u8f93\u51fa\u683c\u5f0f .glb \u683c\u5f0f PBR \u6750\u8d28 \u9ad8\u5206\u8fa8\u7387\u7eb9\u7406 \ud83c\udfaf ComfyUI Hunyuan3D-2 3D \u8d44\u4ea7\u751f\u6210 | \u4ece\u56fe\u50cf\u5230\u9ad8\u8d28\u91cf 3D \u6a21\u578b\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848","title":"Index"},{"location":"hunyuan3d/2.0/doc/#3d-20","text":"[\u6df7\u51433D 2.0](https://github.com/Tencent/Hunyuan3D-2) \u662f\u817e\u8baf\u63a8\u51fa\u7684\u5f00\u6e90 3D \u8d44\u4ea7\u751f\u6210\u6a21\u578b\uff0c\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u3001\u56fe\u50cf\u6216\u8349\u56fe\u751f\u6210\u5e26\u6709\u9ad8\u5206\u8fa8\u7387\u7eb9\u7406\u8d34\u56fe\u7684\u9ad8\u4fdd\u771f 3D \u6a21\u578b\u3002 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u9879\u76ee\u4ed3\u5e93 \uff1a GitHub - Hunyuan3D-2 \u2022 \u6a21\u578b\u4ed3\u5e93 \uff1a \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f \u4e24\u9636\u6bb5\u751f\u6210\u67b6\u6784 \u6df7\u51433D 2.0 \u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\uff0c\u6709\u6548\u5206\u79bb\u4e86\u5f62\u72b6\u548c\u7eb9\u7406\u751f\u6210\u7684\u590d\u6742\u6027\uff1a","title":"\ud83d\udccb \u6df7\u51433D 2.0 \u6a21\u578b\u6982\u89c8"},{"location":"hunyuan3d/2.0/doc/#_1","text":"\u672c\u793a\u4f8b\u5305\u542b\u4e09\u4e2a\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41\uff0c\u6db5\u76d6\u4e0d\u540c\u7684\u4f7f\u7528\u573a\u666f\uff1a \u5747\u53ef\u4ece\u6a21\u7248\u627e\u5230 ![img.png](img.png) \ud83c\udfaf","title":"\ud83d\ude80 \u5de5\u4f5c\u6d41\u793a\u4f8b\u6982\u89c8"},{"location":"hunyuan3d/2.0/doc/#_2","text":"\ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c\uff0c\u539f\u751f\u652f\u6301 Hunyuan3D-2mv \u2022 \u76ee\u524d\u6682\u672a\u652f\u6301\u7eb9\u7406\u548c\u6750\u8d28\u7684\u751f\u6210\u529f\u80fd \u2022 \u5de5\u4f5c\u6d41\u56fe\u7247\u7684 Metadata \u4e2d\u5305\u542b\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41 JSON \u2022 \u751f\u6210\u7684 .glb \u683c\u5f0f\u6a21\u578b\u5c06\u8f93\u51fa\u81f3 ComfyUI/output/mesh \u6587\u4ef6\u5939","title":"\u26a0\ufe0f \u73af\u5883\u8981\u6c42"},{"location":"hunyuan3d/2.0/doc/#hunyuan3d-2mv","text":"Hunyuan3D-2mv \u5de5\u4f5c\u6d41\u652f\u6301\u4f7f\u7528\u591a\u89c6\u89d2\u56fe\u7247\u751f\u6210 3D \u6a21\u578b\u3002\u591a\u4e2a\u89c6\u89d2\u7684\u56fe\u7247\u5e76\u975e\u5fc5\u987b\uff0c\u4e5f\u53ef\u4ee5\u4ec5\u8f93\u5165 `front` \u89c6\u89d2\u7684\u56fe\u7247\u6765\u751f\u6210 3D \u6a21\u578b\u3002 ### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41\u548c\u7d20\u6750 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u591a\u89c6\u89d2\u8f93\u5165\u7d20\u6750","title":"\ud83c\udfaf \u5de5\u4f5c\u6d41\u4e00\uff1aHunyuan3D-2mv \u591a\u89c6\u89d2\u751f\u6210"},{"location":"hunyuan3d/2.0/doc/#hunyuan3d-2mv-turbo","text":"Hunyuan3D-2mv-turbo \u662f Hunyuan3D-2mv \u7684\u5206\u6b65\u84b8\u998f\uff08Step Distillation\uff09\u7248\u672c\uff0c\u53ef\u4ee5\u66f4\u5feb\u5730\u751f\u6210 3D \u6a21\u578b\u3002\u5728\u6b64\u7248\u672c\u4e2d\uff0c\u8bbe\u7f6e `cfg` \u4e3a 1.0 \u5e76\u6dfb\u52a0 `flux guidance` \u8282\u70b9\u6765\u63a7\u5236 `distilled cfg` \u7684\u751f\u6210\u3002 ### \ud83d\udcc1 \u591a\u89c6\u89d2\u8f93\u5165\u7d20\u6750","title":"\u26a1 \u5de5\u4f5c\u6d41\u4e8c\uff1aHunyuan3D-2mv-turbo \u5feb\u901f\u751f\u6210"},{"location":"hunyuan3d/2.0/doc/#hunyuan3d-2","text":"Hunyuan3D-2 \u5de5\u4f5c\u6d41\u4f7f\u7528\u5355\u89c6\u56fe\u8f93\u5165\u751f\u6210 3D \u6a21\u578b\u3002\u5728\u6b64\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f7f\u7528 `Hunyuan3Dv2Conditioning` \u8282\u70b9\u66ff\u6362 `Hunyuan3Dv2ConditioningMultiView` \u8282\u70b9\u3002 ### \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u4e0b\u8f7d\u5de5\u4f5c\u6d41\u548c\u7d20\u6750 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u5355\u89c6\u56fe\u8f93\u5165\u7d20\u6750","title":"\ud83d\uddbc\ufe0f \u5de5\u4f5c\u6d41\u4e09\uff1aHunyuan3D-2 \u5355\u89c6\u56fe\u751f\u6210"},{"location":"hunyuan3d/2.0/doc/#_3","text":"\u4ee5\u4e0b\u662f Hunyuan3D-2 \u76f8\u5173\u7684 ComfyUI \u793e\u533a\u8d44\u6e90\uff1a \ud83d\udd27 ComfyUI-Hunyuan3DWrapper GitHub \u4ed3\u5e93 - \u5b8c\u6574\u7684 Hunyuan3D \u5305\u88c5\u5668 \ud83d\udce6 \u9884\u5904\u7406\u6a21\u578b Kijai/Hunyuan3D-2_safetensors - \u9884\u5904\u7406\u7684 safetensors \u683c\u5f0f\u6a21\u578b \ud83c\udfaf ComfyUI-3D-Pack GitHub \u4ed3\u5e93 - \u7efc\u5408 3D \u751f\u6210\u5de5\u5177\u5305","title":"\ud83d\udee0\ufe0f \u793e\u533a\u8d44\u6e90"},{"location":"hunyuan3d/2.0/doc/#_4","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"hunyuan3d/2.0/doc/#_5","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u6a21\u578b\u7248\u672c \u663e\u5b58\u8981\u6c42 \u63a8\u8350\u914d\u7f6e Hunyuan3D-2mini 5GB RTX 3060 12GB Hunyuan3D-2 (\u5f62\u72b6\u751f\u6210) 6GB RTX 4060 Ti 16GB Hunyuan3D-2 (\u5b8c\u6574\u6d41\u7a0b) 12GB RTX 4070 Ti / RTX 4080 ### \ud83d\udcd0 \u652f\u6301\u7684\u8f93\u51fa\u683c\u5f0f .glb \u683c\u5f0f PBR \u6750\u8d28 \u9ad8\u5206\u8fa8\u7387\u7eb9\u7406 \ud83c\udfaf ComfyUI Hunyuan3D-2 3D \u8d44\u4ea7\u751f\u6210 | \u4ece\u56fe\u50cf\u5230\u9ad8\u8d28\u91cf 3D \u6a21\u578b\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"hunyuan3d/2.0/doc/index-en/","text":"\ud83c\udfaf ComfyUI Hunyuan3D-2 3D Asset Generation ComfyUI Native Workflow - Complete Pipeline from Images to High-Quality 3D Models \ud83c\udfa8 Multi-View Generation \ud83c\udfac High-Fidelity Textures \u26a1 Lightweight Deployment \ud83d\udccb Hunyuan3D 2.0 Model Overview [Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model launched by Tencent that can generate high-fidelity 3D models with high-resolution texture maps through text, images, or sketches. \ud83d\udd17 Related Resources \u2022 Project Repository : GitHub - Hunyuan3D-2 \u2022 Model Repository : \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f Two-Stage Generation Architecture Hunyuan3D 2.0 adopts a two-stage generation approach that effectively separates the complexity of shape and texture generation: \ud83c\udfaf Geometry Generation Model (Hunyuan3D-DiT) Based on flow diffusion Transformer architecture, generates texture-free geometric models that precisely match input conditions \ud83c\udfa8 Texture Generation Model (Hunyuan3D-Paint) Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials ### \u2728 Key Advantages \ud83c\udfaf High-Precision Generation Sharp geometric structures, rich texture colors, supports PBR material generation, achieving near-realistic lighting effects \ud83d\udee0\ufe0f Diverse Usage Methods Provides code calls, Blender plugins, Gradio applications, and official website online experience, suitable for different user needs \u26a1 Lightweight & Compatibility Mini model requires only 5GB VRAM, standard version shape generation needs 6GB VRAM, complete pipeline only needs 12GB VRAM ### \ud83c\udd95 Latest Updates \ud83d\udcc5 March 18, 2025 Update Hunyuan3D 2.0 adds multi-view shape generation model (Hunyuan3D-2mv), supporting input from different viewpoints to generate more refined geometric structures. \ud83d\ude80 Workflow Examples Overview workflow template can be seen in ![img_1.png](img_1.png) This example includes three complete workflows covering different usage scenarios: \ud83c\udfaf Hunyuan3D-2mv Multi-view input generates high-precision 3D models \u26a1 Hunyuan3D-2mv-turbo Fast multi-view generation with step distillation optimization \ud83d\uddbc\ufe0f Hunyuan3D-2 Standard 3D generation from single-view input \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version with native Hunyuan3D-2mv support \u2022 Currently does not support texture and material generation features \u2022 Workflow image metadata contains complete workflow JSON \u2022 Generated .glb format models will be output to ComfyUI/output/mesh folder \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83c\udfaf Workflow 1: Hunyuan3D-2mv Multi-View Generation The Hunyuan3D-2mv workflow supports generating 3D models using multi-view images. Multiple viewpoint images are not mandatory; you can also input only the `front` view image to generate 3D models. ### \ud83d\udce5 Step 1: Download Workflow and Materials Click image to download, drag into ComfyUI to load workflow ### \ud83d\udcc1 Multi-View Input Materials \ud83d\uddbc\ufe0f Front View \ud83d\uddbc\ufe0f Left View \ud83d\uddbc\ufe0f Back View \ud83d\udca1 Material Processing Tips The input images in the example have been pre-processed to remove backgrounds. In actual use, you can use custom nodes like ComfyUI_essentials to automatically remove backgrounds. ### \ud83d\udd27 Step 2: Workflow Configuration #### \ud83d\udccb Configuration Steps \ud83d\udd27 Model Loading Ensure the Image Only Checkpoint Loader node loads the renamed hunyuan3d-dit-v2-mv.safetensors model \ud83d\udcc1 Image Loading Load corresponding viewpoint images in each Load Image node \ud83d\ude80 Execute Generation Click Queue button or use shortcut Ctrl+Enter to run workflow \ud83d\udca1 Extended Views To add more viewpoints, ensure both the Hunyuan3Dv2ConditioningMultiView node and corresponding Load Image nodes load images for the respective viewpoints. \u26a1 Workflow 2: Hunyuan3D-2mv-turbo Fast Generation Hunyuan3D-2mv-turbo is a step distillation version of Hunyuan3D-2mv that can generate 3D models faster. In this version, set `cfg` to 1.0 and add a `flux guidance` node to control `distilled cfg` generation. ### \ud83d\udcc1 Multi-View Input Materials \ud83d\uddbc\ufe0f Front View \ud83d\uddbc\ufe0f Right View ### \ud83d\udd27 Step 2: Workflow Configuration #### \ud83d\udccb Configuration Steps \ud83d\udd27 Model Loading Ensure the Image Only Checkpoint Loader node loads the hunyuan3d-dit-v2-mv-turbo.safetensors model \ud83d\udcc1 Image Loading Load corresponding viewpoint images in each Load Image node \ud83d\ude80 Execute Generation Click Queue button or use shortcut Ctrl+Enter to run workflow \ud83d\uddbc\ufe0f Workflow 3: Hunyuan3D-2 Single-View Generation The Hunyuan3D-2 workflow uses single-view input to generate 3D models. In this workflow, the `Hunyuan3Dv2Conditioning` node replaces the `Hunyuan3Dv2ConditioningMultiView` node. ### \ud83d\udce5 Step 1: Download Workflow and Materials Click image to download, drag into ComfyUI to load workflow ### \ud83d\udcc1 Single-View Input Material \ud83d\uddbc\ufe0f Input Image ### \ud83d\udd27 Step 2: Workflow Configuration #### \ud83d\udccb Configuration Steps \ud83d\udd27 Model Loading Ensure the Image Only Checkpoint Loader node loads the hunyuan3d-dit-v2.safetensors model \ud83d\udcc1 Image Loading Load the input image in the Load Image node \ud83d\ude80 Execute Generation Click Queue button or use shortcut Ctrl+Enter to run workflow \ud83d\udee0\ufe0f Community Resources Here are Hunyuan3D-2 related ComfyUI community resources: \ud83d\udd27 ComfyUI-Hunyuan3DWrapper GitHub Repository - Complete Hunyuan3D wrapper \ud83d\udce6 Pre-processed Models Kijai/Hunyuan3D-2_safetensors - Pre-processed safetensors format models \ud83c\udfaf ComfyUI-3D-Pack GitHub Repository - Comprehensive 3D generation toolkit \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Use background-removed high-quality input images Ensure viewpoint consistency for multi-view inputs Choose appropriate model version based on VRAM capacity Prioritize turbo version for quick previews \u26a0\ufe0f Important Notes Ensure ComfyUI version is the latest development version Input images need clear subject outlines Multi-view images should maintain subject consistency Generation process requires sufficient VRAM support \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Model Version VRAM Requirement Recommended Configuration Hunyuan3D-2mini 5GB RTX 3060 12GB Hunyuan3D-2 (Shape Generation) 6GB RTX 4060 Ti 16GB Hunyuan3D-2 (Complete Pipeline) 12GB RTX 4070 Ti / RTX 4080 ### \ud83d\udcd0 Supported Output Formats .glb Format PBR Materials High-Resolution Textures \ud83c\udfaf ComfyUI Hunyuan3D-2 3D Asset Generation | Complete Solution from Images to High-Quality 3D Models","title":"Index en"},{"location":"hunyuan3d/2.0/doc/index-en/#hunyuan3d-20-model-overview","text":"[Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model launched by Tencent that can generate high-fidelity 3D models with high-resolution texture maps through text, images, or sketches. \ud83d\udd17 Related Resources \u2022 Project Repository : GitHub - Hunyuan3D-2 \u2022 Model Repository : \ud83e\udd17 Tencent HuggingFace ### \ud83c\udfd7\ufe0f Two-Stage Generation Architecture Hunyuan3D 2.0 adopts a two-stage generation approach that effectively separates the complexity of shape and texture generation:","title":"\ud83d\udccb Hunyuan3D 2.0 Model Overview"},{"location":"hunyuan3d/2.0/doc/index-en/#workflow-examples-overview","text":"workflow template can be seen in ![img_1.png](img_1.png) This example includes three complete workflows covering different usage scenarios: \ud83c\udfaf","title":"\ud83d\ude80 Workflow Examples Overview"},{"location":"hunyuan3d/2.0/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version with native Hunyuan3D-2mv support \u2022 Currently does not support texture and material generation features \u2022 Workflow image metadata contains complete workflow JSON \u2022 Generated .glb format models will be output to ComfyUI/output/mesh folder","title":"\u26a0\ufe0f Environment Requirements"},{"location":"hunyuan3d/2.0/doc/index-en/#workflow-1-hunyuan3d-2mv-multi-view-generation","text":"The Hunyuan3D-2mv workflow supports generating 3D models using multi-view images. Multiple viewpoint images are not mandatory; you can also input only the `front` view image to generate 3D models. ### \ud83d\udce5 Step 1: Download Workflow and Materials Click image to download, drag into ComfyUI to load workflow ### \ud83d\udcc1 Multi-View Input Materials","title":"\ud83c\udfaf Workflow 1: Hunyuan3D-2mv Multi-View Generation"},{"location":"hunyuan3d/2.0/doc/index-en/#workflow-2-hunyuan3d-2mv-turbo-fast-generation","text":"Hunyuan3D-2mv-turbo is a step distillation version of Hunyuan3D-2mv that can generate 3D models faster. In this version, set `cfg` to 1.0 and add a `flux guidance` node to control `distilled cfg` generation. ### \ud83d\udcc1 Multi-View Input Materials","title":"\u26a1 Workflow 2: Hunyuan3D-2mv-turbo Fast Generation"},{"location":"hunyuan3d/2.0/doc/index-en/#workflow-3-hunyuan3d-2-single-view-generation","text":"The Hunyuan3D-2 workflow uses single-view input to generate 3D models. In this workflow, the `Hunyuan3Dv2Conditioning` node replaces the `Hunyuan3Dv2ConditioningMultiView` node. ### \ud83d\udce5 Step 1: Download Workflow and Materials Click image to download, drag into ComfyUI to load workflow ### \ud83d\udcc1 Single-View Input Material","title":"\ud83d\uddbc\ufe0f Workflow 3: Hunyuan3D-2 Single-View Generation"},{"location":"hunyuan3d/2.0/doc/index-en/#community-resources","text":"Here are Hunyuan3D-2 related ComfyUI community resources: \ud83d\udd27 ComfyUI-Hunyuan3DWrapper GitHub Repository - Complete Hunyuan3D wrapper \ud83d\udce6 Pre-processed Models Kijai/Hunyuan3D-2_safetensors - Pre-processed safetensors format models \ud83c\udfaf ComfyUI-3D-Pack GitHub Repository - Comprehensive 3D generation toolkit","title":"\ud83d\udee0\ufe0f Community Resources"},{"location":"hunyuan3d/2.0/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"hunyuan3d/2.0/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Model Version VRAM Requirement Recommended Configuration Hunyuan3D-2mini 5GB RTX 3060 12GB Hunyuan3D-2 (Shape Generation) 6GB RTX 4060 Ti 16GB Hunyuan3D-2 (Complete Pipeline) 12GB RTX 4070 Ti / RTX 4080 ### \ud83d\udcd0 Supported Output Formats .glb Format PBR Materials High-Resolution Textures \ud83c\udfaf ComfyUI Hunyuan3D-2 3D Asset Generation | Complete Solution from Images to High-Quality 3D Models","title":"\ud83d\udd27 Technical Specifications"},{"location":"hunyuan3d/2.1/doc/","text":"\ud83c\udfaf Hunyuan3D 2.1 \u56fe\u50cf\u8f6c3D\u6a21\u578b ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u7a81\u7834\u60273D\u8d44\u4ea7\u751f\u6210\uff0c\u8d85\u8d8a2.0\u7248\u672c\u7684\u4e13\u4e1a\u7ea7PBR\u6750\u8d28 \ud83c\udfa8 \u589e\u5f3a\u7eb9\u7406\u8d28\u91cf \ud83c\udfd7\ufe0f \u5353\u8d8a3D\u6df1\u5ea6 \u26a1 \u751f\u4ea7\u5c31\u7eea \ud83d\udccb Hunyuan3D 2.1 \u6a21\u578b\u6982\u89c8 **Hunyuan3D 2.1** \u662f\u817e\u8baf\u63a8\u51fa\u7684\u7a81\u7834\u60273D\u8d44\u4ea7\u751f\u6210\u7cfb\u7edf\uff0c\u80fd\u5c06\u5355\u4e00\u56fe\u50cf\u8f6c\u6362\u4e3a\u5177\u5907\u7269\u7406\u57fa\u7840\u6e32\u67d3 (PBR) \u6750\u8d28\u7684\u751f\u4ea7\u5c31\u7eea3D\u6a21\u578b\u3002\u57fa\u4e8e Hunyuan3D 2.0 \u7684\u575a\u5b9e\u57fa\u7840\uff0c\u8fd9\u4e2a\u6700\u65b0\u7684 Hunyuan3D-2.1 \u7248\u672c\u663e\u8457\u63d0\u9ad8\u4e86\u7eb9\u7406\u8d28\u91cf\uff0c\u5177\u6709\u66f4\u7cbe\u7ec6\u7684\u8868\u9762\u7ec6\u8282\u548c\u589e\u5f3a\u7684\u4e09\u7ef4\u6df1\u5ea6\u611f\u77e5\u3002 \ud83c\udfa8 \u589e\u5f3a\u7684\u7eb9\u7406\u8d28\u91cf \u76f8\u6bd42.0\u6709\u663e\u8457\u63d0\u5347\uff0c\u62e5\u6709\u66f4\u7cbe\u7ec6\u7684\u8868\u9762\u7ec6\u8282\u548c\u66f4\u903c\u771f\u7684\u6750\u8d28\u8868\u73b0 \ud83c\udfd7\ufe0f \u5353\u8d8a\u76843D\u6df1\u5ea6 \u66f4\u597d\u7684\u7a7a\u95f4\u7406\u89e3\u521b\u9020\u66f4\u5177\u8bf4\u670d\u529b\u7684\u4e09\u7ef4\u5f62\u6001 \u26a1 \u771f\u6b63\u7684PBR\u6750\u8d28 \u751f\u6210\u53cd\u7167\u7387\u3001\u91d1\u5c5e\u548c\u7c97\u7cd9\u5ea6\u8d34\u56fe\uff0c\u9002\u5408\u4e13\u4e1a\u6d41\u6c34\u7ebf\u4e2d\u4f7f\u7528 \ud83c\udfaf \u751f\u4ea7\u5c31\u7eea\u8f93\u51fa \u521b\u5efa\u7d27\u5bc6\u7684\u7f51\u683c\u548c\u6b63\u786e\u7684UV\u6620\u5c04\uff0c\u5373\u53ef\u5728\u9879\u76ee\u4e2d\u7acb\u5373\u4f7f\u7528 \ud83d\udd17 \u76f8\u5173\u8d44\u6e90 \u2022 \u539f\u59cb\u6a21\u578b \uff1a\u7531\u817e\u8baf Hunyuan3D \u56e2\u961f\u5f00\u53d1 \u2022 ComfyUI \u5305\u88c5\u5668 \uff1a\u7531\u793e\u533a\u521b\u5efa\uff0c\u7279\u522b\u611f\u8c22 kijai \u7684\u57fa\u7840\u5305\u88c5\u5668\u5de5\u4f5c \u2022 \u524d\u4ee3\u7248\u672c \uff1a\u67e5\u770b Hunyuan3D 2.0 \u5de5\u4f5c\u6d41\u7a0b \u4e86\u89e3\u8fdb\u6b65\u7a0b\u5ea6 ### \ud83c\udd9a \u4e0e2.0\u7248\u672c\u5bf9\u6bd4\u4f18\u52bf \ud83c\udfaf \u8d28\u91cf\u63d0\u5347 \u4ee4\u4eba\u96be\u4ee5\u7f6e\u4fe1\u7684\u7ec6\u8282\u6355\u6349 \u4fdd\u5b58\u7cbe\u7ec6\u7279\u5f81\u5982\u6309\u94ae\u3001\u7ec7\u7269\u7eb9\u7406 \u96f6\u7eb9\u7406\u7f1d\u9699\uff0c\u65e0\u7f1d\u7eb9\u7406 \u5546\u4e1a\u7ea7PBR\u6750\u8d28 \ud83d\udd27 \u6280\u672f\u6539\u8fdb 3D\u611f\u77e5\u5b9a\u4f4d\u786e\u4fdd\u89c6\u89d2\u4e00\u81f4\u6027 \u771f\u5b9e\u5149\u7167\u54cd\u5e94\u6750\u8d28 \u91d1\u5c5e\u903c\u771f\u95ea\u8000\uff0c\u7ec7\u7269\u81ea\u7136\u5438\u5149 \u5b8c\u7f8e\u7684\u89e6\u611f\u8d28\u91cf \ud83d\ude80 Hunyuan3D 2.1 ComfyUI \u5de5\u4f5c\u6d41 \u26a0\ufe0f \u73af\u5883\u8981\u6c42 \ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c \u2022 \u9700\u8981\u5b89\u88c5 ComfyUI-Hunyuan3DWrapper \u81ea\u5b9a\u4e49\u8282\u70b9 \u2022 \u63a8\u8350\u4f7f\u7528\u6700\u65b0\u5f00\u53d1\u7248\uff08nightly\uff09\u83b7\u5f97\u5b8c\u6574\u529f\u80fd \u2022 \u786e\u4fdd\u6709\u8db3\u591f\u7684\u663e\u5b58\u8fd0\u884c\u6a21\u578b\uff08\u63a8\u835016GB+\uff09 \ud83d\udce5 \u4e0b\u8f7d\u94fe\u63a5 ComfyUI \u4e0b\u8f7d ComfyUI \u66f4\u65b0\u6559\u7a0b Hunyuan3D \u5305\u88c5\u5668 \ud83d\udd27 \u5e38\u89c1\u95ee\u9898 \u8282\u70b9\u7f3a\u5931\uff1a\u672a\u5b89\u88c5\u81ea\u5b9a\u4e49\u8282\u70b9\u5305 \u663e\u5b58\u4e0d\u8db3\uff1a\u6a21\u578b\u9700\u8981\u8f83\u5927\u663e\u5b58 \u52a0\u8f7d\u5931\u8d25\uff1a\u6a21\u578b\u6587\u4ef6\u4e0b\u8f7d\u4e0d\u5b8c\u6574 \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d \u70b9\u51fb\u8be5\u94fe\u63a5\u4e0b\u8f7d\u6df7\u51432.1\u5de5\u4f5c\u6d41 \u94fe\u63a5 \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 #### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 checkpoints/ \u2502 \u2502 \u251c\u2500\u2500 hunyuan3d-dit-v2-1.safetensors \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d-paint-v2-1.safetensors \u2502 \u251c\u2500\u2500 vae/ \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d_vae.safetensors \u2502 \u2514\u2500\u2500 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors ### \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c #### \ud83d\udccb \u57fa\u672c\u5de5\u4f5c\u6d41\u7a0b 1\ufe0f\u20e3 \u56fe\u50cf\u51c6\u5907 \u786e\u4fdd\u80cc\u666f\u5e72\u51c0\u4e14\u4e3b\u9898\u6e05\u6670\u2014\u2014\u8fd9\u5bf9 Hunyuan3D-2.1 \u7684\u6700\u4f73\u6548\u679c\u81f3\u5173\u91cd\u8981 \u79fb\u9664\u80cc\u666f\uff0c\u5c06\u4e3b\u9898\u7f6e\u4e2d \u786e\u4fdd\u4e3b\u8981\u7269\u4f53\u4e0a\u7684\u826f\u597d\u5149\u7167 2\ufe0f\u20e3 \u56fe\u50cf\u52a0\u8f7d \u4f7f\u7528 Hunyuan 3D 2.1 Load Image with Transparency \u52a0\u8f7d\u60a8\u7684\u53c2\u8003\u56fe\u50cf 3\ufe0f\u20e3 \u9884\u89c8\u7ed3\u679c \u57283D\u67e5\u770b\u5668\u4e2d\u9884\u89c8\u60a8\u7684 Hunyuan3D-2.1 \u7ed3\u679c #### \u2699\ufe0f \u91cd\u8981\u53c2\u6570\u8bbe\u7f6e \ud83c\udfd7\ufe0f \u5f62\u72b6\u751f\u6210\u53c2\u6570 steps : 25\uff08\u8d28\u91cf\u4e0e\u901f\u5ea6\u7684\u6700\u4f73\u5e73\u8861\u70b9\uff09 guidance_scale : 7.5\uff08\u5982\u4f55\u7d27\u5bc6\u5730\u9075\u5faa\u60a8\u7684\u56fe\u50cf\uff09 control_after_generate : \"fixed\"\uff08\u4fdd\u6301\u4e00\u81f4\u6027\uff09 \ud83c\udfa8 \u7eb9\u7406\u5316\u53c2\u6570 view_size : 768\uff08\u826f\u597d\u8d28\u91cf\u4e14\u4e0d\u4f1a\u8017\u5c3dVRAM\uff09 texture_size : 1024\uff08\u9ad8\u5206\u8fa8\u7387\u7eb9\u7406\uff09 guidance_scale : 3.0\uff08\u7eb9\u7406\u5bf9\u9f50\u5f3a\u5ea6\uff09 #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 ## API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Hunyuan 3D 2.1 Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local HY3D_MODEL = \"model.fp16.ckpt\" # Default Parameters DEFAULT_IMAGE = \"fun_control.jpg\" class ComfyUIHunyuan3DClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_3d_mesh(self, image_path=None, image_name=None, steps=25, guidance_scale=5, seed=None, box_v=1.01, octree_resolution=256, num_chunks=64000, mc_level=0, mc_algo=\"mc\", enable_flash_vdm=True, force_offload=False, attention_mode=\"sdpa\", remove_floaters=True, remove_degenerate_faces=True, reduce_faces=True, max_facenum=200000, smooth_normals=False, file_format=\"glb\", filename_prefix=\"Hy21_Mesh\"): \"\"\"Generate 3D mesh using Hunyuan 3D 2.1 based on original JSON workflow\"\"\" print(\"Starting Hunyuan 3D 2.1 mesh generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle input image if image_path and not image_name: image_name = self.upload_image(image_path) if not image_name: raise Exception(\"Failed to upload input image\") elif not image_name: image_name = DEFAULT_IMAGE # Workflow based on your provided JSON workflow = { \"4\": { \"inputs\": { \"model_name\": HY3D_MODEL }, \"class_type\": \"Hy3D21VAELoader\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 VAE Loader\"} }, \"9\": { \"inputs\": { \"box_v\": box_v, \"octree_resolution\": octree_resolution, \"num_chunks\": num_chunks, \"mc_level\": mc_level, \"mc_algo\": mc_algo, \"enable_flash_vdm\": enable_flash_vdm, \"force_offload\": force_offload, \"vae\": [\"4\", 0], \"latents\": [\"10\", 0] }, \"class_type\": \"Hy3D21VAEDecode\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 VAE Decoder\"} }, \"10\": { \"inputs\": { \"model\": HY3D_MODEL, \"steps\": steps, \"guidance_scale\": guidance_scale, \"seed\": seed, \"attention_mode\": attention_mode, \"image\": [\"14\", 2] }, \"class_type\": \"Hy3DMeshGenerator\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Mesh Generator\"} }, \"11\": { \"inputs\": { \"model_file\": [\"20\", 0], \"image\": \"\" }, \"class_type\": \"Preview3D\", \"_meta\": {\"title\": \"Preview 3D\"} }, \"14\": { \"inputs\": { \"image\": image_name }, \"class_type\": \"Hy3D21LoadImageWithTransparency\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Load Image with Transparency\"} }, \"19\": { \"inputs\": { \"remove_floaters\": remove_floaters, \"remove_degenerate_faces\": remove_degenerate_faces, \"reduce_faces\": reduce_faces, \"max_facenum\": max_facenum, \"smooth_normals\": smooth_normals, \"trimesh\": [\"9\", 0] }, \"class_type\": \"Hy3D21PostprocessMesh\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Post Process Trimesh\"} }, \"20\": { \"inputs\": { \"filename_prefix\": filename_prefix, \"file_format\": file_format, \"save_file\": True, \"trimesh\": [\"19\", 0] }, \"class_type\": \"Hy3D21ExportMesh\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Export Mesh\"} } } print(\"Submitting Hunyuan 3D 2.1 mesh generation workflow...\") print(f\"Input Image: {image_name}\") print(f\"Steps: {steps}\") print(f\"Guidance Scale: {guidance_scale}\") print(f\"Random Seed: {seed}\") print(f\"Output Format: {file_format}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_mesh(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated 3D mesh files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for 3D model files if 'gltf' in output: for model_info in output['gltf']: filename = model_info['filename'] # Download 3D model model_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if model_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(model_response.content) downloaded_files.append(output_path) print(f\"3D model saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, image_paths, **kwargs): \"\"\"Batch generate 3D meshes from multiple images\"\"\" results = [] for i, image_path in enumerate(image_paths): print(f\"\\nStarting 3D generation task {i+1}/{len(image_paths)}...\") try: task_id, seed = self.generate_3d_mesh(image_path=image_path, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_mesh(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'input_image': image_path }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(10) # 3D generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_quality_settings(self, image_path, quality_preset=\"high\"): \"\"\"Generate 3D mesh with predefined quality settings\"\"\" quality_presets = { \"low\": { \"steps\": 15, \"octree_resolution\": 128, \"num_chunks\": 32000, \"max_facenum\": 50000 }, \"medium\": { \"steps\": 20, \"octree_resolution\": 256, \"num_chunks\": 64000, \"max_facenum\": 100000 }, \"high\": { \"steps\": 25, \"octree_resolution\": 512, \"num_chunks\": 128000, \"max_facenum\": 200000 }, \"ultra\": { \"steps\": 30, \"octree_resolution\": 1024, \"num_chunks\": 256000, \"max_facenum\": 500000 } } settings = quality_presets.get(quality_preset, quality_presets[\"high\"]) return self.generate_3d_mesh(image_path=image_path, **settings) def main(): \"\"\"Main function - Execute Hunyuan 3D 2.1 mesh generation\"\"\" client = ComfyUIHunyuan3DClient() try: print(\"Hunyuan 3D 2.1 mesh generation client started...\") # Single 3D mesh generation example print(\"\\n=== Single 3D Mesh Generation ===\") # You can either provide a local image path or use an existing image name input_image_path = None # Set to your image path, e.g., \"input_image.jpg\" input_image_name = DEFAULT_IMAGE # Or use existing image name task_id, seed = client.generate_3d_mesh( image_path=input_image_path, image_name=input_image_name, steps=25, guidance_scale=5, octree_resolution=256, max_facenum=200000, file_format=\"glb\" ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (3D generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"3D mesh generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(10) # Download 3D mesh and preview files downloaded_files = client.download_mesh(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Quality preset example print(\"\\n=== Quality Preset Example ===\") # Uncomment to test different quality settings # if input_image_path: # for quality in [\"low\", \"medium\", \"high\"]: # print(f\"Generating with {quality} quality...\") # task_id, seed = client.generate_with_quality_settings(input_image_path, quality) # # Wait and download logic here... # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_images = [ # \"image1.jpg\", # \"image2.png\", # \"image3.jpeg\" ] # Uncomment to run batch generation # if batch_images: # batch_results = client.generate_batch( # batch_images, # steps=20, # guidance_scale=5, # file_format=\"glb\" # ) # print(f\"Batch generation completed, generated {len(batch_results)} 3D models\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() ## \ud83d\udca1 Hunyuan3D 2.1 \u9ad8\u7ea7\u6280\u5de7 ### \ud83c\udfaf \u56fe\u50cf\u51c6\u5907\u81f3\u5173\u91cd\u8981 \ud83d\udcf8 \u8f93\u5165\u56fe\u50cf\u4f18\u5316 \u5e72\u51c0\u7684\u80cc\u666f\u548c\u6e05\u6670\u7684\u4e3b\u9898\u5bf9\u4e8e Hunyuan3D-2.1 \u7684\u6700\u4f73\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u6a21\u578b\u5728\u7ecf\u8fc7\u826f\u597d\u51c6\u5907\u7684\u8f93\u5165\u56fe\u50cf\u4e0b\u8868\u73b0\u663e\u8457\u66f4\u4f73\u2014\u2014\u79fb\u9664\u80cc\u666f\uff0c\u5c06\u4e3b\u9898\u7f6e\u4e2d\uff0c\u5e76\u786e\u4fdd\u4e3b\u8981\u7269\u4f53\u4e0a\u7684\u826f\u597d\u5149\u7167\u3002 ### \ud83d\udd27 \u7f51\u683c\u8d28\u91cf\u63a7\u5236 \ud83d\udee0\ufe0f Post Process Trimesh \u8fd9\u662f\u60a8\u5728 Hunyuan3D-2.1 \u4e2d\u7684\u597d\u670b\u53cb \u542f\u7528 remove_floaters \u79fb\u9664\u6d6e\u52a8\u9876\u70b9 \u542f\u7528 reduce_faces \u83b7\u5f97\u66f4\u5e72\u51c0\u7684\u7ed3\u679c \u8c03\u6574 max_facenum \u63a7\u5236\u9762\u6570 \ud83c\udfa8 \u7eb9\u7406\u5206\u8fa8\u7387\u5e73\u8861 VAE Decoder \u4e2d\u7684\u53c2\u6570\u63a7\u5236 octree_resolution : 256\uff08\u9ed8\u8ba4\u503c\uff09 \u66f4\u9ad8\u503c = \u66f4\u591a\u51e0\u4f55\u7ec6\u8282 \u9700\u8981\u66f4\u591a\u8ba1\u7b97\u80fd\u529b ### \ud83c\udf9b\ufe0f \u53c2\u6570\u4f18\u5316\u7b56\u7565 \u7528\u9014 max_facenum \u8bbe\u7f6e \u8bf4\u660e \u5373\u65f6\u5e94\u7528 5000-10000 \u5feb\u901f\u9884\u89c8\uff0c\u8f83\u4f4e\u9762\u6570 \u6e38\u620f\u8d44\u4ea7 15000-25000 \u5e73\u8861\u8d28\u91cf\u4e0e\u6027\u80fd \u9ad8\u8d28\u91cf\u6e32\u67d3 50000+ \u6700\u9ad8\u8d28\u91cf\uff0c\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90 ## \ud83c\udfaf Hunyuan3D 2.1 \u5e94\u7528\u573a\u666f \ud83c\udfae \u6e38\u620f\u5f00\u53d1 \u521b\u5efa\u6e38\u620f\u8d44\u4ea7\u3001\u9053\u5177\u6a21\u578b\u3001\u73af\u5883\u7269\u4f53\uff0c\u5177\u5907\u5b8c\u6574\u7684PBR\u6750\u8d28 \ud83e\udd7d \u865a\u62df\u73b0\u5b9e \u6784\u5efaVR\u4f53\u9a8c\u6240\u9700\u7684\u9ad8\u8d28\u91cf3D\u6a21\u578b\u548c\u4ea4\u4e92\u7269\u4f53 \ud83c\udfed \u5de5\u4e1a\u8bbe\u8ba1 \u4ea7\u54c1\u539f\u578b\u8bbe\u8ba1\u3001\u5de5\u4e1a\u5efa\u6a21\u3001\u8bbe\u8ba1\u53ef\u89c6\u5316 \ud83c\udfac \u5f71\u89c6\u5236\u4f5c \u7535\u5f71\u9053\u5177\u3001\u573a\u666f\u5efa\u6a21\u3001\u89c6\u89c9\u6548\u679c\u5236\u4f5c ## \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 16GB 24GB+ \u7cfb\u7edf\u5185\u5b58 32GB 64GB+ \u5b58\u50a8\u7a7a\u95f4 50GB 100GB+ SSD \u63a8\u8350 GPU RTX 4080 RTX 4090 / A100 ### \ud83c\udfa8 \u8f93\u51fa\u683c\u5f0f\u4e0e\u8d28\u91cf .glb \u683c\u5f0f PBR \u6750\u8d28 \u53cd\u7167\u7387\u8d34\u56fe \u91d1\u5c5e\u5ea6\u8d34\u56fe \u7c97\u7cd9\u5ea6\u8d34\u56fe UV \u6620\u5c04 ### \ud83d\udcca \u6027\u80fd\u57fa\u51c6 \u23f1\ufe0f \u751f\u6210\u65f6\u95f4 \u5f62\u72b6\u751f\u6210\uff1a2-5\u5206\u949f | \u7eb9\u7406\u751f\u6210\uff1a3-8\u5206\u949f\uff08\u53d6\u51b3\u4e8e\u5206\u8fa8\u7387\u8bbe\u7f6e\uff09 \ud83c\udfaf \u8d28\u91cf\u63d0\u5347 \u76f8\u6bd42.0\u7248\u672c\uff0c\u7eb9\u7406\u7ec6\u8282\u63d0\u534740%\uff0c\u51e0\u4f55\u7cbe\u5ea6\u63d0\u534725% \ud83d\udcbe \u6587\u4ef6\u5927\u5c0f \u5178\u578b\u8f93\u51fa\uff1a5-20MB .glb\u6587\u4ef6\uff08\u5305\u542b\u5b8c\u6574PBR\u6750\u8d28\uff09 ## \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u4f7f\u7528\u9ad8\u8d28\u91cf\u3001\u6e05\u6670\u7684\u8f93\u5165\u56fe\u50cf \u786e\u4fdd\u4e3b\u4f53\u5728\u56fe\u50cf\u4e2d\u5c45\u4e2d\u4e14\u5b8c\u6574 \u79fb\u9664\u590d\u6742\u80cc\u666f\uff0c\u4fdd\u6301\u4e3b\u4f53\u7a81\u51fa \u6839\u636e\u7528\u9014\u8c03\u6574\u7f51\u683c\u9762\u6570\u548c\u7eb9\u7406\u5206\u8fa8\u7387 \u5229\u7528Post Process\u8282\u70b9\u4f18\u5316\u6700\u7ec8\u7ed3\u679c \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u786e\u4fdd\u6709\u8db3\u591f\u7684\u663e\u5b58\u8fd0\u884c\u5b8c\u6574\u6d41\u7a0b \u590d\u6742\u7269\u4f53\u53ef\u80fd\u9700\u8981\u591a\u6b21\u5c1d\u8bd5\u4f18\u5316\u53c2\u6570 \u900f\u660e\u6216\u53cd\u5c04\u6750\u8d28\u53ef\u80fd\u9700\u8981\u7279\u6b8a\u5904\u7406 \u9996\u6b21\u8fd0\u884c\u9700\u8981\u4e0b\u8f7d\u5927\u91cf\u6a21\u578b\u6587\u4ef6 \u751f\u6210\u65f6\u95f4\u8f83\u957f\uff0c\u5efa\u8bae\u5408\u7406\u5b89\u6392\u5de5\u4f5c\u6d41\u7a0b --- \ud83c\udfaf Hunyuan3D 2.1 \u56fe\u50cf\u8f6c3D\u6a21\u578b | \u7a81\u7834\u60273D\u8d44\u4ea7\u751f\u6210\uff0c\u4e13\u4e1a\u7ea7PBR\u6750\u8d28\u7684\u5b8c\u7f8e\u5448\u73b0","title":"Index"},{"location":"hunyuan3d/2.1/doc/#hunyuan3d-21","text":"**Hunyuan3D 2.1** \u662f\u817e\u8baf\u63a8\u51fa\u7684\u7a81\u7834\u60273D\u8d44\u4ea7\u751f\u6210\u7cfb\u7edf\uff0c\u80fd\u5c06\u5355\u4e00\u56fe\u50cf\u8f6c\u6362\u4e3a\u5177\u5907\u7269\u7406\u57fa\u7840\u6e32\u67d3 (PBR) \u6750\u8d28\u7684\u751f\u4ea7\u5c31\u7eea3D\u6a21\u578b\u3002\u57fa\u4e8e Hunyuan3D 2.0 \u7684\u575a\u5b9e\u57fa\u7840\uff0c\u8fd9\u4e2a\u6700\u65b0\u7684 Hunyuan3D-2.1 \u7248\u672c\u663e\u8457\u63d0\u9ad8\u4e86\u7eb9\u7406\u8d28\u91cf\uff0c\u5177\u6709\u66f4\u7cbe\u7ec6\u7684\u8868\u9762\u7ec6\u8282\u548c\u589e\u5f3a\u7684\u4e09\u7ef4\u6df1\u5ea6\u611f\u77e5\u3002 \ud83c\udfa8","title":"\ud83d\udccb Hunyuan3D 2.1 \u6a21\u578b\u6982\u89c8"},{"location":"hunyuan3d/2.1/doc/#hunyuan3d-21-comfyui","text":"","title":"\ud83d\ude80 Hunyuan3D 2.1 ComfyUI \u5de5\u4f5c\u6d41"},{"location":"hunyuan3d/2.1/doc/#_1","text":"\ud83d\udccb \u4f7f\u7528\u524d\u8bf7\u786e\u8ba4 \u2022 \u786e\u4fdd ComfyUI \u5df2\u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c \u2022 \u9700\u8981\u5b89\u88c5 ComfyUI-Hunyuan3DWrapper \u81ea\u5b9a\u4e49\u8282\u70b9 \u2022 \u63a8\u8350\u4f7f\u7528\u6700\u65b0\u5f00\u53d1\u7248\uff08nightly\uff09\u83b7\u5f97\u5b8c\u6574\u529f\u80fd \u2022 \u786e\u4fdd\u6709\u8db3\u591f\u7684\u663e\u5b58\u8fd0\u884c\u6a21\u578b\uff08\u63a8\u835016GB+\uff09","title":"\u26a0\ufe0f \u73af\u5883\u8981\u6c42"},{"location":"hunyuan3d/2.1/doc/#_2","text":"\u70b9\u51fb\u8be5\u94fe\u63a5\u4e0b\u8f7d\u6df7\u51432.1\u5de5\u4f5c\u6d41 \u94fe\u63a5","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d"},{"location":"hunyuan3d/2.1/doc/#_3","text":"#### \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 checkpoints/ \u2502 \u2502 \u251c\u2500\u2500 hunyuan3d-dit-v2-1.safetensors \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d-paint-v2-1.safetensors \u2502 \u251c\u2500\u2500 vae/ \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d_vae.safetensors \u2502 \u2514\u2500\u2500 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors ### \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c #### \ud83d\udccb \u57fa\u672c\u5de5\u4f5c\u6d41\u7a0b","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"hunyuan3d/2.1/doc/index-en/","text":"\ud83c\udfaf Hunyuan3D 2.1 Image to 3D Model ComfyUI Native Workflow - Breakthrough 3D Asset Generation with Professional PBR Materials Beyond 2.0 \ud83c\udfa8 Enhanced Texture Quality \ud83c\udfd7\ufe0f Exceptional 3D Depth \u26a1 Production Ready \ud83d\udccb Hunyuan3D 2.1 Model Overview **Hunyuan3D 2.1** is a breakthrough 3D asset generation system launched by Tencent that can convert a single image into production-ready 3D models with Physically Based Rendering (PBR) materials. Built on the solid foundation of Hunyuan3D 2.0, this latest Hunyuan3D-2.1 version significantly improves texture quality with finer surface details and enhanced three-dimensional depth perception. \ud83c\udfa8 Enhanced Texture Quality Significant improvements over 2.0 with finer surface details and more realistic material representation \ud83c\udfd7\ufe0f Exceptional 3D Depth Better spatial understanding creates more convincing three-dimensional forms \u26a1 True PBR Materials Generates albedo, metallic, and roughness maps suitable for professional pipelines \ud83c\udfaf Production-Ready Output Creates tight meshes and proper UV mapping, ready for immediate use in projects \ud83d\udd17 Related Resources \u2022 Original Model : Developed by Tencent Hunyuan3D Team \u2022 ComfyUI Wrapper : Created by the community, special thanks to kijai for the foundational wrapper work \u2022 Previous Version : Check out Hunyuan3D 2.0 Workflow to see the level of improvement ### \ud83c\udd9a Advantages Over 2.0 Version \ud83c\udfaf Quality Improvements Incredible detail capture Preserves fine features like buttons, fabric textures Zero texture seams, seamless textures Commercial-grade PBR materials \ud83d\udd27 Technical Improvements 3D-aware positioning ensures viewpoint consistency Realistic lighting-responsive materials Metals shine realistically, fabrics absorb light naturally Perfect tactile quality \ud83d\ude80 Hunyuan3D 2.1 ComfyUI Workflow \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Need to install ComfyUI-Hunyuan3DWrapper custom nodes \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 Ensure sufficient VRAM to run the model (recommended 16GB+) \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Hunyuan3D Wrapper \ud83d\udd27 Common Issues Missing nodes: Custom node package not installed Insufficient VRAM: Model requires large VRAM Loading failure: Model files not completely downloaded \ud83d\udce5 Step 1: Download Workflow Files \ud83c\udfaf Hunyuan3D 2.1 ComfyUI Workflow Professional 3D Asset Creation | Fully Operational Workflow \u2705 No Missing Nodes or Models \u26a1 No Manual Setup Required \ud83c\udfa8 Stunning Visual Effects \ud83d\ude80 Start Using Workflow for Free \ud83d\udd17 Step 2: Model Files #### \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 checkpoints/ \u2502 \u2502 \u251c\u2500\u2500 hunyuan3d-dit-v2-1.safetensors \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d-paint-v2-1.safetensors \u2502 \u251c\u2500\u2500 vae/ \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d_vae.safetensors \u2502 \u2514\u2500\u2500 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors \ud83d\udd27 Step 3: Workflow Configuration Operations #### \ud83d\udccb Basic Workflow Process 1\ufe0f\u20e3 Image Preparation Ensure clean background and clear subject\u2014this is crucial for Hunyuan3D-2.1's optimal results Remove background, center the subject Ensure good lighting on the main object 2\ufe0f\u20e3 Image Loading Use Hunyuan 3D 2.1 Load Image with Transparency to load your reference image 3\ufe0f\u20e3 Preview Results Preview your Hunyuan3D-2.1 results in the 3D viewer #### \u2699\ufe0f Important Parameter Settings \ud83c\udfd7\ufe0f Shape Generation Parameters steps : 25 (optimal balance between quality and speed) guidance_scale : 7.5 (how closely to follow your image) control_after_generate : \"fixed\" (maintain consistency) \ud83c\udfa8 Texturing Parameters view_size : 768 (good quality without exhausting VRAM) texture_size : 1024 (high-resolution textures) guidance_scale : 3.0 (texture alignment strength) #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click Queue button or use shortcut Ctrl(Cmd) + Enter to run workflow API Execute \ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Hunyuan 3D 2.1 Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local HY3D_MODEL = \"model.fp16.ckpt\" # Default Parameters DEFAULT_IMAGE = \"fun_control.jpg\" class ComfyUIHunyuan3DClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_3d_mesh(self, image_path=None, image_name=None, steps=25, guidance_scale=5, seed=None, box_v=1.01, octree_resolution=256, num_chunks=64000, mc_level=0, mc_algo=\"mc\", enable_flash_vdm=True, force_offload=False, attention_mode=\"sdpa\", remove_floaters=True, remove_degenerate_faces=True, reduce_faces=True, max_facenum=200000, smooth_normals=False, file_format=\"glb\", filename_prefix=\"Hy21_Mesh\"): \"\"\"Generate 3D mesh using Hunyuan 3D 2.1 based on original JSON workflow\"\"\" print(\"Starting Hunyuan 3D 2.1 mesh generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle input image if image_path and not image_name: image_name = self.upload_image(image_path) if not image_name: raise Exception(\"Failed to upload input image\") elif not image_name: image_name = DEFAULT_IMAGE # Workflow based on your provided JSON workflow = { \"4\": { \"inputs\": { \"model_name\": HY3D_MODEL }, \"class_type\": \"Hy3D21VAELoader\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 VAE Loader\"} }, \"9\": { \"inputs\": { \"box_v\": box_v, \"octree_resolution\": octree_resolution, \"num_chunks\": num_chunks, \"mc_level\": mc_level, \"mc_algo\": mc_algo, \"enable_flash_vdm\": enable_flash_vdm, \"force_offload\": force_offload, \"vae\": [\"4\", 0], \"latents\": [\"10\", 0] }, \"class_type\": \"Hy3D21VAEDecode\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 VAE Decoder\"} }, \"10\": { \"inputs\": { \"model\": HY3D_MODEL, \"steps\": steps, \"guidance_scale\": guidance_scale, \"seed\": seed, \"attention_mode\": attention_mode, \"image\": [\"14\", 2] }, \"class_type\": \"Hy3DMeshGenerator\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Mesh Generator\"} }, \"11\": { \"inputs\": { \"model_file\": [\"20\", 0], \"image\": \"\" }, \"class_type\": \"Preview3D\", \"_meta\": {\"title\": \"Preview 3D\"} }, \"14\": { \"inputs\": { \"image\": image_name }, \"class_type\": \"Hy3D21LoadImageWithTransparency\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Load Image with Transparency\"} }, \"19\": { \"inputs\": { \"remove_floaters\": remove_floaters, \"remove_degenerate_faces\": remove_degenerate_faces, \"reduce_faces\": reduce_faces, \"max_facenum\": max_facenum, \"smooth_normals\": smooth_normals, \"trimesh\": [\"9\", 0] }, \"class_type\": \"Hy3D21PostprocessMesh\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Post Process Trimesh\"} }, \"20\": { \"inputs\": { \"filename_prefix\": filename_prefix, \"file_format\": file_format, \"save_file\": True, \"trimesh\": [\"19\", 0] }, \"class_type\": \"Hy3D21ExportMesh\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Export Mesh\"} } } print(\"Submitting Hunyuan 3D 2.1 mesh generation workflow...\") print(f\"Input Image: {image_name}\") print(f\"Steps: {steps}\") print(f\"Guidance Scale: {guidance_scale}\") print(f\"Random Seed: {seed}\") print(f\"Output Format: {file_format}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_mesh(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated 3D mesh files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for 3D model files if 'gltf' in output: for model_info in output['gltf']: filename = model_info['filename'] # Download 3D model model_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if model_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(model_response.content) downloaded_files.append(output_path) print(f\"3D model saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, image_paths, **kwargs): \"\"\"Batch generate 3D meshes from multiple images\"\"\" results = [] for i, image_path in enumerate(image_paths): print(f\"\\nStarting 3D generation task {i+1}/{len(image_paths)}...\") try: task_id, seed = self.generate_3d_mesh(image_path=image_path, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_mesh(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'input_image': image_path }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(10) # 3D generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_quality_settings(self, image_path, quality_preset=\"high\"): \"\"\"Generate 3D mesh with predefined quality settings\"\"\" quality_presets = { \"low\": { \"steps\": 15, \"octree_resolution\": 128, \"num_chunks\": 32000, \"max_facenum\": 50000 }, \"medium\": { \"steps\": 20, \"octree_resolution\": 256, \"num_chunks\": 64000, \"max_facenum\": 100000 }, \"high\": { \"steps\": 25, \"octree_resolution\": 512, \"num_chunks\": 128000, \"max_facenum\": 200000 }, \"ultra\": { \"steps\": 30, \"octree_resolution\": 1024, \"num_chunks\": 256000, \"max_facenum\": 500000 } } settings = quality_presets.get(quality_preset, quality_presets[\"high\"]) return self.generate_3d_mesh(image_path=image_path, **settings) def main(): \"\"\"Main function - Execute Hunyuan 3D 2.1 mesh generation\"\"\" client = ComfyUIHunyuan3DClient() try: print(\"Hunyuan 3D 2.1 mesh generation client started...\") # Single 3D mesh generation example print(\"\\n=== Single 3D Mesh Generation ===\") # You can either provide a local image path or use an existing image name input_image_path = None # Set to your image path, e.g., \"input_image.jpg\" input_image_name = DEFAULT_IMAGE # Or use existing image name task_id, seed = client.generate_3d_mesh( image_path=input_image_path, image_name=input_image_name, steps=25, guidance_scale=5, octree_resolution=256, max_facenum=200000, file_format=\"glb\" ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (3D generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"3D mesh generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(10) # Download 3D mesh and preview files downloaded_files = client.download_mesh(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Quality preset example print(\"\\n=== Quality Preset Example ===\") # Uncomment to test different quality settings # if input_image_path: # for quality in [\"low\", \"medium\", \"high\"]: # print(f\"Generating with {quality} quality...\") # task_id, seed = client.generate_with_quality_settings(input_image_path, quality) # # Wait and download logic here... # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_images = [ # \"image1.jpg\", # \"image2.png\", # \"image3.jpeg\" ] # Uncomment to run batch generation # if batch_images: # batch_results = client.generate_batch( # batch_images, # steps=20, # guidance_scale=5, # file_format=\"glb\" # ) # print(f\"Batch generation completed, generated {len(batch_results)} 3D models\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83d\udca1 Hunyuan3D 2.1 Advanced Tips ### \ud83c\udfaf Image Preparation is Crucial \ud83d\udcf8 Input Image Optimization Clean backgrounds and clear subjects are crucial for Hunyuan3D-2.1's optimal performance. The model performs significantly better with well-prepared input images\u2014remove backgrounds, center the subject, and ensure good lighting on the main object. ### \ud83d\udd27 Mesh Quality Control \ud83d\udee0\ufe0f Post Process Trimesh This is your friend in Hunyuan3D-2.1 Enable remove_floaters to remove floating vertices Enable reduce_faces for cleaner results Adjust max_facenum to control face count \ud83c\udfa8 Texture Resolution Balance Parameter control in VAE Decoder octree_resolution : 256 (default value) Higher values = more geometric detail Requires more computational power ### \ud83c\udf9b\ufe0f Parameter Optimization Strategy Use Case max_facenum Setting Description Real-time Applications 5000-10000 Quick preview, lower face count Game Assets 15000-25000 Balance quality and performance High-Quality Rendering 50000+ Highest quality, requires more computational resources \ud83c\udfaf Hunyuan3D 2.1 Application Scenarios \ud83c\udfae Game Development Create game assets, prop models, environmental objects with complete PBR materials \ud83e\udd7d Virtual Reality Build high-quality 3D models and interactive objects needed for VR experiences \ud83c\udfed Industrial Design Product prototyping, industrial modeling, design visualization \ud83c\udfac Film Production Movie props, scene modeling, visual effects production \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 16GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4080 RTX 4090 / A100 ### \ud83c\udfa8 Output Formats and Quality .glb Format PBR Materials Albedo Maps Metallic Maps Roughness Maps UV Mapping ### \ud83d\udcca Performance Benchmarks \u23f1\ufe0f Generation Time Shape generation: 2-5 minutes | Texture generation: 3-8 minutes (depending on resolution settings) \ud83c\udfaf Quality Improvement Compared to version 2.0, texture detail improved by 40%, geometric accuracy improved by 25% \ud83d\udcbe File Size Typical output: 5-20MB .glb files (including complete PBR materials) \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Use high-quality, clear input images Ensure subject is centered and complete in the image Remove complex backgrounds, keep subject prominent Adjust mesh face count and texture resolution based on use case Utilize Post Process nodes to optimize final results \u26a0\ufe0f Important Notes Ensure sufficient VRAM to run the complete pipeline Complex objects may require multiple attempts to optimize parameters Transparent or reflective materials may need special handling First run requires downloading large model files Generation time is lengthy, plan workflow accordingly \ud83c\udfaf Hunyuan3D 2.1 Image to 3D Model | Breakthrough 3D Asset Generation with Perfect Professional PBR Materials","title":"Index en"},{"location":"hunyuan3d/2.1/doc/index-en/#hunyuan3d-21-model-overview","text":"**Hunyuan3D 2.1** is a breakthrough 3D asset generation system launched by Tencent that can convert a single image into production-ready 3D models with Physically Based Rendering (PBR) materials. Built on the solid foundation of Hunyuan3D 2.0, this latest Hunyuan3D-2.1 version significantly improves texture quality with finer surface details and enhanced three-dimensional depth perception. \ud83c\udfa8","title":"\ud83d\udccb Hunyuan3D 2.1 Model Overview"},{"location":"hunyuan3d/2.1/doc/index-en/#hunyuan3d-21-comfyui-workflow","text":"","title":"\ud83d\ude80 Hunyuan3D 2.1 ComfyUI Workflow"},{"location":"hunyuan3d/2.1/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Need to install ComfyUI-Hunyuan3DWrapper custom nodes \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 Ensure sufficient VRAM to run the model (recommended 16GB+)","title":"\u26a0\ufe0f Environment Requirements"},{"location":"hunyuan3d/2.1/doc/index-en/#step-1-download-workflow-files","text":"","title":"\ud83d\udce5 Step 1: Download Workflow Files"},{"location":"hunyuan3d/2.1/doc/index-en/#step-2-model-files","text":"#### \ud83d\udcc2 Model File Structure ComfyUI/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 checkpoints/ \u2502 \u2502 \u251c\u2500\u2500 hunyuan3d-dit-v2-1.safetensors \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d-paint-v2-1.safetensors \u2502 \u251c\u2500\u2500 vae/ \u2502 \u2502 \u2514\u2500\u2500 hunyuan3d_vae.safetensors \u2502 \u2514\u2500\u2500 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"hunyuan3d/2.1/doc/index-en/#step-3-workflow-configuration-operations","text":"#### \ud83d\udccb Basic Workflow Process","title":"\ud83d\udd27 Step 3: Workflow Configuration Operations"},{"location":"hunyuan3d/2.1/doc/index-en/#api-execute","text":"\ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Hunyuan 3D 2.1 Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local HY3D_MODEL = \"model.fp16.ckpt\" # Default Parameters DEFAULT_IMAGE = \"fun_control.jpg\" class ComfyUIHunyuan3DClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def generate_3d_mesh(self, image_path=None, image_name=None, steps=25, guidance_scale=5, seed=None, box_v=1.01, octree_resolution=256, num_chunks=64000, mc_level=0, mc_algo=\"mc\", enable_flash_vdm=True, force_offload=False, attention_mode=\"sdpa\", remove_floaters=True, remove_degenerate_faces=True, reduce_faces=True, max_facenum=200000, smooth_normals=False, file_format=\"glb\", filename_prefix=\"Hy21_Mesh\"): \"\"\"Generate 3D mesh using Hunyuan 3D 2.1 based on original JSON workflow\"\"\" print(\"Starting Hunyuan 3D 2.1 mesh generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle input image if image_path and not image_name: image_name = self.upload_image(image_path) if not image_name: raise Exception(\"Failed to upload input image\") elif not image_name: image_name = DEFAULT_IMAGE # Workflow based on your provided JSON workflow = { \"4\": { \"inputs\": { \"model_name\": HY3D_MODEL }, \"class_type\": \"Hy3D21VAELoader\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 VAE Loader\"} }, \"9\": { \"inputs\": { \"box_v\": box_v, \"octree_resolution\": octree_resolution, \"num_chunks\": num_chunks, \"mc_level\": mc_level, \"mc_algo\": mc_algo, \"enable_flash_vdm\": enable_flash_vdm, \"force_offload\": force_offload, \"vae\": [\"4\", 0], \"latents\": [\"10\", 0] }, \"class_type\": \"Hy3D21VAEDecode\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 VAE Decoder\"} }, \"10\": { \"inputs\": { \"model\": HY3D_MODEL, \"steps\": steps, \"guidance_scale\": guidance_scale, \"seed\": seed, \"attention_mode\": attention_mode, \"image\": [\"14\", 2] }, \"class_type\": \"Hy3DMeshGenerator\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Mesh Generator\"} }, \"11\": { \"inputs\": { \"model_file\": [\"20\", 0], \"image\": \"\" }, \"class_type\": \"Preview3D\", \"_meta\": {\"title\": \"Preview 3D\"} }, \"14\": { \"inputs\": { \"image\": image_name }, \"class_type\": \"Hy3D21LoadImageWithTransparency\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Load Image with Transparency\"} }, \"19\": { \"inputs\": { \"remove_floaters\": remove_floaters, \"remove_degenerate_faces\": remove_degenerate_faces, \"reduce_faces\": reduce_faces, \"max_facenum\": max_facenum, \"smooth_normals\": smooth_normals, \"trimesh\": [\"9\", 0] }, \"class_type\": \"Hy3D21PostprocessMesh\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Post Process Trimesh\"} }, \"20\": { \"inputs\": { \"filename_prefix\": filename_prefix, \"file_format\": file_format, \"save_file\": True, \"trimesh\": [\"19\", 0] }, \"class_type\": \"Hy3D21ExportMesh\", \"_meta\": {\"title\": \"Hunyuan 3D 2.1 Export Mesh\"} } } print(\"Submitting Hunyuan 3D 2.1 mesh generation workflow...\") print(f\"Input Image: {image_name}\") print(f\"Steps: {steps}\") print(f\"Guidance Scale: {guidance_scale}\") print(f\"Random Seed: {seed}\") print(f\"Output Format: {file_format}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_mesh(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated 3D mesh files\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): # Check for 3D model files if 'gltf' in output: for model_info in output['gltf']: filename = model_info['filename'] # Download 3D model model_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if model_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(model_response.content) downloaded_files.append(output_path) print(f\"3D model saved: {output_path}\") # Check for preview images if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download preview image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Preview image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, image_paths, **kwargs): \"\"\"Batch generate 3D meshes from multiple images\"\"\" results = [] for i, image_path in enumerate(image_paths): print(f\"\\nStarting 3D generation task {i+1}/{len(image_paths)}...\") try: task_id, seed = self.generate_3d_mesh(image_path=image_path, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_mesh(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'input_image': image_path }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(10) # 3D generation takes longer except Exception as e: print(f\"Task {i+1} error: {e}\") return results def generate_with_quality_settings(self, image_path, quality_preset=\"high\"): \"\"\"Generate 3D mesh with predefined quality settings\"\"\" quality_presets = { \"low\": { \"steps\": 15, \"octree_resolution\": 128, \"num_chunks\": 32000, \"max_facenum\": 50000 }, \"medium\": { \"steps\": 20, \"octree_resolution\": 256, \"num_chunks\": 64000, \"max_facenum\": 100000 }, \"high\": { \"steps\": 25, \"octree_resolution\": 512, \"num_chunks\": 128000, \"max_facenum\": 200000 }, \"ultra\": { \"steps\": 30, \"octree_resolution\": 1024, \"num_chunks\": 256000, \"max_facenum\": 500000 } } settings = quality_presets.get(quality_preset, quality_presets[\"high\"]) return self.generate_3d_mesh(image_path=image_path, **settings) def main(): \"\"\"Main function - Execute Hunyuan 3D 2.1 mesh generation\"\"\" client = ComfyUIHunyuan3DClient() try: print(\"Hunyuan 3D 2.1 mesh generation client started...\") # Single 3D mesh generation example print(\"\\n=== Single 3D Mesh Generation ===\") # You can either provide a local image path or use an existing image name input_image_path = None # Set to your image path, e.g., \"input_image.jpg\" input_image_name = DEFAULT_IMAGE # Or use existing image name task_id, seed = client.generate_3d_mesh( image_path=input_image_path, image_name=input_image_name, steps=25, guidance_scale=5, octree_resolution=256, max_facenum=200000, file_format=\"glb\" ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion (3D generation takes longer) while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"3D mesh generation completed!\") break elif status == \"failed\": print(\"Generation failed!\") return time.sleep(10) # Download 3D mesh and preview files downloaded_files = client.download_mesh(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Quality preset example print(\"\\n=== Quality Preset Example ===\") # Uncomment to test different quality settings # if input_image_path: # for quality in [\"low\", \"medium\", \"high\"]: # print(f\"Generating with {quality} quality...\") # task_id, seed = client.generate_with_quality_settings(input_image_path, quality) # # Wait and download logic here... # Batch generation example print(\"\\n=== Batch Generation Example ===\") batch_images = [ # \"image1.jpg\", # \"image2.png\", # \"image3.jpeg\" ] # Uncomment to run batch generation # if batch_images: # batch_results = client.generate_batch( # batch_images, # steps=20, # guidance_scale=5, # file_format=\"glb\" # ) # print(f\"Batch generation completed, generated {len(batch_results)} 3D models\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API Execute"},{"location":"hunyuan3d/2.1/doc/index-en/#hunyuan3d-21-advanced-tips","text":"### \ud83c\udfaf Image Preparation is Crucial \ud83d\udcf8 Input Image Optimization Clean backgrounds and clear subjects are crucial for Hunyuan3D-2.1's optimal performance. The model performs significantly better with well-prepared input images\u2014remove backgrounds, center the subject, and ensure good lighting on the main object. ### \ud83d\udd27 Mesh Quality Control","title":"\ud83d\udca1 Hunyuan3D 2.1 Advanced Tips"},{"location":"hunyuan3d/2.1/doc/index-en/#hunyuan3d-21-application-scenarios","text":"\ud83c\udfae","title":"\ud83c\udfaf Hunyuan3D 2.1 Application Scenarios"},{"location":"hunyuan3d/2.1/doc/index-en/#technical-specifications","text":"### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 16GB 24GB+ System RAM 32GB 64GB+ Storage Space 50GB 100GB+ SSD Recommended GPU RTX 4080 RTX 4090 / A100 ### \ud83c\udfa8 Output Formats and Quality .glb Format PBR Materials Albedo Maps Metallic Maps Roughness Maps UV Mapping ### \ud83d\udcca Performance Benchmarks \u23f1\ufe0f Generation Time Shape generation: 2-5 minutes | Texture generation: 3-8 minutes (depending on resolution settings) \ud83c\udfaf Quality Improvement Compared to version 2.0, texture detail improved by 40%, geometric accuracy improved by 25% \ud83d\udcbe File Size Typical output: 5-20MB .glb files (including complete PBR materials)","title":"\ud83d\udd27 Technical Specifications"},{"location":"hunyuan3d/2.1/doc/index-en/#usage-tips-and-recommendations","text":"","title":"\ud83d\udca1 Usage Tips and Recommendations"},{"location":"isaaclab/","text":"\ud83e\udd16 NVIDIA Isaac Lab \u4f7f\u7528\u6307\u5357 \u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f00\u6e90\u7edf\u4e00\u6846\u67b6 - \u5355\u673a\u7248\u4e0e\u96c6\u7fa4\u7248\u5b8c\u6574\u6559\u7a0b \ud83c\udfaf \u6846\u67b6\u7b80\u4ecb **NVIDIA Isaac\u2122 Lab** \u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f00\u6e90\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002 \ud83d\udd27 \u6838\u5fc3\u6280\u672f\u6808 \u2022 \u57fa\u4e8e NVIDIA Isaac Sim\u2122 \u5f00\u53d1 \u2022 \u4f7f\u7528 NVIDIA\u00ae PhysX\u00ae \u7269\u7406\u5f15\u64ce \u2022 \u96c6\u6210 NVIDIA RTX\u2122 \u6e32\u67d3\u6280\u672f \u2022 \u63d0\u4f9b\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u73af\u5883 **\u6838\u5fc3\u4ef7\u503c**\uff1a\u5f25\u5408\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u57fa\u4e8e\u611f\u77e5\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u66f4\u591a\u673a\u5668\u4eba\u5e94\u7528\u3002 \ud83d\udda5\ufe0f IsaacLab \u5355\u673a\u7248\u4f7f\u7528\u6559\u7a0b ### \ud83c\udfae \u670d\u52a1\u7279\u6027 Isaac Lab\u670d\u52a1\u5b9e\u4f8b\u5185\u7f6e\u5b8c\u6574\u7684Isaac Sim\u5e94\u7528\uff0c\u652f\u6301\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f\uff1a - **\u72ec\u7acb\u4eff\u771f\u8bad\u7ec3**\uff1a\u4f7f\u7528Isaac Sim\u8fdb\u884c\u4eff\u771f\u8bad\u7ec3 - **\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3**\uff1a\u57fa\u4e8eIsaac Lab\u6846\u67b6\u8fdb\u884cRL\u8bad\u7ec3 \u2705 \u7cfb\u7edf\u914d\u7f6e ECS\u5b9e\u4f8b\u9884\u88c5Ubuntu\u56fe\u5f62\u754c\u9762\uff0c\u652f\u6301\u901a\u8fc7VNC\u65b9\u5f0f\u5728ECS\u63a7\u5236\u53f0\u76f4\u63a5\u4f7f\u7528 \ud83d\udd17 VNC\u65b9\u5f0f\u8bbf\u95eeECS\u5b9e\u4f8b \ud83d\udda5\ufe0f VncServer + VncRealViewer \u65b9\u5f0f\uff08\u63a8\u8350\uff09 #### \ud83d\udccb \u64cd\u4f5c\u6b65\u9aa4 **\u6b65\u9aa4 1\uff1a\u767b\u5f55ECS\u5b9e\u4f8b** 1. \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b 2. \u70b9\u51fb**\u8fdc\u7a0b\u8fde\u63a5**\u8fdb\u884c\u767b\u5f55 ![img_1.png](img_1.png) **\u6b65\u9aa4 2\uff1a\u914d\u7f6eVNC\u670d\u52a1** # \u5207\u6362\u5230root\u8d26\u6237 sudo su root # \u8bbe\u7f6eVNC\u670d\u52a1\u5bc6\u7801\uff08\u6ce8\u610f\uff1a\u5bc6\u7801\u957f\u5ea6\u6700\u5927\u4e3a8\u4f4d\uff09 /opt/TurboVNC/bin/vncpasswd # \u542f\u52a8VNC Server\u670d\u52a1\uff08\u76d1\u542c5901\u7aef\u53e3\uff09 /opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24 -xstartup ~/.vnc/xstartup **\u6b65\u9aa4 3\uff1a\u5ba2\u6237\u7aef\u8fde\u63a5** 1. \u4e0b\u8f7d [VncRealViewer\u5ba2\u6237\u7aef](https://www.realvnc.com/en/connect/download/viewer/) 2. \u8fde\u63a5\u5730\u5740\uff1a` < \u670d\u52a1\u5668\u516c\u7f51IP>:5901` ![img_2.png](img_2.png) ![img_3.png](img_3.png) **\u6b65\u9aa4 4\uff1a\u542f\u52a8Isaac Sim** cd /home/isaac-sim/isaacsim ./isaac-sim.sh --allow-root ![img_4.png](img_4.png) \ud83d\udda5\ufe0f ECS\u63a7\u5236\u53f0\u539f\u751fVNC\u65b9\u5f0f #### \ud83d\udccb \u64cd\u4f5c\u6b65\u9aa4 **\u6b65\u9aa4 1\uff1a\u8fdb\u5165ECS\u63a7\u5236\u53f0** 1. \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b 2. \u70b9\u51fb**\u53bb\u5230ECS\u63a7\u5236\u53f0** ![img_17.png](img_17.png) **\u6b65\u9aa4 2\uff1aVNC\u767b\u5f55** 1. \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684**\u8fdc\u7a0b\u8fde\u63a5** 2. \u9009\u62e9**VNC\u767b\u5f55\u65b9\u5f0f** 3. \u8f93\u5165isaac-sim\u8d26\u6237\u5bc6\u7801\uff08\u4e0eECS\u5b9e\u4f8b\u5bc6\u7801\u4e00\u81f4\uff09 ![img_18.png](img_18.png) \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u5bc6\u7801\u53ef\u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u67e5\u770b ![img_19.png](img_19.png) \ud83c\udfae Isaac Sim\u4f7f\u7528\u65b9\u5f0f #### \ud83d\udcc1 \u76ee\u5f55\u7ed3\u6784 \u767b\u5f55ECS\u5b9e\u4f8b\u540e\uff0cisaac-sim\u8d26\u6237\u4e0b\u5305\u542b\u4e24\u4e2a\u91cd\u8981\u76ee\u5f55\uff1a \ud83d\udcc2 isaacsim Isaac Sim\u5b89\u88c5\u76ee\u5f55 \u5305\u542b\u542f\u52a8\u548c\u8bad\u7ec3\u811a\u672c \ud83d\udcc2 isaacsim_assets Isaac Sim\u8d44\u6e90\u76ee\u5f55 \u9884\u4e0b\u8f7d\u7684\u8bad\u7ec3\u8d44\u6e90 ![img_21.png](img_21.png) \ud83d\udd2c \u793a\u4f8b1\uff1a\u65e0GUI\u573a\u666f\u5408\u6210\u6570\u636e\u96c6\u751f\u6210 **\u529f\u80fd\u8bf4\u660e**\uff1a\u4f7f\u7528omni.replicator\u6269\u5c55\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u6570\u636e\u79bb\u7ebf\u5b58\u50a8\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3002 # \u521b\u5efa\u5de5\u4f5c\u76ee\u5f55 cd /home/isaac-sim mkdir -p isaacsim_test cd /home/isaac-sim/isaacsim_test mkdir -p scene_based_sdg # \u590d\u5236\u793a\u4f8b\u4ee3\u7801 cp -rf /home/isaac-sim/isaacsim/standalone_examples/replicator/scene_based_sdg/* \\ /home/isaac-sim/isaacsim_test/scene_based_sdg/ # \u6267\u884c\u6e32\u67d3\u5408\u6210 /home/isaac-sim/isaacsim/python.sh ./scene_based_sdg/scene_based_sdg.py \\ --config=\"/home/isaac-sim/isaacsim_test/scene_based_sdg/config/config_coco_writer.yaml\" \\ --/persistent/isaac/asset_root/default=\"/home/isaac-sim/isaacsim_assets/Assets/Isaac/4.5\" \u2705 \u8f93\u51fa\u7ed3\u679c \u751f\u6210\u7ed3\u679c\u5b58\u50a8\u5728 ./isaacsim_test/_out_coco \u76ee\u5f55\u4e2d \u751f\u6210\u7ed3\u679c\u5b58\u50a8\u5728\"./isaacsim_test/_out_coco\"\u4e2d\uff0c\u53ef\u89c6\u5316\u6548\u679c\u5982\u4e0b\uff1a ![img_23.png](img_23.png)![img_22.png](img_22.png) \ud83d\uddbc\ufe0f \u793a\u4f8b2\uff1aGUI\u65b9\u5f0f\u4f7f\u7528Isaac Sim cd /home/isaac-sim/isaacsim ./isaac-sim.sh \u26a0\ufe0f \u542f\u52a8\u63d0\u793a Isaac Sim\u542f\u52a8\u8f83\u6162\uff0c\u4f1a\u5f39\u51fa\u7b49\u5f85\u7a97\u53e3\uff0c\u8bf7\u8010\u5fc3\u7b49\u5f85\uff0c\u65e0\u9700\u64cd\u4f5c ![img_24.png](img_24.png) **\u53c2\u8003\u8d44\u6e90**\uff1a\u53ef\u6309\u7167 [\u5b98\u65b9\u5165\u95e8\u6559\u7a0b](https://docs.isaacsim.omniverse.nvidia.com/4.5.0/introduction/quickstart_isaacsim.html) \u521b\u5efa\u57fa\u7840\u573a\u666f \u4e0b\u9762\u662f\u6309\u5165\u95e8\u6559\u7a0b\u4e2d\u7684\u6b65\u9aa4\u521b\u5efa\u4e86\u4e2a\u6b63\u65b9\u4f53\u3002 ![img_25.png](img_25.png) \ud83e\udd16 Isaac Lab\u4f7f\u7528\u65b9\u5f0f **\u5b89\u88c5\u8def\u5f84**\uff1a`/home/isaac-sim/IsaacLab` \ud83c\udfaf \u793a\u4f8b1\uff1a\u65e0GUI\u6a21\u5f0f\u667a\u80fd\u4f53\u8bad\u7ec3 **\u8bad\u7ec3\u76ee\u6807**\uff1a\u4f7f\u7528Stable-Baselines3\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3Cartpole\u5e73\u8861\u63a7\u5236\u4efb\u52a1 \ud83c\udfaf \u4efb\u52a1\u63cf\u8ff0 \u8ba9\u667a\u80fd\u4f53\u5b66\u4e60\u63a7\u5236\u5c0f\u8f66\u5de6\u53f3\u79fb\u52a8\uff0c\u4fdd\u6301\u6446\u6746\u76f4\u7acb\u4e0d\u5012 # \u521b\u5efa\u5de5\u4f5c\u76ee\u5f55 cd /home/isaac-sim mkdir -p isaaclab_test cd /home/isaac-sim/isaaclab_test mkdir -p sb3 # \u590d\u5236\u793a\u4f8b\u4ee3\u7801 cp -rf /home/isaac-sim/IsaacLab/scripts/reinforcement_learning/sb3/* \\ /home/isaac-sim/isaaclab_test/sb3/ # \u5f00\u59cb\u8bad\u7ec3 /home/isaac-sim/IsaacLab/isaaclab.sh -p ./sb3/train.py \\ --task Isaac-Cartpole-v0 \\ --num_envs 64 \\ --headless \\ --video \u2705 \u8bad\u7ec3\u7ed3\u679c \u7ed3\u679c\u4fdd\u5b58\u81f3\uff1a ./logs/sb3/Isaac-Cartpole-v0 \u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u5230./logs/sb3/Isaac-Cartpole-v0\u4e2d\uff1b\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b ![img_26.png](img_26.png) \ud83c\udfae \u793a\u4f8b2\uff1aGUI\u6a21\u5f0f\u573a\u666f\u751f\u6210 cd /home/isaac-sim/IsaacLab ./isaaclab.sh -p scripts/tutorials/00_sim/spawn_prims.py **\u529f\u80fd**\uff1a\u5728GUI\u754c\u9762\u4e2d\u751f\u6210\u57fa\u672c\u7269\u4f53\u5230\u573a\u666f\u4e2d ![img_27.png](img_27.png) \u2601\ufe0f IsaacLab \u96c6\u7fa4\u7248\u4f7f\u7528\u6559\u7a0b Isaac Lab \u652f\u6301 **Ray** \u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u591a\u4e2a\u8bad\u7ec3\u4efb\u52a1\u7684\u8c03\u5ea6\uff08\u5305\u62ec\u5e76\u884c\u548c\u4e32\u884c\uff09\u4ee5\u53ca\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u9002\u7528\u4e8e\u672c\u5730\u548c\u8fdc\u7a0b\u914d\u7f6e\u3002 \ud83d\udcda \u5b98\u65b9\u6587\u6863 Isaac Lab\u670d\u52a1Ray\u4f5c\u4e1a\u8c03\u5ea6\u548c\u8c03\u4f18\u5b98\u65b9\u6587\u6863\u4e3aRay Job Dispatch and Tuning \ud83d\udee0\ufe0f \u73af\u5883\u51c6\u5907 #### \u6b65\u9aa41\uff1a\u914d\u7f6e\u8fdc\u7a0bRay\u96c6\u7fa4 # \u914d\u7f6e\u96c6\u7fa4\u4fe1\u606f\uff08ISAACRAY_ADDRESS\u4ece\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u83b7\u53d6\uff09 echo \"name: isaacray address: <ISAACRAY_ADDRESS>\" > ~/.cluster_config export RAY_ADDRESS=\"<ISAACRAY_ADDRESS>\" #### \u6b65\u9aa42\uff1a\u4e0b\u8f7d\u6e90\u7801 \u4eceGitHub\u4e0b\u8f7d [Isaac Lab\u6e90\u7801](https://github.com/isaac-sim/IsaacLab) #### \u6b65\u9aa43\uff1a\u5b89\u88c5Ray\u5ba2\u6237\u7aef pip install \"ray[default]\" \ud83e\uddea \u6d4b\u8bd5Job\uff1a\u65e5\u5fd7\u8f93\u51fa\u793a\u4f8b #### \u521b\u5efa\u6d4b\u8bd5\u6587\u4ef6 \u5728 `scripts/reinforcement_learning/ray/` \u76ee\u5f55\u4e0b\u521b\u5efa `test.py`\uff1a import ray import os # \u8fde\u63a5\u672c\u5730\u6216\u8005\u8fdc\u7a0bray cluster ray.init() @ray.remote(num_cpus=1) class Counter: def __init__(self): self.name = \"test_counter\" self.counter = 0 def increment(self): self.counter += 1 def get_counter(self): return \"{} got {}\".format(self.name, self.counter) counter = Counter.remote() for _ in range(10): counter.increment.remote() print(ray.get(counter.get_counter.remote())) #### \u63d0\u4ea4\u4f5c\u4e1a python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p test.py\" \u2705 \u6267\u884c\u7ed3\u679c \u2022 \u63d0\u4ea4\u65f6\u4f1a\u6253\u5305 scripts/reinforcement_learning/ray \u76ee\u5f55 \u2022 \u8fd0\u884c\u5b8c\u6210\u540e\u53ef\u5728\u65e5\u5fd7\u4e2d\u67e5\u770b\u8f93\u51fa\u4fe1\u606f \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u4ece\u65e5\u5fd7\u91cc\u770b\u5230\u4ee5\u4e0b\u4fe1\u606f\uff1a - \u63d0\u4ea4\u4f5c\u4e1a\u65f6\uff0c\u4f1a\u628ascripts/reinforcement_learning/ray\u76ee\u5f55\u5f53\u4f5c\u5de5\u4f5c\u76ee\u5f55\u8fdb\u884c\u6253\u5305\uff0c\u4e0a\u4f20\u5230\u96c6\u7fa4\u4e2d\uff0c\u6240\u4ee5\u6211\u4eec\u7684test.py\u4e5f\u4f1a\u88ab\u4e0a\u4f20\u3002 ![img_5.png](img_5.png) - job\u8fd0\u884c\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u8f93\u51fa\u7684\u8fd0\u884c\u4fe1\u606f\uff1a ![img_6.png](img_6.png) \ud83d\ude80 Isaac Lab\u8bad\u7ec3\u4efb\u52a1\u6267\u884c #### \u63d0\u4ea4\u8bad\u7ec3\u4f5c\u4e1a python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p /workspace/isaaclab/scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Ant-v0 --headless\" #### \u76d1\u63a7\u4f5c\u4e1a\u6267\u884c \ud83d\udd0d \u6b65\u9aa41\uff1a\u67e5\u770b\u65e5\u5fd7 \u63d0\u4ea4\u6210\u529f\u540e\u67e5\u770b\u5de5\u4f5c\u76ee\u5f55 \u4f8b\uff1a _ray_pkg_18b3cac8e32d6f62 \ud83c\udf10 \u6b65\u9aa42\uff1aWeb UI\u76d1\u63a7 \u8bbf\u95eeRay\u96c6\u7fa4URL \u67e5\u770bJob\u8fd0\u884c\u72b6\u6001 \ud83d\udd0e \u6b65\u9aa43\uff1a\u5b9a\u4f4d\u8282\u70b9 \u8bb0\u5f55\u8c03\u5ea6\u8282\u70b9ID \u4f8b\uff1a c9db26a6c016fb43... \ud83d\udcca \u6b65\u9aa44\uff1a\u67e5\u770b\u7ed3\u679c \u5728Cluster Tab\u641c\u7d22\u8282\u70b9 \u767b\u5f55\u5bf9\u5e94Pod\u67e5\u770b\u8bad\u7ec3\u7ed3\u679c \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u65e5\u5fd7\u91cc\u8f93\u51fa\u7684\u4fe1\u606f\uff0c\u8fd9\u91cc\u4e3b\u8981\u53ef\u4ee5\u770b\u5230job\u5728\u96c6\u7fa4\u4e0a\u7684\u5de5\u4f5c\u76ee\u5f55\uff0c\u672c\u4f8b\u4e0a\u4e3a_ray_pkg_18b3cac8e32d6f62\u3002 ![img_7.png](img_7.png) \u70b9\u51fbray\u96c6\u7fa4url, \u53ef\u4ee5\u5230\u96c6\u7fa4\u7684web ui\u4e2d\u67e5\u770bjob\u8fd0\u884c\u60c5\u51b5\u3002 ![img_8.png](img_8.png) \u70b9\u51fb\u6b63\u5728\u8fd0\u884c\u7684\u8fd9\u4e2ajob\uff0c\u53ef\u4ee5\u770b\u5230job\u7684\u8c03\u5ea6\u65e5\u5fd7\uff0c\u672c\u4f8b\u662f\u8c03\u5ea6\u5230\u4e86c9db26a6c016fb4394991190f132afe99cd4a2b0a696f14185001650\u8282\u70b9\uff0c\u5bf9\u5e94\u7684\u8bad\u7ec3\u7ed3\u679c\u4e5f\u8981\u5230\u8fd9\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u3002 ![img_9.png](img_9.png) \u5207\u5230Cluster Tab\u4e0b\uff0c\u8f93\u5165node id\u8fdb\u884c\u641c\u7d22\uff0c\u53ef\u4ee5\u627e\u5230\u5bb9\u5668\u96c6\u7fa4\u4e2d\u5bf9\u5e94\u7684Pod\u3002 ![img_10.png](img_10.png) \u4ece\u670d\u52a1\u5b9e\u4f8b\u8d44\u6e90\u4e2d\u627e\u5230\u5bf9\u5e94\u7684\u5bb9\u5668\u96c6\u7fa4\uff0c\u53bb\u5bb9\u5668\u96c6\u7fa4\u4e2d\u627e\u5230\u4e0a\u9762\u5bf9\u5e94\u7684Pod\uff0c\u5e76\u767b\u5f55\u5230Pod\u4e2d\u3002 ![img_11.png](img_11.png) #### \u8bad\u7ec3\u7ed3\u679c\u67e5\u770b \u767b\u5f55\u5230Pod\u540e\uff0c\u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u8def\u5f84\uff1a # \u4e34\u65f6\u76ee\u5f55\u7ed3\u6784\u8bf4\u660e # _ray_pkg_18b3cac8e32d6f62: \u4e0a\u4f20\u6587\u4ef6\u76ee\u5f55\uff08\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u53d8\u5316\uff09 # 2025-08-21_08-08-24: \u5177\u4f53\u8fd0\u884c\u65f6\u95f4 cd /tmp/ray/session_latest/runtime_resources/working_dir_files/_ray_pkg_18b3cac8e32d6f62/logs/rsl_rl/ant/2025-08-21_08-08-24 \u767b\u5f55\u5230Pod\u4e2d\uff0c\u53ef\u4ee5\u770b\u5230\u8bad\u7ec3\u7ed3\u679c\uff0c\u672c\u4f8b\u8bad\u7ec3\u7684Ant\u73af\u5883\uff0c\u8bad\u7ec3\u7684\u7ed3\u679c\u4fdd\u5b58\u5728\u4e0b\u9762\u7684\u4e34\u65f6\u76ee\u5f55\u4e2d\uff1a ![img_12.png](img_12.png) \ud83e\udd16 NVIDIA Isaac Lab | \u8ba9\u673a\u5668\u4eba\u5b66\u4e60\u66f4\u7b80\u5355\u9ad8\u6548","title":"Index"},{"location":"isaaclab/#_1","text":"**NVIDIA Isaac\u2122 Lab** \u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f00\u6e90\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002 \ud83d\udd27 \u6838\u5fc3\u6280\u672f\u6808 \u2022 \u57fa\u4e8e NVIDIA Isaac Sim\u2122 \u5f00\u53d1 \u2022 \u4f7f\u7528 NVIDIA\u00ae PhysX\u00ae \u7269\u7406\u5f15\u64ce \u2022 \u96c6\u6210 NVIDIA RTX\u2122 \u6e32\u67d3\u6280\u672f \u2022 \u63d0\u4f9b\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u73af\u5883 **\u6838\u5fc3\u4ef7\u503c**\uff1a\u5f25\u5408\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u57fa\u4e8e\u611f\u77e5\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u66f4\u591a\u673a\u5668\u4eba\u5e94\u7528\u3002","title":"\ud83c\udfaf \u6846\u67b6\u7b80\u4ecb"},{"location":"isaaclab/#isaaclab","text":"### \ud83c\udfae \u670d\u52a1\u7279\u6027 Isaac Lab\u670d\u52a1\u5b9e\u4f8b\u5185\u7f6e\u5b8c\u6574\u7684Isaac Sim\u5e94\u7528\uff0c\u652f\u6301\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f\uff1a - **\u72ec\u7acb\u4eff\u771f\u8bad\u7ec3**\uff1a\u4f7f\u7528Isaac Sim\u8fdb\u884c\u4eff\u771f\u8bad\u7ec3 - **\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3**\uff1a\u57fa\u4e8eIsaac Lab\u6846\u67b6\u8fdb\u884cRL\u8bad\u7ec3 \u2705 \u7cfb\u7edf\u914d\u7f6e ECS\u5b9e\u4f8b\u9884\u88c5Ubuntu\u56fe\u5f62\u754c\u9762\uff0c\u652f\u6301\u901a\u8fc7VNC\u65b9\u5f0f\u5728ECS\u63a7\u5236\u53f0\u76f4\u63a5\u4f7f\u7528","title":"\ud83d\udda5\ufe0f IsaacLab \u5355\u673a\u7248\u4f7f\u7528\u6559\u7a0b"},{"location":"isaaclab/#vncecs","text":"\ud83d\udda5\ufe0f VncServer + VncRealViewer \u65b9\u5f0f\uff08\u63a8\u8350\uff09 #### \ud83d\udccb \u64cd\u4f5c\u6b65\u9aa4 **\u6b65\u9aa4 1\uff1a\u767b\u5f55ECS\u5b9e\u4f8b** 1. \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b 2. \u70b9\u51fb**\u8fdc\u7a0b\u8fde\u63a5**\u8fdb\u884c\u767b\u5f55 ![img_1.png](img_1.png) **\u6b65\u9aa4 2\uff1a\u914d\u7f6eVNC\u670d\u52a1** # \u5207\u6362\u5230root\u8d26\u6237 sudo su root # \u8bbe\u7f6eVNC\u670d\u52a1\u5bc6\u7801\uff08\u6ce8\u610f\uff1a\u5bc6\u7801\u957f\u5ea6\u6700\u5927\u4e3a8\u4f4d\uff09 /opt/TurboVNC/bin/vncpasswd # \u542f\u52a8VNC Server\u670d\u52a1\uff08\u76d1\u542c5901\u7aef\u53e3\uff09 /opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24 -xstartup ~/.vnc/xstartup **\u6b65\u9aa4 3\uff1a\u5ba2\u6237\u7aef\u8fde\u63a5** 1. \u4e0b\u8f7d [VncRealViewer\u5ba2\u6237\u7aef](https://www.realvnc.com/en/connect/download/viewer/) 2. \u8fde\u63a5\u5730\u5740\uff1a` < \u670d\u52a1\u5668\u516c\u7f51IP>:5901` ![img_2.png](img_2.png) ![img_3.png](img_3.png) **\u6b65\u9aa4 4\uff1a\u542f\u52a8Isaac Sim** cd /home/isaac-sim/isaacsim ./isaac-sim.sh --allow-root ![img_4.png](img_4.png) \ud83d\udda5\ufe0f ECS\u63a7\u5236\u53f0\u539f\u751fVNC\u65b9\u5f0f #### \ud83d\udccb \u64cd\u4f5c\u6b65\u9aa4 **\u6b65\u9aa4 1\uff1a\u8fdb\u5165ECS\u63a7\u5236\u53f0** 1. \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b 2. \u70b9\u51fb**\u53bb\u5230ECS\u63a7\u5236\u53f0** ![img_17.png](img_17.png) **\u6b65\u9aa4 2\uff1aVNC\u767b\u5f55** 1. \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684**\u8fdc\u7a0b\u8fde\u63a5** 2. \u9009\u62e9**VNC\u767b\u5f55\u65b9\u5f0f** 3. \u8f93\u5165isaac-sim\u8d26\u6237\u5bc6\u7801\uff08\u4e0eECS\u5b9e\u4f8b\u5bc6\u7801\u4e00\u81f4\uff09 ![img_18.png](img_18.png) \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u5bc6\u7801\u53ef\u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u67e5\u770b ![img_19.png](img_19.png)","title":"\ud83d\udd17 VNC\u65b9\u5f0f\u8bbf\u95eeECS\u5b9e\u4f8b"},{"location":"isaaclab/#isaac-sim","text":"#### \ud83d\udcc1 \u76ee\u5f55\u7ed3\u6784 \u767b\u5f55ECS\u5b9e\u4f8b\u540e\uff0cisaac-sim\u8d26\u6237\u4e0b\u5305\u542b\u4e24\u4e2a\u91cd\u8981\u76ee\u5f55\uff1a \ud83d\udcc2 isaacsim Isaac Sim\u5b89\u88c5\u76ee\u5f55 \u5305\u542b\u542f\u52a8\u548c\u8bad\u7ec3\u811a\u672c \ud83d\udcc2 isaacsim_assets Isaac Sim\u8d44\u6e90\u76ee\u5f55 \u9884\u4e0b\u8f7d\u7684\u8bad\u7ec3\u8d44\u6e90 ![img_21.png](img_21.png)","title":"\ud83c\udfae Isaac Sim\u4f7f\u7528\u65b9\u5f0f"},{"location":"isaaclab/#1gui","text":"**\u529f\u80fd\u8bf4\u660e**\uff1a\u4f7f\u7528omni.replicator\u6269\u5c55\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u6570\u636e\u79bb\u7ebf\u5b58\u50a8\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3002 # \u521b\u5efa\u5de5\u4f5c\u76ee\u5f55 cd /home/isaac-sim mkdir -p isaacsim_test cd /home/isaac-sim/isaacsim_test mkdir -p scene_based_sdg # \u590d\u5236\u793a\u4f8b\u4ee3\u7801 cp -rf /home/isaac-sim/isaacsim/standalone_examples/replicator/scene_based_sdg/* \\ /home/isaac-sim/isaacsim_test/scene_based_sdg/ # \u6267\u884c\u6e32\u67d3\u5408\u6210 /home/isaac-sim/isaacsim/python.sh ./scene_based_sdg/scene_based_sdg.py \\ --config=\"/home/isaac-sim/isaacsim_test/scene_based_sdg/config/config_coco_writer.yaml\" \\ --/persistent/isaac/asset_root/default=\"/home/isaac-sim/isaacsim_assets/Assets/Isaac/4.5\" \u2705 \u8f93\u51fa\u7ed3\u679c \u751f\u6210\u7ed3\u679c\u5b58\u50a8\u5728 ./isaacsim_test/_out_coco \u76ee\u5f55\u4e2d \u751f\u6210\u7ed3\u679c\u5b58\u50a8\u5728\"./isaacsim_test/_out_coco\"\u4e2d\uff0c\u53ef\u89c6\u5316\u6548\u679c\u5982\u4e0b\uff1a ![img_23.png](img_23.png)![img_22.png](img_22.png)","title":"\ud83d\udd2c \u793a\u4f8b1\uff1a\u65e0GUI\u573a\u666f\u5408\u6210\u6570\u636e\u96c6\u751f\u6210"},{"location":"isaaclab/#2guiisaac-sim","text":"cd /home/isaac-sim/isaacsim ./isaac-sim.sh \u26a0\ufe0f \u542f\u52a8\u63d0\u793a Isaac Sim\u542f\u52a8\u8f83\u6162\uff0c\u4f1a\u5f39\u51fa\u7b49\u5f85\u7a97\u53e3\uff0c\u8bf7\u8010\u5fc3\u7b49\u5f85\uff0c\u65e0\u9700\u64cd\u4f5c ![img_24.png](img_24.png) **\u53c2\u8003\u8d44\u6e90**\uff1a\u53ef\u6309\u7167 [\u5b98\u65b9\u5165\u95e8\u6559\u7a0b](https://docs.isaacsim.omniverse.nvidia.com/4.5.0/introduction/quickstart_isaacsim.html) \u521b\u5efa\u57fa\u7840\u573a\u666f \u4e0b\u9762\u662f\u6309\u5165\u95e8\u6559\u7a0b\u4e2d\u7684\u6b65\u9aa4\u521b\u5efa\u4e86\u4e2a\u6b63\u65b9\u4f53\u3002 ![img_25.png](img_25.png)","title":"\ud83d\uddbc\ufe0f \u793a\u4f8b2\uff1aGUI\u65b9\u5f0f\u4f7f\u7528Isaac Sim"},{"location":"isaaclab/#isaac-lab","text":"**\u5b89\u88c5\u8def\u5f84**\uff1a`/home/isaac-sim/IsaacLab`","title":"\ud83e\udd16 Isaac Lab\u4f7f\u7528\u65b9\u5f0f"},{"location":"isaaclab/#1gui_1","text":"**\u8bad\u7ec3\u76ee\u6807**\uff1a\u4f7f\u7528Stable-Baselines3\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3Cartpole\u5e73\u8861\u63a7\u5236\u4efb\u52a1 \ud83c\udfaf \u4efb\u52a1\u63cf\u8ff0 \u8ba9\u667a\u80fd\u4f53\u5b66\u4e60\u63a7\u5236\u5c0f\u8f66\u5de6\u53f3\u79fb\u52a8\uff0c\u4fdd\u6301\u6446\u6746\u76f4\u7acb\u4e0d\u5012 # \u521b\u5efa\u5de5\u4f5c\u76ee\u5f55 cd /home/isaac-sim mkdir -p isaaclab_test cd /home/isaac-sim/isaaclab_test mkdir -p sb3 # \u590d\u5236\u793a\u4f8b\u4ee3\u7801 cp -rf /home/isaac-sim/IsaacLab/scripts/reinforcement_learning/sb3/* \\ /home/isaac-sim/isaaclab_test/sb3/ # \u5f00\u59cb\u8bad\u7ec3 /home/isaac-sim/IsaacLab/isaaclab.sh -p ./sb3/train.py \\ --task Isaac-Cartpole-v0 \\ --num_envs 64 \\ --headless \\ --video \u2705 \u8bad\u7ec3\u7ed3\u679c \u7ed3\u679c\u4fdd\u5b58\u81f3\uff1a ./logs/sb3/Isaac-Cartpole-v0 \u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u5230./logs/sb3/Isaac-Cartpole-v0\u4e2d\uff1b\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b ![img_26.png](img_26.png)","title":"\ud83c\udfaf \u793a\u4f8b1\uff1a\u65e0GUI\u6a21\u5f0f\u667a\u80fd\u4f53\u8bad\u7ec3"},{"location":"isaaclab/#2gui","text":"cd /home/isaac-sim/IsaacLab ./isaaclab.sh -p scripts/tutorials/00_sim/spawn_prims.py **\u529f\u80fd**\uff1a\u5728GUI\u754c\u9762\u4e2d\u751f\u6210\u57fa\u672c\u7269\u4f53\u5230\u573a\u666f\u4e2d ![img_27.png](img_27.png)","title":"\ud83c\udfae \u793a\u4f8b2\uff1aGUI\u6a21\u5f0f\u573a\u666f\u751f\u6210"},{"location":"isaaclab/#isaaclab_1","text":"Isaac Lab \u652f\u6301 **Ray** \u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u591a\u4e2a\u8bad\u7ec3\u4efb\u52a1\u7684\u8c03\u5ea6\uff08\u5305\u62ec\u5e76\u884c\u548c\u4e32\u884c\uff09\u4ee5\u53ca\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u9002\u7528\u4e8e\u672c\u5730\u548c\u8fdc\u7a0b\u914d\u7f6e\u3002 \ud83d\udcda \u5b98\u65b9\u6587\u6863 Isaac Lab\u670d\u52a1Ray\u4f5c\u4e1a\u8c03\u5ea6\u548c\u8c03\u4f18\u5b98\u65b9\u6587\u6863\u4e3aRay Job Dispatch and Tuning","title":"\u2601\ufe0f IsaacLab \u96c6\u7fa4\u7248\u4f7f\u7528\u6559\u7a0b"},{"location":"isaaclab/#_2","text":"#### \u6b65\u9aa41\uff1a\u914d\u7f6e\u8fdc\u7a0bRay\u96c6\u7fa4 # \u914d\u7f6e\u96c6\u7fa4\u4fe1\u606f\uff08ISAACRAY_ADDRESS\u4ece\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u83b7\u53d6\uff09 echo \"name: isaacray address: <ISAACRAY_ADDRESS>\" > ~/.cluster_config export RAY_ADDRESS=\"<ISAACRAY_ADDRESS>\" #### \u6b65\u9aa42\uff1a\u4e0b\u8f7d\u6e90\u7801 \u4eceGitHub\u4e0b\u8f7d [Isaac Lab\u6e90\u7801](https://github.com/isaac-sim/IsaacLab) #### \u6b65\u9aa43\uff1a\u5b89\u88c5Ray\u5ba2\u6237\u7aef pip install \"ray[default]\"","title":"\ud83d\udee0\ufe0f \u73af\u5883\u51c6\u5907"},{"location":"isaaclab/#job","text":"#### \u521b\u5efa\u6d4b\u8bd5\u6587\u4ef6 \u5728 `scripts/reinforcement_learning/ray/` \u76ee\u5f55\u4e0b\u521b\u5efa `test.py`\uff1a import ray import os # \u8fde\u63a5\u672c\u5730\u6216\u8005\u8fdc\u7a0bray cluster ray.init() @ray.remote(num_cpus=1) class Counter: def __init__(self): self.name = \"test_counter\" self.counter = 0 def increment(self): self.counter += 1 def get_counter(self): return \"{} got {}\".format(self.name, self.counter) counter = Counter.remote() for _ in range(10): counter.increment.remote() print(ray.get(counter.get_counter.remote())) #### \u63d0\u4ea4\u4f5c\u4e1a python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p test.py\" \u2705 \u6267\u884c\u7ed3\u679c \u2022 \u63d0\u4ea4\u65f6\u4f1a\u6253\u5305 scripts/reinforcement_learning/ray \u76ee\u5f55 \u2022 \u8fd0\u884c\u5b8c\u6210\u540e\u53ef\u5728\u65e5\u5fd7\u4e2d\u67e5\u770b\u8f93\u51fa\u4fe1\u606f \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u4ece\u65e5\u5fd7\u91cc\u770b\u5230\u4ee5\u4e0b\u4fe1\u606f\uff1a - \u63d0\u4ea4\u4f5c\u4e1a\u65f6\uff0c\u4f1a\u628ascripts/reinforcement_learning/ray\u76ee\u5f55\u5f53\u4f5c\u5de5\u4f5c\u76ee\u5f55\u8fdb\u884c\u6253\u5305\uff0c\u4e0a\u4f20\u5230\u96c6\u7fa4\u4e2d\uff0c\u6240\u4ee5\u6211\u4eec\u7684test.py\u4e5f\u4f1a\u88ab\u4e0a\u4f20\u3002 ![img_5.png](img_5.png) - job\u8fd0\u884c\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u8f93\u51fa\u7684\u8fd0\u884c\u4fe1\u606f\uff1a ![img_6.png](img_6.png)","title":"\ud83e\uddea \u6d4b\u8bd5Job\uff1a\u65e5\u5fd7\u8f93\u51fa\u793a\u4f8b"},{"location":"isaaclab/#isaac-lab_1","text":"#### \u63d0\u4ea4\u8bad\u7ec3\u4f5c\u4e1a python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p /workspace/isaaclab/scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Ant-v0 --headless\" #### \u76d1\u63a7\u4f5c\u4e1a\u6267\u884c \ud83d\udd0d \u6b65\u9aa41\uff1a\u67e5\u770b\u65e5\u5fd7 \u63d0\u4ea4\u6210\u529f\u540e\u67e5\u770b\u5de5\u4f5c\u76ee\u5f55 \u4f8b\uff1a _ray_pkg_18b3cac8e32d6f62 \ud83c\udf10 \u6b65\u9aa42\uff1aWeb UI\u76d1\u63a7 \u8bbf\u95eeRay\u96c6\u7fa4URL \u67e5\u770bJob\u8fd0\u884c\u72b6\u6001 \ud83d\udd0e \u6b65\u9aa43\uff1a\u5b9a\u4f4d\u8282\u70b9 \u8bb0\u5f55\u8c03\u5ea6\u8282\u70b9ID \u4f8b\uff1a c9db26a6c016fb43... \ud83d\udcca \u6b65\u9aa44\uff1a\u67e5\u770b\u7ed3\u679c \u5728Cluster Tab\u641c\u7d22\u8282\u70b9 \u767b\u5f55\u5bf9\u5e94Pod\u67e5\u770b\u8bad\u7ec3\u7ed3\u679c \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u65e5\u5fd7\u91cc\u8f93\u51fa\u7684\u4fe1\u606f\uff0c\u8fd9\u91cc\u4e3b\u8981\u53ef\u4ee5\u770b\u5230job\u5728\u96c6\u7fa4\u4e0a\u7684\u5de5\u4f5c\u76ee\u5f55\uff0c\u672c\u4f8b\u4e0a\u4e3a_ray_pkg_18b3cac8e32d6f62\u3002 ![img_7.png](img_7.png) \u70b9\u51fbray\u96c6\u7fa4url, \u53ef\u4ee5\u5230\u96c6\u7fa4\u7684web ui\u4e2d\u67e5\u770bjob\u8fd0\u884c\u60c5\u51b5\u3002 ![img_8.png](img_8.png) \u70b9\u51fb\u6b63\u5728\u8fd0\u884c\u7684\u8fd9\u4e2ajob\uff0c\u53ef\u4ee5\u770b\u5230job\u7684\u8c03\u5ea6\u65e5\u5fd7\uff0c\u672c\u4f8b\u662f\u8c03\u5ea6\u5230\u4e86c9db26a6c016fb4394991190f132afe99cd4a2b0a696f14185001650\u8282\u70b9\uff0c\u5bf9\u5e94\u7684\u8bad\u7ec3\u7ed3\u679c\u4e5f\u8981\u5230\u8fd9\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u3002 ![img_9.png](img_9.png) \u5207\u5230Cluster Tab\u4e0b\uff0c\u8f93\u5165node id\u8fdb\u884c\u641c\u7d22\uff0c\u53ef\u4ee5\u627e\u5230\u5bb9\u5668\u96c6\u7fa4\u4e2d\u5bf9\u5e94\u7684Pod\u3002 ![img_10.png](img_10.png) \u4ece\u670d\u52a1\u5b9e\u4f8b\u8d44\u6e90\u4e2d\u627e\u5230\u5bf9\u5e94\u7684\u5bb9\u5668\u96c6\u7fa4\uff0c\u53bb\u5bb9\u5668\u96c6\u7fa4\u4e2d\u627e\u5230\u4e0a\u9762\u5bf9\u5e94\u7684Pod\uff0c\u5e76\u767b\u5f55\u5230Pod\u4e2d\u3002 ![img_11.png](img_11.png) #### \u8bad\u7ec3\u7ed3\u679c\u67e5\u770b \u767b\u5f55\u5230Pod\u540e\uff0c\u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u8def\u5f84\uff1a # \u4e34\u65f6\u76ee\u5f55\u7ed3\u6784\u8bf4\u660e # _ray_pkg_18b3cac8e32d6f62: \u4e0a\u4f20\u6587\u4ef6\u76ee\u5f55\uff08\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u53d8\u5316\uff09 # 2025-08-21_08-08-24: \u5177\u4f53\u8fd0\u884c\u65f6\u95f4 cd /tmp/ray/session_latest/runtime_resources/working_dir_files/_ray_pkg_18b3cac8e32d6f62/logs/rsl_rl/ant/2025-08-21_08-08-24 \u767b\u5f55\u5230Pod\u4e2d\uff0c\u53ef\u4ee5\u770b\u5230\u8bad\u7ec3\u7ed3\u679c\uff0c\u672c\u4f8b\u8bad\u7ec3\u7684Ant\u73af\u5883\uff0c\u8bad\u7ec3\u7684\u7ed3\u679c\u4fdd\u5b58\u5728\u4e0b\u9762\u7684\u4e34\u65f6\u76ee\u5f55\u4e2d\uff1a ![img_12.png](img_12.png) \ud83e\udd16 NVIDIA Isaac Lab | \u8ba9\u673a\u5668\u4eba\u5b66\u4e60\u66f4\u7b80\u5355\u9ad8\u6548","title":"\ud83d\ude80 Isaac Lab\u8bad\u7ec3\u4efb\u52a1\u6267\u884c"},{"location":"isaaclab/index-en/","text":"\ud83e\udd16 NVIDIA Isaac Lab User Guide Open-source Unified Framework for Robot Learning - Complete Tutorial for Standalone and Cluster Versions \ud83c\udfaf Framework Overview **NVIDIA Isaac\u2122 Lab** is an open-source unified framework for robot learning, designed to help train robot policies. \ud83d\udd27 Core Technology Stack \u2022 Built on NVIDIA Isaac Sim\u2122 \u2022 Uses NVIDIA\u00ae PhysX\u00ae physics engine \u2022 Integrates NVIDIA RTX\u2122 rendering technology \u2022 Provides high-fidelity physics simulation environment **Core Value**: Bridges the gap between high-fidelity simulation and perception-based robot training, helping developers and researchers build more robot applications more efficiently. \ud83d\udda5\ufe0f IsaacLab Standalone Version Tutorial ### \ud83c\udfae Service Features Isaac Lab service instances come with complete Isaac Sim applications built-in, supporting two training modes: - **Independent Simulation Training**: Using Isaac Sim for simulation training - **Reinforcement Learning Training**: RL training based on Isaac Lab framework \u2705 System Configuration ECS instances come pre-installed with Ubuntu graphical interface, supporting direct use through VNC in ECS console \ud83d\udd17 VNC Access to ECS Instance \ud83d\udda5\ufe0f VncServer + VncRealViewer Method (Recommended) #### \ud83d\udccb Operation Steps **Step 1: Login to ECS Instance** 1. In the service instance details page resources, find the corresponding ECS instance 2. Click **Remote Connection** to login ![img_1.png](img_1.png) **Step 2: Configure VNC Service** # Switch to root account sudo su root # Set VNC service password (Note: password length maximum 8 characters) /opt/TurboVNC/bin/vncpasswd # Start VNC Server service (listening on port 5901) /opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24 -xstartup ~/.vnc/xstartup **Step 3: Client Connection** 1. Download [VncRealViewer client](https://www.realvnc.com/en/connect/download/viewer/) 2. Connection address: ` :5901` ![img_2.png](img_2.png) ![img_3.png](img_3.png) **Step 4: Launch Isaac Sim** cd /home/isaac-sim/isaacsim ./isaac-sim.sh --allow-root ![img_4.png](img_4.png) \ud83d\udda5\ufe0f ECS Console Native VNC Method #### \ud83d\udccb Operation Steps **Step 1: Enter ECS Console** 1. In the service instance details page resources, find the corresponding ECS instance 2. Click **Go to ECS Console** ![img_17.png](img_17.png) **Step 2: VNC Login** 1. Click **Remote Connection** in the upper right corner 2. Select **VNC Login Method** 3. Enter isaac-sim account password (same as ECS instance password) ![img_18.png](img_18.png) \u26a0\ufe0f Notes Password can be viewed on the service instance overview page ![img_19.png](img_19.png) \ud83c\udfae Isaac Sim Usage #### \ud83d\udcc1 Directory Structure After logging into the ECS instance, the isaac-sim account contains two important directories: \ud83d\udcc2 isaacsim Isaac Sim installation directory Contains startup and training scripts \ud83d\udcc2 isaacsim_assets Isaac Sim assets directory Pre-downloaded training resources ![img_21.png](img_21.png) \ud83d\udd2c Example 1: Headless Scene Synthetic Dataset Generation **Function Description**: Use omni.replicator extension to generate synthetic datasets, with data stored offline for deep neural network training. # Create working directory cd /home/isaac-sim mkdir -p isaacsim_test cd /home/isaac-sim/isaacsim_test mkdir -p scene_based_sdg # Copy example code cp -rf /home/isaac-sim/isaacsim/standalone_examples/replicator/scene_based_sdg/* \\ /home/isaac-sim/isaacsim_test/scene_based_sdg/ # Execute rendering synthesis /home/isaac-sim/isaacsim/python.sh ./scene_based_sdg/scene_based_sdg.py \\ --config=\"/home/isaac-sim/isaacsim_test/scene_based_sdg/config/config_coco_writer.yaml\" \\ --/persistent/isaac/asset_root/default=\"/home/isaac-sim/isaacsim_assets/Assets/Isaac/4.5\" \u2705 Output Results Generated results are stored in the ./isaacsim_test/_out_coco directory Generated results are stored in \"./isaacsim_test/_out_coco\", visualization effects are as follows: ![img_23.png](img_23.png)![img_22.png](img_22.png) \ud83d\uddbc\ufe0f Example 2: Using Isaac Sim with GUI cd /home/isaac-sim/isaacsim ./isaac-sim.sh \u26a0\ufe0f Startup Notice Isaac Sim starts slowly and will show a waiting window, please be patient and no operation is needed ![img_24.png](img_24.png) **Reference Resources**: You can create basic scenes following the [Official Getting Started Tutorial](https://docs.isaacsim.omniverse.nvidia.com/4.5.0/introduction/quickstart_isaacsim.html) Below is a cube created following the steps in the getting started tutorial. ![img_25.png](img_25.png) \ud83e\udd16 Isaac Lab Usage **Installation Path**: `/home/isaac-sim/IsaacLab` \ud83c\udfaf Example 1: Headless Mode Agent Training **Training Objective**: Use Stable-Baselines3 reinforcement learning framework to solve Cartpole balance control task \ud83c\udfaf Task Description Let the agent learn to control the cart's left-right movement to keep the pole upright # Create working directory cd /home/isaac-sim mkdir -p isaaclab_test cd /home/isaac-sim/isaaclab_test mkdir -p sb3 # Copy example code cp -rf /home/isaac-sim/IsaacLab/scripts/reinforcement_learning/sb3/* \\ /home/isaac-sim/isaaclab_test/sb3/ # Start training /home/isaac-sim/IsaacLab/isaaclab.sh -p ./sb3/train.py \\ --task Isaac-Cartpole-v0 \\ --num_envs 64 \\ --headless \\ --video \u2705 Training Results Results saved to: ./logs/sb3/Isaac-Cartpole-v0 Training results are saved to ./logs/sb3/Isaac-Cartpole-v0; visualization results are as follows: ![img_26.png](img_26.png) \ud83c\udfae Example 2: GUI Mode Scene Generation cd /home/isaac-sim/IsaacLab ./isaaclab.sh -p scripts/tutorials/00_sim/spawn_prims.py **Function**: Generate basic objects into the scene in GUI interface ![img_27.png](img_27.png) \u2601\ufe0f IsaacLab Cluster Version Tutorial Isaac Lab supports the **Ray** framework for simplifying the scheduling of multiple training tasks (including parallel and serial) and hyperparameter tuning, suitable for local and remote configurations. \ud83d\udcda Official Documentation Isaac Lab service Ray job scheduling and tuning official documentation is Ray Job Dispatch and Tuning \ud83d\udee0\ufe0f Environment Setup #### Step 1: Configure Remote Ray Cluster # Configure cluster information (ISAACRAY_ADDRESS obtained from service instance overview page) echo \"name: isaacray address: <ISAACRAY_ADDRESS>\" > ~/.cluster_config export RAY_ADDRESS=\"<ISAACRAY_ADDRESS>\" #### Step 2: Download Source Code Download [Isaac Lab source code](https://github.com/isaac-sim/IsaacLab) from GitHub #### Step 3: Install Ray Client pip install \"ray[default]\" \ud83e\uddea Test Job: Log Output Example #### Create Test File Create `test.py` in the `scripts/reinforcement_learning/ray/` directory: import ray import os # Connect to local or remote ray cluster ray.init() @ray.remote(num_cpus=1) class Counter: def __init__(self): self.name = \"test_counter\" self.counter = 0 def increment(self): self.counter += 1 def get_counter(self): return \"{} got {}\".format(self.name, self.counter) counter = Counter.remote() for _ in range(10): counter.increment.remote() print(ray.get(counter.get_counter.remote())) #### Submit Job python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p test.py\" \u2705 Execution Results \u2022 The scripts/reinforcement_learning/ray directory will be packaged during submission \u2022 Output information can be viewed in logs after completion After successful submission, you can see the following information in the logs: - When submitting the job, the scripts/reinforcement_learning/ray directory will be packaged as the working directory and uploaded to the cluster, so our test.py will also be uploaded. ![img_5.png](img_5.png) - After the job completes, you can see the output runtime information: ![img_6.png](img_6.png) \ud83d\ude80 Isaac Lab Training Task Execution #### Submit Training Job python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p /workspace/isaaclab/scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Ant-v0 --headless\" #### Monitor Job Execution \ud83d\udd0d Step 1: View Logs Check working directory after successful submission Example: _ray_pkg_18b3cac8e32d6f62 \ud83c\udf10 Step 2: Web UI Monitoring Access Ray cluster URL View Job running status \ud83d\udd0e Step 3: Locate Node Record scheduled node ID Example: c9db26a6c016fb43... \ud83d\udcca Step 4: View Results Search for node in Cluster Tab Login to corresponding Pod to view training results After successful submission, you can see the output information in the logs. Here you can mainly see the job's working directory on the cluster, which in this example is _ray_pkg_18b3cac8e32d6f62. ![img_7.png](img_7.png) Click the ray cluster url to view job running status in the cluster's web ui. ![img_8.png](img_8.png) Click on the running job to see the job's scheduling logs. In this example, it was scheduled to node c9db26a6c016fb4394991190f132afe99cd4a2b0a696f14185001650, and the corresponding training results should also be viewed on this node. ![img_9.png](img_9.png) Switch to the Cluster Tab, enter the node id to search, and you can find the corresponding Pod in the container cluster. ![img_10.png](img_10.png) Find the corresponding container cluster from the service instance resources, go to the container cluster to find the corresponding Pod above, and login to the Pod. ![img_11.png](img_11.png) #### View Training Results After logging into the Pod, training results save path: # Temporary directory structure explanation # _ray_pkg_18b3cac8e32d6f62: Upload file directory (varies based on actual situation) # 2025-08-21_08-08-24: Specific runtime cd /tmp/ray/session_latest/runtime_resources/working_dir_files/_ray_pkg_18b3cac8e32d6f62/logs/rsl_rl/ant/2025-08-21_08-08-24 After logging into the Pod, you can see the training results. In this example, the Ant environment was trained, and the training results are saved in the temporary directory below: ![img_12.png](img_12.png) \ud83e\udd16 NVIDIA Isaac Lab | Making Robot Learning Simpler and More Efficient","title":"Index en"},{"location":"isaaclab/index-en/#framework-overview","text":"**NVIDIA Isaac\u2122 Lab** is an open-source unified framework for robot learning, designed to help train robot policies. \ud83d\udd27 Core Technology Stack \u2022 Built on NVIDIA Isaac Sim\u2122 \u2022 Uses NVIDIA\u00ae PhysX\u00ae physics engine \u2022 Integrates NVIDIA RTX\u2122 rendering technology \u2022 Provides high-fidelity physics simulation environment **Core Value**: Bridges the gap between high-fidelity simulation and perception-based robot training, helping developers and researchers build more robot applications more efficiently.","title":"\ud83c\udfaf Framework Overview"},{"location":"isaaclab/index-en/#isaaclab-standalone-version-tutorial","text":"### \ud83c\udfae Service Features Isaac Lab service instances come with complete Isaac Sim applications built-in, supporting two training modes: - **Independent Simulation Training**: Using Isaac Sim for simulation training - **Reinforcement Learning Training**: RL training based on Isaac Lab framework \u2705 System Configuration ECS instances come pre-installed with Ubuntu graphical interface, supporting direct use through VNC in ECS console","title":"\ud83d\udda5\ufe0f IsaacLab Standalone Version Tutorial"},{"location":"isaaclab/index-en/#vnc-access-to-ecs-instance","text":"\ud83d\udda5\ufe0f VncServer + VncRealViewer Method (Recommended) #### \ud83d\udccb Operation Steps **Step 1: Login to ECS Instance** 1. In the service instance details page resources, find the corresponding ECS instance 2. Click **Remote Connection** to login ![img_1.png](img_1.png) **Step 2: Configure VNC Service** # Switch to root account sudo su root # Set VNC service password (Note: password length maximum 8 characters) /opt/TurboVNC/bin/vncpasswd # Start VNC Server service (listening on port 5901) /opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24 -xstartup ~/.vnc/xstartup **Step 3: Client Connection** 1. Download [VncRealViewer client](https://www.realvnc.com/en/connect/download/viewer/) 2. Connection address: ` :5901` ![img_2.png](img_2.png) ![img_3.png](img_3.png) **Step 4: Launch Isaac Sim** cd /home/isaac-sim/isaacsim ./isaac-sim.sh --allow-root ![img_4.png](img_4.png) \ud83d\udda5\ufe0f ECS Console Native VNC Method #### \ud83d\udccb Operation Steps **Step 1: Enter ECS Console** 1. In the service instance details page resources, find the corresponding ECS instance 2. Click **Go to ECS Console** ![img_17.png](img_17.png) **Step 2: VNC Login** 1. Click **Remote Connection** in the upper right corner 2. Select **VNC Login Method** 3. Enter isaac-sim account password (same as ECS instance password) ![img_18.png](img_18.png) \u26a0\ufe0f Notes Password can be viewed on the service instance overview page ![img_19.png](img_19.png)","title":"\ud83d\udd17 VNC Access to ECS Instance"},{"location":"isaaclab/index-en/#isaac-sim-usage","text":"#### \ud83d\udcc1 Directory Structure After logging into the ECS instance, the isaac-sim account contains two important directories: \ud83d\udcc2 isaacsim Isaac Sim installation directory Contains startup and training scripts \ud83d\udcc2 isaacsim_assets Isaac Sim assets directory Pre-downloaded training resources ![img_21.png](img_21.png)","title":"\ud83c\udfae Isaac Sim Usage"},{"location":"isaaclab/index-en/#example-1-headless-scene-synthetic-dataset-generation","text":"**Function Description**: Use omni.replicator extension to generate synthetic datasets, with data stored offline for deep neural network training. # Create working directory cd /home/isaac-sim mkdir -p isaacsim_test cd /home/isaac-sim/isaacsim_test mkdir -p scene_based_sdg # Copy example code cp -rf /home/isaac-sim/isaacsim/standalone_examples/replicator/scene_based_sdg/* \\ /home/isaac-sim/isaacsim_test/scene_based_sdg/ # Execute rendering synthesis /home/isaac-sim/isaacsim/python.sh ./scene_based_sdg/scene_based_sdg.py \\ --config=\"/home/isaac-sim/isaacsim_test/scene_based_sdg/config/config_coco_writer.yaml\" \\ --/persistent/isaac/asset_root/default=\"/home/isaac-sim/isaacsim_assets/Assets/Isaac/4.5\" \u2705 Output Results Generated results are stored in the ./isaacsim_test/_out_coco directory Generated results are stored in \"./isaacsim_test/_out_coco\", visualization effects are as follows: ![img_23.png](img_23.png)![img_22.png](img_22.png)","title":"\ud83d\udd2c Example 1: Headless Scene Synthetic Dataset Generation"},{"location":"isaaclab/index-en/#example-2-using-isaac-sim-with-gui","text":"cd /home/isaac-sim/isaacsim ./isaac-sim.sh \u26a0\ufe0f Startup Notice Isaac Sim starts slowly and will show a waiting window, please be patient and no operation is needed ![img_24.png](img_24.png) **Reference Resources**: You can create basic scenes following the [Official Getting Started Tutorial](https://docs.isaacsim.omniverse.nvidia.com/4.5.0/introduction/quickstart_isaacsim.html) Below is a cube created following the steps in the getting started tutorial. ![img_25.png](img_25.png)","title":"\ud83d\uddbc\ufe0f Example 2: Using Isaac Sim with GUI"},{"location":"isaaclab/index-en/#isaac-lab-usage","text":"**Installation Path**: `/home/isaac-sim/IsaacLab`","title":"\ud83e\udd16 Isaac Lab Usage"},{"location":"isaaclab/index-en/#example-1-headless-mode-agent-training","text":"**Training Objective**: Use Stable-Baselines3 reinforcement learning framework to solve Cartpole balance control task \ud83c\udfaf Task Description Let the agent learn to control the cart's left-right movement to keep the pole upright # Create working directory cd /home/isaac-sim mkdir -p isaaclab_test cd /home/isaac-sim/isaaclab_test mkdir -p sb3 # Copy example code cp -rf /home/isaac-sim/IsaacLab/scripts/reinforcement_learning/sb3/* \\ /home/isaac-sim/isaaclab_test/sb3/ # Start training /home/isaac-sim/IsaacLab/isaaclab.sh -p ./sb3/train.py \\ --task Isaac-Cartpole-v0 \\ --num_envs 64 \\ --headless \\ --video \u2705 Training Results Results saved to: ./logs/sb3/Isaac-Cartpole-v0 Training results are saved to ./logs/sb3/Isaac-Cartpole-v0; visualization results are as follows: ![img_26.png](img_26.png)","title":"\ud83c\udfaf Example 1: Headless Mode Agent Training"},{"location":"isaaclab/index-en/#example-2-gui-mode-scene-generation","text":"cd /home/isaac-sim/IsaacLab ./isaaclab.sh -p scripts/tutorials/00_sim/spawn_prims.py **Function**: Generate basic objects into the scene in GUI interface ![img_27.png](img_27.png)","title":"\ud83c\udfae Example 2: GUI Mode Scene Generation"},{"location":"isaaclab/index-en/#isaaclab-cluster-version-tutorial","text":"Isaac Lab supports the **Ray** framework for simplifying the scheduling of multiple training tasks (including parallel and serial) and hyperparameter tuning, suitable for local and remote configurations. \ud83d\udcda Official Documentation Isaac Lab service Ray job scheduling and tuning official documentation is Ray Job Dispatch and Tuning","title":"\u2601\ufe0f IsaacLab Cluster Version Tutorial"},{"location":"isaaclab/index-en/#environment-setup","text":"#### Step 1: Configure Remote Ray Cluster # Configure cluster information (ISAACRAY_ADDRESS obtained from service instance overview page) echo \"name: isaacray address: <ISAACRAY_ADDRESS>\" > ~/.cluster_config export RAY_ADDRESS=\"<ISAACRAY_ADDRESS>\" #### Step 2: Download Source Code Download [Isaac Lab source code](https://github.com/isaac-sim/IsaacLab) from GitHub #### Step 3: Install Ray Client pip install \"ray[default]\"","title":"\ud83d\udee0\ufe0f Environment Setup"},{"location":"isaaclab/index-en/#test-job-log-output-example","text":"#### Create Test File Create `test.py` in the `scripts/reinforcement_learning/ray/` directory: import ray import os # Connect to local or remote ray cluster ray.init() @ray.remote(num_cpus=1) class Counter: def __init__(self): self.name = \"test_counter\" self.counter = 0 def increment(self): self.counter += 1 def get_counter(self): return \"{} got {}\".format(self.name, self.counter) counter = Counter.remote() for _ in range(10): counter.increment.remote() print(ray.get(counter.get_counter.remote())) #### Submit Job python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p test.py\" \u2705 Execution Results \u2022 The scripts/reinforcement_learning/ray directory will be packaged during submission \u2022 Output information can be viewed in logs after completion After successful submission, you can see the following information in the logs: - When submitting the job, the scripts/reinforcement_learning/ray directory will be packaged as the working directory and uploaded to the cluster, so our test.py will also be uploaded. ![img_5.png](img_5.png) - After the job completes, you can see the output runtime information: ![img_6.png](img_6.png)","title":"\ud83e\uddea Test Job: Log Output Example"},{"location":"isaaclab/index-en/#isaac-lab-training-task-execution","text":"#### Submit Training Job python3 scripts/reinforcement_learning/ray/submit_job.py \\ --aggregate_jobs wrap_resources.py \\ --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p /workspace/isaaclab/scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Ant-v0 --headless\" #### Monitor Job Execution \ud83d\udd0d Step 1: View Logs Check working directory after successful submission Example: _ray_pkg_18b3cac8e32d6f62 \ud83c\udf10 Step 2: Web UI Monitoring Access Ray cluster URL View Job running status \ud83d\udd0e Step 3: Locate Node Record scheduled node ID Example: c9db26a6c016fb43... \ud83d\udcca Step 4: View Results Search for node in Cluster Tab Login to corresponding Pod to view training results After successful submission, you can see the output information in the logs. Here you can mainly see the job's working directory on the cluster, which in this example is _ray_pkg_18b3cac8e32d6f62. ![img_7.png](img_7.png) Click the ray cluster url to view job running status in the cluster's web ui. ![img_8.png](img_8.png) Click on the running job to see the job's scheduling logs. In this example, it was scheduled to node c9db26a6c016fb4394991190f132afe99cd4a2b0a696f14185001650, and the corresponding training results should also be viewed on this node. ![img_9.png](img_9.png) Switch to the Cluster Tab, enter the node id to search, and you can find the corresponding Pod in the container cluster. ![img_10.png](img_10.png) Find the corresponding container cluster from the service instance resources, go to the container cluster to find the corresponding Pod above, and login to the Pod. ![img_11.png](img_11.png) #### View Training Results After logging into the Pod, training results save path: # Temporary directory structure explanation # _ray_pkg_18b3cac8e32d6f62: Upload file directory (varies based on actual situation) # 2025-08-21_08-08-24: Specific runtime cd /tmp/ray/session_latest/runtime_resources/working_dir_files/_ray_pkg_18b3cac8e32d6f62/logs/rsl_rl/ant/2025-08-21_08-08-24 After logging into the Pod, you can see the training results. In this example, the Ant environment was trained, and the training results are saved in the temporary directory below: ![img_12.png](img_12.png) \ud83e\udd16 NVIDIA Isaac Lab | Making Robot Learning Simpler and More Efficient","title":"\ud83d\ude80 Isaac Lab Training Task Execution"},{"location":"isaaclab/original-index/","text":"\u7b80\u4ecb NVIDIA Isaac\u2122 Lab \u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f00\u6e90\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002 Isaac Lab \u57fa\u4e8e NVIDIA Isaac Sim\u2122 \u5f00\u53d1\uff0c\u4f7f\u7528 NVIDIA\u00aePhysX\u00ae \u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u6027\u8d28\u7684 NVIDIA RTX\u2122 \u6e32\u67d3\u63d0\u4f9b\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u3002\u5b83\u5f25\u5408\u4e86\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u57fa\u4e8e\u611f\u77e5\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u66f4\u591a\u673a\u5668\u4eba\u3002 IsaacLab \u5355\u673a\u7248\u4f7f\u7528\u6559\u7a0b Isaac Lab\u670d\u52a1\u5b9e\u4f8b\u5185\u7f6e\u5b8c\u6574\u7684Isaac Sim\u5e94\u7528\uff0c\u652f\u6301\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f\uff1a\u65e2\u53ef\u72ec\u7acb\u4f7f\u7528Isaac Sim\u8fdb\u884c\u4eff\u771f\u8bad\u7ec3\uff0c\u4e5f\u53ef\u57fa\u4e8eIsaac Lab\u6846\u67b6\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002 \u5bf9\u5e94\u7684ECS\u5b9e\u4f8b\u5b89\u88c5\u4e86Ubuntu\u56fe\u5f62\u754c\u9762\uff0c\u652f\u6301\u5728ECS\u63a7\u5236\u53f0\u901a\u8fc7VNC\u7684\u65b9\u5f0f\u76f4\u63a5\u4f7f\u7528\u3002 VNC\u65b9\u5f0f\u8bbf\u95eeECS\u5b9e\u4f8b \u76ee\u524d\u63d0\u4f9b\u4e86\u4e24\u79cd\u65b9\u5f0f\u53bb\u8bbf\u95eeUbuntu\u5b9e\u4f8b\uff0c\u8fd9\u91cc\u63a8\u8350\u4f7f\u7528VncServer+VncRealViewer\u65b9\u5f0f\u8bbf\u95ee\uff0c\u53ef\u4ee5\u6bd4\u8f83\u65b9\u4fbf\u7684\u8c03\u6574\u5206\u8fa8\u7387\uff0c\u6574\u4f53\u4f7f\u7528\u4e5f\u66f4\u52a0\u6d41\u7545\u3002 VncServer+VncRealViewer\u65b9\u5f0f\u8bbf\u95ee \uff08\u63a8\u8350\uff09 \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b\uff0c\u70b9\u51fb\u8fdc\u7a0b\u8fde\u63a5\u8fdb\u884c\u767b\u5f55\u3002 \u767b\u5f55\u5230ECS\u5b9e\u4f8b\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a # \u5207\u6362\u5230root\u8d26\u6237 sudo su root # \u8bbe\u7f6eVNC\u670d\u52a1\u5bc6\u7801\uff0c\u6ce8\u610f\u5bc6\u7801\u957f\u5ea6\u6700\u5927\u4e3a8\u4f4d /opt/TurboVNC/bin/vncpasswd # \u542f\u52a8VNC Server\u670d\u52a1,\u670d\u52a1\u4f1alisten 5901\u7aef\u53e3,\u5b89\u88c5\u7ec4\u5df2\u9884\u5148\u8bbe\u7f6e\u4e3a\u5f00\u653e /opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24 -xstartup ~/.vnc/xstartup \u4e0b\u8f7d VncRealViewer\u5ba2\u6237\u7aef \uff0c\u8fde\u63a5\u8f93\u5165<\u670d\u52a1\u5668\u516c\u7f51IP>:5901, \u5373\u53ef\u8fde\u63a5\u670d\u52a1\u5668\u3002 \u5728/home/isaac-sim/isaacsim\u8def\u5f84\u4e0b\uff0c\u6267\u884c./isaac-sim.sh --allow-root\u547d\u4ee4\uff0c\u5373\u53ef\u6253\u5f00Isaac Sim\u754c\u9762\u3002 Ecs\u63a7\u5236\u53f0\u81ea\u5e26\u539f\u751f\u7684VNC\u767b\u5f55\u65b9\u5f0f \u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b\uff0c\u70b9\u51fb\u53bb\u5230ECS\u63a7\u5236\u53f0\u3002 \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u8fdc\u7a0b\u8fde\u63a5\uff0c\u9009\u62e9VNC\u767b\u5f55\u65b9\u5f0f\uff0c\u5373\u53ef\u8fdb\u5165\u5230Ubuntu\u7cfb\u7edf\u7684\u56fe\u5f62\u754c\u9762\u3002 \u8fd9\u91cc\u9700\u8981\u8f93\u5165isaac-sim\u8d26\u6237\u5bf9\u5e94\u767b\u5f55\u5bc6\u7801\uff0c\u5bf9\u5e94\u7684\u5bc6\u7801\u548cECS\u5b9e\u4f8b\u5bc6\u7801\u4e00\u81f4\uff0c\u53ef\u4ee5\u53bb\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u67e5\u770b\u3002 Isaac Sim\u4f7f\u7528\u65b9\u5f0f \u901a\u8fc7\u4e0a\u9762\u7684\u65b9\u5f0f\u767b\u5f55\u5230ECS\u5b9e\u4f8b\u540e\uff0c\u6253\u5f00Terminal\uff0c\u53ef\u4ee5\u770b\u5230isaac-sim\u8d26\u6237\u4e0b\u6709isaacsim\u548cisaacsim_assets\u4e24\u4e2a\u76ee\u5f55\u3002 - isaacsim\u76ee\u5f55\u4e3aIsaac Sim\u7684\u5b89\u88c5\u76ee\u5f55\uff0c\u91cc\u9762\u6709Isaac Sim\u76f8\u5173\u7684\u542f\u52a8\u548c\u8bad\u7ec3\u811a\u672c\u3002 - isaacsim_assets\u76ee\u5f55\u4e3aIsaac Sim\u7684\u8d44\u6e90\u76ee\u5f55\uff0c\u8fd9\u91cc\u505a\u4e86\u63d0\u524d\u4e0b\u8f7d\uff0c\u65b9\u4fbf\u540e\u9762\u8bad\u7ec3\u4f7f\u7528\u3002 \u793a\u4f8b1: \u4ee5\u65e0GUI\u7684\u65b9\u5f0f\u8fdb\u884c\u573a\u666f\u5408\u6210\u6570\u636e\u96c6\u751f\u6210 \u8be5\u793a\u4f8b\u5c55\u793a\u4f7f\u7528omni.replicator\u6269\u5c55\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u7684\u8fc7\u7a0b\u3002\u751f\u6210\u7684\u6570\u636e\u5c06\u79bb\u7ebf\u5b58\u50a8\uff08\u5728\u78c1\u76d8\u4e0a\uff09\uff0c\u4f7f\u5176\u53ef\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u3002\u793a\u4f8b\u53ef\u4ee5\u5728Isaac Sim\u7684Python\u72ec\u7acb\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5e76\u5229\u7528Isaac Sim\u548cReplicator\u521b\u5efa\u79bb\u7ebf\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u5efa\u8bae\u5c06\u5b98\u65b9\u793a\u4f8b\u4ee3\u7801\u62f7\u8d1d\u5230\u7528\u6237\u76ee\u5f55\u4e0b\u8fdb\u884c\u4fee\u6539\u53ca\u4f7f\u7528\u3002 cd /home/isaac-sim mkdir -p isaacsim_test cd /home/isaac-sim/isaacsim_test mkdir -p scene_based_sdg cp -rf /home/isaac-sim/isaacsim/standalone_examples/replicator/scene_based_sdg/* /home/isaac-sim/isaacsim_test/scene_based_sdg/ ## \u6e32\u67d3\u5408\u6210(--config\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff0c\u5176\u4e2d\u8bbe\u7f6e\u4e86headless=true\uff1b--/persistent/isaac/asset_root/default\u6307\u5b9a3D\u8d44\u4ea7\u5b58\u50a8\u8def\u5f84) /home/isaac-sim/isaacsim/python.sh ./scene_based_sdg/scene_based_sdg.py --config=\"/home/isaac-sim/isaacsim_test/scene_based_sdg/config/config_coco_writer.yaml\" --/persistent/isaac/asset_root/default=\"/home/isaac-sim/isaacsim_assets/Assets/Isaac/4.5\" \u751f\u6210\u7ed3\u679c\u5b58\u50a8\u5728\u201d./isaacsim_test/_out_coco\u201c\u4e2d\uff0c\u53ef\u89c6\u5316\u6548\u679c\u5982\u4e0b\uff1a \u793a\u4f8b2: \u4ee5GUI\u7684\u65b9\u5f0f\u4f7f\u7528Isaac Sim \u5728termial\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5373\u53ef\u8fdb\u5165Isaac Sim\u7684GUI\u754c\u9762\u3002 cd /home/isaac-sim/isaacsim ./isaac-sim.sh \u8fd9\u91cc\u8981\u6ce8\u610f\u7684\u662f\uff0cIsaac Sim\u542f\u52a8\u7684\u65f6\u5019\u4f1a\u6bd4\u8f83\u6162\uff0c\u4f1a\u5f39\u51fa\u7b49\u5f85\u7684\u7a97\u53e3\uff0c\u4e0d\u7528\u8fdb\u884c\u64cd\u4f5c\uff0c\u7b49\u5f85\u4e00\u6bb5\u65f6\u95f4\u5373\u53ef\u3002 \u4e0b\u9762\u662f\u6309 \u5165\u95e8\u6559\u7a0b \u4e2d\u7684\u6b65\u9aa4\u521b\u5efa\u4e86\u4e2a\u6b63\u65b9\u4f53\u3002 Isaac Lab\u7684\u4f7f\u7528\u65b9\u5f0f Isaac Lab\u670d\u52a1\u5b89\u88c5\u76ee\u5f55\u5728/home/isaac-sim/IsaacLab\u4e2d\uff0c\u91cc\u9762\u6709Isaac Lab\u7684\u5b89\u88c5\u76ee\u5f55\u548c\u542f\u52a8\u811a\u672c\u3002 \u793a\u4f8b1\uff1a \u4f7f\u7528\u65e0GUI\u6a21\u5f0f\u8bad\u7ec3\u667a\u80fd\u4f53 \u672c\u6848\u4f8b\uff0c\u6211\u4eec\u5c06\u4f7f\u7528Stable-Baselines3\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3Cartpole\u5e73\u8861\u63a7\u5236\u7684\u667a\u80fd\u4f53\u4efb\u52a1\u3002Stable-Baselines3\u662f\u4e00\u4e2a\u57fa\u4e8e PyTorch \u7684\u5f3a\u5316\u5b66\u4e60\u5e93\uff0c\u63d0\u4f9b\u4e86\u591a\u79cd\u7a33\u5b9a\u4e14\u6613\u7528\u7684 RL \u7b97\u6cd5\uff0c\u5982 PPO\u3001SAC\u3001DQN\u7b49\u3002\u8bad\u7ec3\u76ee\u6807\u662f\uff0c\u8ba9\u667a\u80fd\u4f53\u5b66\u4e60\u5982\u4f55\u63a7\u5236\u5c0f\u8f66\u7684\u5de6\u53f3\u79fb\u52a8\uff0c\u4fdd\u6301\u6446\u6746\u76f4\u7acb\u4e0d\u5012\u3002\u5efa\u8bae\u5c06\u793a\u4f8b\u4ee3\u7801\u62f7\u8d1d\u5230\u4e2a\u4eba\u76ee\u5f55\u8fdb\u884c\u4fee\u6539\u548c\u8c03\u8bd5\u3002 cd /home/isaac-sim mkdir -p isaaclab_test cd /home/isaac-sim/isaaclab_test mkdir -p sb3 cp -rf /home/isaac-sim/IsaacLab/scripts/reinforcement_learning/sb3/* /home/isaac-sim/isaaclab_test/sb3/ ## \u6e32\u67d3\u5408\u6210(--config\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff0c\u5176\u4e2d\u8bbe\u7f6e\u4e86headless=true\uff1b--/persistent/isaac/asset_root/default\u6307\u5b9a3D\u8d44\u4ea7\u5b58\u50a8\u8def\u5f84) /home/isaac-sim/IsaacLab/isaaclab.sh -p ./sb3/train.py --task Isaac-Cartpole-v0 --num_envs 64 --headless --video \u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u5230./logs/sb3/Isaac-Cartpole-v0\u4e2d\uff1b\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b \u793a\u4f8b2\uff1a \u4f7f\u7528GUI\u6a21\u5f0f\u751f\u6210\u57fa\u672c\u7269\u4f53\u5230\u573a\u666f\u4e2d \u5728Terminal\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5373\u53ef\u8fdb\u5165Isaac Lab\u7684GUI\u754c\u9762\u3002 cd /home/isaac-sim/IsaacLab ./isaaclab.sh -p scripts/tutorials/00_sim/spawn_prims.py IsaacLab \u96c6\u7fa4\u7248\u4f7f\u7528\u6559\u7a0b Isaac Lab \u652f\u6301 Ray \uff0c\u7528\u4e8e\u7b80\u5316\u591a\u4e2a\u8bad\u7ec3\u4efb\u52a1\u7684\u8c03\u5ea6\uff08\u5305\u62ec\u5e76\u884c\u548c\u4e32\u884c\uff09\uff0c\u4ee5\u53ca\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u9002\u7528\u4e8e\u672c\u5730\u548c\u8fdc\u7a0b\u914d\u7f6e\u3002 Isaac Lab\u670d\u52a1Ray\u4f5c\u4e1a\u8c03\u5ea6\u548c\u8c03\u4f18\u5b98\u65b9\u6587\u6863\u4e3aRay Job Dispatch and Tuning\u3002 \u73af\u5883\u51c6\u5907 \u5728\u672c\u5730\u7535\u8111\u914d\u7f6e\u8fdc\u7a0bRay\u96c6\u7fa4\u4fe1\u606f\uff0c\u5177\u4f53\u547d\u4ee4\u5982\u4e0b\uff1a # \u8fd9\u91cc\u7684<ISAACRAY_ADDRESS>\u4e3aray\u96c6\u7fa4\u7684url\uff0c\u53ef\u4ee5\u4ece\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u4e2d\u83b7\u53d6 echo \"name: isaacray address: <ISAACRAY_ADDRESS>\" > ~/.cluster_config export RAY_ADDRESS=\"<ISAACRAY_ADDRESS>\" \u4ecegit\u4e0a\u4e0b\u8f7d Isaac Lab\u6e90\u7801 , \u7528\u6765\u505a\u540e\u7eed\u7684\u4f5c\u4e1a\u63d0\u4ea4\u3002 \u5728\u672c\u5730\u7535\u8111\u5b89\u88c5ray\u5ba2\u6237\u7aef\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a pip install \"ray[default]\" \u5728\u65e5\u5fd7\u4e2d\u6253\u5370\u6267\u884c\u7ed3\u679c\u7684\u6d4b\u8bd5job \u5728Isaac Lab\u6e90\u7801\u76ee\u5f55scripts/reinforcement_learning/ray\u4e0b\u65b0\u5efa \u4e2a\u6d4b\u8bd5\u7528\u7684python\u6587\u4ef6test.py\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a import ray import os # \u8fde\u63a5\u672c\u5730\u6216\u8005\u8fdc\u7a0bray cluster ray.init() @ray.remote(num_cpus=1) class Counter: def __init__(self): self.name = \"test_counter\" self.counter = 0 def increment(self): self.counter += 1 def get_counter(self): return \"{} got {}\".format(self.name, self.counter) counter = Counter.remote() for _ in range(10): counter.increment.remote() print(ray.get(counter.get_counter.remote())) \u5728Isaac Lab\u6e90\u4ee3\u7801\u76ee\u5f55\u4e0b\uff0c\u4f7f\u7528ray\u63d0\u4ea4\u4f5c\u4e1a\uff0c\u5177\u4f53\u547d\u4ee4\u5982\u4e0b\uff1a python3 scripts/reinforcement_learning/ray/submit_job.py --aggregate_jobs wrap_resources.py --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p test.py\" \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u4ece\u65e5\u5fd7\u91cc\u770b\u5230\u4ee5\u4e0b\u4fe1\u606f\uff1a \u63d0\u4ea4\u4f5c\u4e1a\u65f6\uff0c\u4f1a\u628ascripts/reinforcement_learning/ray\u76ee\u5f55\u5f53\u4f5c\u5de5\u4f5c\u76ee\u5f55\u8fdb\u884c\u6253\u5305\uff0c\u4e0a\u4f20\u5230\u96c6\u7fa4\u4e2d\uff0c\u6240\u4ee5\u6211\u4eec\u7684test.py\u4e5f\u4f1a\u88ab\u4e0a\u4f20\u3002 job\u8fd0\u884c\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u8f93\u51fa\u7684\u8fd0\u884c\u4fe1\u606f\uff1a \u6267\u884cIsaac Lab\u8bad\u7ec3\u4efb\u52a1 \u5728Isaac Lab\u6e90\u4ee3\u7801\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\u547d\u4ee4\u63d0\u4ea4\u4f5c\u4e1a\uff0c\u5177\u4f53\u547d\u4ee4\u5982\u4e0b\uff1a python3 scripts/reinforcement_learning/ray/submit_job.py --aggregate_jobs wrap_resources.py --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p /workspace/isaaclab/scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Ant-v0 --headless\" \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u65e5\u5fd7\u91cc\u8f93\u51fa\u7684\u4fe1\u606f\uff0c\u8fd9\u91cc\u4e3b\u8981\u53ef\u4ee5\u770b\u5230job\u5728\u96c6\u7fa4\u4e0a\u7684\u5de5\u4f5c\u76ee\u5f55\uff0c\u672c\u4f8b\u4e0a\u4e3a_ray_pkg_18b3cac8e32d6f62\u3002 \u70b9\u51fbray\u96c6\u7fa4url, \u53ef\u4ee5\u5230\u96c6\u7fa4\u7684web ui\u4e2d\u67e5\u770bjob\u8fd0\u884c\u60c5\u51b5\u3002 \u70b9\u51fb\u6b63\u5728\u8fd0\u884c\u7684\u8fd9\u4e2ajob\uff0c\u53ef\u4ee5\u770b\u5230job\u7684\u8c03\u5ea6\u65e5\u5fd7\uff0c\u672c\u4f8b\u662f\u8c03\u5ea6\u5230\u4e86c9db26a6c016fb4394991190f132afe99cd4a2b0a696f14185001650\u8282\u70b9\uff0c \u5bf9\u5e94\u7684\u8bad\u7ec3\u7ed3\u679c\u4e5f\u8981\u5230\u8fd9\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u3002 \u5207\u5230Cluster Tab\u4e0b\uff0c\u8f93\u5165node id\u8fdb\u884c\u641c\u7d22\uff0c\u53ef\u4ee5\u627e\u5230\u5bb9\u5668\u96c6\u7fa4\u4e2d\u5bf9\u5e94\u7684Pod\u3002 \u4ece\u670d\u52a1\u5b9e\u4f8b\u8d44\u6e90\u4e2d\u627e\u5230\u5bf9\u5e94\u7684\u5bb9\u5668\u96c6\u7fa4\uff0c\u53bb\u5bb9\u5668\u96c6\u7fa4\u4e2d\u627e\u5230\u4e0a\u9762\u5bf9\u5e94\u7684Pod\uff0c\u5e76\u767b\u5f55\u5230Pod\u4e2d\u3002 \u767b\u5f55\u5230Pod\u4e2d\uff0c\u53ef\u4ee5\u770b\u5230\u8bad\u7ec3\u7ed3\u679c\uff0c\u672c\u4f8b\u8bad\u7ec3\u7684Ant\u73af\u5883\uff0c\u8bad\u7ec3\u7684\u7ed3\u679c\u4fdd\u5b58\u5728\u4e0b\u9762\u7684\u4e34\u65f6\u76ee\u5f55\u4e2d\uff1a # \u6267\u884cjob\u7684\u4e34\u65f6\u76ee\u5f55\uff0c\u5176\u4e2d_ray_pkg_18b3cac8e32d6f62\u53d6\u51b3\u4e8e\u4e0a\u4f20\u6587\u4ef6\u7684\u76ee\u5f55\uff0c2025-08-21_08-08-24 \u662f\u5177\u4f53\u8fd0\u884c\u65f6\u95f4 cd /tmp/ray/session_latest/runtime_resources/working_dir_files/_ray_pkg_18b3cac8e32d6f62/logs/rsl_rl/ant/2025-08-21_08-08-24","title":"Original index"},{"location":"isaaclab/original-index/#_1","text":"NVIDIA Isaac\u2122 Lab \u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f00\u6e90\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002 Isaac Lab \u57fa\u4e8e NVIDIA Isaac Sim\u2122 \u5f00\u53d1\uff0c\u4f7f\u7528 NVIDIA\u00aePhysX\u00ae \u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u6027\u8d28\u7684 NVIDIA RTX\u2122 \u6e32\u67d3\u63d0\u4f9b\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u3002\u5b83\u5f25\u5408\u4e86\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u57fa\u4e8e\u611f\u77e5\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u66f4\u591a\u673a\u5668\u4eba\u3002","title":"\u7b80\u4ecb"},{"location":"isaaclab/original-index/#isaaclab","text":"Isaac Lab\u670d\u52a1\u5b9e\u4f8b\u5185\u7f6e\u5b8c\u6574\u7684Isaac Sim\u5e94\u7528\uff0c\u652f\u6301\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f\uff1a\u65e2\u53ef\u72ec\u7acb\u4f7f\u7528Isaac Sim\u8fdb\u884c\u4eff\u771f\u8bad\u7ec3\uff0c\u4e5f\u53ef\u57fa\u4e8eIsaac Lab\u6846\u67b6\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002 \u5bf9\u5e94\u7684ECS\u5b9e\u4f8b\u5b89\u88c5\u4e86Ubuntu\u56fe\u5f62\u754c\u9762\uff0c\u652f\u6301\u5728ECS\u63a7\u5236\u53f0\u901a\u8fc7VNC\u7684\u65b9\u5f0f\u76f4\u63a5\u4f7f\u7528\u3002","title":"IsaacLab \u5355\u673a\u7248\u4f7f\u7528\u6559\u7a0b"},{"location":"isaaclab/original-index/#vncecs","text":"\u76ee\u524d\u63d0\u4f9b\u4e86\u4e24\u79cd\u65b9\u5f0f\u53bb\u8bbf\u95eeUbuntu\u5b9e\u4f8b\uff0c\u8fd9\u91cc\u63a8\u8350\u4f7f\u7528VncServer+VncRealViewer\u65b9\u5f0f\u8bbf\u95ee\uff0c\u53ef\u4ee5\u6bd4\u8f83\u65b9\u4fbf\u7684\u8c03\u6574\u5206\u8fa8\u7387\uff0c\u6574\u4f53\u4f7f\u7528\u4e5f\u66f4\u52a0\u6d41\u7545\u3002","title":"VNC\u65b9\u5f0f\u8bbf\u95eeECS\u5b9e\u4f8b"},{"location":"isaaclab/original-index/#vncservervncrealviewer","text":"\u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b\uff0c\u70b9\u51fb\u8fdc\u7a0b\u8fde\u63a5\u8fdb\u884c\u767b\u5f55\u3002 \u767b\u5f55\u5230ECS\u5b9e\u4f8b\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a # \u5207\u6362\u5230root\u8d26\u6237 sudo su root # \u8bbe\u7f6eVNC\u670d\u52a1\u5bc6\u7801\uff0c\u6ce8\u610f\u5bc6\u7801\u957f\u5ea6\u6700\u5927\u4e3a8\u4f4d /opt/TurboVNC/bin/vncpasswd # \u542f\u52a8VNC Server\u670d\u52a1,\u670d\u52a1\u4f1alisten 5901\u7aef\u53e3,\u5b89\u88c5\u7ec4\u5df2\u9884\u5148\u8bbe\u7f6e\u4e3a\u5f00\u653e /opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24 -xstartup ~/.vnc/xstartup \u4e0b\u8f7d VncRealViewer\u5ba2\u6237\u7aef \uff0c\u8fde\u63a5\u8f93\u5165<\u670d\u52a1\u5668\u516c\u7f51IP>:5901, \u5373\u53ef\u8fde\u63a5\u670d\u52a1\u5668\u3002 \u5728/home/isaac-sim/isaacsim\u8def\u5f84\u4e0b\uff0c\u6267\u884c./isaac-sim.sh --allow-root\u547d\u4ee4\uff0c\u5373\u53ef\u6253\u5f00Isaac Sim\u754c\u9762\u3002","title":"VncServer+VncRealViewer\u65b9\u5f0f\u8bbf\u95ee \uff08\u63a8\u8350\uff09"},{"location":"isaaclab/original-index/#ecsvnc","text":"\u5728\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u9875\u7684\u8d44\u6e90\u4e2d\uff0c\u627e\u5230\u5bf9\u5e94\u7684ECS\u5b9e\u4f8b\uff0c\u70b9\u51fb\u53bb\u5230ECS\u63a7\u5236\u53f0\u3002 \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u8fdc\u7a0b\u8fde\u63a5\uff0c\u9009\u62e9VNC\u767b\u5f55\u65b9\u5f0f\uff0c\u5373\u53ef\u8fdb\u5165\u5230Ubuntu\u7cfb\u7edf\u7684\u56fe\u5f62\u754c\u9762\u3002 \u8fd9\u91cc\u9700\u8981\u8f93\u5165isaac-sim\u8d26\u6237\u5bf9\u5e94\u767b\u5f55\u5bc6\u7801\uff0c\u5bf9\u5e94\u7684\u5bc6\u7801\u548cECS\u5b9e\u4f8b\u5bc6\u7801\u4e00\u81f4\uff0c\u53ef\u4ee5\u53bb\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u67e5\u770b\u3002","title":"Ecs\u63a7\u5236\u53f0\u81ea\u5e26\u539f\u751f\u7684VNC\u767b\u5f55\u65b9\u5f0f"},{"location":"isaaclab/original-index/#isaac-sim","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u65b9\u5f0f\u767b\u5f55\u5230ECS\u5b9e\u4f8b\u540e\uff0c\u6253\u5f00Terminal\uff0c\u53ef\u4ee5\u770b\u5230isaac-sim\u8d26\u6237\u4e0b\u6709isaacsim\u548cisaacsim_assets\u4e24\u4e2a\u76ee\u5f55\u3002 - isaacsim\u76ee\u5f55\u4e3aIsaac Sim\u7684\u5b89\u88c5\u76ee\u5f55\uff0c\u91cc\u9762\u6709Isaac Sim\u76f8\u5173\u7684\u542f\u52a8\u548c\u8bad\u7ec3\u811a\u672c\u3002 - isaacsim_assets\u76ee\u5f55\u4e3aIsaac Sim\u7684\u8d44\u6e90\u76ee\u5f55\uff0c\u8fd9\u91cc\u505a\u4e86\u63d0\u524d\u4e0b\u8f7d\uff0c\u65b9\u4fbf\u540e\u9762\u8bad\u7ec3\u4f7f\u7528\u3002","title":"Isaac Sim\u4f7f\u7528\u65b9\u5f0f"},{"location":"isaaclab/original-index/#1-gui","text":"\u8be5\u793a\u4f8b\u5c55\u793a\u4f7f\u7528omni.replicator\u6269\u5c55\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u7684\u8fc7\u7a0b\u3002\u751f\u6210\u7684\u6570\u636e\u5c06\u79bb\u7ebf\u5b58\u50a8\uff08\u5728\u78c1\u76d8\u4e0a\uff09\uff0c\u4f7f\u5176\u53ef\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u3002\u793a\u4f8b\u53ef\u4ee5\u5728Isaac Sim\u7684Python\u72ec\u7acb\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5e76\u5229\u7528Isaac Sim\u548cReplicator\u521b\u5efa\u79bb\u7ebf\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u5efa\u8bae\u5c06\u5b98\u65b9\u793a\u4f8b\u4ee3\u7801\u62f7\u8d1d\u5230\u7528\u6237\u76ee\u5f55\u4e0b\u8fdb\u884c\u4fee\u6539\u53ca\u4f7f\u7528\u3002 cd /home/isaac-sim mkdir -p isaacsim_test cd /home/isaac-sim/isaacsim_test mkdir -p scene_based_sdg cp -rf /home/isaac-sim/isaacsim/standalone_examples/replicator/scene_based_sdg/* /home/isaac-sim/isaacsim_test/scene_based_sdg/ ## \u6e32\u67d3\u5408\u6210(--config\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff0c\u5176\u4e2d\u8bbe\u7f6e\u4e86headless=true\uff1b--/persistent/isaac/asset_root/default\u6307\u5b9a3D\u8d44\u4ea7\u5b58\u50a8\u8def\u5f84) /home/isaac-sim/isaacsim/python.sh ./scene_based_sdg/scene_based_sdg.py --config=\"/home/isaac-sim/isaacsim_test/scene_based_sdg/config/config_coco_writer.yaml\" --/persistent/isaac/asset_root/default=\"/home/isaac-sim/isaacsim_assets/Assets/Isaac/4.5\" \u751f\u6210\u7ed3\u679c\u5b58\u50a8\u5728\u201d./isaacsim_test/_out_coco\u201c\u4e2d\uff0c\u53ef\u89c6\u5316\u6548\u679c\u5982\u4e0b\uff1a","title":"\u793a\u4f8b1: \u4ee5\u65e0GUI\u7684\u65b9\u5f0f\u8fdb\u884c\u573a\u666f\u5408\u6210\u6570\u636e\u96c6\u751f\u6210"},{"location":"isaaclab/original-index/#2-guiisaac-sim","text":"\u5728termial\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5373\u53ef\u8fdb\u5165Isaac Sim\u7684GUI\u754c\u9762\u3002 cd /home/isaac-sim/isaacsim ./isaac-sim.sh \u8fd9\u91cc\u8981\u6ce8\u610f\u7684\u662f\uff0cIsaac Sim\u542f\u52a8\u7684\u65f6\u5019\u4f1a\u6bd4\u8f83\u6162\uff0c\u4f1a\u5f39\u51fa\u7b49\u5f85\u7684\u7a97\u53e3\uff0c\u4e0d\u7528\u8fdb\u884c\u64cd\u4f5c\uff0c\u7b49\u5f85\u4e00\u6bb5\u65f6\u95f4\u5373\u53ef\u3002 \u4e0b\u9762\u662f\u6309 \u5165\u95e8\u6559\u7a0b \u4e2d\u7684\u6b65\u9aa4\u521b\u5efa\u4e86\u4e2a\u6b63\u65b9\u4f53\u3002","title":"\u793a\u4f8b2: \u4ee5GUI\u7684\u65b9\u5f0f\u4f7f\u7528Isaac Sim"},{"location":"isaaclab/original-index/#isaac-lab","text":"Isaac Lab\u670d\u52a1\u5b89\u88c5\u76ee\u5f55\u5728/home/isaac-sim/IsaacLab\u4e2d\uff0c\u91cc\u9762\u6709Isaac Lab\u7684\u5b89\u88c5\u76ee\u5f55\u548c\u542f\u52a8\u811a\u672c\u3002","title":"Isaac Lab\u7684\u4f7f\u7528\u65b9\u5f0f"},{"location":"isaaclab/original-index/#1-gui_1","text":"\u672c\u6848\u4f8b\uff0c\u6211\u4eec\u5c06\u4f7f\u7528Stable-Baselines3\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3Cartpole\u5e73\u8861\u63a7\u5236\u7684\u667a\u80fd\u4f53\u4efb\u52a1\u3002Stable-Baselines3\u662f\u4e00\u4e2a\u57fa\u4e8e PyTorch \u7684\u5f3a\u5316\u5b66\u4e60\u5e93\uff0c\u63d0\u4f9b\u4e86\u591a\u79cd\u7a33\u5b9a\u4e14\u6613\u7528\u7684 RL \u7b97\u6cd5\uff0c\u5982 PPO\u3001SAC\u3001DQN\u7b49\u3002\u8bad\u7ec3\u76ee\u6807\u662f\uff0c\u8ba9\u667a\u80fd\u4f53\u5b66\u4e60\u5982\u4f55\u63a7\u5236\u5c0f\u8f66\u7684\u5de6\u53f3\u79fb\u52a8\uff0c\u4fdd\u6301\u6446\u6746\u76f4\u7acb\u4e0d\u5012\u3002\u5efa\u8bae\u5c06\u793a\u4f8b\u4ee3\u7801\u62f7\u8d1d\u5230\u4e2a\u4eba\u76ee\u5f55\u8fdb\u884c\u4fee\u6539\u548c\u8c03\u8bd5\u3002 cd /home/isaac-sim mkdir -p isaaclab_test cd /home/isaac-sim/isaaclab_test mkdir -p sb3 cp -rf /home/isaac-sim/IsaacLab/scripts/reinforcement_learning/sb3/* /home/isaac-sim/isaaclab_test/sb3/ ## \u6e32\u67d3\u5408\u6210(--config\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff0c\u5176\u4e2d\u8bbe\u7f6e\u4e86headless=true\uff1b--/persistent/isaac/asset_root/default\u6307\u5b9a3D\u8d44\u4ea7\u5b58\u50a8\u8def\u5f84) /home/isaac-sim/IsaacLab/isaaclab.sh -p ./sb3/train.py --task Isaac-Cartpole-v0 --num_envs 64 --headless --video \u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u5230./logs/sb3/Isaac-Cartpole-v0\u4e2d\uff1b\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b","title":"\u793a\u4f8b1\uff1a \u4f7f\u7528\u65e0GUI\u6a21\u5f0f\u8bad\u7ec3\u667a\u80fd\u4f53"},{"location":"isaaclab/original-index/#2-gui","text":"\u5728Terminal\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5373\u53ef\u8fdb\u5165Isaac Lab\u7684GUI\u754c\u9762\u3002 cd /home/isaac-sim/IsaacLab ./isaaclab.sh -p scripts/tutorials/00_sim/spawn_prims.py","title":"\u793a\u4f8b2\uff1a \u4f7f\u7528GUI\u6a21\u5f0f\u751f\u6210\u57fa\u672c\u7269\u4f53\u5230\u573a\u666f\u4e2d"},{"location":"isaaclab/original-index/#isaaclab_1","text":"Isaac Lab \u652f\u6301 Ray \uff0c\u7528\u4e8e\u7b80\u5316\u591a\u4e2a\u8bad\u7ec3\u4efb\u52a1\u7684\u8c03\u5ea6\uff08\u5305\u62ec\u5e76\u884c\u548c\u4e32\u884c\uff09\uff0c\u4ee5\u53ca\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u9002\u7528\u4e8e\u672c\u5730\u548c\u8fdc\u7a0b\u914d\u7f6e\u3002 Isaac Lab\u670d\u52a1Ray\u4f5c\u4e1a\u8c03\u5ea6\u548c\u8c03\u4f18\u5b98\u65b9\u6587\u6863\u4e3aRay Job Dispatch and Tuning\u3002","title":"IsaacLab \u96c6\u7fa4\u7248\u4f7f\u7528\u6559\u7a0b"},{"location":"isaaclab/original-index/#_2","text":"\u5728\u672c\u5730\u7535\u8111\u914d\u7f6e\u8fdc\u7a0bRay\u96c6\u7fa4\u4fe1\u606f\uff0c\u5177\u4f53\u547d\u4ee4\u5982\u4e0b\uff1a # \u8fd9\u91cc\u7684<ISAACRAY_ADDRESS>\u4e3aray\u96c6\u7fa4\u7684url\uff0c\u53ef\u4ee5\u4ece\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u4e2d\u83b7\u53d6 echo \"name: isaacray address: <ISAACRAY_ADDRESS>\" > ~/.cluster_config export RAY_ADDRESS=\"<ISAACRAY_ADDRESS>\" \u4ecegit\u4e0a\u4e0b\u8f7d Isaac Lab\u6e90\u7801 , \u7528\u6765\u505a\u540e\u7eed\u7684\u4f5c\u4e1a\u63d0\u4ea4\u3002 \u5728\u672c\u5730\u7535\u8111\u5b89\u88c5ray\u5ba2\u6237\u7aef\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a pip install \"ray[default]\"","title":"\u73af\u5883\u51c6\u5907"},{"location":"isaaclab/original-index/#job","text":"\u5728Isaac Lab\u6e90\u7801\u76ee\u5f55scripts/reinforcement_learning/ray\u4e0b\u65b0\u5efa \u4e2a\u6d4b\u8bd5\u7528\u7684python\u6587\u4ef6test.py\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a import ray import os # \u8fde\u63a5\u672c\u5730\u6216\u8005\u8fdc\u7a0bray cluster ray.init() @ray.remote(num_cpus=1) class Counter: def __init__(self): self.name = \"test_counter\" self.counter = 0 def increment(self): self.counter += 1 def get_counter(self): return \"{} got {}\".format(self.name, self.counter) counter = Counter.remote() for _ in range(10): counter.increment.remote() print(ray.get(counter.get_counter.remote())) \u5728Isaac Lab\u6e90\u4ee3\u7801\u76ee\u5f55\u4e0b\uff0c\u4f7f\u7528ray\u63d0\u4ea4\u4f5c\u4e1a\uff0c\u5177\u4f53\u547d\u4ee4\u5982\u4e0b\uff1a python3 scripts/reinforcement_learning/ray/submit_job.py --aggregate_jobs wrap_resources.py --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p test.py\" \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u4ece\u65e5\u5fd7\u91cc\u770b\u5230\u4ee5\u4e0b\u4fe1\u606f\uff1a \u63d0\u4ea4\u4f5c\u4e1a\u65f6\uff0c\u4f1a\u628ascripts/reinforcement_learning/ray\u76ee\u5f55\u5f53\u4f5c\u5de5\u4f5c\u76ee\u5f55\u8fdb\u884c\u6253\u5305\uff0c\u4e0a\u4f20\u5230\u96c6\u7fa4\u4e2d\uff0c\u6240\u4ee5\u6211\u4eec\u7684test.py\u4e5f\u4f1a\u88ab\u4e0a\u4f20\u3002 job\u8fd0\u884c\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u8f93\u51fa\u7684\u8fd0\u884c\u4fe1\u606f\uff1a","title":"\u5728\u65e5\u5fd7\u4e2d\u6253\u5370\u6267\u884c\u7ed3\u679c\u7684\u6d4b\u8bd5job"},{"location":"isaaclab/original-index/#isaac-lab_1","text":"\u5728Isaac Lab\u6e90\u4ee3\u7801\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\u547d\u4ee4\u63d0\u4ea4\u4f5c\u4e1a\uff0c\u5177\u4f53\u547d\u4ee4\u5982\u4e0b\uff1a python3 scripts/reinforcement_learning/ray/submit_job.py --aggregate_jobs wrap_resources.py --sub_jobs \"/workspace/isaaclab/isaaclab.sh -p /workspace/isaaclab/scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Ant-v0 --headless\" \u63d0\u4ea4\u6210\u529f\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u65e5\u5fd7\u91cc\u8f93\u51fa\u7684\u4fe1\u606f\uff0c\u8fd9\u91cc\u4e3b\u8981\u53ef\u4ee5\u770b\u5230job\u5728\u96c6\u7fa4\u4e0a\u7684\u5de5\u4f5c\u76ee\u5f55\uff0c\u672c\u4f8b\u4e0a\u4e3a_ray_pkg_18b3cac8e32d6f62\u3002 \u70b9\u51fbray\u96c6\u7fa4url, \u53ef\u4ee5\u5230\u96c6\u7fa4\u7684web ui\u4e2d\u67e5\u770bjob\u8fd0\u884c\u60c5\u51b5\u3002 \u70b9\u51fb\u6b63\u5728\u8fd0\u884c\u7684\u8fd9\u4e2ajob\uff0c\u53ef\u4ee5\u770b\u5230job\u7684\u8c03\u5ea6\u65e5\u5fd7\uff0c\u672c\u4f8b\u662f\u8c03\u5ea6\u5230\u4e86c9db26a6c016fb4394991190f132afe99cd4a2b0a696f14185001650\u8282\u70b9\uff0c \u5bf9\u5e94\u7684\u8bad\u7ec3\u7ed3\u679c\u4e5f\u8981\u5230\u8fd9\u4e2a\u8282\u70b9\u4e0a\u67e5\u770b\u3002 \u5207\u5230Cluster Tab\u4e0b\uff0c\u8f93\u5165node id\u8fdb\u884c\u641c\u7d22\uff0c\u53ef\u4ee5\u627e\u5230\u5bb9\u5668\u96c6\u7fa4\u4e2d\u5bf9\u5e94\u7684Pod\u3002 \u4ece\u670d\u52a1\u5b9e\u4f8b\u8d44\u6e90\u4e2d\u627e\u5230\u5bf9\u5e94\u7684\u5bb9\u5668\u96c6\u7fa4\uff0c\u53bb\u5bb9\u5668\u96c6\u7fa4\u4e2d\u627e\u5230\u4e0a\u9762\u5bf9\u5e94\u7684Pod\uff0c\u5e76\u767b\u5f55\u5230Pod\u4e2d\u3002 \u767b\u5f55\u5230Pod\u4e2d\uff0c\u53ef\u4ee5\u770b\u5230\u8bad\u7ec3\u7ed3\u679c\uff0c\u672c\u4f8b\u8bad\u7ec3\u7684Ant\u73af\u5883\uff0c\u8bad\u7ec3\u7684\u7ed3\u679c\u4fdd\u5b58\u5728\u4e0b\u9762\u7684\u4e34\u65f6\u76ee\u5f55\u4e2d\uff1a # \u6267\u884cjob\u7684\u4e34\u65f6\u76ee\u5f55\uff0c\u5176\u4e2d_ray_pkg_18b3cac8e32d6f62\u53d6\u51b3\u4e8e\u4e0a\u4f20\u6587\u4ef6\u7684\u76ee\u5f55\uff0c2025-08-21_08-08-24 \u662f\u5177\u4f53\u8fd0\u884c\u65f6\u95f4 cd /tmp/ray/session_latest/runtime_resources/working_dir_files/_ray_pkg_18b3cac8e32d6f62/logs/rsl_rl/ant/2025-08-21_08-08-24","title":"\u6267\u884cIsaac Lab\u8bad\u7ec3\u4efb\u52a1"},{"location":"kimi/index-kimi-dev-en/","text":"\ud83d\ude80 Kimi-Dev-72B Open-Source Coding Large Model Next-generation AI programming assistant designed specifically for software engineering tasks \ud83c\udfaf Model Overview **Kimi-Dev-72B** is a groundbreaking open-source coding large model that has been deeply optimized specifically for software engineering tasks. This model has set new records for open-source models in industry-standard benchmarks, bringing unprecedented AI programming capabilities to the developer community. \ud83c\udfc6 Key Highlights Achieved exceptional performance of 60.4% on the SWE-bench Verified test, surpassing all other open-source competitors and establishing a new industry benchmark. \ud83d\udcca Performance Results \ud83c\udfaf SWE-bench Verified Benchmark \ud83e\udd47 Kimi-Dev-72B 60.4% Open-Source Model SOTA \ud83d\udcc8 Performance Advantages Surpasses all open-source competitors Establishes new industry benchmark Validated in real software engineering scenarios \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83d\ude80 Kimi-Dev-72B | Ushering in a new era of AI programming, making every line of code more intelligent","title":"Index kimi dev en"},{"location":"kimi/index-kimi-dev-en/#model-overview","text":"**Kimi-Dev-72B** is a groundbreaking open-source coding large model that has been deeply optimized specifically for software engineering tasks. This model has set new records for open-source models in industry-standard benchmarks, bringing unprecedented AI programming capabilities to the developer community. \ud83c\udfc6 Key Highlights Achieved exceptional performance of 60.4% on the SWE-bench Verified test, surpassing all other open-source competitors and establishing a new industry benchmark.","title":"\ud83c\udfaf Model Overview"},{"location":"kimi/index-kimi-dev-en/#performance-results","text":"","title":"\ud83d\udcca Performance Results"},{"location":"kimi/index-kimi-dev-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"kimi/index-kimi-dev-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"kimi/index-kimi-dev-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"kimi/index-kimi-dev-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"kimi/index-kimi-dev-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"kimi/index-kimi-dev-original/","text":"\u7b80\u4ecb Kimi-Dev-72B\u662f\u4e00\u4e2a\u65b0\u7684\u5f00\u6e90\u7f16\u7801\u5927\u6a21\u578b\uff0c\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002Kimi-Dev-72B \u5728 SWE-bench Verified \u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u65b0\u9ad8\u5ea6\u3002 Kimi-Dev-72B \u5728 SWE-bench Verified \u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86 60.4% \u7684\u6027\u80fd\u3002\u5b83\u8d85\u8d8a\u4e86\u5176\u4ed6\u7ade\u4e89\u8005\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002 Kimi-Dev-72B \u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5b83\u80fd\u591f\u81ea\u4e3b\u4fee\u8865 Docker \u4e2d\u7684\u771f\u5b9e\u4ed3\u5e93\uff0c\u5e76\u4e14\u53ea\u6709\u5728\u6240\u6709\u6d4b\u8bd5\u5957\u4ef6\u901a\u8fc7\u65f6\u624d\u80fd\u83b7\u5f97\u5956\u52b1\u3002\u8fd9\u786e\u4fdd\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7b26\u5408\u5b9e\u9645\u5f00\u53d1\u6807\u51c6\u3002 Kimi-Dev-72B \u53ef\u4ee5\u5728 Hugging Face \u548c GitHub \u4e0a\u4e0b\u8f7d\u548c\u90e8\u7f72\u3002\u6211\u4eec\u6b22\u8fce\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u5176\u529f\u80fd\u5e76\u4e3a\u5f00\u53d1\u505a\u51fa\u8d21\u732e\u3002","title":"\u7b80\u4ecb"},{"location":"kimi/index-kimi-dev-original/#_1","text":"Kimi-Dev-72B\u662f\u4e00\u4e2a\u65b0\u7684\u5f00\u6e90\u7f16\u7801\u5927\u6a21\u578b\uff0c\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002Kimi-Dev-72B \u5728 SWE-bench Verified \u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u65b0\u9ad8\u5ea6\u3002 Kimi-Dev-72B \u5728 SWE-bench Verified \u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86 60.4% \u7684\u6027\u80fd\u3002\u5b83\u8d85\u8d8a\u4e86\u5176\u4ed6\u7ade\u4e89\u8005\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002 Kimi-Dev-72B \u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5b83\u80fd\u591f\u81ea\u4e3b\u4fee\u8865 Docker \u4e2d\u7684\u771f\u5b9e\u4ed3\u5e93\uff0c\u5e76\u4e14\u53ea\u6709\u5728\u6240\u6709\u6d4b\u8bd5\u5957\u4ef6\u901a\u8fc7\u65f6\u624d\u80fd\u83b7\u5f97\u5956\u52b1\u3002\u8fd9\u786e\u4fdd\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7b26\u5408\u5b9e\u9645\u5f00\u53d1\u6807\u51c6\u3002 Kimi-Dev-72B \u53ef\u4ee5\u5728 Hugging Face \u548c GitHub \u4e0a\u4e0b\u8f7d\u548c\u90e8\u7f72\u3002\u6211\u4eec\u6b22\u8fce\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u5176\u529f\u80fd\u5e76\u4e3a\u5f00\u53d1\u505a\u51fa\u8d21\u732e\u3002","title":"\u7b80\u4ecb"},{"location":"kimi/index-kimi-dev/","text":"\ud83d\ude80 Kimi-Dev-72B \u5f00\u6e90\u7f16\u7801\u5927\u6a21\u578b \u4e13\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8bbe\u8ba1\u7684\u65b0\u4e00\u4ee3AI\u7f16\u7a0b\u52a9\u624b \ud83c\udfaf \u6a21\u578b\u6982\u8ff0 **Kimi-Dev-72B** \u662f\u4e00\u4e2a\u7a81\u7834\u6027\u7684\u5f00\u6e90\u7f16\u7801\u5927\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8fdb\u884c\u4e86\u6df1\u5ea6\u4f18\u5316\u3002\u8be5\u6a21\u578b\u5728\u4e1a\u754c\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u9020\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u65b0\u7eaa\u5f55\uff0c\u4e3a\u5f00\u53d1\u8005\u793e\u533a\u5e26\u6765\u4e86\u524d\u6240\u672a\u6709\u7684\u7f16\u7a0bAI\u80fd\u529b\u3002 \ud83c\udfc6 \u6838\u5fc3\u4eae\u70b9 \u5728 SWE-bench Verified \u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86 60.4% \u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u6240\u6709\u5176\u4ed6\u5f00\u6e90\u7ade\u4e89\u8005\uff0c\u6811\u7acb\u4e86\u65b0\u7684\u884c\u4e1a\u6807\u6746\u3002 \ud83d\udcca \u6027\u80fd\u8868\u73b0 \ud83c\udfaf SWE-bench Verified \u57fa\u51c6\u6d4b\u8bd5 \ud83e\udd47 Kimi-Dev-72B 60.4% \u5f00\u6e90\u6a21\u578b SOTA \ud83d\udcc8 \u6027\u80fd\u4f18\u52bf \u8d85\u8d8a\u6240\u6709\u5f00\u6e90\u7ade\u4e89\u8005 \u6811\u7acb\u65b0\u7684\u884c\u4e1a\u6807\u6746 \u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u573a\u666f\u9a8c\u8bc1 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83d\ude80 Kimi-Dev-72B | \u5f00\u542fAI\u7f16\u7a0b\u65b0\u65f6\u4ee3\uff0c\u8ba9\u6bcf\u4e00\u884c\u4ee3\u7801\u90fd\u66f4\u52a0\u667a\u80fd","title":"Index kimi dev"},{"location":"kimi/index-kimi-dev/#_1","text":"**Kimi-Dev-72B** \u662f\u4e00\u4e2a\u7a81\u7834\u6027\u7684\u5f00\u6e90\u7f16\u7801\u5927\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8fdb\u884c\u4e86\u6df1\u5ea6\u4f18\u5316\u3002\u8be5\u6a21\u578b\u5728\u4e1a\u754c\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u9020\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u65b0\u7eaa\u5f55\uff0c\u4e3a\u5f00\u53d1\u8005\u793e\u533a\u5e26\u6765\u4e86\u524d\u6240\u672a\u6709\u7684\u7f16\u7a0bAI\u80fd\u529b\u3002 \ud83c\udfc6 \u6838\u5fc3\u4eae\u70b9 \u5728 SWE-bench Verified \u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86 60.4% \u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u6240\u6709\u5176\u4ed6\u5f00\u6e90\u7ade\u4e89\u8005\uff0c\u6811\u7acb\u4e86\u65b0\u7684\u884c\u4e1a\u6807\u6746\u3002","title":"\ud83c\udfaf \u6a21\u578b\u6982\u8ff0"},{"location":"kimi/index-kimi-dev/#_2","text":"","title":"\ud83d\udcca \u6027\u80fd\u8868\u73b0"},{"location":"kimi/index-kimi-dev/#_3","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"kimi/index-kimi-dev/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"kimi/index-kimi-dev/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"kimi/index-kimi-dev/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"kimi/index-kimi-dev/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"kimi/index-kimi-k2-en/","text":"\ud83e\udd16 Kimi K2 Language Model Advanced Mixture of Experts (MoE) Language Model Technology Overview \ud83c\udfaf Core Overview Kimi K2 is a state-of-the-art Mixture of Experts (MoE) language model with **32 billion activated parameters** and **1 trillion total parameters**. Trained with the Muon optimizer, Kimi K2 excels in cutting-edge knowledge, reasoning, and programming tasks, while being specifically optimized for agentic capabilities. \ud83c\udfd7\ufe0f Technical Architecture Features \ud83d\udcca Large-Scale Training Training Scale : Pre-trained on 15.5T tokens Model Parameters : 1T parameter MoE architecture Stability : No training instability issues \u26a1 MuonClip Optimizer Innovative Application : Muon optimizer at unprecedented scale Technical Breakthrough : Novel optimization techniques Stability Assurance : Resolves instabilities during scaling \ud83e\udde0 Agentic Intelligence Tool Usage : Specially optimized tool calling capabilities Reasoning Ability : Advanced logical reasoning Autonomous Problem-Solving : Independent problem resolution capabilities \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83e\udd16 Kimi K2 | Next-Generation Mixture of Experts Language Model Technology Breakthrough","title":"Index kimi k2 en"},{"location":"kimi/index-kimi-k2-en/#core-overview","text":"Kimi K2 is a state-of-the-art Mixture of Experts (MoE) language model with **32 billion activated parameters** and **1 trillion total parameters**. Trained with the Muon optimizer, Kimi K2 excels in cutting-edge knowledge, reasoning, and programming tasks, while being specifically optimized for agentic capabilities.","title":"\ud83c\udfaf Core Overview"},{"location":"kimi/index-kimi-k2-en/#technical-architecture-features","text":"","title":"\ud83c\udfd7\ufe0f Technical Architecture Features"},{"location":"kimi/index-kimi-k2-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"kimi/index-kimi-k2-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"kimi/index-kimi-k2-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"kimi/index-kimi-k2-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"kimi/index-kimi-k2-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"kimi/index-kimi-k2-original/","text":"\u7b80\u4ecb Kimi K2 \u662f\u4e00\u6b3e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8bed\u8a00\u6a21\u578b\uff0c\u62e5\u6709 320 \u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c 1 \u4e07\u4ebf\u603b\u53c2\u6570\u3002\u901a\u8fc7 Muon \u4f18\u5316\u5668\u8bad\u7ec3\uff0cKimi K2 \u5728\u524d\u6cbf\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5728\u667a\u80fd\u4f53\u80fd\u529b\u65b9\u9762\u8fdb\u884c\u4e86\u7cbe\u5fc3\u4f18\u5316\u3002 \u4e3b\u8981\u7279\u70b9\uff1a - \u5927\u89c4\u6a21\u8bad\u7ec3\uff1a\u5728 15.5T \u4e2a\u6807\u8bb0\u4e0a\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a 1T \u53c2\u6570\u7684 MoE \u6a21\u578b\uff0c\u4e14\u65e0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002 - MuonClip \u4f18\u5316\u5668\uff1a\u6211\u4eec\u5c06 Muon \u4f18\u5316\u5668\u5e94\u7528\u5230\u4e86\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u4f18\u5316\u6280\u672f\u6765\u89e3\u51b3\u6269\u5c55\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u3002 - \u667a\u80fd\u4f53\u667a\u80fd\uff1a\u7279\u522b\u8bbe\u8ba1\u7528\u4e8e\u5de5\u5177\u4f7f\u7528\u3001\u63a8\u7406\u548c\u81ea\u4e3b\u95ee\u9898\u89e3\u51b3\u3002","title":"\u7b80\u4ecb"},{"location":"kimi/index-kimi-k2-original/#_1","text":"Kimi K2 \u662f\u4e00\u6b3e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8bed\u8a00\u6a21\u578b\uff0c\u62e5\u6709 320 \u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c 1 \u4e07\u4ebf\u603b\u53c2\u6570\u3002\u901a\u8fc7 Muon \u4f18\u5316\u5668\u8bad\u7ec3\uff0cKimi K2 \u5728\u524d\u6cbf\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5728\u667a\u80fd\u4f53\u80fd\u529b\u65b9\u9762\u8fdb\u884c\u4e86\u7cbe\u5fc3\u4f18\u5316\u3002 \u4e3b\u8981\u7279\u70b9\uff1a - \u5927\u89c4\u6a21\u8bad\u7ec3\uff1a\u5728 15.5T \u4e2a\u6807\u8bb0\u4e0a\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a 1T \u53c2\u6570\u7684 MoE \u6a21\u578b\uff0c\u4e14\u65e0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002 - MuonClip \u4f18\u5316\u5668\uff1a\u6211\u4eec\u5c06 Muon \u4f18\u5316\u5668\u5e94\u7528\u5230\u4e86\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u4f18\u5316\u6280\u672f\u6765\u89e3\u51b3\u6269\u5c55\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u3002 - \u667a\u80fd\u4f53\u667a\u80fd\uff1a\u7279\u522b\u8bbe\u8ba1\u7528\u4e8e\u5de5\u5177\u4f7f\u7528\u3001\u63a8\u7406\u548c\u81ea\u4e3b\u95ee\u9898\u89e3\u51b3\u3002","title":"\u7b80\u4ecb"},{"location":"kimi/index-kimi-k2/","text":"\ud83e\udd16 Kimi K2 \u8bed\u8a00\u6a21\u578b \u6700\u5148\u8fdb\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8bed\u8a00\u6a21\u578b\u6280\u672f\u89e3\u6790 \ud83c\udfaf \u6838\u5fc3\u6982\u8ff0 Kimi K2 \u662f\u4e00\u6b3e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8bed\u8a00\u6a21\u578b\uff0c\u62e5\u6709 **320 \u4ebf\u6fc0\u6d3b\u53c2\u6570**\u548c **1 \u4e07\u4ebf\u603b\u53c2\u6570**\u3002\u901a\u8fc7 Muon \u4f18\u5316\u5668\u8bad\u7ec3\uff0cKimi K2 \u5728\u524d\u6cbf\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5728\u667a\u80fd\u4f53\u80fd\u529b\u65b9\u9762\u8fdb\u884c\u4e86\u7cbe\u5fc3\u4f18\u5316\u3002 \ud83c\udfd7\ufe0f \u6280\u672f\u67b6\u6784\u7279\u70b9 \ud83d\udcca \u5927\u89c4\u6a21\u8bad\u7ec3 \u8bad\u7ec3\u89c4\u6a21 \uff1a15.5T \u4e2a\u6807\u8bb0\u9884\u8bad\u7ec3 \u6a21\u578b\u53c2\u6570 \uff1a1T \u53c2\u6570\u7684 MoE \u67b6\u6784 \u7a33\u5b9a\u6027 \uff1a\u65e0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898 \u26a1 MuonClip \u4f18\u5316\u5668 \u521b\u65b0\u5e94\u7528 \uff1aMuon \u4f18\u5316\u5668\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21 \u6280\u672f\u7a81\u7834 \uff1a\u65b0\u7684\u4f18\u5316\u6280\u672f \u7a33\u5b9a\u6027\u4fdd\u969c \uff1a\u89e3\u51b3\u6269\u5c55\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027 \ud83e\udde0 \u667a\u80fd\u4f53\u667a\u80fd \u5de5\u5177\u4f7f\u7528 \uff1a\u4e13\u95e8\u4f18\u5316\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b \u63a8\u7406\u80fd\u529b \uff1a\u9ad8\u7ea7\u903b\u8f91\u63a8\u7406 \u81ea\u4e3b\u89e3\u51b3 \uff1a\u72ec\u7acb\u95ee\u9898\u89e3\u51b3\u80fd\u529b \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83e\udd16 Kimi K2 | \u4e0b\u4e00\u4ee3\u6df7\u5408\u4e13\u5bb6\u8bed\u8a00\u6a21\u578b\u7684\u6280\u672f\u7a81\u7834","title":"Index kimi k2"},{"location":"kimi/index-kimi-k2/#_1","text":"Kimi K2 \u662f\u4e00\u6b3e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8bed\u8a00\u6a21\u578b\uff0c\u62e5\u6709 **320 \u4ebf\u6fc0\u6d3b\u53c2\u6570**\u548c **1 \u4e07\u4ebf\u603b\u53c2\u6570**\u3002\u901a\u8fc7 Muon \u4f18\u5316\u5668\u8bad\u7ec3\uff0cKimi K2 \u5728\u524d\u6cbf\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5728\u667a\u80fd\u4f53\u80fd\u529b\u65b9\u9762\u8fdb\u884c\u4e86\u7cbe\u5fc3\u4f18\u5316\u3002","title":"\ud83c\udfaf \u6838\u5fc3\u6982\u8ff0"},{"location":"kimi/index-kimi-k2/#_2","text":"","title":"\ud83c\udfd7\ufe0f \u6280\u672f\u67b6\u6784\u7279\u70b9"},{"location":"kimi/index-kimi-k2/#_3","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"kimi/index-kimi-k2/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"kimi/index-kimi-k2/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"kimi/index-kimi-k2/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"kimi/index-kimi-k2/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"qwen/index-qwen2.5-en/","text":"\ud83c\udf1f Qwen2.5 Open Source Model Tongyi Qianwen Next-Generation Open Source Model - Breakthrough Performance Surpassing Llama-405B \ud83c\udfaf Product Overview At the Yunqi Conference on September 19th, Alibaba Cloud released Tongyi Qianwen's next-generation open source model Qwen2.5, with the flagship model Qwen2.5-72B surpassing Llama-405B in performance. All Qwen2.5 series models are pre-trained on 18T tokens of data, achieving an overall performance improvement of over 18% compared to Qwen2, with more knowledge and stronger programming and mathematical capabilities. \ud83c\udfc6 Performance Breakthrough The Qwen2.5-72B model demonstrates excellence in multiple authoritative benchmark tests, achieving industry-leading levels in general knowledge, coding capabilities, and mathematical abilities. \u2728 Core Features \ud83d\udcda Extended Context Support Supports context lengths up to 128K and can generate up to 8K content, meeting the needs for long document processing and complex conversations. \ud83c\udf0d Powerful Multilingual Capabilities Supports over 29 languages including Chinese, English, French, Spanish, Russian, Japanese, Vietnamese, and Arabic. \ud83c\udfad Flexible Role-Playing Can smoothly respond to diverse system prompts, enabling role-playing and chatbot tasks. \ud83d\udcca Structured Data Processing Shows significant improvement in instruction following, understanding structured data (such as tables), and generating structured output (especially JSON). \ud83d\udd27 Model Specifications \ud83d\udce6 Complete Model Series 0.5B Lightweight Deployment 1.5B Mobile Optimization 3B Edge Computing 7B General Applications 14B Enterprise Applications 32B High-Performance Services 72B Flagship Performance \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83c\udf1f Qwen2.5 Open Source Model | Performance Surpassing Llama-405B, New Benchmark for Open Source AI","title":"Index qwen2.5 en"},{"location":"qwen/index-qwen2.5-en/#product-overview","text":"At the Yunqi Conference on September 19th, Alibaba Cloud released Tongyi Qianwen's next-generation open source model Qwen2.5, with the flagship model Qwen2.5-72B surpassing Llama-405B in performance. All Qwen2.5 series models are pre-trained on 18T tokens of data, achieving an overall performance improvement of over 18% compared to Qwen2, with more knowledge and stronger programming and mathematical capabilities. \ud83c\udfc6 Performance Breakthrough The Qwen2.5-72B model demonstrates excellence in multiple authoritative benchmark tests, achieving industry-leading levels in general knowledge, coding capabilities, and mathematical abilities.","title":"\ud83c\udfaf Product Overview"},{"location":"qwen/index-qwen2.5-en/#core-features","text":"\ud83d\udcda Extended Context Support Supports context lengths up to 128K and can generate up to 8K content, meeting the needs for long document processing and complex conversations. \ud83c\udf0d Powerful Multilingual Capabilities Supports over 29 languages including Chinese, English, French, Spanish, Russian, Japanese, Vietnamese, and Arabic. \ud83c\udfad Flexible Role-Playing Can smoothly respond to diverse system prompts, enabling role-playing and chatbot tasks. \ud83d\udcca Structured Data Processing Shows significant improvement in instruction following, understanding structured data (such as tables), and generating structured output (especially JSON).","title":"\u2728 Core Features"},{"location":"qwen/index-qwen2.5-en/#model-specifications","text":"","title":"\ud83d\udd27 Model Specifications"},{"location":"qwen/index-qwen2.5-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"qwen/index-qwen2.5-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"qwen/index-qwen2.5-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"qwen/index-qwen2.5-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"qwen/index-qwen2.5-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"qwen/index-qwen2.5-original/","text":"\u7b80\u4ecb 9 \u6708 19 \u65e5\u4e91\u6816\u5927\u4f1a\uff0c\u963f\u91cc\u4e91\u53d1\u5e03\u901a\u4e49\u5343\u95ee\u65b0\u4e00\u4ee3\u5f00\u6e90\u6a21\u578b Qwen2.5\uff0c\u65d7\u8230\u6a21\u578b Qwen2.5-72B \u6027\u80fd\u8d85\u8d8a Llama-405B\u3002Qwen2.5 \u5168\u7cfb\u5217\u6a21\u578b\u90fd\u5728 18T tokens \u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u76f8\u6bd4 Qwen2\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347 18%\u4ee5\u4e0a\uff0c\u62e5\u6709\u66f4\u591a\u7684\u77e5\u8bc6\u3001\u66f4\u5f3a\u7684\u7f16\u7a0b\u548c\u6570\u5b66\u80fd\u529b\u3002Qwen2.5-72B \u6a21\u578b\u5728 MMLU-rudex \u57fa\u51c6\uff08\u8003\u5bdf\u901a\u7528\u77e5\u8bc6\uff09\u3001MBPP \u57fa\u51c6\uff08\u8003\u5bdf\u4ee3\u7801\u80fd\u529b\uff09\u548c MATH \u57fa\u51c6\uff08\u8003\u5bdf\u6570\u5b66\u80fd\u529b\uff09\u7684\u5f97\u5206\u9ad8\u8fbe 86.8\u300188.2\u300183.1\u3002 Qwen2.5 \u652f\u6301\u9ad8\u8fbe 128K \u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u53ef\u751f\u6210\u6700\u591a 8K \u5185\u5bb9\u3002\u6a21\u578b\u62e5\u6709\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u652f\u6301\u4e2d\u6587\u3001\u82f1\u6587\u3001\u6cd5\u6587\u3001\u897f\u73ed\u7259\u6587\u3001\u4fc4\u6587\u3001\u65e5\u6587\u3001\u8d8a\u5357\u6587\u3001\u963f\u62c9\u4f2f\u6587\u7b49 29 \u79cd\u4ee5\u4e0a\u8bed\u8a00\u3002\u6a21\u578b\u80fd\u591f\u4e1d\u6ed1\u54cd\u5e94\u591a\u6837\u5316\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u5b9e\u73b0\u89d2\u8272\u626e\u6f14\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u4efb\u52a1\u3002\u5728\u6307\u4ee4\u8ddf\u968f\u3001\u7406\u89e3\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u8868\u683c\uff09\u3001\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5c24\u5176\u662f JSON\uff09\u7b49\u65b9\u9762 Qwen2.5 \u90fd\u8fdb\u6b65\u660e\u663e\u3002 \u8bed\u8a00\u6a21\u578b\u65b9\u9762\uff0cQwen2.5 \u5f00\u6e90\u4e86 7 \u4e2a\u5c3a\u5bf8\uff0c0.5B\u30011.5B\u30013B\u30017B\u300114B\u300132B\u300172B\u3002\u901a\u4e49\u5343\u95ee 2.5-32B-Instruct\uff08Qwen2.5-32B-Instruct\uff09\u662f\u7531\u901a\u4e49\u5343\u95ee 2.5-32B\uff08Qwen2.5-32B\uff09\u7ecf\u8fc7\u6307\u4ee4\u8ddf\u968f\u5fae\u8c03\u540e\u5f97\u5230\u7684\u7248\u672c\uff0c\u80fd\u591f\u6839\u636e\u6307\u4ee4\u548c\u5386\u53f2\u5bf9\u8bdd\uff0c\u751f\u6210\u7b26\u5408\u6307\u4ee4\u7684\u6587\u672c\u3002 \u4f7f\u7528\u8bf4\u660e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86Api\u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548cApiKey\uff0c\u4e0b\u9762\u4f1a\u5206\u522b\u4ecb\u7ecd\u5982\u4f55\u8bbf\u95ee\u4f7f\u7528\u3002 API\u8c03\u7528 Curl\u547d\u4ee4\u8c03\u7528 Curl\u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684Api\u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578bAPI\u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a \u5176\u4e2d${ServerIP}\u53ef\u4ee5\u586b\u5199\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684IP\u5730\u5740\uff0c${ApiKey}\u4e3aApiKey\uff0c${ModelName}\u4e3a\u6a21\u578b\u540d\u79f0\u3002 curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' Python\u8c03\u7528 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a \u5176\u4e2d${ApiKey}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684ApiKey\uff1b${ServerUrl}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a/v1\u3002 from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"Index qwen2.5 original"},{"location":"qwen/index-qwen2.5-original/#_1","text":"9 \u6708 19 \u65e5\u4e91\u6816\u5927\u4f1a\uff0c\u963f\u91cc\u4e91\u53d1\u5e03\u901a\u4e49\u5343\u95ee\u65b0\u4e00\u4ee3\u5f00\u6e90\u6a21\u578b Qwen2.5\uff0c\u65d7\u8230\u6a21\u578b Qwen2.5-72B \u6027\u80fd\u8d85\u8d8a Llama-405B\u3002Qwen2.5 \u5168\u7cfb\u5217\u6a21\u578b\u90fd\u5728 18T tokens \u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u76f8\u6bd4 Qwen2\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347 18%\u4ee5\u4e0a\uff0c\u62e5\u6709\u66f4\u591a\u7684\u77e5\u8bc6\u3001\u66f4\u5f3a\u7684\u7f16\u7a0b\u548c\u6570\u5b66\u80fd\u529b\u3002Qwen2.5-72B \u6a21\u578b\u5728 MMLU-rudex \u57fa\u51c6\uff08\u8003\u5bdf\u901a\u7528\u77e5\u8bc6\uff09\u3001MBPP \u57fa\u51c6\uff08\u8003\u5bdf\u4ee3\u7801\u80fd\u529b\uff09\u548c MATH \u57fa\u51c6\uff08\u8003\u5bdf\u6570\u5b66\u80fd\u529b\uff09\u7684\u5f97\u5206\u9ad8\u8fbe 86.8\u300188.2\u300183.1\u3002 Qwen2.5 \u652f\u6301\u9ad8\u8fbe 128K \u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u53ef\u751f\u6210\u6700\u591a 8K \u5185\u5bb9\u3002\u6a21\u578b\u62e5\u6709\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u652f\u6301\u4e2d\u6587\u3001\u82f1\u6587\u3001\u6cd5\u6587\u3001\u897f\u73ed\u7259\u6587\u3001\u4fc4\u6587\u3001\u65e5\u6587\u3001\u8d8a\u5357\u6587\u3001\u963f\u62c9\u4f2f\u6587\u7b49 29 \u79cd\u4ee5\u4e0a\u8bed\u8a00\u3002\u6a21\u578b\u80fd\u591f\u4e1d\u6ed1\u54cd\u5e94\u591a\u6837\u5316\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u5b9e\u73b0\u89d2\u8272\u626e\u6f14\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u4efb\u52a1\u3002\u5728\u6307\u4ee4\u8ddf\u968f\u3001\u7406\u89e3\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u8868\u683c\uff09\u3001\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5c24\u5176\u662f JSON\uff09\u7b49\u65b9\u9762 Qwen2.5 \u90fd\u8fdb\u6b65\u660e\u663e\u3002 \u8bed\u8a00\u6a21\u578b\u65b9\u9762\uff0cQwen2.5 \u5f00\u6e90\u4e86 7 \u4e2a\u5c3a\u5bf8\uff0c0.5B\u30011.5B\u30013B\u30017B\u300114B\u300132B\u300172B\u3002\u901a\u4e49\u5343\u95ee 2.5-32B-Instruct\uff08Qwen2.5-32B-Instruct\uff09\u662f\u7531\u901a\u4e49\u5343\u95ee 2.5-32B\uff08Qwen2.5-32B\uff09\u7ecf\u8fc7\u6307\u4ee4\u8ddf\u968f\u5fae\u8c03\u540e\u5f97\u5230\u7684\u7248\u672c\uff0c\u80fd\u591f\u6839\u636e\u6307\u4ee4\u548c\u5386\u53f2\u5bf9\u8bdd\uff0c\u751f\u6210\u7b26\u5408\u6307\u4ee4\u7684\u6587\u672c\u3002","title":"\u7b80\u4ecb"},{"location":"qwen/index-qwen2.5-original/#_2","text":"\u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86Api\u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548cApiKey\uff0c\u4e0b\u9762\u4f1a\u5206\u522b\u4ecb\u7ecd\u5982\u4f55\u8bbf\u95ee\u4f7f\u7528\u3002","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"qwen/index-qwen2.5-original/#api","text":"","title":"API\u8c03\u7528"},{"location":"qwen/index-qwen2.5-original/#curl","text":"Curl\u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684Api\u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578bAPI\u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a \u5176\u4e2d${ServerIP}\u53ef\u4ee5\u586b\u5199\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684IP\u5730\u5740\uff0c${ApiKey}\u4e3aApiKey\uff0c${ModelName}\u4e3a\u6a21\u578b\u540d\u79f0\u3002 curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"Curl\u547d\u4ee4\u8c03\u7528"},{"location":"qwen/index-qwen2.5-original/#python","text":"\u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a \u5176\u4e2d${ApiKey}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684ApiKey\uff1b${ServerUrl}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a/v1\u3002 from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"Python\u8c03\u7528"},{"location":"qwen/index-qwen2.5-vl-en/","text":"\ud83d\udc41\ufe0f Qwen2.5-VL Vision-Language Model More Useful Vision-Language Model - AI Expert for Understanding Images, Videos, and Documents \ud83c\udfaf Product Overview Qwen2.5-VL is a more useful vision-language model built by Alibaba Cloud's Qwen team based on Qwen2-VL and incorporating valuable feedback from numerous developers. This model has achieved significant improvements in visual understanding, agent capabilities, video analysis, visual localization, and structured output. \ud83d\ude80 Core Upgrades Continuously optimized based on developer feedback, Qwen2.5-VL has achieved major breakthroughs in practicality and functional completeness, becoming a more practical vision-language AI assistant. \u2728 Core Features \ud83d\udc41\ufe0f Visual Understanding of Objects Not only can skillfully recognize common objects such as flowers, birds, fish, and insects, but can also analyze text, charts, icons, graphics, and layouts in images. \ud83e\udd16 Intelligent Agency Directly acts as a visual agent with reasoning and dynamic tool command functions, applicable for computer and mobile device operation control. \ud83c\udfac Long Video Understanding Can understand videos longer than 1 hour, with new functionality to capture events through precise localization of relevant video segments. \ud83c\udfaf Visual Localization Capability Can accurately locate objects in images by generating bounding boxes or points, and can provide stable JSON output for coordinates and attributes. \ud83d\udcca Structured Output For scanned data such as invoices, tables, and forms, supports structured output of their content, beneficial for applications in finance, business, and other fields. \ud83d\udd0d Visual Understanding Capabilities \ud83c\udf1f Comprehensive Visual Recognition \ud83c\udf3a Natural Object Recognition Recognition Scope: \u2022 Flower and plant classification \u2022 Bird and animal identification \u2022 Fish and aquatic life \u2022 Insect and microorganism detection Professional-grade Accuracy \ud83d\udcdd Text and Chart Analysis Analysis Capabilities: \u2022 Image text extraction \u2022 Chart data interpretation \u2022 Icon and symbol recognition \u2022 Layout structure analysis Multimodal Understanding \ud83c\udfa8 Graphic Design Understanding Design Analysis: \u2022 Graphic element recognition \u2022 Design style analysis \u2022 Color scheme understanding \u2022 Visual hierarchy deconstruction Aesthetic Perception \ud83c\udfac Long Video Understanding \u23f0 Extended Duration Video Analysis Video Length Analysis Capabilities Application Scenarios Short Videos ( < 10 minutes) Fine-grained action recognition, emotion analysis Social media content analysis Medium Videos (10-30 minutes) Event sequence understanding, topic extraction Educational training video analysis Long Videos (> 1 hour) Global understanding, event localization, segment retrieval Movie analysis, meeting records \ud83c\udfaf Event Capture \u2022 Automatic key event recognition \u2022 Precise timestamp localization \u2022 Intelligent related segment retrieval \u2022 Event correlation analysis \ud83d\udcca Content Understanding \u2022 Video topic extraction \u2022 Plot development tracking \u2022 Character behavior analysis \u2022 Scene change detection \ud83d\ude80 Application Scenarios \ud83d\udcbc Business Office \u2022 Document automation processing \u2022 Invoice and receipt recognition \u2022 Table data extraction \u2022 Office workflow automation \ud83c\udf93 Education and Training \u2022 Educational video analysis \u2022 Learning content extraction \u2022 Knowledge point localization \u2022 Interactive teaching assistance \ud83c\udfac Media Entertainment \u2022 Video content analysis \u2022 Highlight segment extraction \u2022 Subtitle generation optimization \u2022 Content recommendation systems \ud83c\udfe5 Healthcare \u2022 Medical image analysis \u2022 Medical record document processing \u2022 Examination report interpretation \u2022 Diagnostic assistance support \ud83d\udcd6 User Guide \ud83d\udca1 After Deployment After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses (available after enabling public network access), and Api_Key. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name \ud83d\uddbc\ufe0f Image Format Support The image_url parameter supports two formats: \u2022 HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 Encoding : data:image/jpeg;base64,<base64 encoded image format> Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image format>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code that supports image and video processing: import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Answer in Chinese, what number should be in the box in the image?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Please describe the video content\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83d\udc41\ufe0f Qwen2.5-VL | More Useful Vision-Language Model, AI's Eye for Understanding the World","title":"Index qwen2.5 vl en"},{"location":"qwen/index-qwen2.5-vl-en/#product-overview","text":"Qwen2.5-VL is a more useful vision-language model built by Alibaba Cloud's Qwen team based on Qwen2-VL and incorporating valuable feedback from numerous developers. This model has achieved significant improvements in visual understanding, agent capabilities, video analysis, visual localization, and structured output. \ud83d\ude80 Core Upgrades Continuously optimized based on developer feedback, Qwen2.5-VL has achieved major breakthroughs in practicality and functional completeness, becoming a more practical vision-language AI assistant.","title":"\ud83c\udfaf Product Overview"},{"location":"qwen/index-qwen2.5-vl-en/#core-features","text":"\ud83d\udc41\ufe0f Visual Understanding of Objects Not only can skillfully recognize common objects such as flowers, birds, fish, and insects, but can also analyze text, charts, icons, graphics, and layouts in images. \ud83e\udd16 Intelligent Agency Directly acts as a visual agent with reasoning and dynamic tool command functions, applicable for computer and mobile device operation control. \ud83c\udfac Long Video Understanding Can understand videos longer than 1 hour, with new functionality to capture events through precise localization of relevant video segments. \ud83c\udfaf Visual Localization Capability Can accurately locate objects in images by generating bounding boxes or points, and can provide stable JSON output for coordinates and attributes. \ud83d\udcca Structured Output For scanned data such as invoices, tables, and forms, supports structured output of their content, beneficial for applications in finance, business, and other fields.","title":"\u2728 Core Features"},{"location":"qwen/index-qwen2.5-vl-en/#visual-understanding-capabilities","text":"","title":"\ud83d\udd0d Visual Understanding Capabilities"},{"location":"qwen/index-qwen2.5-vl-en/#long-video-understanding","text":"","title":"\ud83c\udfac Long Video Understanding"},{"location":"qwen/index-qwen2.5-vl-en/#application-scenarios","text":"\ud83d\udcbc Business Office \u2022 Document automation processing \u2022 Invoice and receipt recognition \u2022 Table data extraction \u2022 Office workflow automation \ud83c\udf93 Education and Training \u2022 Educational video analysis \u2022 Learning content extraction \u2022 Knowledge point localization \u2022 Interactive teaching assistance \ud83c\udfac Media Entertainment \u2022 Video content analysis \u2022 Highlight segment extraction \u2022 Subtitle generation optimization \u2022 Content recommendation systems \ud83c\udfe5 Healthcare \u2022 Medical image analysis \u2022 Medical record document processing \u2022 Examination report interpretation \u2022 Diagnostic assistance support","title":"\ud83d\ude80 Application Scenarios"},{"location":"qwen/index-qwen2.5-vl-en/#user-guide","text":"\ud83d\udca1 After Deployment After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses (available after enabling public network access), and Api_Key.","title":"\ud83d\udcd6 User Guide"},{"location":"qwen/index-qwen2.5-vl-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"qwen/index-qwen2.5-vl-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name \ud83d\uddbc\ufe0f Image Format Support The image_url parameter supports two formats: \u2022 HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 Encoding : data:image/jpeg;base64,<base64 encoded image format> Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image format>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"qwen/index-qwen2.5-vl-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code that supports image and video processing: import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Answer in Chinese, what number should be in the box in the image?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Please describe the video content\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video()","title":"\ud83d\udc0d Python SDK Call"},{"location":"qwen/index-qwen2.5-vl-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"qwen/index-qwen2.5-vl/","text":"\ud83d\udc41\ufe0f Qwen2.5-VL \u89c6\u89c9\u8bed\u8a00\u6a21\u578b \u66f4\u6709\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b - \u7406\u89e3\u56fe\u50cf\u3001\u89c6\u9891\u4e0e\u6587\u6863\u7684 AI \u4e13\u5bb6 \ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb Qwen2.5-VL \u662f\u963f\u91cc\u4e91 Qwen \u56e2\u961f\u5728 Qwen2-VL \u57fa\u7840\u4e0a\u7ed3\u5408\u4f17\u591a\u5f00\u53d1\u8005\u7684\u5b9d\u8d35\u53cd\u9988\u6784\u5efa\u7684\u66f4\u6709\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u3001\u4ee3\u7406\u80fd\u529b\u3001\u89c6\u9891\u5206\u6790\u3001\u89c6\u89c9\u5b9a\u4f4d\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u7b49\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002 \ud83d\ude80 \u6838\u5fc3\u5347\u7ea7 \u57fa\u4e8e\u5f00\u53d1\u8005\u53cd\u9988\u6301\u7eed\u4f18\u5316\uff0cQwen2.5-VL \u5728\u5b9e\u7528\u6027\u548c\u529f\u80fd\u5b8c\u6574\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u6210\u4e3a\u66f4\u52a0\u5b9e\u7528\u7684\u89c6\u89c9\u8bed\u8a00 AI \u52a9\u624b\u3002 \u2728 \u6838\u5fc3\u529f\u80fd \ud83d\udc41\ufe0f \u89c6\u89c9\u7406\u89e3\u4e8b\u7269 \u4e0d\u4ec5\u80fd\u591f\u719f\u7ec3\u8bc6\u522b\u82b1\u3001\u9e1f\u3001\u9c7c\u3001\u6606\u866b\u7b49\u5e38\u89c1\u7269\u4f53\uff0c\u800c\u4e14\u8fd8\u80fd\u591f\u5206\u6790\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u3001\u56fe\u8868\u3001\u56fe\u6807\u3001\u56fe\u5f62\u548c\u5e03\u5c40\u3002 \ud83e\udd16 \u667a\u80fd\u4ee3\u7406\u6027 \u76f4\u63a5\u626e\u6f14\u89c6\u89c9\u4ee3\u7406\u7684\u89d2\u8272\uff0c\u5177\u6709\u63a8\u7406\u548c\u52a8\u6001\u6307\u6325\u5de5\u5177\u7684\u529f\u80fd\uff0c\u53ef\u7528\u4e8e\u7535\u8111\u548c\u624b\u673a\u64cd\u4f5c\u63a7\u5236\u3002 \ud83c\udfac \u957f\u89c6\u9891\u7406\u89e3 \u53ef\u4ee5\u7406\u89e3\u8d85\u8fc7 1 \u5c0f\u65f6\u7684\u89c6\u9891\uff0c\u5177\u6709\u901a\u8fc7\u7cbe\u786e\u5b9a\u4f4d\u76f8\u5173\u89c6\u9891\u7247\u6bb5\u6765\u6355\u6349\u4e8b\u4ef6\u7684\u65b0\u529f\u80fd\u3002 \ud83c\udfaf \u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b \u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u8fb9\u754c\u6846\u6216\u70b9\u6765\u51c6\u786e\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e3a\u5750\u6807\u548c\u5c5e\u6027\u63d0\u4f9b\u7a33\u5b9a\u7684 JSON \u8f93\u51fa\u3002 \ud83d\udcca \u7ed3\u6784\u5316\u8f93\u51fa \u5bf9\u4e8e\u53d1\u7968\u3001\u8868\u683c\u3001\u8868\u5355\u7b49\u626b\u63cf\u4ef6\u6570\u636e\uff0c\u652f\u6301\u5176\u5185\u5bb9\u7684\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u6709\u5229\u4e8e\u91d1\u878d\u3001\u5546\u4e1a\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002 \ud83d\udd0d \u89c6\u89c9\u7406\u89e3\u80fd\u529b \ud83c\udf1f \u5168\u65b9\u4f4d\u89c6\u89c9\u8bc6\u522b \ud83c\udf3a \u81ea\u7136\u7269\u4f53\u8bc6\u522b \u8bc6\u522b\u8303\u56f4\uff1a \u2022 \u82b1\u5349\u690d\u7269\u5206\u7c7b \u2022 \u9e1f\u7c7b\u52a8\u7269\u8bc6\u522b \u2022 \u9c7c\u7c7b\u6c34\u751f\u751f\u7269 \u2022 \u6606\u866b\u5fae\u751f\u7269\u68c0\u6d4b \u4e13\u4e1a\u7ea7\u7cbe\u5ea6 \ud83d\udcdd \u6587\u672c\u56fe\u8868\u5206\u6790 \u5206\u6790\u80fd\u529b\uff1a \u2022 \u56fe\u50cf\u6587\u672c\u63d0\u53d6 \u2022 \u56fe\u8868\u6570\u636e\u89e3\u8bfb \u2022 \u56fe\u6807\u7b26\u53f7\u8bc6\u522b \u2022 \u5e03\u5c40\u7ed3\u6784\u5206\u6790 \u591a\u6a21\u6001\u7406\u89e3 \ud83c\udfa8 \u56fe\u5f62\u8bbe\u8ba1\u7406\u89e3 \u8bbe\u8ba1\u5206\u6790\uff1a \u2022 \u56fe\u5f62\u5143\u7d20\u8bc6\u522b \u2022 \u8bbe\u8ba1\u98ce\u683c\u5206\u6790 \u2022 \u8272\u5f69\u642d\u914d\u7406\u89e3 \u2022 \u89c6\u89c9\u5c42\u6b21\u89e3\u6784 \u7f8e\u5b66\u611f\u77e5 \ud83c\udfac \u957f\u89c6\u9891\u7406\u89e3 \u23f0 \u8d85\u957f\u65f6\u95f4\u89c6\u9891\u5206\u6790 \u89c6\u9891\u957f\u5ea6 \u5206\u6790\u80fd\u529b \u5e94\u7528\u573a\u666f \u77ed\u89c6\u9891 ( < 10\u5206\u949f) \u7cbe\u7ec6\u52a8\u4f5c\u8bc6\u522b\u3001\u60c5\u611f\u5206\u6790 \u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5206\u6790 \u4e2d\u7b49\u89c6\u9891 (10-30\u5206\u949f) \u4e8b\u4ef6\u5e8f\u5217\u7406\u89e3\u3001\u4e3b\u9898\u63d0\u53d6 \u6559\u80b2\u57f9\u8bad\u89c6\u9891\u5206\u6790 \u957f\u89c6\u9891 (> 1\u5c0f\u65f6) \u5168\u5c40\u7406\u89e3\u3001\u4e8b\u4ef6\u5b9a\u4f4d\u3001\u7247\u6bb5\u68c0\u7d22 \u7535\u5f71\u5206\u6790\u3001\u4f1a\u8bae\u8bb0\u5f55 \ud83c\udfaf \u4e8b\u4ef6\u6355\u6349 \u2022 \u5173\u952e\u4e8b\u4ef6\u81ea\u52a8\u8bc6\u522b \u2022 \u7cbe\u786e\u65f6\u95f4\u6233\u5b9a\u4f4d \u2022 \u76f8\u5173\u7247\u6bb5\u667a\u80fd\u68c0\u7d22 \u2022 \u4e8b\u4ef6\u5173\u8054\u6027\u5206\u6790 \ud83d\udcca \u5185\u5bb9\u7406\u89e3 \u2022 \u89c6\u9891\u4e3b\u9898\u63d0\u53d6 \u2022 \u60c5\u8282\u53d1\u5c55\u8ddf\u8e2a \u2022 \u4eba\u7269\u884c\u4e3a\u5206\u6790 \u2022 \u573a\u666f\u53d8\u5316\u68c0\u6d4b \ud83d\ude80 \u5e94\u7528\u573a\u666f \ud83d\udcbc \u5546\u4e1a\u529e\u516c \u2022 \u6587\u6863\u81ea\u52a8\u5316\u5904\u7406 \u2022 \u53d1\u7968\u7968\u636e\u8bc6\u522b \u2022 \u8868\u683c\u6570\u636e\u63d0\u53d6 \u2022 \u529e\u516c\u6d41\u7a0b\u81ea\u52a8\u5316 \ud83c\udf93 \u6559\u80b2\u57f9\u8bad \u2022 \u6559\u5b66\u89c6\u9891\u5206\u6790 \u2022 \u5b66\u4e60\u5185\u5bb9\u63d0\u53d6 \u2022 \u77e5\u8bc6\u70b9\u5b9a\u4f4d \u2022 \u4e92\u52a8\u6559\u5b66\u8f85\u52a9 \ud83c\udfac \u5a92\u4f53\u5a31\u4e50 \u2022 \u89c6\u9891\u5185\u5bb9\u5206\u6790 \u2022 \u7cbe\u5f69\u7247\u6bb5\u63d0\u53d6 \u2022 \u5b57\u5e55\u751f\u6210\u4f18\u5316 \u2022 \u5185\u5bb9\u63a8\u8350\u7cfb\u7edf \ud83c\udfe5 \u533b\u7597\u5065\u5eb7 \u2022 \u533b\u5b66\u5f71\u50cf\u5206\u6790 \u2022 \u75c5\u5386\u6587\u6863\u5904\u7406 \u2022 \u68c0\u67e5\u62a5\u544a\u89e3\u8bfb \u2022 \u8bca\u65ad\u8f85\u52a9\u652f\u6301 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u90e8\u7f72\u5b8c\u6210\u540e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\uff08\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u540e\u4f1a\u6709\uff09\u548c Api_Key\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \ud83d\uddbc\ufe0f \u56fe\u7247\u683c\u5f0f\u652f\u6301 image_url \u53c2\u6570\u652f\u6301\u4e24\u79cd\u683c\u5f0f\uff1a \u2022 HTTP URL \uff1a\u5982 https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f> Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff1a import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f7f\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u56fe\u4e2d\u65b9\u6846\u5904\u5e94\u8be5\u662f\u6570\u5b57\u591a\u5c11?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"\u8bf7\u63cf\u8ff0\u4e0b\u89c6\u9891\u5185\u5bb9\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\uff0c\u5bf9\u4e8e\u591a\u6a21\u6001\u9700\u8981\u8f93\u5165\u56fe\u7247\u7684\u573a\u666f\uff0c\u53ef\u4ee5\u5728\u8f93\u5165\u6846\u53f3\u4e0b\u89d2\u9009\u62e9\u56fe\u7247\u8fdb\u884c\u4e0a\u4f20\u3002 \ud83d\udc41\ufe0f Qwen2.5-VL | \u66f4\u6709\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7406\u89e3\u4e16\u754c\u7684 AI \u4e4b\u773c","title":"Index qwen2.5 vl"},{"location":"qwen/index-qwen2.5-vl/#_1","text":"Qwen2.5-VL \u662f\u963f\u91cc\u4e91 Qwen \u56e2\u961f\u5728 Qwen2-VL \u57fa\u7840\u4e0a\u7ed3\u5408\u4f17\u591a\u5f00\u53d1\u8005\u7684\u5b9d\u8d35\u53cd\u9988\u6784\u5efa\u7684\u66f4\u6709\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u3001\u4ee3\u7406\u80fd\u529b\u3001\u89c6\u9891\u5206\u6790\u3001\u89c6\u89c9\u5b9a\u4f4d\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u7b49\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002 \ud83d\ude80 \u6838\u5fc3\u5347\u7ea7 \u57fa\u4e8e\u5f00\u53d1\u8005\u53cd\u9988\u6301\u7eed\u4f18\u5316\uff0cQwen2.5-VL \u5728\u5b9e\u7528\u6027\u548c\u529f\u80fd\u5b8c\u6574\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u6210\u4e3a\u66f4\u52a0\u5b9e\u7528\u7684\u89c6\u89c9\u8bed\u8a00 AI \u52a9\u624b\u3002","title":"\ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb"},{"location":"qwen/index-qwen2.5-vl/#_2","text":"\ud83d\udc41\ufe0f \u89c6\u89c9\u7406\u89e3\u4e8b\u7269 \u4e0d\u4ec5\u80fd\u591f\u719f\u7ec3\u8bc6\u522b\u82b1\u3001\u9e1f\u3001\u9c7c\u3001\u6606\u866b\u7b49\u5e38\u89c1\u7269\u4f53\uff0c\u800c\u4e14\u8fd8\u80fd\u591f\u5206\u6790\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u3001\u56fe\u8868\u3001\u56fe\u6807\u3001\u56fe\u5f62\u548c\u5e03\u5c40\u3002 \ud83e\udd16 \u667a\u80fd\u4ee3\u7406\u6027 \u76f4\u63a5\u626e\u6f14\u89c6\u89c9\u4ee3\u7406\u7684\u89d2\u8272\uff0c\u5177\u6709\u63a8\u7406\u548c\u52a8\u6001\u6307\u6325\u5de5\u5177\u7684\u529f\u80fd\uff0c\u53ef\u7528\u4e8e\u7535\u8111\u548c\u624b\u673a\u64cd\u4f5c\u63a7\u5236\u3002 \ud83c\udfac \u957f\u89c6\u9891\u7406\u89e3 \u53ef\u4ee5\u7406\u89e3\u8d85\u8fc7 1 \u5c0f\u65f6\u7684\u89c6\u9891\uff0c\u5177\u6709\u901a\u8fc7\u7cbe\u786e\u5b9a\u4f4d\u76f8\u5173\u89c6\u9891\u7247\u6bb5\u6765\u6355\u6349\u4e8b\u4ef6\u7684\u65b0\u529f\u80fd\u3002 \ud83c\udfaf \u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b \u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u8fb9\u754c\u6846\u6216\u70b9\u6765\u51c6\u786e\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e3a\u5750\u6807\u548c\u5c5e\u6027\u63d0\u4f9b\u7a33\u5b9a\u7684 JSON \u8f93\u51fa\u3002 \ud83d\udcca \u7ed3\u6784\u5316\u8f93\u51fa \u5bf9\u4e8e\u53d1\u7968\u3001\u8868\u683c\u3001\u8868\u5355\u7b49\u626b\u63cf\u4ef6\u6570\u636e\uff0c\u652f\u6301\u5176\u5185\u5bb9\u7684\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u6709\u5229\u4e8e\u91d1\u878d\u3001\u5546\u4e1a\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002","title":"\u2728 \u6838\u5fc3\u529f\u80fd"},{"location":"qwen/index-qwen2.5-vl/#_3","text":"","title":"\ud83d\udd0d \u89c6\u89c9\u7406\u89e3\u80fd\u529b"},{"location":"qwen/index-qwen2.5-vl/#_4","text":"","title":"\ud83c\udfac \u957f\u89c6\u9891\u7406\u89e3"},{"location":"qwen/index-qwen2.5-vl/#_5","text":"\ud83d\udcbc \u5546\u4e1a\u529e\u516c \u2022 \u6587\u6863\u81ea\u52a8\u5316\u5904\u7406 \u2022 \u53d1\u7968\u7968\u636e\u8bc6\u522b \u2022 \u8868\u683c\u6570\u636e\u63d0\u53d6 \u2022 \u529e\u516c\u6d41\u7a0b\u81ea\u52a8\u5316 \ud83c\udf93 \u6559\u80b2\u57f9\u8bad \u2022 \u6559\u5b66\u89c6\u9891\u5206\u6790 \u2022 \u5b66\u4e60\u5185\u5bb9\u63d0\u53d6 \u2022 \u77e5\u8bc6\u70b9\u5b9a\u4f4d \u2022 \u4e92\u52a8\u6559\u5b66\u8f85\u52a9 \ud83c\udfac \u5a92\u4f53\u5a31\u4e50 \u2022 \u89c6\u9891\u5185\u5bb9\u5206\u6790 \u2022 \u7cbe\u5f69\u7247\u6bb5\u63d0\u53d6 \u2022 \u5b57\u5e55\u751f\u6210\u4f18\u5316 \u2022 \u5185\u5bb9\u63a8\u8350\u7cfb\u7edf \ud83c\udfe5 \u533b\u7597\u5065\u5eb7 \u2022 \u533b\u5b66\u5f71\u50cf\u5206\u6790 \u2022 \u75c5\u5386\u6587\u6863\u5904\u7406 \u2022 \u68c0\u67e5\u62a5\u544a\u89e3\u8bfb \u2022 \u8bca\u65ad\u8f85\u52a9\u652f\u6301","title":"\ud83d\ude80 \u5e94\u7528\u573a\u666f"},{"location":"qwen/index-qwen2.5-vl/#_6","text":"\ud83d\udca1 \u90e8\u7f72\u5b8c\u6210\u540e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\uff08\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u540e\u4f1a\u6709\uff09\u548c Api_Key\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"qwen/index-qwen2.5-vl/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"qwen/index-qwen2.5-vl/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \ud83d\uddbc\ufe0f \u56fe\u7247\u683c\u5f0f\u652f\u6301 image_url \u53c2\u6570\u652f\u6301\u4e24\u79cd\u683c\u5f0f\uff1a \u2022 HTTP URL \uff1a\u5982 https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f> Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"qwen/index-qwen2.5-vl/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff1a import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f7f\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u56fe\u4e2d\u65b9\u6846\u5904\u5e94\u8be5\u662f\u6570\u5b57\u591a\u5c11?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"\u8bf7\u63cf\u8ff0\u4e0b\u89c6\u9891\u5185\u5bb9\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"qwen/index-qwen2.5-vl/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"qwen/index-qwen2.5/","text":"\ud83c\udf1f Qwen2.5 \u5f00\u6e90\u6a21\u578b \u901a\u4e49\u5343\u95ee\u65b0\u4e00\u4ee3\u5f00\u6e90\u6a21\u578b - \u6027\u80fd\u8d85\u8d8a Llama-405B \u7684\u7a81\u7834\u4e4b\u4f5c \ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb 9 \u6708 19 \u65e5\u4e91\u6816\u5927\u4f1a\uff0c\u963f\u91cc\u4e91\u53d1\u5e03\u901a\u4e49\u5343\u95ee\u65b0\u4e00\u4ee3\u5f00\u6e90\u6a21\u578b Qwen2.5\uff0c\u65d7\u8230\u6a21\u578b Qwen2.5-72B \u6027\u80fd\u8d85\u8d8a Llama-405B\u3002Qwen2.5 \u5168\u7cfb\u5217\u6a21\u578b\u90fd\u5728 18T tokens \u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u76f8\u6bd4 Qwen2\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347 18% \u4ee5\u4e0a\uff0c\u62e5\u6709\u66f4\u591a\u7684\u77e5\u8bc6\u3001\u66f4\u5f3a\u7684\u7f16\u7a0b\u548c\u6570\u5b66\u80fd\u529b\u3002 \ud83c\udfc6 \u6027\u80fd\u7a81\u7834 Qwen2.5-72B \u6a21\u578b\u5728\u591a\u9879\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5728\u901a\u7528\u77e5\u8bc6\u3001\u4ee3\u7801\u80fd\u529b\u548c\u6570\u5b66\u80fd\u529b\u65b9\u9762\u5747\u8fbe\u5230\u4e1a\u754c\u9886\u5148\u6c34\u5e73\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \ud83d\udcda \u8d85\u957f\u4e0a\u4e0b\u6587\u652f\u6301 \u652f\u6301\u9ad8\u8fbe 128K \u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u53ef\u751f\u6210\u6700\u591a 8K \u5185\u5bb9\uff0c\u6ee1\u8db3\u957f\u6587\u6863\u5904\u7406\u548c\u590d\u6742\u5bf9\u8bdd\u9700\u6c42\u3002 \ud83c\udf0d \u5f3a\u5927\u591a\u8bed\u8a00\u80fd\u529b \u652f\u6301\u4e2d\u6587\u3001\u82f1\u6587\u3001\u6cd5\u6587\u3001\u897f\u73ed\u7259\u6587\u3001\u4fc4\u6587\u3001\u65e5\u6587\u3001\u8d8a\u5357\u6587\u3001\u963f\u62c9\u4f2f\u6587\u7b49 29 \u79cd\u4ee5\u4e0a\u8bed\u8a00\u3002 \ud83c\udfad \u7075\u6d3b\u89d2\u8272\u626e\u6f14 \u80fd\u591f\u4e1d\u6ed1\u54cd\u5e94\u591a\u6837\u5316\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u5b9e\u73b0\u89d2\u8272\u626e\u6f14\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u4efb\u52a1\u3002 \ud83d\udcca \u7ed3\u6784\u5316\u6570\u636e\u5904\u7406 \u5728\u6307\u4ee4\u8ddf\u968f\u3001\u7406\u89e3\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u8868\u683c\uff09\u3001\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5c24\u5176\u662f JSON\uff09\u7b49\u65b9\u9762\u8fdb\u6b65\u660e\u663e\u3002 \ud83d\udd27 \u6a21\u578b\u89c4\u683c \ud83d\udce6 \u5b8c\u6574\u6a21\u578b\u7cfb\u5217 0.5B \u8f7b\u91cf\u7ea7\u90e8\u7f72 1.5B \u79fb\u52a8\u7aef\u4f18\u5316 3B \u8fb9\u7f18\u8ba1\u7b97 7B \u901a\u7528\u5e94\u7528 14B \u4f01\u4e1a\u7ea7\u5e94\u7528 32B \u9ad8\u6027\u80fd\u670d\u52a1 72B \u65d7\u8230\u6027\u80fd \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83c\udf1f Qwen2.5 \u5f00\u6e90\u6a21\u578b | \u6027\u80fd\u8d85\u8d8a Llama-405B\uff0c\u5f00\u6e90 AI \u7684\u65b0\u6807\u6746","title":"Index qwen2.5"},{"location":"qwen/index-qwen2.5/#_1","text":"9 \u6708 19 \u65e5\u4e91\u6816\u5927\u4f1a\uff0c\u963f\u91cc\u4e91\u53d1\u5e03\u901a\u4e49\u5343\u95ee\u65b0\u4e00\u4ee3\u5f00\u6e90\u6a21\u578b Qwen2.5\uff0c\u65d7\u8230\u6a21\u578b Qwen2.5-72B \u6027\u80fd\u8d85\u8d8a Llama-405B\u3002Qwen2.5 \u5168\u7cfb\u5217\u6a21\u578b\u90fd\u5728 18T tokens \u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u76f8\u6bd4 Qwen2\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347 18% \u4ee5\u4e0a\uff0c\u62e5\u6709\u66f4\u591a\u7684\u77e5\u8bc6\u3001\u66f4\u5f3a\u7684\u7f16\u7a0b\u548c\u6570\u5b66\u80fd\u529b\u3002 \ud83c\udfc6 \u6027\u80fd\u7a81\u7834 Qwen2.5-72B \u6a21\u578b\u5728\u591a\u9879\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5728\u901a\u7528\u77e5\u8bc6\u3001\u4ee3\u7801\u80fd\u529b\u548c\u6570\u5b66\u80fd\u529b\u65b9\u9762\u5747\u8fbe\u5230\u4e1a\u754c\u9886\u5148\u6c34\u5e73\u3002","title":"\ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb"},{"location":"qwen/index-qwen2.5/#_2","text":"\ud83d\udcda \u8d85\u957f\u4e0a\u4e0b\u6587\u652f\u6301 \u652f\u6301\u9ad8\u8fbe 128K \u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u53ef\u751f\u6210\u6700\u591a 8K \u5185\u5bb9\uff0c\u6ee1\u8db3\u957f\u6587\u6863\u5904\u7406\u548c\u590d\u6742\u5bf9\u8bdd\u9700\u6c42\u3002 \ud83c\udf0d \u5f3a\u5927\u591a\u8bed\u8a00\u80fd\u529b \u652f\u6301\u4e2d\u6587\u3001\u82f1\u6587\u3001\u6cd5\u6587\u3001\u897f\u73ed\u7259\u6587\u3001\u4fc4\u6587\u3001\u65e5\u6587\u3001\u8d8a\u5357\u6587\u3001\u963f\u62c9\u4f2f\u6587\u7b49 29 \u79cd\u4ee5\u4e0a\u8bed\u8a00\u3002 \ud83c\udfad \u7075\u6d3b\u89d2\u8272\u626e\u6f14 \u80fd\u591f\u4e1d\u6ed1\u54cd\u5e94\u591a\u6837\u5316\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u5b9e\u73b0\u89d2\u8272\u626e\u6f14\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u4efb\u52a1\u3002 \ud83d\udcca \u7ed3\u6784\u5316\u6570\u636e\u5904\u7406 \u5728\u6307\u4ee4\u8ddf\u968f\u3001\u7406\u89e3\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u8868\u683c\uff09\u3001\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5c24\u5176\u662f JSON\uff09\u7b49\u65b9\u9762\u8fdb\u6b65\u660e\u663e\u3002","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"qwen/index-qwen2.5/#_3","text":"","title":"\ud83d\udd27 \u6a21\u578b\u89c4\u683c"},{"location":"qwen/index-qwen2.5/#_4","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"qwen/index-qwen2.5/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"qwen/index-qwen2.5/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"qwen/index-qwen2.5/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"qwen/index-qwen2.5/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"qwen/index-qwen3-en/","text":"\ud83d\ude80 Qwen3 Large Language Model Next-Generation Intelligent Dialogue Model - Seamless Switching Between Thinking and Non-Thinking Modes \ud83c\udfaf Product Overview Qwen3 is the latest generation large language model in the Qwen series, providing a series of Dense and Mixture of Experts (MOE) models. Based on extensive training, Qwen3 has achieved breakthrough progress in reasoning, instruction following, agent capabilities, and multilingual support. \ud83d\udca1 Core Innovation Qwen3 is the first to achieve intelligent switching between thinking mode and non-thinking mode, ensuring reasoning accuracy while considering response efficiency, providing optimal solutions for different application scenarios. \u2728 Core Features \ud83e\udde0 Dual-Mode Intelligent Switching Uniquely supports seamless switching between thinking mode (for complex logical reasoning, mathematics, and coding) and non-thinking mode (for efficient general dialogue), ensuring optimal performance in various scenarios. \u26a1 Superior Reasoning Capabilities Significantly enhanced reasoning capabilities that surpass previous QwQ (in thinking mode) and Qwen2.5 instruction models (in non-thinking mode) in mathematics, code generation, and common sense logical reasoning. \ud83c\udfad Excellent Human Preference Alignment Outstanding performance in creative writing, role-playing, multi-turn dialogue, and instruction following, providing more natural, engaging, and immersive conversational experiences. \ud83e\udd16 Leading Agent Capabilities Can precisely integrate external tools in both thinking and non-thinking modes, leading among open-source models in complex agent-based tasks. \ud83c\udf0d Powerful Multilingual Support Supports over 100 languages and dialects with strong multilingual understanding, reasoning, instruction following, and generation capabilities. \ud83d\udd04 Dual-Mode Working Mechanism \ud83e\udd14 Thinking Mode Applicable Scenarios: \u2022 Complex mathematical reasoning \u2022 Code logic analysis \u2022 Multi-step problem solving \u2022 Scientific computation verification Characteristics: \u2022 Deep thinking and reasoning \u2022 Transparent and traceable process \u2022 Higher accuracy \u2022 Longer response time \ud83d\udcac Non-Thinking Mode Applicable Scenarios: \u2022 Daily conversation \u2022 Quick information queries \u2022 Creative writing assistance \u2022 Role-playing interaction Characteristics: \u2022 Quick response \u2022 Natural and fluent \u2022 Efficient interaction \u2022 User-friendly experience \ud83c\udfaf Intelligent Switching Mechanism Qwen3 can automatically select the most suitable working mode based on problem complexity and user needs, and also supports manual mode specification by users, ensuring optimal performance in various scenarios. \ud83d\udcbb Deployment Configuration \ud83d\udd27 Hardware Configuration Requirements Model Specification Minimum VRAM Requirement Recommended Scenarios Performance Characteristics 0.6B / 1.7B / 4B / 8B 24G VRAM Lightweight applications, mobile deployment Fast response, resource-friendly 14B 48G VRAM Medium complexity tasks, enterprise applications Balanced performance and efficiency 30B / 32B 96G VRAM High-performance applications, professional services Powerful reasoning capabilities 235B 8\u00d7 96G VRAM Enterprise deployment, research applications Top-tier performance \ud83d\udca1 Selection Recommendations \u2022 Individual Developers : Recommend 0.6B-8B models, low cost and simple deployment \u2022 Small and Medium Enterprises : Recommend 14B model, optimal cost-performance ratio \u2022 Large Enterprises : Recommend 30B-32B models, powerful performance \u2022 Research Institutions : Recommend 235B model, top-tier capability support \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83d\ude80 Qwen3 Large Language Model | A New Era of Intelligent Dialogue, Perfect Integration of Thinking and Efficiency","title":"Index qwen3 en"},{"location":"qwen/index-qwen3-en/#product-overview","text":"Qwen3 is the latest generation large language model in the Qwen series, providing a series of Dense and Mixture of Experts (MOE) models. Based on extensive training, Qwen3 has achieved breakthrough progress in reasoning, instruction following, agent capabilities, and multilingual support. \ud83d\udca1 Core Innovation Qwen3 is the first to achieve intelligent switching between thinking mode and non-thinking mode, ensuring reasoning accuracy while considering response efficiency, providing optimal solutions for different application scenarios.","title":"\ud83c\udfaf Product Overview"},{"location":"qwen/index-qwen3-en/#core-features","text":"\ud83e\udde0 Dual-Mode Intelligent Switching Uniquely supports seamless switching between thinking mode (for complex logical reasoning, mathematics, and coding) and non-thinking mode (for efficient general dialogue), ensuring optimal performance in various scenarios. \u26a1 Superior Reasoning Capabilities Significantly enhanced reasoning capabilities that surpass previous QwQ (in thinking mode) and Qwen2.5 instruction models (in non-thinking mode) in mathematics, code generation, and common sense logical reasoning. \ud83c\udfad Excellent Human Preference Alignment Outstanding performance in creative writing, role-playing, multi-turn dialogue, and instruction following, providing more natural, engaging, and immersive conversational experiences. \ud83e\udd16 Leading Agent Capabilities Can precisely integrate external tools in both thinking and non-thinking modes, leading among open-source models in complex agent-based tasks. \ud83c\udf0d Powerful Multilingual Support Supports over 100 languages and dialects with strong multilingual understanding, reasoning, instruction following, and generation capabilities.","title":"\u2728 Core Features"},{"location":"qwen/index-qwen3-en/#dual-mode-working-mechanism","text":"","title":"\ud83d\udd04 Dual-Mode Working Mechanism"},{"location":"qwen/index-qwen3-en/#deployment-configuration","text":"","title":"\ud83d\udcbb Deployment Configuration"},{"location":"qwen/index-qwen3-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"qwen/index-qwen3-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"qwen/index-qwen3-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"qwen/index-qwen3-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"qwen/index-qwen3-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"qwen/index-qwen3/","text":"\ud83d\ude80 Qwen3 \u5927\u8bed\u8a00\u6a21\u578b \u65b0\u4e00\u4ee3\u667a\u80fd\u5bf9\u8bdd\u6a21\u578b - \u601d\u8003\u4e0e\u975e\u601d\u8003\u6a21\u5f0f\u65e0\u7f1d\u5207\u6362 \ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb Qwen3 \u662f Qwen \u7cfb\u5217\u6700\u65b0\u4e00\u4ee3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u5bc6\u96c6\uff08Dense\uff09\u548c\u6df7\u5408\u4e13\u5bb6\uff08MOE\uff09\u6a21\u578b\u3002\u57fa\u4e8e\u5e7f\u6cdb\u7684\u8bad\u7ec3\uff0cQwen3 \u5728\u63a8\u7406\u3001\u6307\u4ee4\u8ddf\u968f\u3001\u4ee3\u7406\u80fd\u529b\u548c\u591a\u8bed\u8a00\u652f\u6301\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u7684\u8fdb\u5c55\u3002 \ud83d\udca1 \u6838\u5fc3\u521b\u65b0 Qwen3 \u9996\u6b21\u5b9e\u73b0\u4e86\u601d\u8003\u6a21\u5f0f\u4e0e\u975e\u601d\u8003\u6a21\u5f0f\u7684\u667a\u80fd\u5207\u6362\uff0c\u5728\u4fdd\u8bc1\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u517c\u987e\u54cd\u5e94\u6548\u7387\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \ud83e\udde0 \u53cc\u6a21\u5f0f\u667a\u80fd\u5207\u6362 \u72ec\u7279\u652f\u6301\u5728\u601d\u8003\u6a21\u5f0f\uff08\u7528\u4e8e\u590d\u6742\u903b\u8f91\u63a8\u7406\u3001\u6570\u5b66\u548c\u7f16\u7801\uff09\u548c\u975e\u601d\u8003\u6a21\u5f0f\uff08\u7528\u4e8e\u9ad8\u6548\u901a\u7528\u5bf9\u8bdd\uff09\u4e4b\u95f4\u65e0\u7f1d\u5207\u6362\uff0c\u786e\u4fdd\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u6700\u4f73\u6027\u80fd\u3002 \u26a1 \u8d85\u8d8a\u524d\u4ee3\u63a8\u7406\u80fd\u529b \u663e\u8457\u589e\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\u548c\u5e38\u8bc6\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684 QwQ\uff08\u5728\u601d\u8003\u6a21\u5f0f\u4e0b\uff09\u548c Qwen2.5 \u6307\u4ee4\u6a21\u578b\uff08\u5728\u975e\u601d\u8003\u6a21\u5f0f\u4e0b\uff09\u3002 \ud83c\udfad \u5353\u8d8a\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50 \u5728\u521b\u610f\u5199\u4f5c\u3001\u89d2\u8272\u626e\u6f14\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u66f4\u5438\u5f15\u4eba\u548c\u66f4\u5177\u6c89\u6d78\u611f\u7684\u5bf9\u8bdd\u4f53\u9a8c\u3002 \ud83e\udd16 \u9886\u5148 Agent \u80fd\u529b \u53ef\u4ee5\u5728\u601d\u8003\u548c\u975e\u601d\u8003\u6a21\u5f0f\u4e0b\u7cbe\u786e\u96c6\u6210\u5916\u90e8\u5de5\u5177\uff0c\u5728\u590d\u6742\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u4efb\u52a1\u4e2d\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u9886\u5148\u3002 \ud83c\udf0d \u5f3a\u5927\u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301 100 \u591a\u79cd\u8bed\u8a00\u548c\u65b9\u8a00\uff0c\u5177\u6709\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u7406\u89e3\u3001\u63a8\u7406\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u751f\u6210\u80fd\u529b\u3002 \ud83d\udd04 \u53cc\u6a21\u5f0f\u5de5\u4f5c\u673a\u5236 \ud83e\udd14 \u601d\u8003\u6a21\u5f0f \u9002\u7528\u573a\u666f\uff1a \u2022 \u590d\u6742\u6570\u5b66\u63a8\u7406 \u2022 \u4ee3\u7801\u903b\u8f91\u5206\u6790 \u2022 \u591a\u6b65\u9aa4\u95ee\u9898\u89e3\u51b3 \u2022 \u79d1\u5b66\u8ba1\u7b97\u9a8c\u8bc1 \u7279\u70b9\uff1a \u2022 \u6df1\u5ea6\u601d\u8003\u63a8\u7406 \u2022 \u8fc7\u7a0b\u900f\u660e\u53ef\u8ffd\u6eaf \u2022 \u51c6\u786e\u7387\u66f4\u9ad8 \u2022 \u54cd\u5e94\u65f6\u95f4\u8f83\u957f \ud83d\udcac \u975e\u601d\u8003\u6a21\u5f0f \u9002\u7528\u573a\u666f\uff1a \u2022 \u65e5\u5e38\u5bf9\u8bdd\u4ea4\u6d41 \u2022 \u5feb\u901f\u4fe1\u606f\u67e5\u8be2 \u2022 \u521b\u610f\u5199\u4f5c\u8f85\u52a9 \u2022 \u89d2\u8272\u626e\u6f14\u4e92\u52a8 \u7279\u70b9\uff1a \u2022 \u5feb\u901f\u54cd\u5e94 \u2022 \u81ea\u7136\u6d41\u7545 \u2022 \u9ad8\u6548\u4e92\u52a8 \u2022 \u4f53\u9a8c\u53cb\u597d \ud83c\udfaf \u667a\u80fd\u5207\u6362\u673a\u5236 Qwen3 \u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u548c\u7528\u6237\u9700\u6c42\u81ea\u52a8\u9009\u62e9\u6700\u9002\u5408\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u4e5f\u652f\u6301\u7528\u6237\u624b\u52a8\u6307\u5b9a\u6a21\u5f0f\uff0c\u786e\u4fdd\u5728\u5404\u79cd\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u4f9b\u6700\u4f18\u7684\u6027\u80fd\u8868\u73b0\u3002 \ud83d\udcbb \u90e8\u7f72\u914d\u7f6e \ud83d\udd27 \u786c\u4ef6\u914d\u7f6e\u8981\u6c42 \u6a21\u578b\u89c4\u683c \u6700\u4f4e\u663e\u5b58\u8981\u6c42 \u63a8\u8350\u573a\u666f \u6027\u80fd\u7279\u70b9 0.6B / 1.7B / 4B / 8B 24G \u663e\u5b58 \u8f7b\u91cf\u7ea7\u5e94\u7528\u3001\u79fb\u52a8\u7aef\u90e8\u7f72 \u5feb\u901f\u54cd\u5e94\u3001\u8d44\u6e90\u53cb\u597d 14B 48G \u663e\u5b58 \u4e2d\u7b49\u590d\u6742\u5ea6\u4efb\u52a1\u3001\u4f01\u4e1a\u5e94\u7528 \u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387 30B / 32B 96G \u663e\u5b58 \u9ad8\u6027\u80fd\u5e94\u7528\u3001\u4e13\u4e1a\u670d\u52a1 \u5f3a\u5927\u63a8\u7406\u80fd\u529b 235B 8\u00d7 96G \u663e\u5b58 \u4f01\u4e1a\u7ea7\u90e8\u7f72\u3001\u79d1\u7814\u5e94\u7528 \u9876\u7ea7\u6027\u80fd\u8868\u73b0 \ud83d\udca1 \u9009\u62e9\u5efa\u8bae \u2022 \u4e2a\u4eba\u5f00\u53d1\u8005 \uff1a\u63a8\u8350 0.6B-8B \u6a21\u578b\uff0c\u6210\u672c\u4f4e\u3001\u90e8\u7f72\u7b80\u5355 \u2022 \u4e2d\u5c0f\u4f01\u4e1a \uff1a\u63a8\u8350 14B \u6a21\u578b\uff0c\u6027\u4ef7\u6bd4\u6700\u4f18 \u2022 \u5927\u578b\u4f01\u4e1a \uff1a\u63a8\u8350 30B-32B \u6a21\u578b\uff0c\u6027\u80fd\u5f3a\u52b2 \u2022 \u79d1\u7814\u673a\u6784 \uff1a\u63a8\u8350 235B \u6a21\u578b\uff0c\u9876\u7ea7\u80fd\u529b\u652f\u6301 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83d\ude80 Qwen3 \u5927\u8bed\u8a00\u6a21\u578b | \u667a\u80fd\u5bf9\u8bdd\u7684\u65b0\u7eaa\u5143\uff0c\u601d\u8003\u4e0e\u6548\u7387\u7684\u5b8c\u7f8e\u878d\u5408","title":"Index qwen3"},{"location":"qwen/index-qwen3/#_1","text":"Qwen3 \u662f Qwen \u7cfb\u5217\u6700\u65b0\u4e00\u4ee3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u5bc6\u96c6\uff08Dense\uff09\u548c\u6df7\u5408\u4e13\u5bb6\uff08MOE\uff09\u6a21\u578b\u3002\u57fa\u4e8e\u5e7f\u6cdb\u7684\u8bad\u7ec3\uff0cQwen3 \u5728\u63a8\u7406\u3001\u6307\u4ee4\u8ddf\u968f\u3001\u4ee3\u7406\u80fd\u529b\u548c\u591a\u8bed\u8a00\u652f\u6301\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u7684\u8fdb\u5c55\u3002 \ud83d\udca1 \u6838\u5fc3\u521b\u65b0 Qwen3 \u9996\u6b21\u5b9e\u73b0\u4e86\u601d\u8003\u6a21\u5f0f\u4e0e\u975e\u601d\u8003\u6a21\u5f0f\u7684\u667a\u80fd\u5207\u6362\uff0c\u5728\u4fdd\u8bc1\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u517c\u987e\u54cd\u5e94\u6548\u7387\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u3002","title":"\ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb"},{"location":"qwen/index-qwen3/#_2","text":"\ud83e\udde0 \u53cc\u6a21\u5f0f\u667a\u80fd\u5207\u6362 \u72ec\u7279\u652f\u6301\u5728\u601d\u8003\u6a21\u5f0f\uff08\u7528\u4e8e\u590d\u6742\u903b\u8f91\u63a8\u7406\u3001\u6570\u5b66\u548c\u7f16\u7801\uff09\u548c\u975e\u601d\u8003\u6a21\u5f0f\uff08\u7528\u4e8e\u9ad8\u6548\u901a\u7528\u5bf9\u8bdd\uff09\u4e4b\u95f4\u65e0\u7f1d\u5207\u6362\uff0c\u786e\u4fdd\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u6700\u4f73\u6027\u80fd\u3002 \u26a1 \u8d85\u8d8a\u524d\u4ee3\u63a8\u7406\u80fd\u529b \u663e\u8457\u589e\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\u548c\u5e38\u8bc6\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684 QwQ\uff08\u5728\u601d\u8003\u6a21\u5f0f\u4e0b\uff09\u548c Qwen2.5 \u6307\u4ee4\u6a21\u578b\uff08\u5728\u975e\u601d\u8003\u6a21\u5f0f\u4e0b\uff09\u3002 \ud83c\udfad \u5353\u8d8a\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50 \u5728\u521b\u610f\u5199\u4f5c\u3001\u89d2\u8272\u626e\u6f14\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u66f4\u5438\u5f15\u4eba\u548c\u66f4\u5177\u6c89\u6d78\u611f\u7684\u5bf9\u8bdd\u4f53\u9a8c\u3002 \ud83e\udd16 \u9886\u5148 Agent \u80fd\u529b \u53ef\u4ee5\u5728\u601d\u8003\u548c\u975e\u601d\u8003\u6a21\u5f0f\u4e0b\u7cbe\u786e\u96c6\u6210\u5916\u90e8\u5de5\u5177\uff0c\u5728\u590d\u6742\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u4efb\u52a1\u4e2d\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u9886\u5148\u3002 \ud83c\udf0d \u5f3a\u5927\u591a\u8bed\u8a00\u652f\u6301 \u652f\u6301 100 \u591a\u79cd\u8bed\u8a00\u548c\u65b9\u8a00\uff0c\u5177\u6709\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u7406\u89e3\u3001\u63a8\u7406\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u751f\u6210\u80fd\u529b\u3002","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"qwen/index-qwen3/#_3","text":"","title":"\ud83d\udd04 \u53cc\u6a21\u5f0f\u5de5\u4f5c\u673a\u5236"},{"location":"qwen/index-qwen3/#_4","text":"","title":"\ud83d\udcbb \u90e8\u7f72\u914d\u7f6e"},{"location":"qwen/index-qwen3/#_5","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"qwen/index-qwen3/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"qwen/index-qwen3/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"qwen/index-qwen3/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"qwen/index-qwen3/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"qwen/index-qwq-en/","text":"\ud83e\udde0 QwQ Reasoning Model Qwen Series Professional Reasoning Model - Breakthrough in Thinking and Reasoning Capabilities \ud83c\udfaf Product Overview QwQ is a reasoning model in the Qwen series. Compared to traditional instruction-tuned models, QwQ with thinking and reasoning capabilities can significantly improve performance in downstream tasks, especially when solving difficult problems. \ud83d\udca1 Core Advantage Through the \"Chain-of-Thought\" mechanism, QwQ can perform step-by-step reasoning like humans, demonstrating excellence in complex problem solving. \u2728 Core Features \ud83d\udcca Mathematical Reasoning Capability Supports analysis and solving of mathematical problems including algebra, geometry, calculus, and can derive complex formulas step by step, providing strong support for mathematics education and research. \ud83d\udcbb Code Generation and Understanding Can generate high-quality Python/Java/C++ code based on natural language descriptions, and supports code debugging and optimization, improving development efficiency. \ud83d\udd17 Multi-step Reasoning Through the \"Chain-of-Thought\" mechanism, solves complex problems step by step, such as step-by-step derivation of mathematical proofs, ensuring the reasoning process is clear and traceable. \ud83c\udfaf Reinforcement Learning Fine-tuning Optimizes reasoning accuracy through feedback mechanisms, performing exceptionally well in tasks requiring precise calculations, continuously improving model performance. \ud83d\ude80 Application Scenarios \ud83c\udf93 Education Field \ud83d\udcd0 Mathematics Teaching \u2022 Step-by-step analysis of complex math problems \u2022 Providing detailed derivation processes \u2022 Supporting multiple solution method demonstrations \ud83d\udca1 Programming Education \u2022 Code logic analysis and explanation \u2022 Detailed algorithm concept elaboration \u2022 Programming error diagnosis and correction \ud83c\udfe2 Professional Applications \ud83d\udd2c Research Analysis \u2022 Complex data analysis reasoning \u2022 Experimental result logical verification \u2022 Theoretical model construction support \u2696\ufe0f Logical Reasoning \u2022 Legal clause logical analysis \u2022 Business decision reasoning support \u2022 Complex problem decomposition and solving \ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services. \ud83e\udde0 QwQ Reasoning Model | Making AI Think and Reason Like Humans","title":"Index qwq en"},{"location":"qwen/index-qwq-en/#product-overview","text":"QwQ is a reasoning model in the Qwen series. Compared to traditional instruction-tuned models, QwQ with thinking and reasoning capabilities can significantly improve performance in downstream tasks, especially when solving difficult problems. \ud83d\udca1 Core Advantage Through the \"Chain-of-Thought\" mechanism, QwQ can perform step-by-step reasoning like humans, demonstrating excellence in complex problem solving.","title":"\ud83c\udfaf Product Overview"},{"location":"qwen/index-qwq-en/#core-features","text":"\ud83d\udcca Mathematical Reasoning Capability Supports analysis and solving of mathematical problems including algebra, geometry, calculus, and can derive complex formulas step by step, providing strong support for mathematics education and research. \ud83d\udcbb Code Generation and Understanding Can generate high-quality Python/Java/C++ code based on natural language descriptions, and supports code debugging and optimization, improving development efficiency. \ud83d\udd17 Multi-step Reasoning Through the \"Chain-of-Thought\" mechanism, solves complex problems step by step, such as step-by-step derivation of mathematical proofs, ensuring the reasoning process is clear and traceable. \ud83c\udfaf Reinforcement Learning Fine-tuning Optimizes reasoning accuracy through feedback mechanisms, performing exceptionally well in tasks requiring precise calculations, continuously improving model performance.","title":"\u2728 Core Features"},{"location":"qwen/index-qwq-en/#application-scenarios","text":"","title":"\ud83d\ude80 Application Scenarios"},{"location":"qwen/index-qwq-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"qwen/index-qwq-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"qwen/index-qwq-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"qwen/index-qwq-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"qwen/index-qwq-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"qwen/index-qwq/","text":"\ud83e\udde0 QwQ \u63a8\u7406\u6a21\u578b Qwen \u7cfb\u5217\u4e13\u4e1a\u63a8\u7406\u6a21\u578b - \u601d\u8003\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u7a81\u7834 \ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb QwQ \u662f Qwen \u7cfb\u5217\u7684\u63a8\u7406\u6a21\u578b\u3002\u4e0e\u4f20\u7edf\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u76f8\u6bd4\uff0c\u5177\u5907\u601d\u8003\u548c\u63a8\u7406\u80fd\u529b\u7684 QwQ \u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u96be\u9898\u65f6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002 \ud83d\udca1 \u6838\u5fc3\u4f18\u52bf \u901a\u8fc7\"\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\"\u673a\u5236\uff0cQwQ \u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u5353\u8d8a\u3002 \u2728 \u6838\u5fc3\u7279\u6027 \ud83d\udcca \u6570\u5b66\u63a8\u7406\u80fd\u529b \u652f\u6301\u4ee3\u6570\u3001\u51e0\u4f55\u3001\u5fae\u79ef\u5206\u7b49\u6570\u5b66\u95ee\u9898\u7684\u89e3\u6790\u4e0e\u89e3\u7b54\uff0c\u53ef\u9010\u6b65\u63a8\u5bfc\u590d\u6742\u516c\u5f0f\uff0c\u4e3a\u6570\u5b66\u6559\u80b2\u548c\u79d1\u7814\u63d0\u4f9b\u5f3a\u5927\u652f\u6301\u3002 \ud83d\udcbb \u4ee3\u7801\u751f\u6210\u4e0e\u7406\u89e3 \u80fd\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u9ad8\u8d28\u91cf\u7684 Python/Java/C++ \u7b49\u4ee3\u7801\uff0c\u5e76\u652f\u6301\u4ee3\u7801\u8c03\u8bd5\u4e0e\u4f18\u5316\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002 \ud83d\udd17 \u591a\u6b65\u9aa4\u63a8\u7406 \u901a\u8fc7\"\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\"\u673a\u5236\uff0c\u5206\u6b65\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f8b\u5982\u5206\u6b65\u63a8\u5bfc\u6570\u5b66\u8bc1\u660e\uff0c\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u6e05\u6670\u53ef\u8ffd\u6eaf\u3002 \ud83c\udfaf \u5f3a\u5316\u5b66\u4e60\u5fae\u8c03 \u901a\u8fc7\u53cd\u9988\u673a\u5236\u4f18\u5316\u63a8\u7406\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u8ba1\u7b97\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002 \ud83d\ude80 \u5e94\u7528\u573a\u666f \ud83c\udf93 \u6559\u80b2\u9886\u57df \ud83d\udcd0 \u6570\u5b66\u6559\u5b66 \u2022 \u9010\u6b65\u89e3\u6790\u590d\u6742\u6570\u5b66\u9898\u76ee \u2022 \u63d0\u4f9b\u8be6\u7ec6\u7684\u63a8\u5bfc\u8fc7\u7a0b \u2022 \u652f\u6301\u591a\u79cd\u89e3\u9898\u65b9\u6cd5\u5c55\u793a \ud83d\udca1 \u7f16\u7a0b\u6559\u80b2 \u2022 \u4ee3\u7801\u903b\u8f91\u5206\u6790\u4e0e\u8bb2\u89e3 \u2022 \u7b97\u6cd5\u601d\u8def\u8be6\u7ec6\u9610\u8ff0 \u2022 \u7f16\u7a0b\u9519\u8bef\u8bca\u65ad\u4e0e\u4fee\u6b63 \ud83c\udfe2 \u4e13\u4e1a\u5e94\u7528 \ud83d\udd2c \u79d1\u7814\u5206\u6790 \u2022 \u590d\u6742\u6570\u636e\u5206\u6790\u63a8\u7406 \u2022 \u5b9e\u9a8c\u7ed3\u679c\u903b\u8f91\u9a8c\u8bc1 \u2022 \u7406\u8bba\u6a21\u578b\u6784\u5efa\u652f\u6301 \u2696\ufe0f \u903b\u8f91\u63a8\u7406 \u2022 \u6cd5\u5f8b\u6761\u6587\u903b\u8f91\u5206\u6790 \u2022 \u5546\u4e1a\u51b3\u7b56\u63a8\u7406\u652f\u6301 \u2022 \u590d\u6742\u95ee\u9898\u5206\u89e3\u6c42\u89e3 \ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002 \ud83e\udde0 QwQ \u63a8\u7406\u6a21\u578b | \u8ba9 AI \u50cf\u4eba\u7c7b\u4e00\u6837\u601d\u8003\u63a8\u7406","title":"Index qwq"},{"location":"qwen/index-qwq/#_1","text":"QwQ \u662f Qwen \u7cfb\u5217\u7684\u63a8\u7406\u6a21\u578b\u3002\u4e0e\u4f20\u7edf\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u76f8\u6bd4\uff0c\u5177\u5907\u601d\u8003\u548c\u63a8\u7406\u80fd\u529b\u7684 QwQ \u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u96be\u9898\u65f6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002 \ud83d\udca1 \u6838\u5fc3\u4f18\u52bf \u901a\u8fc7\"\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\"\u673a\u5236\uff0cQwQ \u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u5353\u8d8a\u3002","title":"\ud83c\udfaf \u4ea7\u54c1\u7b80\u4ecb"},{"location":"qwen/index-qwq/#_2","text":"\ud83d\udcca \u6570\u5b66\u63a8\u7406\u80fd\u529b \u652f\u6301\u4ee3\u6570\u3001\u51e0\u4f55\u3001\u5fae\u79ef\u5206\u7b49\u6570\u5b66\u95ee\u9898\u7684\u89e3\u6790\u4e0e\u89e3\u7b54\uff0c\u53ef\u9010\u6b65\u63a8\u5bfc\u590d\u6742\u516c\u5f0f\uff0c\u4e3a\u6570\u5b66\u6559\u80b2\u548c\u79d1\u7814\u63d0\u4f9b\u5f3a\u5927\u652f\u6301\u3002 \ud83d\udcbb \u4ee3\u7801\u751f\u6210\u4e0e\u7406\u89e3 \u80fd\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u9ad8\u8d28\u91cf\u7684 Python/Java/C++ \u7b49\u4ee3\u7801\uff0c\u5e76\u652f\u6301\u4ee3\u7801\u8c03\u8bd5\u4e0e\u4f18\u5316\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002 \ud83d\udd17 \u591a\u6b65\u9aa4\u63a8\u7406 \u901a\u8fc7\"\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\"\u673a\u5236\uff0c\u5206\u6b65\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f8b\u5982\u5206\u6b65\u63a8\u5bfc\u6570\u5b66\u8bc1\u660e\uff0c\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u6e05\u6670\u53ef\u8ffd\u6eaf\u3002 \ud83c\udfaf \u5f3a\u5316\u5b66\u4e60\u5fae\u8c03 \u901a\u8fc7\u53cd\u9988\u673a\u5236\u4f18\u5316\u63a8\u7406\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u8ba1\u7b97\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002","title":"\u2728 \u6838\u5fc3\u7279\u6027"},{"location":"qwen/index-qwq/#_3","text":"","title":"\ud83d\ude80 \u5e94\u7528\u573a\u666f"},{"location":"qwen/index-qwq/#_4","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"qwen/index-qwq/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"qwen/index-qwq/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"qwen/index-qwq/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"qwen/index-qwq/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"qwen-image/doc/","text":"\ud83c\udfa8 Qwen-Image Qwen \u7cfb\u5217\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b - \u4e13\u7cbe\u590d\u6742\u6587\u672c\u6e32\u67d3\u4e0e\u7cbe\u786e\u56fe\u50cf\u7f16\u8f91 \ud83c\udfaf \u6587\u672c\u6e32\u67d3 \u2728 \u56fe\u50cf\u7f16\u8f91 \ud83e\udde0 \u667a\u80fd\u7406\u89e3 \ud83c\udf1f \u6a21\u578b\u7b80\u4ecb \u6211\u4eec\u975e\u5e38\u6fc0\u52a8\u5730\u53d1\u5e03\u4e86 **Qwen-Image**\uff0c\u8fd9\u662f Qwen \u7cfb\u5217\u4e2d\u7684\u4e00\u4e2a\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5728 **\u590d\u6742\u6587\u672c\u6e32\u67d3** \u548c **\u7cbe\u786e\u56fe\u50cf\u7f16\u8f91** \u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002 \ud83c\udfaf \u6838\u5fc3\u4f18\u52bf \u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e2d\u6587\u3002 \ud83d\udd27 \u6280\u672f\u89c4\u683c \u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08T2I\uff09 \u67b6\u6784 Diffusion Transformer \u652f\u6301\u5206\u8fa8\u7387 \u591a\u79cd\u5bbd\u9ad8\u6bd4\uff081:1, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3\uff09 \u6700\u5927\u5206\u8fa8\u7387 1664\u00d7928 \u63a8\u7406\u6b65\u6570 \u63a8\u8350 50 \u6b65 CFG \u5f15\u5bfc true_cfg_scale: 4.0 \u8bb8\u53ef\u534f\u8bae Apache 2.0 \ud83d\ude80 \u5feb\u901f\u5f00\u59cb \ud83c\udfa8 ComfyUI \u4f7f\u7528\u6307\u5357 \ud83c\udf10 ComfyUI Web \u754c\u9762\u4f7f\u7528 ### \ud83d\udccd \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 ![img.png](img.png) \u901a\u8fc7\u670d\u52a1\u5b9e\u4f8b\u7684\u8bbf\u95ee\u94fe\u63a5\u8fdb\u5165 ComfyUI \u754c\u9762 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u52a0\u8f7d\u5de5\u4f5c\u6d41 \u9009\u62e9 Qwen-Image \u4e13\u7528\u5de5\u4f5c\u6d41\u6a21\u677f\uff0c\u5e76\u53bb\u6389Lora\u8282\u70b9\u914d\u7f6e ![img_4.png](img_4.png) ### \u270d\ufe0f \u6b65\u9aa4\u4e09\uff1a\u8bbe\u7f6e\u63d0\u793a\u8bcd \u5728 **TextEncode** \u8282\u70b9\u586b\u5199\u63cf\u8ff0\u8bcd\uff1a \u2705 \u6b63\u5411\u63d0\u793a\u8bcd \u63cf\u8ff0\u5e0c\u671b\u751f\u6210\u7684\u56fe\u50cf\u5185\u5bb9\u548c\u98ce\u683c \u274c \u8d1f\u5411\u63d0\u793a\u8bcd \u4e0d\u60f3\u8981\u751f\u6210\u7684\u5185\u5bb9 ### \u2699\ufe0f \u6b65\u9aa4\u56db\uff1a\u914d\u7f6e\u53c2\u6570 \u8bbe\u7f6e\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u63a8\u7406\u6b65\u6570\u7b49\u53c2\u6570 ### \ud83c\udfac \u6b65\u9aa4\u4e94\uff1a\u6267\u884c\u751f\u6210 \u70b9\u51fb\u6267\u884c\u6309\u94ae\u5f00\u59cb\u751f\u6210\u56fe\u50cf \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f \ud83c\udf10 \u83b7\u53d6\u670d\u52a1\u5668\u5730\u5740 \u8bb0\u5f55 ComfyUI \u670d\u52a1\u5668\u7684\u8bbf\u95ee\u5730\u5740 ![img_2.png](img_2.png) \ud83d\udd10 \u83b7\u53d6 API Token \u5728 ComfyUI \u754c\u9762\u53f3\u4e0a\u89d2\u83b7\u53d6\u8bbf\u95ee\u4ee4\u724c ![img_1.png](img_1.png) \ud83d\udcbb Python API \u8c03\u7528\u793a\u4f8b \ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests import json import uuid import time import random import os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER = \"your-server-address\" COMFYUI_TOKEN = \"your-api-token\" # \ud83c\udfaf Qwen-Image \u6a21\u578b\u6587\u4ef6\u540d UNET_MODEL = \"qwen_image_fp8_e4m3fn.safetensors\" CLIP_MODEL = \"qwen_2.5_vl_7b_fp8_scaled.safetensors\" VAE_MODEL = \"qwen_image_vae.safetensors\" class QwenImageClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_image(self, prompt, negative_prompt=\"\", width=1328, height=1328, steps=20, cfg=2.5, shift=3.1, seed=None): \"\"\"\ud83c\udfa8 \u751f\u6210\u56fe\u50cf\"\"\" if seed is None: seed = random.randint(1, 1000000000000000) workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"66\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"58\", 0] }, \"class_type\": \"KSampler\", \"_meta\": { \"title\": \"KSampler\" } }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Positive Prompt)\" } }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Negative Prompt)\" } }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": { \"title\": \"VAE Decode\" } }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": { \"title\": \"Load Diffusion Model\" } }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"qwen_image\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": { \"title\": \"Load CLIP\" } }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": { \"title\": \"Load VAE\" } }, \"58\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": { \"title\": \"EmptySD3LatentImage\" } }, \"60\": { \"inputs\": { \"filename_prefix\": \"qwen-image\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": { \"title\": \"Save Image\" } }, \"66\": { \"inputs\": { \"shift\": shift, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingAuraFlow\", \"_meta\": { \"title\": \"ModelSamplingAuraFlow\" } } } print(\"\ud83d\udce4 \u63d0\u4ea4 Qwen-Image \u751f\u6210\u4efb\u52a1...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u5de5\u4f5c\u6d41\u9519\u8bef: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u54cd\u5e94\u4e2d\u6ca1\u6709 prompt_id: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: # \u68c0\u67e5\u961f\u5217\u72b6\u6001 queue_response = requests.get(f\"{self.base_url}/queue\", headers=self.headers) queue_data = queue_response.json() # \u68c0\u67e5\u662f\u5426\u5728\u8fd0\u884c\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # \u68c0\u67e5\u662f\u5426\u5728\u7b49\u5f85\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # \u68c0\u67e5\u5386\u53f2\u8bb0\u5f55 history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"\u72b6\u6001\u68c0\u67e5\u9519\u8bef: {e}\") return \"processing\" def download_image(self, task_id, output_path=\"qwen_image_output.png\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u56fe\u50cf\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] for node_id, output in outputs.items(): if 'images' in output: for image_info in output['images']: filename = image_info['filename'] image_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(image_response.content) return output_path return None except Exception as e: print(f\"\u4e0b\u8f7d\u9519\u8bef: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Qwen-Image \u751f\u6210\u4efb\u52a1\"\"\" client = QwenImageClient() try: print(\"\ud83c\udfa8 \u5f00\u59cb Qwen-Image \u56fe\u50cf\u751f\u6210\u4efb\u52a1...\") # \ud83c\udfaf \u793a\u4f8b\u63d0\u793a\u8bcd - \u9999\u6e2f\u9713\u8679\u8857\u666f prompt = '''A vibrant, warm neon-lit street scene in Hong Kong at the afternoon, with a mix of colorful Chinese and English signs glowing brightly. The atmosphere is lively, cinematic, and rain-washed with reflections on the pavement. The colors are vivid, full of pink, blue, red, and green hues. Crowded buildings with overlapping neon signs. 1980s Hong Kong style. Signs include: \"\u9f8d\u9cf3\u51b0\u5ba4\" \"\u91d1\u83ef\u71d2\u81d8\" \"HAPPY HAIR\" \"\u9d3b\u904b\u8336\u9910\u5ef3\" \"EASY BAR\" \"\u6c38\u767c\u9b5a\u86cb\u7c89\" \"\u6dfb\u8a18\u7ca5\u9eb5\" \"SUNSHINE MOTEL\" \"\u7f8e\u90fd\u9910\u5ba4\" \"\u5bcc\u8a18\u7cd6\u6c34\" \"\u592a\u5e73\u9928\" \"\u96c5\u82b3\u9aee\u578b\u5c4b\" \"STAR KTV\" \"\u9280\u6cb3\u5a1b\u6a02\u57ce\" \"\u767e\u6a02\u9580\u821e\u5ef3\" \"BUBBLE CAFE\" \"\u842c\u8c6a\u9ebb\u96c0\u9928\" \"CITY LIGHTS BAR\" \"\u745e\u7965\u9999\u71ed\u838a\" \"\u6587\u8a18\u6587\u5177\" \"GOLDEN JADE HOTEL\" \"LOVELY BEAUTY\" \"\u5408\u8208\u767e\u8ca8\" \"\u8208\u65fa\u96fb\u5668\" And the background is warm yellow street and with all stores' lights on.''' negative_prompt = \"low quality, blurry, distorted, bad anatomy, deformed text\" print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt[:100]}...\") # \ud83c\udfa8 \u751f\u6210\u56fe\u50cf task_id = client.generate_image( prompt=prompt, negative_prompt=negative_prompt, width=1328, height=1328, steps=20, cfg=2.5, shift=3.1 ) print(f\"\ud83c\udd94 \u4efb\u52a1 ID: {task_id}\") # \u23f3 \u7b49\u5f85\u5b8c\u6210 while True: status = client.get_status(task_id) print(f\"\ud83d\udcca \u5f53\u524d\u72b6\u6001: {status}\") if status == \"completed\": print(\"\u2705 \u56fe\u50cf\u751f\u6210\u5b8c\u6210!\") break elif status == \"failed\": print(\"\u274c \u56fe\u50cf\u751f\u6210\u5931\u8d25!\") exit(1) time.sleep(10) # \ud83d\udce5 \u4e0b\u8f7d\u56fe\u50cf output_file = client.download_image(task_id, \"qwen_image_hongkong.png\") if output_file: print(f\"\ud83c\udf89 \u56fe\u50cf\u4e0b\u8f7d\u6210\u529f! \u4fdd\u5b58\u4e3a: {output_file}\") else: print(\"\u274c \u56fe\u50cf\u4e0b\u8f7d\u5931\u8d25\") except Exception as e: print(f\"\u274c \u9519\u8bef: {e}\") # \ud83c\udfaf \u9884\u8bbe\u793a\u4f8b\u51fd\u6570 def generate_text_examples(): \"\"\"\ud83d\udcdd \u751f\u6210\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u672c\u6e32\u67d3\u793a\u4f8b\"\"\" client = QwenImageClient() examples = [ { \"name\": \"\u5496\u5561\u5e97\u62db\u724c\", \"prompt\": '''A cozy coffee shop entrance with a wooden chalkboard sign reading \"Qwen Coffee \u2615 \u901a\u4e49\u5496\u5561\" in beautiful handwritten style. Below it shows \"\u4eca\u65e5\u7279\u4ef7 Today's Special: \u62ff\u94c1 Latte \u00a525\". The scene has warm lighting and vintage atmosphere.''', \"filename\": \"coffee_shop_sign.png\" }, { \"name\": \"\u6570\u5b66\u516c\u5f0f\u9ed1\u677f\", \"prompt\": '''A university classroom blackboard with mathematical equations written in white chalk: \"E=mc\u00b2\", \"\u03c0\u22483.14159265359\", \"\u222bf(x)dx\", \"\u2211(n=1 to \u221e)\", \"\u221a(a\u00b2+b\u00b2)=c\". The handwriting is clear and academic style.''', \"filename\": \"math_blackboard.png\" }, { \"name\": \"\u4e2d\u82f1\u6587\u4e66\u5e97\", \"prompt\": '''A traditional bookstore with bilingual signs: \"\u4e66\u9999\u95e8\u7b2c Book Paradise\", \"\u65b0\u4e66\u4e0a\u67b6 New Arrivals\", \"\u6587\u5b66\u5c0f\u8bf4 Literature\", \"\u5386\u53f2\u4f20\u8bb0 Biography\", \"\u513f\u7ae5\u8bfb\u7269 Children's Books\". Warm wooden shelves filled with books.''', \"filename\": \"bilingual_bookstore.png\" }, { \"name\": \"\u65e5\u5f0f\u62c9\u9762\u5e97\", \"prompt\": '''A Japanese ramen shop with neon signs displaying: \"\u3089\u30fc\u3081\u3093 Ramen\", \"\u5473\u564c\u30e9\u30fc\u30e1\u30f3 Miso Ramen \u00a5800\", \"\u91a4\u6cb9\u30e9\u30fc\u30e1\u30f3 Shoyu Ramen \u00a5750\", \"\u8c5a\u9aa8\u30e9\u30fc\u30e1\u30f3 Tonkotsu Ramen \u00a5850\". Traditional red lanterns and warm lighting.''', \"filename\": \"ramen_shop_signs.png\" } ] for example in examples: try: print(f\"\\n\ud83c\udfa8 \u751f\u6210\u793a\u4f8b: {example['name']}\") task_id = client.generate_image( prompt=example['prompt'], negative_prompt=\"low quality, blurry, illegible text, distorted characters\", width=1328, height=1328, steps=20, cfg=2.5 ) # \u7b49\u5f85\u5b8c\u6210 while client.get_status(task_id) != \"completed\": time.sleep(5) # \u4e0b\u8f7d output_file = client.download_image(task_id, example['filename']) if output_file: print(f\"\u2705 {example['name']} \u751f\u6210\u5b8c\u6210: {output_file}\") except Exception as e: print(f\"\u274c {example['name']} \u751f\u6210\u5931\u8d25: {e}\") if __name__ == \"__main__\": # \u8fd0\u884c\u4e3b\u793a\u4f8b main() # \u53ef\u9009\uff1a\u8fd0\u884c\u591a\u4e2a\u6587\u672c\u6e32\u67d3\u793a\u4f8b # generate_text_examples()","title":"Index"},{"location":"qwen-image/doc/#_1","text":"\u6211\u4eec\u975e\u5e38\u6fc0\u52a8\u5730\u53d1\u5e03\u4e86 **Qwen-Image**\uff0c\u8fd9\u662f Qwen \u7cfb\u5217\u4e2d\u7684\u4e00\u4e2a\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5728 **\u590d\u6742\u6587\u672c\u6e32\u67d3** \u548c **\u7cbe\u786e\u56fe\u50cf\u7f16\u8f91** \u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002 \ud83c\udfaf \u6838\u5fc3\u4f18\u52bf \u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e2d\u6587\u3002","title":"\ud83c\udf1f \u6a21\u578b\u7b80\u4ecb"},{"location":"qwen-image/doc/#_2","text":"\u89c4\u683c\u9879\u76ee \u8be6\u7ec6\u4fe1\u606f \u6a21\u578b\u7c7b\u578b \u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff08T2I\uff09 \u67b6\u6784 Diffusion Transformer \u652f\u6301\u5206\u8fa8\u7387 \u591a\u79cd\u5bbd\u9ad8\u6bd4\uff081:1, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3\uff09 \u6700\u5927\u5206\u8fa8\u7387 1664\u00d7928 \u63a8\u7406\u6b65\u6570 \u63a8\u8350 50 \u6b65 CFG \u5f15\u5bfc true_cfg_scale: 4.0 \u8bb8\u53ef\u534f\u8bae Apache 2.0","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"qwen-image/doc/#_3","text":"","title":"\ud83d\ude80 \u5feb\u901f\u5f00\u59cb"},{"location":"qwen-image/doc/#comfyui","text":"","title":"\ud83c\udfa8 ComfyUI \u4f7f\u7528\u6307\u5357"},{"location":"qwen-image/doc/#comfyui-web","text":"### \ud83d\udccd \u6b65\u9aa4\u4e00\uff1a\u8bbf\u95ee\u754c\u9762 ![img.png](img.png) \u901a\u8fc7\u670d\u52a1\u5b9e\u4f8b\u7684\u8bbf\u95ee\u94fe\u63a5\u8fdb\u5165 ComfyUI \u754c\u9762 ### \ud83d\udd27 \u6b65\u9aa4\u4e8c\uff1a\u52a0\u8f7d\u5de5\u4f5c\u6d41 \u9009\u62e9 Qwen-Image \u4e13\u7528\u5de5\u4f5c\u6d41\u6a21\u677f\uff0c\u5e76\u53bb\u6389Lora\u8282\u70b9\u914d\u7f6e ![img_4.png](img_4.png) ### \u270d\ufe0f \u6b65\u9aa4\u4e09\uff1a\u8bbe\u7f6e\u63d0\u793a\u8bcd \u5728 **TextEncode** \u8282\u70b9\u586b\u5199\u63cf\u8ff0\u8bcd\uff1a","title":"\ud83c\udf10 ComfyUI Web \u754c\u9762\u4f7f\u7528"},{"location":"qwen-image/doc/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"qwen-image/doc/#_4","text":"","title":"\ud83d\udd11 \u83b7\u53d6\u8ba4\u8bc1\u4fe1\u606f"},{"location":"qwen-image/doc/#python-api","text":"\ud83d\udc0d \u70b9\u51fb\u5c55\u5f00\u5b8c\u6574 Python API \u8c03\u7528\u4ee3\u7801 import requests import json import uuid import time import random import os # \ud83d\udd27 \u914d\u7f6e\u53c2\u6570 COMFYUI_SERVER = \"your-server-address\" COMFYUI_TOKEN = \"your-api-token\" # \ud83c\udfaf Qwen-Image \u6a21\u578b\u6587\u4ef6\u540d UNET_MODEL = \"qwen_image_fp8_e4m3fn.safetensors\" CLIP_MODEL = \"qwen_2.5_vl_7b_fp8_scaled.safetensors\" VAE_MODEL = \"qwen_image_vae.safetensors\" class QwenImageClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_image(self, prompt, negative_prompt=\"\", width=1328, height=1328, steps=20, cfg=2.5, shift=3.1, seed=None): \"\"\"\ud83c\udfa8 \u751f\u6210\u56fe\u50cf\"\"\" if seed is None: seed = random.randint(1, 1000000000000000) workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"66\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"58\", 0] }, \"class_type\": \"KSampler\", \"_meta\": { \"title\": \"KSampler\" } }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Positive Prompt)\" } }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Negative Prompt)\" } }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": { \"title\": \"VAE Decode\" } }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": { \"title\": \"Load Diffusion Model\" } }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"qwen_image\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": { \"title\": \"Load CLIP\" } }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": { \"title\": \"Load VAE\" } }, \"58\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": { \"title\": \"EmptySD3LatentImage\" } }, \"60\": { \"inputs\": { \"filename_prefix\": \"qwen-image\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": { \"title\": \"Save Image\" } }, \"66\": { \"inputs\": { \"shift\": shift, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingAuraFlow\", \"_meta\": { \"title\": \"ModelSamplingAuraFlow\" } } } print(\"\ud83d\udce4 \u63d0\u4ea4 Qwen-Image \u751f\u6210\u4efb\u52a1...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"\u5de5\u4f5c\u6d41\u9519\u8bef: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"\u54cd\u5e94\u4e2d\u6ca1\u6709 prompt_id: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca \u83b7\u53d6\u4efb\u52a1\u72b6\u6001\"\"\" try: # \u68c0\u67e5\u961f\u5217\u72b6\u6001 queue_response = requests.get(f\"{self.base_url}/queue\", headers=self.headers) queue_data = queue_response.json() # \u68c0\u67e5\u662f\u5426\u5728\u8fd0\u884c\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # \u68c0\u67e5\u662f\u5426\u5728\u7b49\u5f85\u961f\u5217\u4e2d if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # \u68c0\u67e5\u5386\u53f2\u8bb0\u5f55 history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"\u72b6\u6001\u68c0\u67e5\u9519\u8bef: {e}\") return \"processing\" def download_image(self, task_id, output_path=\"qwen_image_output.png\"): \"\"\"\ud83d\udce5 \u4e0b\u8f7d\u751f\u6210\u7684\u56fe\u50cf\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] for node_id, output in outputs.items(): if 'images' in output: for image_info in output['images']: filename = image_info['filename'] image_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(image_response.content) return output_path return None except Exception as e: print(f\"\u4e0b\u8f7d\u9519\u8bef: {e}\") return None def main(): \"\"\"\ud83d\ude80 \u4e3b\u51fd\u6570 - \u6267\u884c Qwen-Image \u751f\u6210\u4efb\u52a1\"\"\" client = QwenImageClient() try: print(\"\ud83c\udfa8 \u5f00\u59cb Qwen-Image \u56fe\u50cf\u751f\u6210\u4efb\u52a1...\") # \ud83c\udfaf \u793a\u4f8b\u63d0\u793a\u8bcd - \u9999\u6e2f\u9713\u8679\u8857\u666f prompt = '''A vibrant, warm neon-lit street scene in Hong Kong at the afternoon, with a mix of colorful Chinese and English signs glowing brightly. The atmosphere is lively, cinematic, and rain-washed with reflections on the pavement. The colors are vivid, full of pink, blue, red, and green hues. Crowded buildings with overlapping neon signs. 1980s Hong Kong style. Signs include: \"\u9f8d\u9cf3\u51b0\u5ba4\" \"\u91d1\u83ef\u71d2\u81d8\" \"HAPPY HAIR\" \"\u9d3b\u904b\u8336\u9910\u5ef3\" \"EASY BAR\" \"\u6c38\u767c\u9b5a\u86cb\u7c89\" \"\u6dfb\u8a18\u7ca5\u9eb5\" \"SUNSHINE MOTEL\" \"\u7f8e\u90fd\u9910\u5ba4\" \"\u5bcc\u8a18\u7cd6\u6c34\" \"\u592a\u5e73\u9928\" \"\u96c5\u82b3\u9aee\u578b\u5c4b\" \"STAR KTV\" \"\u9280\u6cb3\u5a1b\u6a02\u57ce\" \"\u767e\u6a02\u9580\u821e\u5ef3\" \"BUBBLE CAFE\" \"\u842c\u8c6a\u9ebb\u96c0\u9928\" \"CITY LIGHTS BAR\" \"\u745e\u7965\u9999\u71ed\u838a\" \"\u6587\u8a18\u6587\u5177\" \"GOLDEN JADE HOTEL\" \"LOVELY BEAUTY\" \"\u5408\u8208\u767e\u8ca8\" \"\u8208\u65fa\u96fb\u5668\" And the background is warm yellow street and with all stores' lights on.''' negative_prompt = \"low quality, blurry, distorted, bad anatomy, deformed text\" print(f\"\ud83d\udcdd \u63d0\u793a\u8bcd: {prompt[:100]}...\") # \ud83c\udfa8 \u751f\u6210\u56fe\u50cf task_id = client.generate_image( prompt=prompt, negative_prompt=negative_prompt, width=1328, height=1328, steps=20, cfg=2.5, shift=3.1 ) print(f\"\ud83c\udd94 \u4efb\u52a1 ID: {task_id}\") # \u23f3 \u7b49\u5f85\u5b8c\u6210 while True: status = client.get_status(task_id) print(f\"\ud83d\udcca \u5f53\u524d\u72b6\u6001: {status}\") if status == \"completed\": print(\"\u2705 \u56fe\u50cf\u751f\u6210\u5b8c\u6210!\") break elif status == \"failed\": print(\"\u274c \u56fe\u50cf\u751f\u6210\u5931\u8d25!\") exit(1) time.sleep(10) # \ud83d\udce5 \u4e0b\u8f7d\u56fe\u50cf output_file = client.download_image(task_id, \"qwen_image_hongkong.png\") if output_file: print(f\"\ud83c\udf89 \u56fe\u50cf\u4e0b\u8f7d\u6210\u529f! \u4fdd\u5b58\u4e3a: {output_file}\") else: print(\"\u274c \u56fe\u50cf\u4e0b\u8f7d\u5931\u8d25\") except Exception as e: print(f\"\u274c \u9519\u8bef: {e}\") # \ud83c\udfaf \u9884\u8bbe\u793a\u4f8b\u51fd\u6570 def generate_text_examples(): \"\"\"\ud83d\udcdd \u751f\u6210\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u672c\u6e32\u67d3\u793a\u4f8b\"\"\" client = QwenImageClient() examples = [ { \"name\": \"\u5496\u5561\u5e97\u62db\u724c\", \"prompt\": '''A cozy coffee shop entrance with a wooden chalkboard sign reading \"Qwen Coffee \u2615 \u901a\u4e49\u5496\u5561\" in beautiful handwritten style. Below it shows \"\u4eca\u65e5\u7279\u4ef7 Today's Special: \u62ff\u94c1 Latte \u00a525\". The scene has warm lighting and vintage atmosphere.''', \"filename\": \"coffee_shop_sign.png\" }, { \"name\": \"\u6570\u5b66\u516c\u5f0f\u9ed1\u677f\", \"prompt\": '''A university classroom blackboard with mathematical equations written in white chalk: \"E=mc\u00b2\", \"\u03c0\u22483.14159265359\", \"\u222bf(x)dx\", \"\u2211(n=1 to \u221e)\", \"\u221a(a\u00b2+b\u00b2)=c\". The handwriting is clear and academic style.''', \"filename\": \"math_blackboard.png\" }, { \"name\": \"\u4e2d\u82f1\u6587\u4e66\u5e97\", \"prompt\": '''A traditional bookstore with bilingual signs: \"\u4e66\u9999\u95e8\u7b2c Book Paradise\", \"\u65b0\u4e66\u4e0a\u67b6 New Arrivals\", \"\u6587\u5b66\u5c0f\u8bf4 Literature\", \"\u5386\u53f2\u4f20\u8bb0 Biography\", \"\u513f\u7ae5\u8bfb\u7269 Children's Books\". Warm wooden shelves filled with books.''', \"filename\": \"bilingual_bookstore.png\" }, { \"name\": \"\u65e5\u5f0f\u62c9\u9762\u5e97\", \"prompt\": '''A Japanese ramen shop with neon signs displaying: \"\u3089\u30fc\u3081\u3093 Ramen\", \"\u5473\u564c\u30e9\u30fc\u30e1\u30f3 Miso Ramen \u00a5800\", \"\u91a4\u6cb9\u30e9\u30fc\u30e1\u30f3 Shoyu Ramen \u00a5750\", \"\u8c5a\u9aa8\u30e9\u30fc\u30e1\u30f3 Tonkotsu Ramen \u00a5850\". Traditional red lanterns and warm lighting.''', \"filename\": \"ramen_shop_signs.png\" } ] for example in examples: try: print(f\"\\n\ud83c\udfa8 \u751f\u6210\u793a\u4f8b: {example['name']}\") task_id = client.generate_image( prompt=example['prompt'], negative_prompt=\"low quality, blurry, illegible text, distorted characters\", width=1328, height=1328, steps=20, cfg=2.5 ) # \u7b49\u5f85\u5b8c\u6210 while client.get_status(task_id) != \"completed\": time.sleep(5) # \u4e0b\u8f7d output_file = client.download_image(task_id, example['filename']) if output_file: print(f\"\u2705 {example['name']} \u751f\u6210\u5b8c\u6210: {output_file}\") except Exception as e: print(f\"\u274c {example['name']} \u751f\u6210\u5931\u8d25: {e}\") if __name__ == \"__main__\": # \u8fd0\u884c\u4e3b\u793a\u4f8b main() # \u53ef\u9009\uff1a\u8fd0\u884c\u591a\u4e2a\u6587\u672c\u6e32\u67d3\u793a\u4f8b # generate_text_examples()","title":"\ud83d\udcbb Python API \u8c03\u7528\u793a\u4f8b"},{"location":"qwen-image/doc/index-en/","text":"\ud83c\udfa8 Qwen-Image Qwen Series Image Generation Foundation Model - Specialized in Complex Text Rendering & Precise Image Editing \ud83c\udfaf Text Rendering \u2728 Image Editing \ud83e\udde0 Intelligent Understanding \ud83c\udf1f Model Overview We are thrilled to introduce **Qwen-Image**, an image generation foundation model in the Qwen series that achieves significant breakthroughs in **complex text rendering** and **precise image editing**. \ud83c\udfaf Core Advantages Experiments demonstrate that this model possesses powerful general capabilities in image generation and editing, with particularly outstanding performance in text rendering, especially for Chinese characters. \ud83d\udd27 Technical Specifications Specification Details Model Type Text-to-Image Generation (T2I) Architecture Diffusion Transformer Supported Resolutions Multiple aspect ratios (1:1, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3) Maximum Resolution 1664\u00d7928 Inference Steps Recommended 50 steps CFG Guidance true_cfg_scale: 4.0 License Apache 2.0 \ud83d\ude80 Quick Start \ud83c\udfa8 ComfyUI Usage Guide \ud83c\udf10 ComfyUI Web Interface Usage ### \ud83d\udccd Step 1: Access Interface ![img_3.png](img_3.png) Access the ComfyUI interface through the service instance's access link ### \ud83d\udd27 Step 2: Load Workflow Select the Qwen-Image dedicated workflow template and remove the Lora node configuration [img_4.png](img_4.png) ### \u270d\ufe0f Step 3: Set Prompts Fill in the description in the **TextEncode** node: \u2705 Positive Prompt Describe the desired image content and style \u274c Negative Prompt Content you don't want to generate ### \u2699\ufe0f Step 4: Configure Parameters Set image resolution, inference steps, and other parameters ### \ud83c\udfac Step 5: Execute Generation Click the execute button to start image generation \ud83d\udd0c API Integration \ud83d\udd11 Authentication Setup \ud83c\udf10 Get Server Address Record the ComfyUI server's access address ![img_3.png](img_3.png) \ud83d\udd10 Get API Token Obtain the access token from the top-right corner of ComfyUI interface ![img_1.png](img_1.png) \ud83d\udcbb Python API Integration Example \ud83d\udc0d Click to expand complete Python API integration code import requests import json import uuid import time import random import os # \ud83d\udd27 Configuration Parameters COMFYUI_SERVER = \"your-server-address\" COMFYUI_TOKEN = \"your-api-token\" # \ud83c\udfaf Qwen-Image Model Files UNET_MODEL = \"qwen_image_fp8_e4m3fn.safetensors\" CLIP_MODEL = \"qwen_2.5_vl_7b_fp8_scaled.safetensors\" VAE_MODEL = \"qwen_image_vae.safetensors\" class QwenImageClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_image(self, prompt, negative_prompt=\"\", width=1328, height=1328, steps=20, cfg=2.5, shift=3.1, seed=None): \"\"\"\ud83c\udfa8 Generate Image\"\"\" if seed is None: seed = random.randint(1, 1000000000000000) workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"66\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"58\", 0] }, \"class_type\": \"KSampler\", \"_meta\": { \"title\": \"KSampler\" } }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Positive Prompt)\" } }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Negative Prompt)\" } }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": { \"title\": \"VAE Decode\" } }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": { \"title\": \"Load Diffusion Model\" } }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"qwen_image\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": { \"title\": \"Load CLIP\" } }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": { \"title\": \"Load VAE\" } }, \"58\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": { \"title\": \"EmptySD3LatentImage\" } }, \"60\": { \"inputs\": { \"filename_prefix\": \"qwen-image\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": { \"title\": \"Save Image\" } }, \"66\": { \"inputs\": { \"shift\": shift, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingAuraFlow\", \"_meta\": { \"title\": \"ModelSamplingAuraFlow\" } } } print(\"\ud83d\udce4 Submitting Qwen-Image generation task...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get Task Status\"\"\" try: # Check queue status queue_response = requests.get(f\"{self.base_url}/queue\", headers=self.headers) queue_data = queue_response.json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_path=\"qwen_image_output.png\"): \"\"\"\ud83d\udce5 Download Generated Image\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] for node_id, output in outputs.items(): if 'images' in output: for image_info in output['images']: filename = image_info['filename'] image_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(image_response.content) return output_path return None except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main Function - Execute Qwen-Image Generation Task\"\"\" client = QwenImageClient() try: print(\"\ud83c\udfa8 Starting Qwen-Image generation task...\") # \ud83c\udfaf Example Prompt - Hong Kong Neon Street Scene prompt = '''A vibrant, warm neon-lit street scene in Hong Kong at the afternoon, with a mix of colorful Chinese and English signs glowing brightly. The atmosphere is lively, cinematic, and rain-washed with reflections on the pavement. The colors are vivid, full of pink, blue, red, and green hues. Crowded buildings with overlapping neon signs. 1980s Hong Kong style. Signs include: \"\u9f8d\u9cf3\u51b0\u5ba4\" \"\u91d1\u83ef\u71d2\u81d8\" \"HAPPY HAIR\" \"\u9d3b\u904b\u8336\u9910\u5ef3\" \"EASY BAR\" \"\u6c38\u767c\u9b5a\u86cb\u7c89\" \"\u6dfb\u8a18\u7ca5\u9eb5\" \"SUNSHINE MOTEL\" \"\u7f8e\u90fd\u9910\u5ba4\" \"\u5bcc\u8a18\u7cd6\u6c34\" \"\u592a\u5e73\u9928\" \"\u96c5\u82b3\u9aee\u578b\u5c4b\" \"STAR KTV\" \"\u9280\u6cb3\u5a1b\u6a02\u57ce\" \"\u767e\u6a02\u9580\u821e\u5ef3\" \"BUBBLE CAFE\" \"\u842c\u8c6a\u9ebb\u96c0\u9928\" \"CITY LIGHTS BAR\" \"\u745e\u7965\u9999\u71ed\u838a\" \"\u6587\u8a18\u6587\u5177\" \"GOLDEN JADE HOTEL\" \"LOVELY BEAUTY\" \"\u5408\u8208\u767e\u8ca8\" \"\u8208\u65fa\u96fb\u5668\" And the background is warm yellow street and with all stores' lights on.''' negative_prompt = \"low quality, blurry, distorted, bad anatomy, deformed text\" print(f\"\ud83d\udcdd Prompt: {prompt[:100]}...\") # \ud83c\udfa8 Generate Image task_id = client.generate_image( prompt=prompt, negative_prompt=negative_prompt, width=1328, height=1328, steps=20, cfg=2.5, shift=3.1 ) print(f\"\ud83c\udd94 Task ID: {task_id}\") # \u23f3 Wait for completion while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Image generation completed!\") break elif status == \"failed\": print(\"\u274c Image generation failed!\") exit(1) time.sleep(10) # \ud83d\udce5 Download image output_file = client.download_image(task_id, \"qwen_image_hongkong.png\") if output_file: print(f\"\ud83c\udf89 Image downloaded successfully! Saved as: {output_file}\") else: print(\"\u274c Image download failed\") except Exception as e: print(f\"\u274c Error: {e}\") # \ud83c\udfaf Preset Example Functions def generate_text_examples(): \"\"\"\ud83d\udcdd Generate Different Types of Text Rendering Examples\"\"\" client = QwenImageClient() examples = [ { \"name\": \"Coffee Shop Sign\", \"prompt\": '''A cozy coffee shop entrance with a wooden chalkboard sign reading \"Qwen Coffee \u2615 \u901a\u4e49\u5496\u5561\" in beautiful handwritten style. Below it shows \"\u4eca\u65e5\u7279\u4ef7 Today's Special: \u62ff\u94c1 Latte \u00a525\". The scene has warm lighting and vintage atmosphere.''', \"filename\": \"coffee_shop_sign.png\" }, { \"name\": \"Math Formula Blackboard\", \"prompt\": '''A university classroom blackboard with mathematical equations written in white chalk: \"E=mc\u00b2\", \"\u03c0\u22483.14159265359\", \"\u222bf(x)dx\", \"\u2211(n=1 to \u221e)\", \"\u221a(a\u00b2+b\u00b2)=c\". The handwriting is clear and academic style.''', \"filename\": \"math_blackboard.png\" }, { \"name\": \"Bilingual Bookstore\", \"prompt\": '''A traditional bookstore with bilingual signs: \"\u4e66\u9999\u95e8\u7b2c Book Paradise\", \"\u65b0\u4e66\u4e0a\u67b6 New Arrivals\", \"\u6587\u5b66\u5c0f\u8bf4 Literature\", \"\u5386\u53f2\u4f20\u8bb0 Biography\", \"\u513f\u7ae5\u8bfb\u7269 Children's Books\". Warm wooden shelves filled with books.''', \"filename\": \"bilingual_bookstore.png\" }, { \"name\": \"Japanese Ramen Shop\", \"prompt\": '''A Japanese ramen shop with neon signs displaying: \"\u3089\u30fc\u3081\u3093 Ramen\", \"\u5473\u564c\u30e9\u30fc\u30e1\u30f3 Miso Ramen \u00a5800\", \"\u91a4\u6cb9\u30e9\u30fc\u30e1\u30f3 Shoyu Ramen \u00a5750\", \"\u8c5a\u9aa8\u30e9\u30fc\u30e1\u30f3 Tonkotsu Ramen \u00a5850\". Traditional red lanterns and warm lighting.''', \"filename\": \"ramen_shop_signs.png\" } ] for example in examples: try: print(f\"\\n\ud83c\udfa8 Generating example: {example['name']}\") task_id = client.generate_image( prompt=example['prompt'], negative_prompt=\"low quality, blurry, illegible text, distorted characters\", width=1328, height=1328, steps=20, cfg=2.5 ) # Wait for completion while client.get_status(task_id) != \"completed\": time.sleep(5) # Download output_file = client.download_image(task_id, example['filename']) if output_file: print(f\"\u2705 {example['name']} generation completed: {output_file}\") except Exception as e: print(f\"\u274c {example['name']} generation failed: {e}\") if __name__ == \"__main__\": # Run main example main() # Optional: Run multiple text rendering examples # generate_text_examples()","title":"Index en"},{"location":"qwen-image/doc/index-en/#model-overview","text":"We are thrilled to introduce **Qwen-Image**, an image generation foundation model in the Qwen series that achieves significant breakthroughs in **complex text rendering** and **precise image editing**. \ud83c\udfaf Core Advantages Experiments demonstrate that this model possesses powerful general capabilities in image generation and editing, with particularly outstanding performance in text rendering, especially for Chinese characters.","title":"\ud83c\udf1f Model Overview"},{"location":"qwen-image/doc/index-en/#technical-specifications","text":"Specification Details Model Type Text-to-Image Generation (T2I) Architecture Diffusion Transformer Supported Resolutions Multiple aspect ratios (1:1, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3) Maximum Resolution 1664\u00d7928 Inference Steps Recommended 50 steps CFG Guidance true_cfg_scale: 4.0 License Apache 2.0","title":"\ud83d\udd27 Technical Specifications"},{"location":"qwen-image/doc/index-en/#quick-start","text":"","title":"\ud83d\ude80 Quick Start"},{"location":"qwen-image/doc/index-en/#comfyui-usage-guide","text":"","title":"\ud83c\udfa8 ComfyUI Usage Guide"},{"location":"qwen-image/doc/index-en/#comfyui-web-interface-usage","text":"### \ud83d\udccd Step 1: Access Interface ![img_3.png](img_3.png) Access the ComfyUI interface through the service instance's access link ### \ud83d\udd27 Step 2: Load Workflow Select the Qwen-Image dedicated workflow template and remove the Lora node configuration [img_4.png](img_4.png) ### \u270d\ufe0f Step 3: Set Prompts Fill in the description in the **TextEncode** node:","title":"\ud83c\udf10 ComfyUI Web Interface Usage"},{"location":"qwen-image/doc/index-en/#api-integration","text":"","title":"\ud83d\udd0c API Integration"},{"location":"qwen-image/doc/index-en/#authentication-setup","text":"","title":"\ud83d\udd11 Authentication Setup"},{"location":"qwen-image/doc/index-en/#python-api-integration-example","text":"\ud83d\udc0d Click to expand complete Python API integration code import requests import json import uuid import time import random import os # \ud83d\udd27 Configuration Parameters COMFYUI_SERVER = \"your-server-address\" COMFYUI_TOKEN = \"your-api-token\" # \ud83c\udfaf Qwen-Image Model Files UNET_MODEL = \"qwen_image_fp8_e4m3fn.safetensors\" CLIP_MODEL = \"qwen_2.5_vl_7b_fp8_scaled.safetensors\" VAE_MODEL = \"qwen_image_vae.safetensors\" class QwenImageClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = { \"Content-Type\": \"application/json\", **({\"Authorization\": f\"Bearer {token}\"} if token else {}) } def generate_image(self, prompt, negative_prompt=\"\", width=1328, height=1328, steps=20, cfg=2.5, shift=3.1, seed=None): \"\"\"\ud83c\udfa8 Generate Image\"\"\" if seed is None: seed = random.randint(1, 1000000000000000) workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"66\", 0], \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"latent_image\": [\"58\", 0] }, \"class_type\": \"KSampler\", \"_meta\": { \"title\": \"KSampler\" } }, \"6\": { \"inputs\": { \"text\": prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Positive Prompt)\" } }, \"7\": { \"inputs\": { \"text\": negative_prompt, \"clip\": [\"38\", 0] }, \"class_type\": \"CLIPTextEncode\", \"_meta\": { \"title\": \"CLIP Text Encode (Negative Prompt)\" } }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": { \"title\": \"VAE Decode\" } }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": { \"title\": \"Load Diffusion Model\" } }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"qwen_image\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": { \"title\": \"Load CLIP\" } }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": { \"title\": \"Load VAE\" } }, \"58\": { \"inputs\": { \"width\": width, \"height\": height, \"batch_size\": 1 }, \"class_type\": \"EmptySD3LatentImage\", \"_meta\": { \"title\": \"EmptySD3LatentImage\" } }, \"60\": { \"inputs\": { \"filename_prefix\": \"qwen-image\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": { \"title\": \"Save Image\" } }, \"66\": { \"inputs\": { \"shift\": shift, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingAuraFlow\", \"_meta\": { \"title\": \"ModelSamplingAuraFlow\" } } } print(\"\ud83d\udce4 Submitting Qwen-Image generation task...\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"] def get_status(self, task_id): \"\"\"\ud83d\udcca Get Task Status\"\"\" try: # Check queue status queue_response = requests.get(f\"{self.base_url}/queue\", headers=self.headers) queue_data = queue_response.json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_path=\"qwen_image_output.png\"): \"\"\"\ud83d\udce5 Download Generated Image\"\"\" try: response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] for node_id, output in outputs.items(): if 'images' in output: for image_info in output['images']: filename = image_info['filename'] image_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) with open(output_path, \"wb\") as f: f.write(image_response.content) return output_path return None except Exception as e: print(f\"Download error: {e}\") return None def main(): \"\"\"\ud83d\ude80 Main Function - Execute Qwen-Image Generation Task\"\"\" client = QwenImageClient() try: print(\"\ud83c\udfa8 Starting Qwen-Image generation task...\") # \ud83c\udfaf Example Prompt - Hong Kong Neon Street Scene prompt = '''A vibrant, warm neon-lit street scene in Hong Kong at the afternoon, with a mix of colorful Chinese and English signs glowing brightly. The atmosphere is lively, cinematic, and rain-washed with reflections on the pavement. The colors are vivid, full of pink, blue, red, and green hues. Crowded buildings with overlapping neon signs. 1980s Hong Kong style. Signs include: \"\u9f8d\u9cf3\u51b0\u5ba4\" \"\u91d1\u83ef\u71d2\u81d8\" \"HAPPY HAIR\" \"\u9d3b\u904b\u8336\u9910\u5ef3\" \"EASY BAR\" \"\u6c38\u767c\u9b5a\u86cb\u7c89\" \"\u6dfb\u8a18\u7ca5\u9eb5\" \"SUNSHINE MOTEL\" \"\u7f8e\u90fd\u9910\u5ba4\" \"\u5bcc\u8a18\u7cd6\u6c34\" \"\u592a\u5e73\u9928\" \"\u96c5\u82b3\u9aee\u578b\u5c4b\" \"STAR KTV\" \"\u9280\u6cb3\u5a1b\u6a02\u57ce\" \"\u767e\u6a02\u9580\u821e\u5ef3\" \"BUBBLE CAFE\" \"\u842c\u8c6a\u9ebb\u96c0\u9928\" \"CITY LIGHTS BAR\" \"\u745e\u7965\u9999\u71ed\u838a\" \"\u6587\u8a18\u6587\u5177\" \"GOLDEN JADE HOTEL\" \"LOVELY BEAUTY\" \"\u5408\u8208\u767e\u8ca8\" \"\u8208\u65fa\u96fb\u5668\" And the background is warm yellow street and with all stores' lights on.''' negative_prompt = \"low quality, blurry, distorted, bad anatomy, deformed text\" print(f\"\ud83d\udcdd Prompt: {prompt[:100]}...\") # \ud83c\udfa8 Generate Image task_id = client.generate_image( prompt=prompt, negative_prompt=negative_prompt, width=1328, height=1328, steps=20, cfg=2.5, shift=3.1 ) print(f\"\ud83c\udd94 Task ID: {task_id}\") # \u23f3 Wait for completion while True: status = client.get_status(task_id) print(f\"\ud83d\udcca Current status: {status}\") if status == \"completed\": print(\"\u2705 Image generation completed!\") break elif status == \"failed\": print(\"\u274c Image generation failed!\") exit(1) time.sleep(10) # \ud83d\udce5 Download image output_file = client.download_image(task_id, \"qwen_image_hongkong.png\") if output_file: print(f\"\ud83c\udf89 Image downloaded successfully! Saved as: {output_file}\") else: print(\"\u274c Image download failed\") except Exception as e: print(f\"\u274c Error: {e}\") # \ud83c\udfaf Preset Example Functions def generate_text_examples(): \"\"\"\ud83d\udcdd Generate Different Types of Text Rendering Examples\"\"\" client = QwenImageClient() examples = [ { \"name\": \"Coffee Shop Sign\", \"prompt\": '''A cozy coffee shop entrance with a wooden chalkboard sign reading \"Qwen Coffee \u2615 \u901a\u4e49\u5496\u5561\" in beautiful handwritten style. Below it shows \"\u4eca\u65e5\u7279\u4ef7 Today's Special: \u62ff\u94c1 Latte \u00a525\". The scene has warm lighting and vintage atmosphere.''', \"filename\": \"coffee_shop_sign.png\" }, { \"name\": \"Math Formula Blackboard\", \"prompt\": '''A university classroom blackboard with mathematical equations written in white chalk: \"E=mc\u00b2\", \"\u03c0\u22483.14159265359\", \"\u222bf(x)dx\", \"\u2211(n=1 to \u221e)\", \"\u221a(a\u00b2+b\u00b2)=c\". The handwriting is clear and academic style.''', \"filename\": \"math_blackboard.png\" }, { \"name\": \"Bilingual Bookstore\", \"prompt\": '''A traditional bookstore with bilingual signs: \"\u4e66\u9999\u95e8\u7b2c Book Paradise\", \"\u65b0\u4e66\u4e0a\u67b6 New Arrivals\", \"\u6587\u5b66\u5c0f\u8bf4 Literature\", \"\u5386\u53f2\u4f20\u8bb0 Biography\", \"\u513f\u7ae5\u8bfb\u7269 Children's Books\". Warm wooden shelves filled with books.''', \"filename\": \"bilingual_bookstore.png\" }, { \"name\": \"Japanese Ramen Shop\", \"prompt\": '''A Japanese ramen shop with neon signs displaying: \"\u3089\u30fc\u3081\u3093 Ramen\", \"\u5473\u564c\u30e9\u30fc\u30e1\u30f3 Miso Ramen \u00a5800\", \"\u91a4\u6cb9\u30e9\u30fc\u30e1\u30f3 Shoyu Ramen \u00a5750\", \"\u8c5a\u9aa8\u30e9\u30fc\u30e1\u30f3 Tonkotsu Ramen \u00a5850\". Traditional red lanterns and warm lighting.''', \"filename\": \"ramen_shop_signs.png\" } ] for example in examples: try: print(f\"\\n\ud83c\udfa8 Generating example: {example['name']}\") task_id = client.generate_image( prompt=example['prompt'], negative_prompt=\"low quality, blurry, illegible text, distorted characters\", width=1328, height=1328, steps=20, cfg=2.5 ) # Wait for completion while client.get_status(task_id) != \"completed\": time.sleep(5) # Download output_file = client.download_image(task_id, example['filename']) if output_file: print(f\"\u2705 {example['name']} generation completed: {output_file}\") except Exception as e: print(f\"\u274c {example['name']} generation failed: {e}\") if __name__ == \"__main__\": # Run main example main() # Optional: Run multiple text rendering examples # generate_text_examples()","title":"\ud83d\udcbb Python API Integration Example"},{"location":"qwen-image-edit/doc/","text":"\u270f\ufe0f Qwen-Image-Edit \u56fe\u50cf\u7f16\u8f91 ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 - \u7cbe\u51c6\u6587\u5b57\u7f16\u8f91\u4e0e\u8bed\u4e49\u5916\u89c2\u53cc\u91cd\u7f16\u8f91\u80fd\u529b \ud83d\udcdd \u7cbe\u51c6\u6587\u5b57\u7f16\u8f91 \ud83c\udfa8 \u8bed\u4e49\u5916\u89c2\u7f16\u8f91 \ud83c\udf10 \u4e2d\u82f1\u53cc\u8bed \ud83d\udccb Qwen-Image-Edit \u6a21\u578b\u6982\u89c8 **Qwen-Image-Edit** \u662f Qwen-Image \u7684\u56fe\u50cf\u7f16\u8f91\u7248\u672c\u3002\u5b83\u57fa\u4e8e 20B \u7684 Qwen-Image \u6a21\u578b\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff0c\u6210\u529f\u5c06 Qwen-Image \u7684\u6587\u672c\u6e32\u67d3\u7279\u8272\u80fd\u529b\u62d3\u5c55\u5230\u7f16\u8f91\u4efb\u52a1\u4e0a\uff0c\u4ee5\u652f\u6301\u7cbe\u51c6\u7684\u6587\u5b57\u7f16\u8f91\u3002\u6b64\u5916\uff0cQwen-Image-Edit \u5c06\u8f93\u5165\u56fe\u50cf\u540c\u65f6\u8f93\u5165\u5230 Qwen2.5-VL\uff08\u83b7\u53d6\u89c6\u89c9\u8bed\u4e49\u63a7\u5236\uff09\u548c VAE Encoder\uff08\u83b7\u5f97\u89c6\u89c9\u5916\u89c2\u63a7\u5236\uff09\uff0c\u4ee5\u540c\u65f6\u83b7\u5f97\u8bed\u4e49/\u5916\u89c2\u53cc\u91cd\u7f16\u8f91\u80fd\u529b\u3002 \ud83d\udcdd \u7cbe\u51c6\u6587\u5b57\u7f16\u8f91 \u652f\u6301\u4e2d\u82f1\u53cc\u8bed\u6587\u5b57\u7f16\u8f91\uff0c\u4fdd\u7559\u6587\u5b57\u5927\u5c0f/\u5b57\u4f53/\u98ce\u683c\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u589e\u5220\u6539 \ud83c\udfa8 \u8bed\u4e49/\u5916\u89c2\u53cc\u91cd\u7f16\u8f91 \u652f\u6301 low-level \u89c6\u89c9\u5916\u89c2\u7f16\u8f91\u548c high-level \u89c6\u89c9\u8bed\u4e49\u7f16\u8f91 \ud83c\udfc6 SOTA \u6027\u80fd\u8868\u73b0 \u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u83b7\u5f97 SOTA\uff0c\u5f3a\u5927\u7684\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b \ud83d\udd17 \u5b98\u65b9\u8d44\u6e90 \u2022 GitHub \u4ed3\u5e93 \uff1a QwenLM/Qwen-Image \u2022 Hugging Face \uff1a \ud83e\udd17 Qwen/Qwen-Image-Edit \u2022 ModelScope \uff1a ModelScope ### \ud83c\udfaf \u6838\u5fc3\u7f16\u8f91\u80fd\u529b \ud83d\udd24 Low-Level \u89c6\u89c9\u5916\u89c2\u7f16\u8f91 \u98ce\u683c\u8fc1\u79fb \u56fe\u50cf\u589e\u5220\u6539 \u8272\u5f69\u8c03\u6574 \u7eb9\u7406\u4fee\u6539 \ud83e\udde0 High-Level \u89c6\u89c9\u8bed\u4e49\u7f16\u8f91 IP \u5236\u4f5c \u7269\u4f53\u65cb\u8f6c \u573a\u666f\u91cd\u6784 \u6982\u5ff5\u66ff\u6362 \ud83d\ude80 Qwen-Image-Edit ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41 \ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d \u6216\u76f4\u63a5\u4ece\u5b98\u65b9\u6a21\u7248\u6253\u5f00\uff1a ![img.png](img.png) \u66f4\u65b0 ComfyUI \u540e\u53ef\u4ee5\u4ece\u6a21\u677f\u4e2d\u627e\u5230\u5de5\u4f5c\u6d41\u6587\u4ef6\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u7684\u5de5\u4f5c\u6d41\u62d6\u5165 ComfyUI \u4e2d\u52a0\u8f7d\u3002 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u8f93\u5165\u56fe\u7247 \ud83d\uddbc\ufe0f \u793a\u4f8b\u8f93\u5165\u56fe\u7247 \u53f3\u952e\u4fdd\u5b58\u56fe\u7247\uff0c\u7528\u4e8e\u5de5\u4f5c\u6d41\u6d4b\u8bd5 \ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6 \ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784 \ud83d\udcc2 ComfyUI/ \u251c\u2500\u2500 \ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_edit_fp8_e4m3fn.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 loras/ \u2502 \u2502 \u2514\u2500\u2500 Qwen-Image-Lightning-4steps-V1.0.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 vae/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_vae.safetensors \u2502 \u2514\u2500\u2500 \ud83d\udcc2 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors \ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c #### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4 \ud83d\udd27 \u6a21\u578b\u52a0\u8f7d Load Diffusion Model \uff1a qwen_image_edit_fp8_e4m3fn.safetensors Load CLIP \uff1a qwen_2.5_vl_7b_fp8_scaled.safetensors Load VAE \uff1a qwen_image_vae.safetensors \ud83d\udcc1 \u56fe\u7247\u52a0\u8f7d \u5728 Load Image \u8282\u70b9\u4e2d\u4e0a\u4f20\u7528\u4e8e\u7f16\u8f91\u7684\u56fe\u7247 \ud83d\udcdd \u63d0\u793a\u8bcd\u8bbe\u7f6e \u5728 CLIP Text Encoder \u8282\u70b9\u4e2d\u8bbe\u7f6e\u7f16\u8f91\u63d0\u793a\u8bcd #### \u2699\ufe0f \u9ad8\u7ea7\u914d\u7f6e\u9009\u9879 \ud83d\udcd0 \u56fe\u50cf\u5c3a\u5bf8\u5904\u7406 Scale Image to Total Pixels \u8282\u70b9\u4f1a\u5c06\u8f93\u5165\u56fe\u7247\u7f29\u653e\u5230\u603b\u50cf\u7d20\u4e3a\u4e00\u767e\u4e07\u50cf\u7d20 \u907f\u514d\u8f93\u5165\u56fe\u7247\u5c3a\u5bf8\u8fc7\u5927\uff08\u5982 2048\u00d72048\uff09\u5bfc\u81f4\u7684\u8f93\u51fa\u56fe\u50cf\u8d28\u91cf\u635f\u5931 \u5982\u679c\u4e86\u89e3\u8f93\u5165\u56fe\u7247\u5c3a\u5bf8\uff0c\u53ef\u4f7f\u7528 Ctrl+B \u7ed5\u8fc7\u6b64\u8282\u70b9 \u26a1 Lightning LoRA \u52a0\u901f \u4f7f\u7528 4 \u6b65 Lightning LoRA \u5b9e\u73b0\u56fe\u7247\u751f\u6210\u63d0\u901f \u9009\u4e2d LoraLoaderModelOnly \u8282\u70b9 \u6309 Ctrl+B \u542f\u7528\u8be5\u8282\u70b9 #### \ud83c\udf9b\ufe0f KSampler \u53c2\u6570\u8c03\u4f18 \ud83d\udd27 KSampler \u8282\u70b9\u53c2\u6570\u8bbe\u7f6e \u5bf9\u4e8e steps \u548c cfg \u8bbe\u7f6e\uff0c\u5de5\u4f5c\u6d41\u5728\u8282\u70b9\u4e0b\u65b9\u63d0\u4f9b\u4e86\u53c2\u6570\u5efa\u8bae\u7b14\u8bb0\uff0c\u53ef\u4ee5\u6d4b\u8bd5\u6700\u4f73\u7684\u53c2\u6570\u8bbe\u7f6e\uff1a \u2022 Steps \uff1a\u63a8\u8350 20-50 \u6b65 \u2022 CFG \uff1a\u63a8\u8350 3.5-7.5 \u2022 Sampler \uff1a\u63a8\u8350 DPM++ 2M Karras #### \ud83d\ude80 \u6267\u884c\u751f\u6210 \u2328\ufe0f \u70b9\u51fb Queue \u6309\u94ae\u6216\u4f7f\u7528\u5feb\u6377\u952e Ctrl(Cmd) + Enter \u8fd0\u884c\u5de5\u4f5c\u6d41 API\u8c03\u7528 \ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Qwen Image Edit Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration UNET_MODEL = \"qwen_image_edit_fp8_e4m3fn.safetensors\" CLIP_MODEL = \"qwen_2.5_vl_7b_fp8_scaled.safetensors\" VAE_MODEL = \"qwen_image_vae.safetensors\" # Default Parameters DEFAULT_EDIT_PROMPT = \"Remove all UI text elements from the image. Keep the feeling that the characters and scene are in water. Also, remove the green UI elements at the bottom.\" DEFAULT_NEGATIVE_PROMPT = \"\" DEFAULT_INPUT_IMAGE = \"Qwen-Image_00043_.png\" class ComfyUIQwenImageEditClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def edit_image_with_qwen(self, edit_prompt, input_image_path=None, input_image_name=None, negative_prompt=\"\", steps=20, cfg=2.5, seed=None, shift=3.0, cfg_norm_strength=1.0, megapixels=1.0, upscale_method=\"lanczos\"): \"\"\"Edit image using Qwen Image Edit model based on original JSON workflow\"\"\" print(\"Starting Qwen Image Edit generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle input image if input_image_path and not input_image_name: input_image_name = self.upload_image(input_image_path) if not input_image_name: raise Exception(\"Failed to upload input image\") elif not input_image_name: input_image_name = DEFAULT_INPUT_IMAGE # Workflow based on your provided JSON workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"75\", 0], \"positive\": [\"76\", 0], \"negative\": [\"77\", 0], \"latent_image\": [\"88\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"qwen_image\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"60\": { \"inputs\": { \"filename_prefix\": \"qwen_image_edit\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"66\": { \"inputs\": { \"shift\": shift, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingAuraFlow\", \"_meta\": {\"title\": \"Model Sampling AuraFlow\"} }, \"75\": { \"inputs\": { \"strength\": cfg_norm_strength, \"model\": [\"66\", 0] }, \"class_type\": \"CFGNorm\", \"_meta\": {\"title\": \"CFG Norm\"} }, \"76\": { \"inputs\": { \"prompt\": edit_prompt, \"clip\": [\"38\", 0], \"vae\": [\"39\", 0], \"image\": [\"93\", 0] }, \"class_type\": \"TextEncodeQwenImageEdit\", \"_meta\": {\"title\": \"Text Encode Qwen Image Edit (Positive)\"} }, \"77\": { \"inputs\": { \"prompt\": negative_prompt, \"clip\": [\"38\", 0], \"vae\": [\"39\", 0], \"image\": [\"93\", 0] }, \"class_type\": \"TextEncodeQwenImageEdit\", \"_meta\": {\"title\": \"Text Encode Qwen Image Edit (Negative)\"} }, \"78\": { \"inputs\": { \"image\": input_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Input Image\"} }, \"88\": { \"inputs\": { \"pixels\": [\"93\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEEncode\", \"_meta\": {\"title\": \"VAE Encode\"} }, \"93\": { \"inputs\": { \"upscale_method\": upscale_method, \"megapixels\": megapixels, \"image\": [\"78\", 0] }, \"class_type\": \"ImageScaleToTotalPixels\", \"_meta\": {\"title\": \"Image Scale To Total Pixels\"} } } print(\"Submitting Qwen Image Edit workflow...\") print(f\"Edit Prompt: {edit_prompt}\") print(f\"Input Image: {input_image_name}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Megapixels: {megapixels}\") print(f\"Upscale Method: {upscale_method}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, edit_configs, **kwargs): \"\"\"Batch edit images with different configurations\"\"\" results = [] for i, config in enumerate(edit_configs): print(f\"\\nStarting image edit task {i+1}/{len(edit_configs)}...\") try: task_id, seed = self.edit_image_with_qwen(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def edit_with_quality_presets(self, edit_prompt, input_image_path, quality_preset=\"high\"): \"\"\"Edit image with predefined quality settings\"\"\" quality_presets = { \"fast\": { \"steps\": 10, \"cfg\": 2.0, \"megapixels\": 0.5, \"upscale_method\": \"nearest-exact\" }, \"balanced\": { \"steps\": 15, \"cfg\": 2.5, \"megapixels\": 1.0, \"upscale_method\": \"bilinear\" }, \"high\": { \"steps\": 20, \"cfg\": 3.0, \"megapixels\": 1.5, \"upscale_method\": \"lanczos\" }, \"ultra\": { \"steps\": 30, \"cfg\": 3.5, \"megapixels\": 2.0, \"upscale_method\": \"lanczos\" } } settings = quality_presets.get(quality_preset, quality_presets[\"high\"]) return self.edit_image_with_qwen( edit_prompt=edit_prompt, input_image_path=input_image_path, **settings ) def generate_edit_variations(self, base_image_path, edit_prompts): \"\"\"Generate multiple edit variations of the same image\"\"\" results = [] for i, prompt in enumerate(edit_prompts): print(f\"\\nGenerating edit variation {i+1}: {prompt[:50]}...\") try: task_id, seed = self.edit_image_with_qwen( edit_prompt=prompt, input_image_path=base_image_path ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_image(task_id) results.append({ 'prompt': prompt, 'files': files, 'seed': seed }) break elif status == \"failed\": print(f\"Edit variation {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Edit variation {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Qwen Image Edit\"\"\" client = ComfyUIQwenImageEditClient() try: print(\"Qwen Image Edit client started...\") # Single image edit example print(\"\\n=== Single Image Edit ===\") # You can provide a local image path or use existing image name input_image_path = None # Set to your image path, e.g., \"input.jpg\" task_id, seed = client.edit_image_with_qwen( edit_prompt=DEFAULT_EDIT_PROMPT, input_image_path=input_image_path, input_image_name=DEFAULT_INPUT_IMAGE, negative_prompt=DEFAULT_NEGATIVE_PROMPT, steps=20, cfg=2.5, shift=3.0, megapixels=1.0, upscale_method=\"lanczos\" ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image edit completed!\") break elif status == \"failed\": print(\"Edit failed!\") return time.sleep(5) # Download edited images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Quality preset example print(\"\\n=== Quality Preset Example ===\") # Uncomment to test different quality settings # if input_image_path: # for preset in [\"fast\", \"balanced\", \"high\"]: # print(f\"Editing with {preset} quality...\") # task_id, seed = client.edit_with_quality_presets( # \"Remove background and make it white\", input_image_path, preset # ) # # Wait and download logic here... # Edit variations example print(\"\\n=== Edit Variations Example ===\") edit_variations = [ \"Remove all text and UI elements from the image\", \"Change the background to a sunset sky\", \"Make the image black and white\", \"Add more vibrant colors to the scene\", \"Remove people from the image\" ] # Uncomment to run edit variations # if input_image_path: # variation_results = client.generate_edit_variations(input_image_path, edit_variations) # print(f\"Generated {len(variation_results)} edit variations\") # Batch edit example print(\"\\n=== Batch Edit Example ===\") batch_configs = [ { 'edit_prompt': \"Remove all UI elements and make background transparent\", 'input_image_name': DEFAULT_INPUT_IMAGE, 'steps': 15, 'cfg': 2.0 }, { 'edit_prompt': \"Change the lighting to golden hour\", 'input_image_name': DEFAULT_INPUT_IMAGE, 'steps': 20, 'cfg': 2.5 } ] # Uncomment to run batch editing # batch_results = client.generate_batch(batch_configs, megapixels=1.0) # print(f\"Batch editing completed, processed {len(batch_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() \ud83c\udfaf \u7f16\u8f91\u5e94\u7528\u573a\u666f \ud83d\udcdd \u6587\u5b57\u7f16\u8f91 \u6d77\u62a5\u6587\u5b57\u4fee\u6539\u3001\u6807\u8bc6\u66f4\u6362\u3001\u591a\u8bed\u8a00\u672c\u5730\u5316 \ud83c\udfa8 \u98ce\u683c\u8f6c\u6362 \u827a\u672f\u98ce\u683c\u8fc1\u79fb\u3001\u8272\u8c03\u8c03\u6574\u3001\u89c6\u89c9\u6548\u679c\u589e\u5f3a \ud83c\udfe2 \u5546\u4e1a\u5e94\u7528 \u4ea7\u54c1\u56fe\u7247\u7f16\u8f91\u3001\u5e7f\u544a\u7d20\u6750\u5236\u4f5c\u3001\u54c1\u724c\u89c6\u89c9\u7edf\u4e00 \ud83c\udfad \u521b\u610f\u8bbe\u8ba1 \u6982\u5ff5\u827a\u672f\u521b\u4f5c\u3001IP \u89d2\u8272\u8bbe\u8ba1\u3001\u573a\u666f\u91cd\u6784 \ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae \u2705 \u6700\u4f73\u5b9e\u8df5 \u4f7f\u7528\u6e05\u6670\u3001\u5177\u4f53\u7684\u7f16\u8f91\u6307\u4ee4 \u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u8fa8\u7387\u9002\u4e2d \u6587\u5b57\u7f16\u8f91\u65f6\u63cf\u8ff0\u5177\u4f53\u7684\u5b57\u4f53\u548c\u6837\u5f0f\u8981\u6c42 \u5229\u7528 Lightning LoRA \u8fdb\u884c\u5feb\u901f\u9884\u89c8 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u907f\u514d\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u8fc7\u5927\u5f71\u54cd\u751f\u6210\u8d28\u91cf \u590d\u6742\u7f16\u8f91\u4efb\u52a1\u53ef\u80fd\u9700\u8981\u591a\u6b21\u8fed\u4ee3 \u6587\u5b57\u7f16\u8f91\u6548\u679c\u4e0e\u539f\u56fe\u6587\u5b57\u6e05\u6670\u5ea6\u76f8\u5173 \u5408\u7406\u8bbe\u7f6e CFG \u503c\u907f\u514d\u8fc7\u5ea6\u62df\u5408 \ud83d\udd27 \u6280\u672f\u89c4\u683c ### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 12GB 16GB+ \u7cfb\u7edf\u5185\u5b58 16GB 32GB+ \u5b58\u50a8\u7a7a\u95f4 20GB 50GB+ SSD \u63a8\u8350 GPU RTX 4060 Ti RTX 4080 / RTX 4090 ### \ud83c\udfa8 \u652f\u6301\u7684\u7f16\u8f91\u7c7b\u578b \u6587\u5b57\u7f16\u8f91 \u98ce\u683c\u8fc1\u79fb \u7269\u4f53\u66ff\u6362 \u573a\u666f\u91cd\u6784 \u8272\u5f69\u8c03\u6574 \u7eb9\u7406\u4fee\u6539 ### \ud83c\udf10 \u8bed\u8a00\u652f\u6301 \ud83d\udd24 \u6587\u5b57\u7f16\u8f91\u652f\u6301 \u4e2d\u6587\u3001\u82f1\u6587\u53cc\u8bed\u6587\u5b57\u7684\u7cbe\u51c6\u7f16\u8f91\u548c\u66ff\u6362 \ud83d\udcac \u6307\u4ee4\u8bed\u8a00 \u652f\u6301\u4e2d\u82f1\u6587\u7f16\u8f91\u6307\u4ee4\uff0c\u7406\u89e3\u590d\u6742\u7684\u7f16\u8f91\u9700\u6c42 \u270f\ufe0f Qwen-Image-Edit \u56fe\u50cf\u7f16\u8f91 | \u7cbe\u51c6\u6587\u5b57\u7f16\u8f91\u4e0e\u8bed\u4e49\u5916\u89c2\u53cc\u91cd\u7f16\u8f91\u7684\u5b8c\u7f8e\u7ed3\u5408 \u00a9 2025 \u963f\u91cc\u5df4\u5df4\u901a\u4e49\u5343\u95ee\u56e2\u961f | \u5f00\u6e90\u534f\u8bae | \u8ba9\u56fe\u50cf\u7f16\u8f91\u53d8\u5f97\u667a\u80fd\u9ad8\u6548","title":"Index"},{"location":"qwen-image-edit/doc/#qwen-image-edit","text":"**Qwen-Image-Edit** \u662f Qwen-Image \u7684\u56fe\u50cf\u7f16\u8f91\u7248\u672c\u3002\u5b83\u57fa\u4e8e 20B \u7684 Qwen-Image \u6a21\u578b\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff0c\u6210\u529f\u5c06 Qwen-Image \u7684\u6587\u672c\u6e32\u67d3\u7279\u8272\u80fd\u529b\u62d3\u5c55\u5230\u7f16\u8f91\u4efb\u52a1\u4e0a\uff0c\u4ee5\u652f\u6301\u7cbe\u51c6\u7684\u6587\u5b57\u7f16\u8f91\u3002\u6b64\u5916\uff0cQwen-Image-Edit \u5c06\u8f93\u5165\u56fe\u50cf\u540c\u65f6\u8f93\u5165\u5230 Qwen2.5-VL\uff08\u83b7\u53d6\u89c6\u89c9\u8bed\u4e49\u63a7\u5236\uff09\u548c VAE Encoder\uff08\u83b7\u5f97\u89c6\u89c9\u5916\u89c2\u63a7\u5236\uff09\uff0c\u4ee5\u540c\u65f6\u83b7\u5f97\u8bed\u4e49/\u5916\u89c2\u53cc\u91cd\u7f16\u8f91\u80fd\u529b\u3002 \ud83d\udcdd","title":"\ud83d\udccb Qwen-Image-Edit \u6a21\u578b\u6982\u89c8"},{"location":"qwen-image-edit/doc/#qwen-image-edit-comfyui","text":"","title":"\ud83d\ude80 Qwen-Image-Edit ComfyUI \u539f\u751f\u5de5\u4f5c\u6d41"},{"location":"qwen-image-edit/doc/#_1","text":"\u6216\u76f4\u63a5\u4ece\u5b98\u65b9\u6a21\u7248\u6253\u5f00\uff1a ![img.png](img.png) \u66f4\u65b0 ComfyUI \u540e\u53ef\u4ee5\u4ece\u6a21\u677f\u4e2d\u627e\u5230\u5de5\u4f5c\u6d41\u6587\u4ef6\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u7684\u5de5\u4f5c\u6d41\u62d6\u5165 ComfyUI \u4e2d\u52a0\u8f7d\u3002 \u70b9\u51fb\u56fe\u7247\u4e0b\u8f7d\uff0c\u62d6\u5165 ComfyUI \u52a0\u8f7d\u5de5\u4f5c\u6d41 \ud83d\udcc4 \u4e0b\u8f7d JSON \u683c\u5f0f\u5de5\u4f5c\u6d41 ### \ud83d\udcc1 \u793a\u4f8b\u8f93\u5165\u56fe\u7247","title":"\ud83d\udce5 \u6b65\u9aa4\u4e00\uff1a\u5de5\u4f5c\u6d41\u6587\u4ef6\u4e0b\u8f7d"},{"location":"qwen-image-edit/doc/#_2","text":"","title":"\ud83d\udd17 \u6b65\u9aa4\u4e8c\uff1a\u6a21\u578b\u6587\u4ef6"},{"location":"qwen-image-edit/doc/#_3","text":"\ud83d\udcc2 ComfyUI/ \u251c\u2500\u2500 \ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_edit_fp8_e4m3fn.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 loras/ \u2502 \u2502 \u2514\u2500\u2500 Qwen-Image-Lightning-4steps-V1.0.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 vae/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_vae.safetensors \u2502 \u2514\u2500\u2500 \ud83d\udcc2 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors","title":"\ud83d\udcc2 \u6a21\u578b\u6587\u4ef6\u7ed3\u6784"},{"location":"qwen-image-edit/doc/#_4","text":"#### \ud83d\udccb \u8be6\u7ec6\u914d\u7f6e\u6b65\u9aa4","title":"\ud83d\udd27 \u6b65\u9aa4\u4e09\uff1a\u5de5\u4f5c\u6d41\u914d\u7f6e\u64cd\u4f5c"},{"location":"qwen-image-edit/doc/#api","text":"\ud83d\udccb \u70b9\u51fb\u5c55\u5f00ComfyUI API\u8c03\u7528Python\u4ee3\u7801 import requests import json import uuid import time import random import os # Configuration Parameters - Qwen Image Edit Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration UNET_MODEL = \"qwen_image_edit_fp8_e4m3fn.safetensors\" CLIP_MODEL = \"qwen_2.5_vl_7b_fp8_scaled.safetensors\" VAE_MODEL = \"qwen_image_vae.safetensors\" # Default Parameters DEFAULT_EDIT_PROMPT = \"Remove all UI text elements from the image. Keep the feeling that the characters and scene are in water. Also, remove the green UI elements at the bottom.\" DEFAULT_NEGATIVE_PROMPT = \"\" DEFAULT_INPUT_IMAGE = \"Qwen-Image_00043_.png\" class ComfyUIQwenImageEditClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def edit_image_with_qwen(self, edit_prompt, input_image_path=None, input_image_name=None, negative_prompt=\"\", steps=20, cfg=2.5, seed=None, shift=3.0, cfg_norm_strength=1.0, megapixels=1.0, upscale_method=\"lanczos\"): \"\"\"Edit image using Qwen Image Edit model based on original JSON workflow\"\"\" print(\"Starting Qwen Image Edit generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle input image if input_image_path and not input_image_name: input_image_name = self.upload_image(input_image_path) if not input_image_name: raise Exception(\"Failed to upload input image\") elif not input_image_name: input_image_name = DEFAULT_INPUT_IMAGE # Workflow based on your provided JSON workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"75\", 0], \"positive\": [\"76\", 0], \"negative\": [\"77\", 0], \"latent_image\": [\"88\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"qwen_image\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"60\": { \"inputs\": { \"filename_prefix\": \"qwen_image_edit\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"66\": { \"inputs\": { \"shift\": shift, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingAuraFlow\", \"_meta\": {\"title\": \"Model Sampling AuraFlow\"} }, \"75\": { \"inputs\": { \"strength\": cfg_norm_strength, \"model\": [\"66\", 0] }, \"class_type\": \"CFGNorm\", \"_meta\": {\"title\": \"CFG Norm\"} }, \"76\": { \"inputs\": { \"prompt\": edit_prompt, \"clip\": [\"38\", 0], \"vae\": [\"39\", 0], \"image\": [\"93\", 0] }, \"class_type\": \"TextEncodeQwenImageEdit\", \"_meta\": {\"title\": \"Text Encode Qwen Image Edit (Positive)\"} }, \"77\": { \"inputs\": { \"prompt\": negative_prompt, \"clip\": [\"38\", 0], \"vae\": [\"39\", 0], \"image\": [\"93\", 0] }, \"class_type\": \"TextEncodeQwenImageEdit\", \"_meta\": {\"title\": \"Text Encode Qwen Image Edit (Negative)\"} }, \"78\": { \"inputs\": { \"image\": input_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Input Image\"} }, \"88\": { \"inputs\": { \"pixels\": [\"93\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEEncode\", \"_meta\": {\"title\": \"VAE Encode\"} }, \"93\": { \"inputs\": { \"upscale_method\": upscale_method, \"megapixels\": megapixels, \"image\": [\"78\", 0] }, \"class_type\": \"ImageScaleToTotalPixels\", \"_meta\": {\"title\": \"Image Scale To Total Pixels\"} } } print(\"Submitting Qwen Image Edit workflow...\") print(f\"Edit Prompt: {edit_prompt}\") print(f\"Input Image: {input_image_name}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Megapixels: {megapixels}\") print(f\"Upscale Method: {upscale_method}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, edit_configs, **kwargs): \"\"\"Batch edit images with different configurations\"\"\" results = [] for i, config in enumerate(edit_configs): print(f\"\\nStarting image edit task {i+1}/{len(edit_configs)}...\") try: task_id, seed = self.edit_image_with_qwen(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def edit_with_quality_presets(self, edit_prompt, input_image_path, quality_preset=\"high\"): \"\"\"Edit image with predefined quality settings\"\"\" quality_presets = { \"fast\": { \"steps\": 10, \"cfg\": 2.0, \"megapixels\": 0.5, \"upscale_method\": \"nearest-exact\" }, \"balanced\": { \"steps\": 15, \"cfg\": 2.5, \"megapixels\": 1.0, \"upscale_method\": \"bilinear\" }, \"high\": { \"steps\": 20, \"cfg\": 3.0, \"megapixels\": 1.5, \"upscale_method\": \"lanczos\" }, \"ultra\": { \"steps\": 30, \"cfg\": 3.5, \"megapixels\": 2.0, \"upscale_method\": \"lanczos\" } } settings = quality_presets.get(quality_preset, quality_presets[\"high\"]) return self.edit_image_with_qwen( edit_prompt=edit_prompt, input_image_path=input_image_path, **settings ) def generate_edit_variations(self, base_image_path, edit_prompts): \"\"\"Generate multiple edit variations of the same image\"\"\" results = [] for i, prompt in enumerate(edit_prompts): print(f\"\\nGenerating edit variation {i+1}: {prompt[:50]}...\") try: task_id, seed = self.edit_image_with_qwen( edit_prompt=prompt, input_image_path=base_image_path ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_image(task_id) results.append({ 'prompt': prompt, 'files': files, 'seed': seed }) break elif status == \"failed\": print(f\"Edit variation {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Edit variation {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Qwen Image Edit\"\"\" client = ComfyUIQwenImageEditClient() try: print(\"Qwen Image Edit client started...\") # Single image edit example print(\"\\n=== Single Image Edit ===\") # You can provide a local image path or use existing image name input_image_path = None # Set to your image path, e.g., \"input.jpg\" task_id, seed = client.edit_image_with_qwen( edit_prompt=DEFAULT_EDIT_PROMPT, input_image_path=input_image_path, input_image_name=DEFAULT_INPUT_IMAGE, negative_prompt=DEFAULT_NEGATIVE_PROMPT, steps=20, cfg=2.5, shift=3.0, megapixels=1.0, upscale_method=\"lanczos\" ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image edit completed!\") break elif status == \"failed\": print(\"Edit failed!\") return time.sleep(5) # Download edited images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Quality preset example print(\"\\n=== Quality Preset Example ===\") # Uncomment to test different quality settings # if input_image_path: # for preset in [\"fast\", \"balanced\", \"high\"]: # print(f\"Editing with {preset} quality...\") # task_id, seed = client.edit_with_quality_presets( # \"Remove background and make it white\", input_image_path, preset # ) # # Wait and download logic here... # Edit variations example print(\"\\n=== Edit Variations Example ===\") edit_variations = [ \"Remove all text and UI elements from the image\", \"Change the background to a sunset sky\", \"Make the image black and white\", \"Add more vibrant colors to the scene\", \"Remove people from the image\" ] # Uncomment to run edit variations # if input_image_path: # variation_results = client.generate_edit_variations(input_image_path, edit_variations) # print(f\"Generated {len(variation_results)} edit variations\") # Batch edit example print(\"\\n=== Batch Edit Example ===\") batch_configs = [ { 'edit_prompt': \"Remove all UI elements and make background transparent\", 'input_image_name': DEFAULT_INPUT_IMAGE, 'steps': 15, 'cfg': 2.0 }, { 'edit_prompt': \"Change the lighting to golden hour\", 'input_image_name': DEFAULT_INPUT_IMAGE, 'steps': 20, 'cfg': 2.5 } ] # Uncomment to run batch editing # batch_results = client.generate_batch(batch_configs, megapixels=1.0) # print(f\"Batch editing completed, processed {len(batch_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main()","title":"API\u8c03\u7528"},{"location":"qwen-image-edit/doc/#_5","text":"\ud83d\udcdd","title":"\ud83c\udfaf \u7f16\u8f91\u5e94\u7528\u573a\u666f"},{"location":"qwen-image-edit/doc/#_6","text":"","title":"\ud83d\udca1 \u4f7f\u7528\u6280\u5de7\u4e0e\u5efa\u8bae"},{"location":"qwen-image-edit/doc/#_7","text":"### \ud83d\udcbb \u7cfb\u7edf\u8981\u6c42 \u914d\u7f6e\u9879 \u6700\u4f4e\u8981\u6c42 \u63a8\u8350\u914d\u7f6e GPU \u663e\u5b58 12GB 16GB+ \u7cfb\u7edf\u5185\u5b58 16GB 32GB+ \u5b58\u50a8\u7a7a\u95f4 20GB 50GB+ SSD \u63a8\u8350 GPU RTX 4060 Ti RTX 4080 / RTX 4090 ### \ud83c\udfa8 \u652f\u6301\u7684\u7f16\u8f91\u7c7b\u578b \u6587\u5b57\u7f16\u8f91 \u98ce\u683c\u8fc1\u79fb \u7269\u4f53\u66ff\u6362 \u573a\u666f\u91cd\u6784 \u8272\u5f69\u8c03\u6574 \u7eb9\u7406\u4fee\u6539 ### \ud83c\udf10 \u8bed\u8a00\u652f\u6301 \ud83d\udd24 \u6587\u5b57\u7f16\u8f91\u652f\u6301 \u4e2d\u6587\u3001\u82f1\u6587\u53cc\u8bed\u6587\u5b57\u7684\u7cbe\u51c6\u7f16\u8f91\u548c\u66ff\u6362 \ud83d\udcac \u6307\u4ee4\u8bed\u8a00 \u652f\u6301\u4e2d\u82f1\u6587\u7f16\u8f91\u6307\u4ee4\uff0c\u7406\u89e3\u590d\u6742\u7684\u7f16\u8f91\u9700\u6c42 \u270f\ufe0f Qwen-Image-Edit \u56fe\u50cf\u7f16\u8f91 | \u7cbe\u51c6\u6587\u5b57\u7f16\u8f91\u4e0e\u8bed\u4e49\u5916\u89c2\u53cc\u91cd\u7f16\u8f91\u7684\u5b8c\u7f8e\u7ed3\u5408 \u00a9 2025 \u963f\u91cc\u5df4\u5df4\u901a\u4e49\u5343\u95ee\u56e2\u961f | \u5f00\u6e90\u534f\u8bae | \u8ba9\u56fe\u50cf\u7f16\u8f91\u53d8\u5f97\u667a\u80fd\u9ad8\u6548","title":"\ud83d\udd27 \u6280\u672f\u89c4\u683c"},{"location":"qwen-image-edit/doc/index-en/","text":"\u270f\ufe0f Qwen-Image-Edit Image Editing ComfyUI Native Workflow - Precise Text Editing with Dual Semantic & Appearance Control \ud83d\udcdd Precise Text Editing \ud83c\udfa8 Semantic & Appearance \ud83c\udf10 Multilingual \ud83d\udccb Qwen-Image-Edit Model Overview **Qwen-Image-Edit** is the image editing version of Qwen-Image. Built upon the 20B Qwen-Image model with further training, it successfully extends Qwen-Image's text rendering capabilities to editing tasks, supporting precise text editing. Additionally, Qwen-Image-Edit simultaneously inputs images to Qwen2.5-VL (for visual semantic control) and VAE Encoder (for visual appearance control), achieving dual semantic/appearance editing capabilities. \ud83d\udcdd Precise Text Editing Supports Chinese and English text editing while preserving text size, font, and style \ud83c\udfa8 Dual Semantic & Appearance Editing Supports both low-level visual appearance and high-level visual semantic editing \ud83c\udfc6 SOTA Performance Achieves SOTA results across multiple public benchmarks as a powerful image generation foundation model \ud83d\udd17 Official Resources \u2022 GitHub Repository : QwenLM/Qwen-Image \u2022 Hugging Face : \ud83e\udd17 Qwen/Qwen-Image-Edit \u2022 ModelScope : ModelScope ### \ud83c\udfaf Core Editing Capabilities \ud83d\udd24 Low-Level Visual Appearance Editing Style Transfer Image Addition/Deletion/Modification Color Adjustment Texture Modification \ud83e\udde0 High-Level Visual Semantic Editing IP Creation Object Rotation Scene Reconstruction Concept Replacement \ud83c\udfa5 ComfyOrg Qwen-Image-Edit Live Stream Replay Watch the official live stream replay to learn detailed usage methods and best practices for Qwen-Image-Edit in ComfyUI. \ud83d\ude80 Qwen-Image-Edit ComfyUI Native Workflow \u26a0\ufe0f Environment Requirements \ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status \ud83d\udce5 Download Links ComfyUI Download ComfyUI Update Tutorial Workflow Templates \ud83d\udd27 Common Issues Missing nodes: Version too old or import failed Incomplete features: Using stable version instead of dev version Loading failure: Node import exception during startup \ud83d\udce5 Step 1: Download Workflow Files After updating ComfyUI, you can find the workflow files in templates, or drag the workflow below into ComfyUI to load it. Click image to download, drag into ComfyUI to load workflow \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Input Image \ud83d\uddbc\ufe0f Sample Input Image Right-click to save image for workflow testing \ud83d\udd17 Step 2: Model Files All model files can be found at Comfy-Org/Qwen-Image_ComfyUI or Comfy-Org/Qwen-Image-Edit_ComfyUI . #### \ud83d\udcc2 Model File Structure \ud83d\udcc2 ComfyUI/ \u251c\u2500\u2500 \ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_edit_fp8_e4m3fn.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 loras/ \u2502 \u2502 \u2514\u2500\u2500 Qwen-Image-Lightning-4steps-V1.0.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 vae/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_vae.safetensors \u2502 \u2514\u2500\u2500 \ud83d\udcc2 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors ### \ud83d\udd27 Step 3: Workflow Configuration Operations you can find the template from ComfyUI's workflow templates ![img_1.png](img_1.png) #### \ud83d\udccb Detailed Configuration Steps \ud83d\udd27 Model Loading Load Diffusion Model : qwen_image_edit_fp8_e4m3fn.safetensors Load CLIP : qwen_2.5_vl_7b_fp8_scaled.safetensors Load VAE : qwen_image_vae.safetensors \ud83d\udcc1 Image Loading Upload the image to be edited in the Load Image node \ud83d\udcdd Prompt Settings Set editing prompts in the CLIP Text Encoder node #### \u2699\ufe0f Advanced Configuration Options \ud83d\udcd0 Image Size Processing The Scale Image to Total Pixels node scales input images to one million total pixels Prevents quality loss from oversized inputs (e.g., 2048\u00d72048) Use Ctrl+B to bypass if you know your input image size \u26a1 Lightning LoRA Acceleration Use 4-step Lightning LoRA for faster image generation Select the LoraLoaderModelOnly node Press Ctrl+B to enable the node #### \ud83c\udf9b\ufe0f KSampler Parameter Tuning \ud83d\udd27 KSampler Node Parameter Settings For steps and cfg settings, the workflow provides parameter suggestion notes below the node for testing optimal settings: \u2022 Steps : Recommended 20-50 steps \u2022 CFG : Recommended 3.5-7.5 \u2022 Sampler : Recommended DPM++ 2M Karras #### \ud83d\ude80 Execute Generation \u2328\ufe0f Click Queue button or use shortcut Ctrl(Cmd) + Enter to run workflow ## API Execute \ud83d\udccb ComfyUI API Python import requests import json import uuid import time import random import os # Configuration Parameters - Qwen Image Edit Specific COMFYUI_SERVER = \"127.0.0.1:8188\" # Local server COMFYUI_TOKEN = \"\" # Usually no token needed for local # Model Configuration UNET_MODEL = \"qwen_image_edit_fp8_e4m3fn.safetensors\" CLIP_MODEL = \"qwen_2.5_vl_7b_fp8_scaled.safetensors\" VAE_MODEL = \"qwen_image_vae.safetensors\" # Default Parameters DEFAULT_EDIT_PROMPT = \"Remove all UI text elements from the image. Keep the feeling that the characters and scene are in water. Also, remove the green UI elements at the bottom.\" DEFAULT_NEGATIVE_PROMPT = \"\" DEFAULT_INPUT_IMAGE = \"Qwen-Image_00043_.png\" class ComfyUIQwenImageEditClient: def __init__(self, server=COMFYUI_SERVER, token=COMFYUI_TOKEN): self.base_url = f\"http://{server}\" self.token = token self.client_id = str(uuid.uuid4()) self.headers = {\"Content-Type\": \"application/json\"} if token: self.headers[\"Authorization\"] = f\"Bearer {token}\" def upload_image(self, image_path): \"\"\"Upload image to ComfyUI server\"\"\" try: with open(image_path, 'rb') as f: files = {'image': f} response = requests.post(f\"{self.base_url}/upload/image\", files=files) if response.status_code == 200: result = response.json() return result.get('name', os.path.basename(image_path)) else: raise Exception(f\"Failed to upload image: {response.text}\") except Exception as e: print(f\"Image upload error: {e}\") return None def edit_image_with_qwen(self, edit_prompt, input_image_path=None, input_image_name=None, negative_prompt=\"\", steps=20, cfg=2.5, seed=None, shift=3.0, cfg_norm_strength=1.0, megapixels=1.0, upscale_method=\"lanczos\"): \"\"\"Edit image using Qwen Image Edit model based on original JSON workflow\"\"\" print(\"Starting Qwen Image Edit generation...\") # Generate random seed if not provided if seed is None: seed = random.randint(1, 1000000000000000) # Handle input image if input_image_path and not input_image_name: input_image_name = self.upload_image(input_image_path) if not input_image_name: raise Exception(\"Failed to upload input image\") elif not input_image_name: input_image_name = DEFAULT_INPUT_IMAGE # Workflow based on your provided JSON workflow = { \"3\": { \"inputs\": { \"seed\": seed, \"steps\": steps, \"cfg\": cfg, \"sampler_name\": \"euler\", \"scheduler\": \"simple\", \"denoise\": 1, \"model\": [\"75\", 0], \"positive\": [\"76\", 0], \"negative\": [\"77\", 0], \"latent_image\": [\"88\", 0] }, \"class_type\": \"KSampler\", \"_meta\": {\"title\": \"K Sampler\"} }, \"8\": { \"inputs\": { \"samples\": [\"3\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEDecode\", \"_meta\": {\"title\": \"VAE Decode\"} }, \"37\": { \"inputs\": { \"unet_name\": UNET_MODEL, \"weight_dtype\": \"default\" }, \"class_type\": \"UNETLoader\", \"_meta\": {\"title\": \"UNet Loader\"} }, \"38\": { \"inputs\": { \"clip_name\": CLIP_MODEL, \"type\": \"qwen_image\", \"device\": \"default\" }, \"class_type\": \"CLIPLoader\", \"_meta\": {\"title\": \"CLIP Loader\"} }, \"39\": { \"inputs\": { \"vae_name\": VAE_MODEL }, \"class_type\": \"VAELoader\", \"_meta\": {\"title\": \"VAE Loader\"} }, \"60\": { \"inputs\": { \"filename_prefix\": \"qwen_image_edit\", \"images\": [\"8\", 0] }, \"class_type\": \"SaveImage\", \"_meta\": {\"title\": \"Save Image\"} }, \"66\": { \"inputs\": { \"shift\": shift, \"model\": [\"37\", 0] }, \"class_type\": \"ModelSamplingAuraFlow\", \"_meta\": {\"title\": \"Model Sampling AuraFlow\"} }, \"75\": { \"inputs\": { \"strength\": cfg_norm_strength, \"model\": [\"66\", 0] }, \"class_type\": \"CFGNorm\", \"_meta\": {\"title\": \"CFG Norm\"} }, \"76\": { \"inputs\": { \"prompt\": edit_prompt, \"clip\": [\"38\", 0], \"vae\": [\"39\", 0], \"image\": [\"93\", 0] }, \"class_type\": \"TextEncodeQwenImageEdit\", \"_meta\": {\"title\": \"Text Encode Qwen Image Edit (Positive)\"} }, \"77\": { \"inputs\": { \"prompt\": negative_prompt, \"clip\": [\"38\", 0], \"vae\": [\"39\", 0], \"image\": [\"93\", 0] }, \"class_type\": \"TextEncodeQwenImageEdit\", \"_meta\": {\"title\": \"Text Encode Qwen Image Edit (Negative)\"} }, \"78\": { \"inputs\": { \"image\": input_image_name }, \"class_type\": \"LoadImage\", \"_meta\": {\"title\": \"Load Input Image\"} }, \"88\": { \"inputs\": { \"pixels\": [\"93\", 0], \"vae\": [\"39\", 0] }, \"class_type\": \"VAEEncode\", \"_meta\": {\"title\": \"VAE Encode\"} }, \"93\": { \"inputs\": { \"upscale_method\": upscale_method, \"megapixels\": megapixels, \"image\": [\"78\", 0] }, \"class_type\": \"ImageScaleToTotalPixels\", \"_meta\": {\"title\": \"Image Scale To Total Pixels\"} } } print(\"Submitting Qwen Image Edit workflow...\") print(f\"Edit Prompt: {edit_prompt}\") print(f\"Input Image: {input_image_name}\") print(f\"Steps: {steps}\") print(f\"CFG: {cfg}\") print(f\"Seed: {seed}\") print(f\"Megapixels: {megapixels}\") print(f\"Upscale Method: {upscale_method}\") response = requests.post( f\"{self.base_url}/prompt\", headers=self.headers, json={\"prompt\": workflow, \"client_id\": self.client_id} ) print(f\"API Response: {response.text}\") if response.status_code != 200: raise Exception(f\"API request failed with status code: {response.status_code}\") result = response.json() if \"error\" in result: raise Exception(f\"Workflow error: {result['error']}\") if \"prompt_id\" not in result: raise Exception(f\"No prompt_id in response: {result}\") return result[\"prompt_id\"], seed def get_status(self, task_id): \"\"\"Get task status\"\"\" try: # Check queue status queue_data = requests.get(f\"{self.base_url}/queue\", headers=self.headers).json() # Check if in running queue if any(item[1] == task_id for item in queue_data.get(\"queue_running\", [])): return \"processing\" # Check if in pending queue if any(item[1] == task_id for item in queue_data.get(\"queue_pending\", [])): return \"pending\" # Check history history_response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) if history_response.status_code == 200: history = history_response.json() if task_id in history: return \"completed\" return \"processing\" except Exception as e: print(f\"Status check error: {e}\") return \"processing\" def download_image(self, task_id, output_dir=\"outputs\"): \"\"\"Download generated images\"\"\" try: # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) response = requests.get(f\"{self.base_url}/history/{task_id}\", headers=self.headers) history = response.json() if task_id in history: outputs = history[task_id]['outputs'] downloaded_files = [] for node_id, output in outputs.items(): if 'images' in output: for img_info in output['images']: filename = img_info['filename'] # Download image img_response = requests.get( f\"{self.base_url}/view?filename={filename}\", headers=self.headers ) if img_response.status_code == 200: output_path = os.path.join(output_dir, filename) with open(output_path, \"wb\") as f: f.write(img_response.content) downloaded_files.append(output_path) print(f\"Image saved: {output_path}\") return downloaded_files except Exception as e: print(f\"Download error: {e}\") return [] def generate_batch(self, edit_configs, **kwargs): \"\"\"Batch edit images with different configurations\"\"\" results = [] for i, config in enumerate(edit_configs): print(f\"\\nStarting image edit task {i+1}/{len(edit_configs)}...\") try: task_id, seed = self.edit_image_with_qwen(**config, **kwargs) # Wait for completion while True: status = self.get_status(task_id) print(f\"Task {i+1} status: {status}\") if status == \"completed\": files = self.download_image(task_id) results.append({ 'task_id': task_id, 'seed': seed, 'files': files, 'config': config }) break elif status == \"failed\": print(f\"Task {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Task {i+1} error: {e}\") return results def edit_with_quality_presets(self, edit_prompt, input_image_path, quality_preset=\"high\"): \"\"\"Edit image with predefined quality settings\"\"\" quality_presets = { \"fast\": { \"steps\": 10, \"cfg\": 2.0, \"megapixels\": 0.5, \"upscale_method\": \"nearest-exact\" }, \"balanced\": { \"steps\": 15, \"cfg\": 2.5, \"megapixels\": 1.0, \"upscale_method\": \"bilinear\" }, \"high\": { \"steps\": 20, \"cfg\": 3.0, \"megapixels\": 1.5, \"upscale_method\": \"lanczos\" }, \"ultra\": { \"steps\": 30, \"cfg\": 3.5, \"megapixels\": 2.0, \"upscale_method\": \"lanczos\" } } settings = quality_presets.get(quality_preset, quality_presets[\"high\"]) return self.edit_image_with_qwen( edit_prompt=edit_prompt, input_image_path=input_image_path, **settings ) def generate_edit_variations(self, base_image_path, edit_prompts): \"\"\"Generate multiple edit variations of the same image\"\"\" results = [] for i, prompt in enumerate(edit_prompts): print(f\"\\nGenerating edit variation {i+1}: {prompt[:50]}...\") try: task_id, seed = self.edit_image_with_qwen( edit_prompt=prompt, input_image_path=base_image_path ) # Wait for completion while True: status = self.get_status(task_id) if status == \"completed\": files = self.download_image(task_id) results.append({ 'prompt': prompt, 'files': files, 'seed': seed }) break elif status == \"failed\": print(f\"Edit variation {i+1} failed\") break time.sleep(5) except Exception as e: print(f\"Edit variation {i+1} error: {e}\") return results def main(): \"\"\"Main function - Execute Qwen Image Edit\"\"\" client = ComfyUIQwenImageEditClient() try: print(\"Qwen Image Edit client started...\") # Single image edit example print(\"\\n=== Single Image Edit ===\") # You can provide a local image path or use existing image name input_image_path = None # Set to your image path, e.g., \"input.jpg\" task_id, seed = client.edit_image_with_qwen( edit_prompt=DEFAULT_EDIT_PROMPT, input_image_path=input_image_path, input_image_name=DEFAULT_INPUT_IMAGE, negative_prompt=DEFAULT_NEGATIVE_PROMPT, steps=20, cfg=2.5, shift=3.0, megapixels=1.0, upscale_method=\"lanczos\" ) print(f\"Task ID: {task_id}\") print(f\"Seed: {seed}\") # Wait for task completion while True: status = client.get_status(task_id) print(f\"Current status: {status}\") if status == \"completed\": print(\"Image edit completed!\") break elif status == \"failed\": print(\"Edit failed!\") return time.sleep(5) # Download edited images downloaded_files = client.download_image(task_id) if downloaded_files: print(f\"Successfully downloaded {len(downloaded_files)} files!\") for file in downloaded_files: print(f\"File path: {file}\") else: print(\"Download failed\") # Quality preset example print(\"\\n=== Quality Preset Example ===\") # Uncomment to test different quality settings # if input_image_path: # for preset in [\"fast\", \"balanced\", \"high\"]: # print(f\"Editing with {preset} quality...\") # task_id, seed = client.edit_with_quality_presets( # \"Remove background and make it white\", input_image_path, preset # ) # # Wait and download logic here... # Edit variations example print(\"\\n=== Edit Variations Example ===\") edit_variations = [ \"Remove all text and UI elements from the image\", \"Change the background to a sunset sky\", \"Make the image black and white\", \"Add more vibrant colors to the scene\", \"Remove people from the image\" ] # Uncomment to run edit variations # if input_image_path: # variation_results = client.generate_edit_variations(input_image_path, edit_variations) # print(f\"Generated {len(variation_results)} edit variations\") # Batch edit example print(\"\\n=== Batch Edit Example ===\") batch_configs = [ { 'edit_prompt': \"Remove all UI elements and make background transparent\", 'input_image_name': DEFAULT_INPUT_IMAGE, 'steps': 15, 'cfg': 2.0 }, { 'edit_prompt': \"Change the lighting to golden hour\", 'input_image_name': DEFAULT_INPUT_IMAGE, 'steps': 20, 'cfg': 2.5 } ] # Uncomment to run batch editing # batch_results = client.generate_batch(batch_configs, megapixels=1.0) # print(f\"Batch editing completed, processed {len(batch_results)} images\") except Exception as e: print(f\"Error: {e}\") if __name__ == \"__main__\": main() ## \ud83c\udfaf Editing Application Scenarios \ud83d\udcdd Text Editing Poster text modification, logo replacement, multilingual localization \ud83c\udfa8 Style Transfer Artistic style transfer, tone adjustment, visual effect enhancement \ud83c\udfe2 Commercial Applications Product image editing, advertising material creation, brand visual consistency \ud83c\udfad Creative Design Concept art creation, IP character design, scene reconstruction ## \ud83d\udca1 Usage Tips and Recommendations \u2705 Best Practices Use clear, specific editing instructions Maintain appropriate input image quality and resolution Describe specific font and style requirements for text editing Use Lightning LoRA for quick previews \u26a0\ufe0f Important Notes Avoid oversized input images that affect generation quality Complex editing tasks may require multiple iterations Text editing effectiveness depends on original text clarity Set CFG values reasonably to avoid overfitting ## \ud83d\udd27 Technical Specifications ### \ud83d\udcbb System Requirements Component Minimum Recommended GPU VRAM 12GB 16GB+ System RAM 16GB 32GB+ Storage Space 20GB 50GB+ SSD Recommended GPU RTX 4060 Ti RTX 4080 / RTX 4090 ### \ud83c\udfa8 Supported Editing Types Text Editing Style Transfer Object Replacement Scene Reconstruction Color Adjustment Texture Modification ### \ud83c\udf10 Language Support \ud83d\udd24 Text Editing Support Precise editing and replacement of Chinese and English text \ud83d\udcac Instruction Language Supports Chinese and English editing instructions, understands complex editing requirements ### \ud83c\udfaf Performance Benchmarks \ud83c\udfc6 SOTA Results Achieves state-of-the-art performance across multiple public benchmarks \u26a1 Generation Speed 4-step Lightning LoRA enables 5x faster generation with minimal quality loss \ud83c\udfaf Editing Precision Maintains original text style, font, and size during precise text editing --- \u270f\ufe0f Qwen-Image-Edit Image Editing | Perfect Combination of Precise Text Editing and Dual Semantic & Appearance Control \u00a9 2025 Alibaba Qwen Team | Open Source License | Making Image Editing Intelligent and Efficient","title":"Index en"},{"location":"qwen-image-edit/doc/index-en/#qwen-image-edit-model-overview","text":"**Qwen-Image-Edit** is the image editing version of Qwen-Image. Built upon the 20B Qwen-Image model with further training, it successfully extends Qwen-Image's text rendering capabilities to editing tasks, supporting precise text editing. Additionally, Qwen-Image-Edit simultaneously inputs images to Qwen2.5-VL (for visual semantic control) and VAE Encoder (for visual appearance control), achieving dual semantic/appearance editing capabilities. \ud83d\udcdd","title":"\ud83d\udccb Qwen-Image-Edit Model Overview"},{"location":"qwen-image-edit/doc/index-en/#comfyorg-qwen-image-edit-live-stream-replay","text":"Watch the official live stream replay to learn detailed usage methods and best practices for Qwen-Image-Edit in ComfyUI.","title":"\ud83c\udfa5 ComfyOrg Qwen-Image-Edit Live Stream Replay"},{"location":"qwen-image-edit/doc/index-en/#qwen-image-edit-comfyui-native-workflow","text":"","title":"\ud83d\ude80 Qwen-Image-Edit ComfyUI Native Workflow"},{"location":"qwen-image-edit/doc/index-en/#environment-requirements","text":"\ud83d\udccb Pre-usage Checklist \u2022 Ensure ComfyUI is updated to the latest version \u2022 Recommend using the latest development version (nightly) for full functionality \u2022 The workflow in this guide can be found in ComfyUI's workflow templates \u2022 If nodes are missing when loading the workflow, check ComfyUI version or node import status","title":"\u26a0\ufe0f Environment Requirements"},{"location":"qwen-image-edit/doc/index-en/#step-1-download-workflow-files","text":"After updating ComfyUI, you can find the workflow files in templates, or drag the workflow below into ComfyUI to load it. Click image to download, drag into ComfyUI to load workflow \ud83d\udcc4 Download JSON Workflow File ### \ud83d\udcc1 Sample Input Image","title":"\ud83d\udce5 Step 1: Download Workflow Files"},{"location":"qwen-image-edit/doc/index-en/#step-2-model-files","text":"All model files can be found at Comfy-Org/Qwen-Image_ComfyUI or Comfy-Org/Qwen-Image-Edit_ComfyUI . #### \ud83d\udcc2 Model File Structure \ud83d\udcc2 ComfyUI/ \u251c\u2500\u2500 \ud83d\udcc2 models/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_edit_fp8_e4m3fn.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 loras/ \u2502 \u2502 \u2514\u2500\u2500 Qwen-Image-Lightning-4steps-V1.0.safetensors \u2502 \u251c\u2500\u2500 \ud83d\udcc2 vae/ \u2502 \u2502 \u2514\u2500\u2500 qwen_image_vae.safetensors \u2502 \u2514\u2500\u2500 \ud83d\udcc2 text_encoders/ \u2502 \u2514\u2500\u2500 qwen_2.5_vl_7b_fp8_scaled.safetensors ### \ud83d\udd27 Step 3: Workflow Configuration Operations you can find the template from ComfyUI's workflow templates ![img_1.png](img_1.png) #### \ud83d\udccb Detailed Configuration Steps","title":"\ud83d\udd17 Step 2: Model Files"},{"location":"qwen3-embedding/","text":"\u7b80\u4ecb Qwen3 Embedding \u6a21\u578b\u7cfb\u5217\u662f Qwen \u5bb6\u65cf\u7684\u6700\u65b0\u4e13\u6709\u6a21\u578b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6587\u672c\u5d4c\u5165\u548c\u6392\u5e8f\u4efb\u52a1\u3002\u57fa\u4e8e Qwen3 \u7cfb\u5217\u7684\u5bc6\u96c6\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u63d0\u4f9b\u4e86\u5404\u79cd\u5927\u5c0f\uff080.6B\u30014B \u548c 8B\uff09\u7684\u5168\u9762\u6587\u672c\u5d4c\u5165\u548c\u91cd\u6392\u5e8f\u6a21\u578b\u3002\u8be5\u7cfb\u5217\u7ee7\u627f\u4e86\u5176\u57fa\u7840\u6a21\u578b\u5353\u8d8a\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3001\u957f\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u6280\u80fd\u3002Qwen3 Embedding \u7cfb\u5217\u8868\u73b0\u51fa\u4e86\u5728\u591a\u79cd\u6587\u672c\u5d4c\u5165\u548c\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u5305\u62ec\u6587\u672c\u68c0\u7d22\u3001\u4ee3\u7801\u68c0\u7d22\u3001\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u805a\u7c7b\u548c\u53cc\u8bed\u6587\u672c\u6316\u6398\u3002 \u4f7f\u7528\u8bf4\u660e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86Api\u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548cApiKey\uff0c\u4e0b\u9762\u4f1a\u5206\u522b\u4ecb\u7ecd\u5982\u4f55\u8bbf\u95ee\u4f7f\u7528\u3002 API\u8c03\u7528 Curl\u547d\u4ee4\u8c03\u7528 Curl\u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684Api\u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578bAPI\u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a \u5176\u4e2d${ServerIP}\u53ef\u4ee5\u586b\u5199\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684IP\u5730\u5740\uff0c${ApiKey}\u4e3aApiKey\uff0c${ModelName}\u4e3a\u6a21\u578b\u540d\u79f0\u3002 curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"input\": [ \"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", \"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", \"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\" ] }' Python\u8c03\u7528 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a \u5176\u4e2d${ApiKey}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684ApiKey\uff1b${ServerUrl}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u4e0d\u9700\u8981\u5e26\u4e0a/v1\u3002 import requests import json import math url = '${ServerUrl}/v1/embeddings' token = '${ApiKey}' def cosine_similarity(v1, v2): dot_product = sum(x * y for x, y in zip(v1, v2)) norm_v1 = math.sqrt(sum(x**2 for x in v1)) norm_v2 = math.sqrt(sum(y**2 for y in v2)) if norm_v1 == 0 or norm_v2 == 0: return 0.0 return dot_product / (norm_v1 * norm_v2) request_body = { \"input\": [ \"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", \"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", \"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\" ], \"model\": \"Qwen3-Embedding-8B\", # \"dimensions\": 4096 } headers = {\"Authorization\": token} resp = requests.post(url=url, headers=headers, json=request_body) embeddings_data = json.loads(resp.content.decode())[\"data\"] embeddings_0 = embeddings_data[0][\"embedding\"] embeddings_1 = embeddings_data[1][\"embedding\"] embeddings_2 = embeddings_data[2][\"embedding\"] sim_0_1 = cosine_similarity(embeddings_0, embeddings_1) sim_0_2 = cosine_similarity(embeddings_0, embeddings_2) sim_1_2 = cosine_similarity(embeddings_1, embeddings_2) print(f\"similarity({embeddings_data[0][\"index\"]},{embeddings_data[1][\"index\"]}): {sim_0_1:.6f}\") print(f\"similarity({embeddings_data[0][\"index\"]},{embeddings_data[2][\"index\"]}): {sim_0_2:.6f}\") print(f\"similarity({embeddings_data[1][\"index\"]},{embeddings_data[2][\"index\"]}): {sim_1_2:.6f}\")","title":"Index"},{"location":"qwen3-embedding/#_1","text":"Qwen3 Embedding \u6a21\u578b\u7cfb\u5217\u662f Qwen \u5bb6\u65cf\u7684\u6700\u65b0\u4e13\u6709\u6a21\u578b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6587\u672c\u5d4c\u5165\u548c\u6392\u5e8f\u4efb\u52a1\u3002\u57fa\u4e8e Qwen3 \u7cfb\u5217\u7684\u5bc6\u96c6\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u63d0\u4f9b\u4e86\u5404\u79cd\u5927\u5c0f\uff080.6B\u30014B \u548c 8B\uff09\u7684\u5168\u9762\u6587\u672c\u5d4c\u5165\u548c\u91cd\u6392\u5e8f\u6a21\u578b\u3002\u8be5\u7cfb\u5217\u7ee7\u627f\u4e86\u5176\u57fa\u7840\u6a21\u578b\u5353\u8d8a\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3001\u957f\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u6280\u80fd\u3002Qwen3 Embedding \u7cfb\u5217\u8868\u73b0\u51fa\u4e86\u5728\u591a\u79cd\u6587\u672c\u5d4c\u5165\u548c\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u5305\u62ec\u6587\u672c\u68c0\u7d22\u3001\u4ee3\u7801\u68c0\u7d22\u3001\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u805a\u7c7b\u548c\u53cc\u8bed\u6587\u672c\u6316\u6398\u3002","title":"\u7b80\u4ecb"},{"location":"qwen3-embedding/#_2","text":"\u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86Api\u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548cApiKey\uff0c\u4e0b\u9762\u4f1a\u5206\u522b\u4ecb\u7ecd\u5982\u4f55\u8bbf\u95ee\u4f7f\u7528\u3002","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"qwen3-embedding/#api","text":"","title":"API\u8c03\u7528"},{"location":"qwen3-embedding/#curl","text":"Curl\u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684Api\u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578bAPI\u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a \u5176\u4e2d${ServerIP}\u53ef\u4ee5\u586b\u5199\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684IP\u5730\u5740\uff0c${ApiKey}\u4e3aApiKey\uff0c${ModelName}\u4e3a\u6a21\u578b\u540d\u79f0\u3002 curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"input\": [ \"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", \"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", \"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\" ] }'","title":"Curl\u547d\u4ee4\u8c03\u7528"},{"location":"qwen3-embedding/#python","text":"\u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a \u5176\u4e2d${ApiKey}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684ApiKey\uff1b${ServerUrl}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u4e0d\u9700\u8981\u5e26\u4e0a/v1\u3002 import requests import json import math url = '${ServerUrl}/v1/embeddings' token = '${ApiKey}' def cosine_similarity(v1, v2): dot_product = sum(x * y for x, y in zip(v1, v2)) norm_v1 = math.sqrt(sum(x**2 for x in v1)) norm_v2 = math.sqrt(sum(y**2 for y in v2)) if norm_v1 == 0 or norm_v2 == 0: return 0.0 return dot_product / (norm_v1 * norm_v2) request_body = { \"input\": [ \"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", \"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", \"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\" ], \"model\": \"Qwen3-Embedding-8B\", # \"dimensions\": 4096 } headers = {\"Authorization\": token} resp = requests.post(url=url, headers=headers, json=request_body) embeddings_data = json.loads(resp.content.decode())[\"data\"] embeddings_0 = embeddings_data[0][\"embedding\"] embeddings_1 = embeddings_data[1][\"embedding\"] embeddings_2 = embeddings_data[2][\"embedding\"] sim_0_1 = cosine_similarity(embeddings_0, embeddings_1) sim_0_2 = cosine_similarity(embeddings_0, embeddings_2) sim_1_2 = cosine_similarity(embeddings_1, embeddings_2) print(f\"similarity({embeddings_data[0][\"index\"]},{embeddings_data[1][\"index\"]}): {sim_0_1:.6f}\") print(f\"similarity({embeddings_data[0][\"index\"]},{embeddings_data[2][\"index\"]}): {sim_0_2:.6f}\") print(f\"similarity({embeddings_data[1][\"index\"]},{embeddings_data[2][\"index\"]}): {sim_1_2:.6f}\")","title":"Python\u8c03\u7528"},{"location":"qwen3-embedding/index-en/","text":"Qwen3 Embedding Model Usage Guide Introduction The Qwen3 Embedding model series is the latest proprietary model from the Qwen family, specifically designed for text embedding and ranking tasks. Built upon the dense foundation models of the Qwen3 series, it offers comprehensive text embedding and reranking models in various sizes (0.6B, 4B, and 8B). The series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its base models. Qwen3 Embedding demonstrates significant improvements across multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bilingual text mining. Usage Instructions After completing the model deployment, you can view the usage methods on the ComputeNest service instance overview page, which provides API call examples, private network access address, public network access address, and ApiKey. The following sections will introduce how to access and use the service. API Calls Curl Command Call You can directly use the API call example from the service instance overview page for Curl command calls. The specific structure for calling the model API is as follows: Where ${ServerIP} can be filled with the IP address from either the private or public network address, ${ApiKey} is the ApiKey, and ${ModelName} is the model name. curl -X POST http://${ServerIP}:8000/v1/embeddings \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"input\": [ \"The capital of China is Beijing.\", \"The capital of the United States is Washington.\", \"Today is Friday.\" ] }' Python Call Below is a Python example code: Where ${ApiKey} needs to be filled with the ApiKey from the page; ${ServerUrl} needs to be filled with the public or private network address from the page, without the /v1 suffix. import requests import json import math url = '${ServerUrl}/v1/embeddings' token = '${ApiKey}' def cosine_similarity(v1, v2): \"\"\"Calculate cosine similarity between two vectors\"\"\" dot_product = sum(x * y for x, y in zip(v1, v2)) norm_v1 = math.sqrt(sum(x**2 for x in v1)) norm_v2 = math.sqrt(sum(y**2 for y in v2)) if norm_v1 == 0 or norm_v2 == 0: return 0.0 return dot_product / (norm_v1 * norm_v2) # Build request body request_body = { \"input\": [ \"The capital of China is Beijing.\", \"The capital of the United States is Washington.\", \"Today is Friday.\" ], \"model\": \"Qwen3-Embedding-8B\", # \"dimensions\": 4096 # Optional: specify output dimensions } # Send request headers = {\"Authorization\": token} resp = requests.post(url=url, headers=headers, json=request_body) # Parse response embeddings_data = json.loads(resp.content.decode())[\"data\"] embeddings_0 = embeddings_data[0][\"embedding\"] embeddings_1 = embeddings_data[1][\"embedding\"] embeddings_2 = embeddings_data[2][\"embedding\"] # Calculate similarity sim_0_1 = cosine_similarity(embeddings_0, embeddings_1) sim_0_2 = cosine_similarity(embeddings_0, embeddings_2) sim_1_2 = cosine_similarity(embeddings_1, embeddings_2) # Output results print(f\"similarity({embeddings_data[0]['index']},{embeddings_data[1]['index']}): {sim_0_1:.6f}\") print(f\"similarity({embeddings_data[0]['index']},{embeddings_data[2]['index']}): {sim_0_2:.6f}\") print(f\"similarity({embeddings_data[1]['index']},{embeddings_data[2]['index']}): {sim_1_2:.6f}\")","title":"Qwen3 Embedding Model Usage Guide"},{"location":"qwen3-embedding/index-en/#qwen3-embedding-model-usage-guide","text":"","title":"Qwen3 Embedding Model Usage Guide"},{"location":"qwen3-embedding/index-en/#introduction","text":"The Qwen3 Embedding model series is the latest proprietary model from the Qwen family, specifically designed for text embedding and ranking tasks. Built upon the dense foundation models of the Qwen3 series, it offers comprehensive text embedding and reranking models in various sizes (0.6B, 4B, and 8B). The series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its base models. Qwen3 Embedding demonstrates significant improvements across multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bilingual text mining.","title":"Introduction"},{"location":"qwen3-embedding/index-en/#usage-instructions","text":"After completing the model deployment, you can view the usage methods on the ComputeNest service instance overview page, which provides API call examples, private network access address, public network access address, and ApiKey. The following sections will introduce how to access and use the service.","title":"Usage Instructions"},{"location":"qwen3-embedding/index-en/#api-calls","text":"","title":"API Calls"},{"location":"qwen3-embedding/index-en/#curl-command-call","text":"You can directly use the API call example from the service instance overview page for Curl command calls. The specific structure for calling the model API is as follows: Where ${ServerIP} can be filled with the IP address from either the private or public network address, ${ApiKey} is the ApiKey, and ${ModelName} is the model name. curl -X POST http://${ServerIP}:8000/v1/embeddings \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"input\": [ \"The capital of China is Beijing.\", \"The capital of the United States is Washington.\", \"Today is Friday.\" ] }'","title":"Curl Command Call"},{"location":"qwen3-embedding/index-en/#python-call","text":"Below is a Python example code: Where ${ApiKey} needs to be filled with the ApiKey from the page; ${ServerUrl} needs to be filled with the public or private network address from the page, without the /v1 suffix. import requests import json import math url = '${ServerUrl}/v1/embeddings' token = '${ApiKey}' def cosine_similarity(v1, v2): \"\"\"Calculate cosine similarity between two vectors\"\"\" dot_product = sum(x * y for x, y in zip(v1, v2)) norm_v1 = math.sqrt(sum(x**2 for x in v1)) norm_v2 = math.sqrt(sum(y**2 for y in v2)) if norm_v1 == 0 or norm_v2 == 0: return 0.0 return dot_product / (norm_v1 * norm_v2) # Build request body request_body = { \"input\": [ \"The capital of China is Beijing.\", \"The capital of the United States is Washington.\", \"Today is Friday.\" ], \"model\": \"Qwen3-Embedding-8B\", # \"dimensions\": 4096 # Optional: specify output dimensions } # Send request headers = {\"Authorization\": token} resp = requests.post(url=url, headers=headers, json=request_body) # Parse response embeddings_data = json.loads(resp.content.decode())[\"data\"] embeddings_0 = embeddings_data[0][\"embedding\"] embeddings_1 = embeddings_data[1][\"embedding\"] embeddings_2 = embeddings_data[2][\"embedding\"] # Calculate similarity sim_0_1 = cosine_similarity(embeddings_0, embeddings_1) sim_0_2 = cosine_similarity(embeddings_0, embeddings_2) sim_1_2 = cosine_similarity(embeddings_1, embeddings_2) # Output results print(f\"similarity({embeddings_data[0]['index']},{embeddings_data[1]['index']}): {sim_0_1:.6f}\") print(f\"similarity({embeddings_data[0]['index']},{embeddings_data[2]['index']}): {sim_0_2:.6f}\") print(f\"similarity({embeddings_data[1]['index']},{embeddings_data[2]['index']}): {sim_1_2:.6f}\")","title":"Python Call"},{"location":"qwen3-reranker/","text":"\u7b80\u4ecb Qwen3 \u5d4c\u5165\u6a21\u578b\u7cfb\u5217\u662f Qwen \u5bb6\u65cf\u7684\u6700\u65b0\u4e13\u6709\u6a21\u578b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6587\u672c\u5d4c\u5165\u548c\u6392\u540d\u4efb\u52a1\u3002\u57fa\u4e8e Qwen3 \u7cfb\u5217\u7684\u5bc6\u96c6\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u63d0\u4f9b\u4e86\u5404\u79cd\u5927\u5c0f\uff080.6B\u30014B \u548c 8B\uff09\u7684\u5168\u9762\u6587\u672c\u5d4c\u5165\u548c\u91cd\u65b0\u6392\u540d\u6a21\u578b\u3002\u8be5\u7cfb\u5217\u7ee7\u627f\u4e86\u5176\u57fa\u7840\u6a21\u578b\u51fa\u8272\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3001\u957f\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u6280\u80fd\u3002Qwen3 \u5d4c\u5165\u7cfb\u5217\u5728\u591a\u4e2a\u6587\u672c\u5d4c\u5165\u548c\u6392\u540d\u4efb\u52a1\u4e2d\u4ee3\u8868\u4e86\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u5305\u62ec\u6587\u672c\u68c0\u7d22\u3001\u4ee3\u7801\u68c0\u7d22\u3001\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u805a\u7c7b\u548c\u53cc\u8bed\u6587\u672c\u6316\u6398\u3002 \u4f7f\u7528\u8bf4\u660e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548cApiKey\uff0c\u4e0b\u9762\u4f1a\u5206\u522b\u4ecb\u7ecd\u5982\u4f55\u8bbf\u95ee\u4f7f\u7528\u3002 API\u8c03\u7528 Python\u8c03\u7528 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a \u5176\u4e2d${ApiKey}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684ApiKey\uff1b${ServerUrl}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u4e0d\u9700\u8981\u5e26\u4e0a/v1\u3002 import json import requests url = \"${ServerUrl}/v1/rerank\" token = \"${ApiKey}\" headers = {\"Authorization\": token} prefix = '<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\\n<|im_start|>user\\n' suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\" query_template = \"{prefix}<Instruct>: {instruction}\\n<Query>: {query}\\n\" document_template = \"<Document>: {doc}{suffix}\" instruction = \"Given a web search query, retrieve relevant passages that answer the query\" data = { \"model\": \"Qwen3-Reranker-8B\", \"query\": query_template.format(prefix=prefix, instruction=instruction, query=\"\u4e2d\u56fd\u9996\u90fd\u662f\u54ea\u513f?\"), \"documents\": [ document_template.format(doc=\"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", suffix=suffix), document_template.format(doc=\"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", suffix=suffix), document_template.format(doc=\"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\", suffix=suffix) ], } def main(): response = requests.post(url, headers=headers, json=data) # Check the response if response.status_code == 200: print(\"Request successful!\") print(json.dumps(response.json(), indent=2)) else: print(f\"Request failed with status code: {response.status_code}\") print(response.text) if __name__ == \"__main__\": main()","title":"Index"},{"location":"qwen3-reranker/#_1","text":"Qwen3 \u5d4c\u5165\u6a21\u578b\u7cfb\u5217\u662f Qwen \u5bb6\u65cf\u7684\u6700\u65b0\u4e13\u6709\u6a21\u578b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6587\u672c\u5d4c\u5165\u548c\u6392\u540d\u4efb\u52a1\u3002\u57fa\u4e8e Qwen3 \u7cfb\u5217\u7684\u5bc6\u96c6\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u63d0\u4f9b\u4e86\u5404\u79cd\u5927\u5c0f\uff080.6B\u30014B \u548c 8B\uff09\u7684\u5168\u9762\u6587\u672c\u5d4c\u5165\u548c\u91cd\u65b0\u6392\u540d\u6a21\u578b\u3002\u8be5\u7cfb\u5217\u7ee7\u627f\u4e86\u5176\u57fa\u7840\u6a21\u578b\u51fa\u8272\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3001\u957f\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u6280\u80fd\u3002Qwen3 \u5d4c\u5165\u7cfb\u5217\u5728\u591a\u4e2a\u6587\u672c\u5d4c\u5165\u548c\u6392\u540d\u4efb\u52a1\u4e2d\u4ee3\u8868\u4e86\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u5305\u62ec\u6587\u672c\u68c0\u7d22\u3001\u4ee3\u7801\u68c0\u7d22\u3001\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u805a\u7c7b\u548c\u53cc\u8bed\u6587\u672c\u6316\u6398\u3002","title":"\u7b80\u4ecb"},{"location":"qwen3-reranker/#_2","text":"\u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u548cApiKey\uff0c\u4e0b\u9762\u4f1a\u5206\u522b\u4ecb\u7ecd\u5982\u4f55\u8bbf\u95ee\u4f7f\u7528\u3002","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"qwen3-reranker/#api","text":"","title":"API\u8c03\u7528"},{"location":"qwen3-reranker/#python","text":"\u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a \u5176\u4e2d${ApiKey}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684ApiKey\uff1b${ServerUrl}\u9700\u8981\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u4e0d\u9700\u8981\u5e26\u4e0a/v1\u3002 import json import requests url = \"${ServerUrl}/v1/rerank\" token = \"${ApiKey}\" headers = {\"Authorization\": token} prefix = '<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\\n<|im_start|>user\\n' suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\" query_template = \"{prefix}<Instruct>: {instruction}\\n<Query>: {query}\\n\" document_template = \"<Document>: {doc}{suffix}\" instruction = \"Given a web search query, retrieve relevant passages that answer the query\" data = { \"model\": \"Qwen3-Reranker-8B\", \"query\": query_template.format(prefix=prefix, instruction=instruction, query=\"\u4e2d\u56fd\u9996\u90fd\u662f\u54ea\u513f?\"), \"documents\": [ document_template.format(doc=\"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", suffix=suffix), document_template.format(doc=\"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", suffix=suffix), document_template.format(doc=\"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\", suffix=suffix) ], } def main(): response = requests.post(url, headers=headers, json=data) # Check the response if response.status_code == 200: print(\"Request successful!\") print(json.dumps(response.json(), indent=2)) else: print(f\"Request failed with status code: {response.status_code}\") print(response.text) if __name__ == \"__main__\": main()","title":"Python\u8c03\u7528"},{"location":"qwen3-reranker/index-en/","text":"Qwen3 Reranker Model Usage Guide Introduction The Qwen3 Embedding model series is the latest proprietary model from the Qwen family, specifically designed for text embedding and ranking tasks. Built upon the dense foundation models of the Qwen3 series, it offers comprehensive text embedding and reranking models in various sizes (0.6B, 4B, and 8B). The series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its base models. The Qwen3 Embedding series represents significant advancements across multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bilingual text mining. Usage Instructions After completing the model deployment, you can view the usage methods on the ComputeNest service instance overview page, which provides private network access address, public network access address, and ApiKey. The following sections will introduce how to access and use the service. API Calls Python Call Below is a Python example code: Where ${ApiKey} needs to be filled with the ApiKey from the page; ${ServerUrl} needs to be filled with the public or private network address from the page, without the /v1 suffix. import json import requests url = \"${ServerUrl}/v1/rerank\" token = \"${ApiKey}\" headers = {\"Authorization\": token} prefix = '<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\\n<|im_start|>user\\n' suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\" query_template = \"{prefix}<Instruct>: {instruction}\\n<Query>: {query}\\n\" document_template = \"<Document>: {doc}{suffix}\" instruction = \"Given a web search query, retrieve relevant passages that answer the query\" data = { \"model\": \"Qwen3-Reranker-8B\", \"query\": query_template.format(prefix=prefix, instruction=instruction, query=\"\u4e2d\u56fd\u9996\u90fd\u662f\u54ea\u513f?\"), \"documents\": [ document_template.format(doc=\"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", suffix=suffix), document_template.format(doc=\"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", suffix=suffix), document_template.format(doc=\"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\", suffix=suffix) ], } def main(): response = requests.post(url, headers=headers, json=data) # Check the response if response.status_code == 200: print(\"Request successful!\") print(json.dumps(response.json(), indent=2)) else: print(f\"Request failed with status code: {response.status_code}\") print(response.text) if __name__ == \"__main__\": main()","title":"Qwen3 Reranker Model Usage Guide"},{"location":"qwen3-reranker/index-en/#qwen3-reranker-model-usage-guide","text":"","title":"Qwen3 Reranker Model Usage Guide"},{"location":"qwen3-reranker/index-en/#introduction","text":"The Qwen3 Embedding model series is the latest proprietary model from the Qwen family, specifically designed for text embedding and ranking tasks. Built upon the dense foundation models of the Qwen3 series, it offers comprehensive text embedding and reranking models in various sizes (0.6B, 4B, and 8B). The series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its base models. The Qwen3 Embedding series represents significant advancements across multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bilingual text mining.","title":"Introduction"},{"location":"qwen3-reranker/index-en/#usage-instructions","text":"After completing the model deployment, you can view the usage methods on the ComputeNest service instance overview page, which provides private network access address, public network access address, and ApiKey. The following sections will introduce how to access and use the service.","title":"Usage Instructions"},{"location":"qwen3-reranker/index-en/#api-calls","text":"","title":"API Calls"},{"location":"qwen3-reranker/index-en/#python-call","text":"Below is a Python example code: Where ${ApiKey} needs to be filled with the ApiKey from the page; ${ServerUrl} needs to be filled with the public or private network address from the page, without the /v1 suffix. import json import requests url = \"${ServerUrl}/v1/rerank\" token = \"${ApiKey}\" headers = {\"Authorization\": token} prefix = '<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\\n<|im_start|>user\\n' suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\" query_template = \"{prefix}<Instruct>: {instruction}\\n<Query>: {query}\\n\" document_template = \"<Document>: {doc}{suffix}\" instruction = \"Given a web search query, retrieve relevant passages that answer the query\" data = { \"model\": \"Qwen3-Reranker-8B\", \"query\": query_template.format(prefix=prefix, instruction=instruction, query=\"\u4e2d\u56fd\u9996\u90fd\u662f\u54ea\u513f?\"), \"documents\": [ document_template.format(doc=\"\u4e2d\u56fd\u9996\u90fd\u662f\u5317\u4eac\u3002\", suffix=suffix), document_template.format(doc=\"\u7f8e\u56fd\u9996\u90fd\u662f\u534e\u76db\u987f\u3002\", suffix=suffix), document_template.format(doc=\"\u4eca\u5929\u662f\u661f\u671f\u4e94\u3002\", suffix=suffix) ], } def main(): response = requests.post(url, headers=headers, json=data) # Check the response if response.status_code == 200: print(\"Request successful!\") print(json.dumps(response.json(), indent=2)) else: print(f\"Request failed with status code: {response.status_code}\") print(response.text) if __name__ == \"__main__\": main()","title":"Python Call"},{"location":"rvc/","text":"\ud83c\udfa4 RVC\u58f0\u97f3\u514b\u9686\u6280\u672f\u6307\u5357 \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e13\u4e1a\u58f0\u97f3\u5408\u6210\u4e0e\u53d8\u58f0\u89e3\u51b3\u65b9\u6848 \ud83d\udd2c \u6280\u672f\u7b80\u4ecb **RVC\u58f0\u97f3\u514b\u9686\u6280\u672f**\uff08Retrieval-based-Voice-Conversion-WebUI\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u58f0\u97f3\u5408\u6210\u6280\u672f\u3002\u5176\u6838\u5fc3\u539f\u7406\u5728\u4e8e\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c\u5c06\u8f93\u5165\u7684\u8bed\u97f3\u6837\u672c\u4e0e\u76ee\u6807\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\u7279\u5f81\u8fdb\u884c\u5b66\u4e60\u548c\u5339\u914d\u3002\u968f\u540e\uff0c\u5229\u7528\u8fd9\u4e2a\u6a21\u578b\u5bf9\u65b0\u7684\u6587\u672c\u8fdb\u884c\u8bed\u97f3\u5408\u6210\uff0c\u4f7f\u5f97\u5408\u6210\u7684\u8bed\u97f3\u542c\u8d77\u6765\u5c31\u50cf\u76ee\u6807\u8bf4\u8bdd\u8005\u4e00\u6837\u3002 \ud83c\udfaf \u5de5\u4f5c\u6d41\u7a0b \u8bed\u97f3\u6837\u672c\u8bad\u7ec3 \u2192 \u7279\u5f81\u5b66\u4e60\u5339\u914d \u2192 \u6a21\u578b\u751f\u6210 \u2192 \u65b0\u97f3\u9891\u63a8\u7406 \u2192 \u58f0\u97f3\u514b\u9686\u5b8c\u6210 \ud83d\ude80 \u5feb\u901f\u5f00\u59cb ### \ud83d\udccd \u8bbf\u95ee\u670d\u52a1 \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7684\u516c\u7f51\u5730\u5740\u6253\u5f00\u5c31\u662f\u5bf9\u5e94\u7684Web\u9875\u9762\u3002 \ud83d\udca1 \u4f7f\u7528\u6d41\u7a0b RVC\u7684\u4f7f\u7528\u8981\u5148\u7528\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u83b7\u53d6\u5bf9\u5e94\u7684\u6a21\u578b\u540e\uff0c\u518d\u5bf9\u5f85\u5904\u7406\u97f3\u9891\u8fdb\u884c\u63a8\u7406\uff0c\u5c31\u53ef\u4ee5\u5c06\u5f85\u5904\u7406\u97f3\u9891\u8f6c\u6362\u4e3a\u8bad\u7ec3\u6240\u7528\u7684\u8bed\u58f0\uff0c\u8fbe\u5230\u53d8\u58f0\u7684\u6548\u679c\u3002 \ud83c\udf93 \u8bad\u7ec3\u6559\u7a0b \u6b65\u9aa4 1\uff1a\u8fdb\u5165\u8bad\u7ec3\u9875\u9762 \u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u4e2d\u7684\u516c\u7f51\u5730\u5740\uff0c\u5373\u53ef\u8fdb\u5165\u5230RVC Web\u9875\u9762\uff0c\u9996\u5148\u8fdb\u5230\u8bad\u7ec3\u9875\u9762\u3002 \u6b65\u9aa4 2\uff1a\u914d\u7f6e\u8bad\u7ec3\u53c2\u6570 \u8fdb\u884c\u8bad\u7ec3\u76f8\u5173\u7684\u914d\u7f6e\uff0c\u4e3b\u8981\u8981\u8bbe\u7f6e\u5b9e\u9a8c\u540d\u79f0\uff0c\u8bad\u7ec3\u6587\u4ef6\u5939\uff0c\u6ce8\u610f\u8fd9\u91cc\u7684\u6587\u4ef6\u5939\u4e3a\u5bb9\u5668Pod\u91cc\u5bf9\u5e94\u7684\u76ee\u5f55\u3002 \u26a0\ufe0f \u91cd\u8981\u63d0\u793a \u6587\u4ef6\u5939\u8def\u5f84\u5fc5\u987b\u6307\u5411\u5bb9\u5668Pod\u5185\u7684\u5b9e\u9645\u76ee\u5f55\uff0c\u786e\u4fdd\u8def\u5f84\u6b63\u786e\u6027 \u6b65\u9aa4 3\uff1a\u4e0a\u4f20\u8bed\u97f3\u6837\u672c \u5c06\u8981\u8bad\u7ec3\u7684\u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5230\u8bbe\u7f6e\u7684\u8bad\u7ec3\u6587\u4ef6\u5939\u4e2d\uff1a #### 3.1 \u8fde\u63a5\u5230\u5bb9\u5668Pod \u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u4e2d\uff0c\u70b9\u51fb\u300a\u8d44\u6e90\u300b\u2192\u300a\u5bb9\u5668Pod\u8d44\u6e90\u300b\uff0c\u627e\u5230rvc\u5bf9\u5e94\u7684Pod\uff0c\u70b9\u51fb\u300a\u8fdc\u7a0b\u8fde\u63a5\u300b\u3002 #### 3.2 \u521b\u5efa\u8bad\u7ec3\u76ee\u5f55 \u5728Pod\u5185\u90e8/workspace/rvc-git\u76ee\u5f55\u4e0b\u521b\u5efatrain\u76ee\u5f55\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6587\u4ef6\u5939\u3002 #### 3.3 \u4e0a\u4f20\u8bed\u97f3\u6587\u4ef6 \u901a\u8fc7\u6587\u4ef6\u6811\u754c\u9762\u4e0a\u4f20\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u5230train\u76ee\u5f55\u3002 \u6b65\u9aa4 4\uff1a\u6570\u636e\u5904\u7406 \u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u300a\u5904\u7406\u6570\u636e\u300b\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u5904\u7406\u8fdb\u5ea6\u3002 \u6b65\u9aa4 5\uff1a\u7279\u5f81\u63d0\u53d6 \u70b9\u51fb\u300a\u7279\u5f81\u63d0\u53d6\u300b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u7279\u5f81\u63d0\u53d6\u8fdb\u5ea6\u3002 \u6b65\u9aa4 6\uff1a\u6a21\u578b\u8bad\u7ec3 \u70b9\u51fb\u300a\u8bad\u7ec3\u6a21\u578b\u300b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u8fd9\u91cc\u4f1a\u63d0\u793aError\uff0c\u4f46\u5b9e\u9645\u4e0a\u662f\u8bef\u62a5\uff0c\u8bad\u7ec3\u8fd8\u662f\u5728\u6b63\u5e38\u8fdb\u884c\u3002\u8bad\u7ec3\u8fdb\u5ea6\u53ef\u4ee5\u5728Pod\u4e2d\u6267\u884c tail -f /var/logs/app.log \u547d\u4ee4\u67e5\u770b\u3002 \u6b65\u9aa4 7\uff1a\u6784\u5efa\u7279\u5f81\u7d22\u5f15 \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u300a\u8bad\u7ec3\u7279\u5f81\u7d22\u5f15\u300b\uff0c\u770b\u5230\u6210\u529f\u6784\u5efa\u7d22\u5f15\uff0c\u5c31\u662f\u8bad\u7ec3\u6210\u529f\u4e86\u3002 \u2705 \u8bad\u7ec3\u5b8c\u6210 \u6210\u529f\u6784\u5efa\u7d22\u5f15\u8868\u793a\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u53ef\u4ee5\u8fdb\u884c\u63a8\u7406\u64cd\u4f5c \ud83c\udfaf \u63a8\u7406\u6559\u7a0b \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5bf9\u6211\u4eec\u60f3\u8981\u53d8\u58f0\u7684\u8bed\u97f3\u8fdb\u884c\u63a8\u7406\u4e86\uff1a \u6b65\u9aa4 1\uff1a\u52a0\u8f7d\u8bad\u7ec3\u6a21\u578b RVC web\u9875\u9762\u56de\u5230\u6a21\u578b\u63a8\u7406\u9875\u9762\uff0c\u70b9\u51fb\u300a\u5237\u65b0\u97f3\u8272\u5217\u8868\u548c\u7d22\u5f15\u8def\u5f84\u300b\uff0c\u53bb\u52a0\u8f7d\u521a\u624d\u8bad\u7ec3\u5b8c\u6210\u7684\u6a21\u578b\u3002 \u6b65\u9aa4 2\uff1a\u914d\u7f6e\u63a8\u7406\u53c2\u6570 \u9009\u62e9\u6211\u4eec\u521a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u8bbe\u7f6e\u5f85\u5904\u7406\u97f3\u9891\u6587\u4ef6\u8def\u5f84\u3002 \ud83d\udca1 \u8def\u5f84\u8bbe\u7f6e\u8bf4\u660e \u2022 \u5355\u6b21\u63a8\u7406 \uff1a\u8def\u5f84\u8981\u5230\u5177\u4f53\u6587\u4ef6\u540d\u79f0 \u2022 \u6279\u91cf\u63a8\u7406 \uff1a\u8bbe\u7f6e\u5230\u76ee\u5f55\u5373\u53ef \u2022 \u6587\u4ef6\u9700\u8981\u5148\u4e0a\u4f20\u5230Pod\u5bb9\u5668\u4e2d\uff08\u53c2\u8003\u8bad\u7ec3\u6b65\u9aa43\uff09 \u6b65\u9aa4 3\uff1a\u6267\u884c\u58f0\u97f3\u8f6c\u6362 \u70b9\u51fb\u8f6c\u6362\uff0c\u5f00\u59cb\u5c06\u5f85\u5904\u7406\u7684\u97f3\u9891\u8fdb\u884c\u53d8\u58f0\uff0c\u53d8\u58f0\u5b8c\u6210\u540e\uff0c\u8f93\u51fa\u97f3\u9891\u53ef\u4ee5\u76f4\u63a5\u64ad\u653e\u6216\u4e0b\u8f7d\u3002 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u8f6c\u6362\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u5931\u8d25\uff0c\u5931\u8d25\u540e\u91cd\u8bd5\u5373\u53ef\u3002\u5efa\u8bae\u5728\u7f51\u7edc\u7a33\u5b9a\u7684\u73af\u5883\u4e0b\u8fdb\u884c\u64cd\u4f5c\u3002 \ud83d\udccb \u64cd\u4f5c\u8981\u70b9\u603b\u7ed3 \u2705 \u6210\u529f\u8981\u7d20 \u9ad8\u8d28\u91cf\u8bed\u97f3\u6837\u672c \uff1a\u6e05\u6670\u3001\u65e0\u566a\u97f3 \u5145\u8db3\u7684\u8bad\u7ec3\u6570\u636e \uff1a\u5efa\u8bae10-30\u5206\u949f\u97f3\u9891 \u6b63\u786e\u7684\u8def\u5f84\u914d\u7f6e \uff1a\u786e\u4fdd\u6587\u4ef6\u8def\u5f84\u51c6\u786e \u8010\u5fc3\u7b49\u5f85\u8bad\u7ec3 \uff1a\u8bad\u7ec3\u65f6\u95f4\u8f83\u957f\u5c5e\u6b63\u5e38 \u274c \u5e38\u89c1\u95ee\u9898 \u6587\u4ef6\u8def\u5f84\u9519\u8bef\u5bfc\u81f4\u627e\u4e0d\u5230\u6587\u4ef6 \u8bed\u97f3\u6837\u672c\u8d28\u91cf\u5dee\u5f71\u54cd\u6548\u679c \u8bad\u7ec3\u65f6\u95f4\u4e0d\u8db3\u6a21\u578b\u672a\u6536\u655b \u7f51\u7edc\u4e0d\u7a33\u5b9a\u5bfc\u81f4\u64cd\u4f5c\u5931\u8d25 \ud83c\udfa4 RVC\u58f0\u97f3\u514b\u9686\u6280\u672f | \u8ba9\u6bcf\u4e2a\u58f0\u97f3\u90fd\u80fd\u88ab\u5b8c\u7f8e\u590d\u5236","title":"Index"},{"location":"rvc/#_1","text":"**RVC\u58f0\u97f3\u514b\u9686\u6280\u672f**\uff08Retrieval-based-Voice-Conversion-WebUI\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u58f0\u97f3\u5408\u6210\u6280\u672f\u3002\u5176\u6838\u5fc3\u539f\u7406\u5728\u4e8e\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c\u5c06\u8f93\u5165\u7684\u8bed\u97f3\u6837\u672c\u4e0e\u76ee\u6807\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\u7279\u5f81\u8fdb\u884c\u5b66\u4e60\u548c\u5339\u914d\u3002\u968f\u540e\uff0c\u5229\u7528\u8fd9\u4e2a\u6a21\u578b\u5bf9\u65b0\u7684\u6587\u672c\u8fdb\u884c\u8bed\u97f3\u5408\u6210\uff0c\u4f7f\u5f97\u5408\u6210\u7684\u8bed\u97f3\u542c\u8d77\u6765\u5c31\u50cf\u76ee\u6807\u8bf4\u8bdd\u8005\u4e00\u6837\u3002 \ud83c\udfaf \u5de5\u4f5c\u6d41\u7a0b \u8bed\u97f3\u6837\u672c\u8bad\u7ec3 \u2192 \u7279\u5f81\u5b66\u4e60\u5339\u914d \u2192 \u6a21\u578b\u751f\u6210 \u2192 \u65b0\u97f3\u9891\u63a8\u7406 \u2192 \u58f0\u97f3\u514b\u9686\u5b8c\u6210","title":"\ud83d\udd2c \u6280\u672f\u7b80\u4ecb"},{"location":"rvc/#_2","text":"### \ud83d\udccd \u8bbf\u95ee\u670d\u52a1 \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7684\u516c\u7f51\u5730\u5740\u6253\u5f00\u5c31\u662f\u5bf9\u5e94\u7684Web\u9875\u9762\u3002 \ud83d\udca1 \u4f7f\u7528\u6d41\u7a0b RVC\u7684\u4f7f\u7528\u8981\u5148\u7528\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u83b7\u53d6\u5bf9\u5e94\u7684\u6a21\u578b\u540e\uff0c\u518d\u5bf9\u5f85\u5904\u7406\u97f3\u9891\u8fdb\u884c\u63a8\u7406\uff0c\u5c31\u53ef\u4ee5\u5c06\u5f85\u5904\u7406\u97f3\u9891\u8f6c\u6362\u4e3a\u8bad\u7ec3\u6240\u7528\u7684\u8bed\u58f0\uff0c\u8fbe\u5230\u53d8\u58f0\u7684\u6548\u679c\u3002","title":"\ud83d\ude80 \u5feb\u901f\u5f00\u59cb"},{"location":"rvc/#_3","text":"","title":"\ud83c\udf93 \u8bad\u7ec3\u6559\u7a0b"},{"location":"rvc/#1","text":"\u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u4e2d\u7684\u516c\u7f51\u5730\u5740\uff0c\u5373\u53ef\u8fdb\u5165\u5230RVC Web\u9875\u9762\uff0c\u9996\u5148\u8fdb\u5230\u8bad\u7ec3\u9875\u9762\u3002","title":"\u6b65\u9aa4 1\uff1a\u8fdb\u5165\u8bad\u7ec3\u9875\u9762"},{"location":"rvc/#2","text":"\u8fdb\u884c\u8bad\u7ec3\u76f8\u5173\u7684\u914d\u7f6e\uff0c\u4e3b\u8981\u8981\u8bbe\u7f6e\u5b9e\u9a8c\u540d\u79f0\uff0c\u8bad\u7ec3\u6587\u4ef6\u5939\uff0c\u6ce8\u610f\u8fd9\u91cc\u7684\u6587\u4ef6\u5939\u4e3a\u5bb9\u5668Pod\u91cc\u5bf9\u5e94\u7684\u76ee\u5f55\u3002 \u26a0\ufe0f \u91cd\u8981\u63d0\u793a \u6587\u4ef6\u5939\u8def\u5f84\u5fc5\u987b\u6307\u5411\u5bb9\u5668Pod\u5185\u7684\u5b9e\u9645\u76ee\u5f55\uff0c\u786e\u4fdd\u8def\u5f84\u6b63\u786e\u6027","title":"\u6b65\u9aa4 2\uff1a\u914d\u7f6e\u8bad\u7ec3\u53c2\u6570"},{"location":"rvc/#3","text":"\u5c06\u8981\u8bad\u7ec3\u7684\u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5230\u8bbe\u7f6e\u7684\u8bad\u7ec3\u6587\u4ef6\u5939\u4e2d\uff1a #### 3.1 \u8fde\u63a5\u5230\u5bb9\u5668Pod \u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u4e2d\uff0c\u70b9\u51fb\u300a\u8d44\u6e90\u300b\u2192\u300a\u5bb9\u5668Pod\u8d44\u6e90\u300b\uff0c\u627e\u5230rvc\u5bf9\u5e94\u7684Pod\uff0c\u70b9\u51fb\u300a\u8fdc\u7a0b\u8fde\u63a5\u300b\u3002 #### 3.2 \u521b\u5efa\u8bad\u7ec3\u76ee\u5f55 \u5728Pod\u5185\u90e8/workspace/rvc-git\u76ee\u5f55\u4e0b\u521b\u5efatrain\u76ee\u5f55\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6587\u4ef6\u5939\u3002 #### 3.3 \u4e0a\u4f20\u8bed\u97f3\u6587\u4ef6 \u901a\u8fc7\u6587\u4ef6\u6811\u754c\u9762\u4e0a\u4f20\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u5230train\u76ee\u5f55\u3002","title":"\u6b65\u9aa4 3\uff1a\u4e0a\u4f20\u8bed\u97f3\u6837\u672c"},{"location":"rvc/#4","text":"\u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u300a\u5904\u7406\u6570\u636e\u300b\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u5904\u7406\u8fdb\u5ea6\u3002","title":"\u6b65\u9aa4 4\uff1a\u6570\u636e\u5904\u7406"},{"location":"rvc/#5","text":"\u70b9\u51fb\u300a\u7279\u5f81\u63d0\u53d6\u300b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u7279\u5f81\u63d0\u53d6\u8fdb\u5ea6\u3002","title":"\u6b65\u9aa4 5\uff1a\u7279\u5f81\u63d0\u53d6"},{"location":"rvc/#6","text":"\u70b9\u51fb\u300a\u8bad\u7ec3\u6a21\u578b\u300b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u8fd9\u91cc\u4f1a\u63d0\u793aError\uff0c\u4f46\u5b9e\u9645\u4e0a\u662f\u8bef\u62a5\uff0c\u8bad\u7ec3\u8fd8\u662f\u5728\u6b63\u5e38\u8fdb\u884c\u3002\u8bad\u7ec3\u8fdb\u5ea6\u53ef\u4ee5\u5728Pod\u4e2d\u6267\u884c tail -f /var/logs/app.log \u547d\u4ee4\u67e5\u770b\u3002","title":"\u6b65\u9aa4 6\uff1a\u6a21\u578b\u8bad\u7ec3"},{"location":"rvc/#7","text":"\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u300a\u8bad\u7ec3\u7279\u5f81\u7d22\u5f15\u300b\uff0c\u770b\u5230\u6210\u529f\u6784\u5efa\u7d22\u5f15\uff0c\u5c31\u662f\u8bad\u7ec3\u6210\u529f\u4e86\u3002 \u2705 \u8bad\u7ec3\u5b8c\u6210 \u6210\u529f\u6784\u5efa\u7d22\u5f15\u8868\u793a\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff0c\u53ef\u4ee5\u8fdb\u884c\u63a8\u7406\u64cd\u4f5c","title":"\u6b65\u9aa4 7\uff1a\u6784\u5efa\u7279\u5f81\u7d22\u5f15"},{"location":"rvc/#_4","text":"\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5bf9\u6211\u4eec\u60f3\u8981\u53d8\u58f0\u7684\u8bed\u97f3\u8fdb\u884c\u63a8\u7406\u4e86\uff1a","title":"\ud83c\udfaf \u63a8\u7406\u6559\u7a0b"},{"location":"rvc/#1_1","text":"RVC web\u9875\u9762\u56de\u5230\u6a21\u578b\u63a8\u7406\u9875\u9762\uff0c\u70b9\u51fb\u300a\u5237\u65b0\u97f3\u8272\u5217\u8868\u548c\u7d22\u5f15\u8def\u5f84\u300b\uff0c\u53bb\u52a0\u8f7d\u521a\u624d\u8bad\u7ec3\u5b8c\u6210\u7684\u6a21\u578b\u3002","title":"\u6b65\u9aa4 1\uff1a\u52a0\u8f7d\u8bad\u7ec3\u6a21\u578b"},{"location":"rvc/#2_1","text":"\u9009\u62e9\u6211\u4eec\u521a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u8bbe\u7f6e\u5f85\u5904\u7406\u97f3\u9891\u6587\u4ef6\u8def\u5f84\u3002 \ud83d\udca1 \u8def\u5f84\u8bbe\u7f6e\u8bf4\u660e \u2022 \u5355\u6b21\u63a8\u7406 \uff1a\u8def\u5f84\u8981\u5230\u5177\u4f53\u6587\u4ef6\u540d\u79f0 \u2022 \u6279\u91cf\u63a8\u7406 \uff1a\u8bbe\u7f6e\u5230\u76ee\u5f55\u5373\u53ef \u2022 \u6587\u4ef6\u9700\u8981\u5148\u4e0a\u4f20\u5230Pod\u5bb9\u5668\u4e2d\uff08\u53c2\u8003\u8bad\u7ec3\u6b65\u9aa43\uff09","title":"\u6b65\u9aa4 2\uff1a\u914d\u7f6e\u63a8\u7406\u53c2\u6570"},{"location":"rvc/#3_1","text":"\u70b9\u51fb\u8f6c\u6362\uff0c\u5f00\u59cb\u5c06\u5f85\u5904\u7406\u7684\u97f3\u9891\u8fdb\u884c\u53d8\u58f0\uff0c\u53d8\u58f0\u5b8c\u6210\u540e\uff0c\u8f93\u51fa\u97f3\u9891\u53ef\u4ee5\u76f4\u63a5\u64ad\u653e\u6216\u4e0b\u8f7d\u3002 \u26a0\ufe0f \u6ce8\u610f\u4e8b\u9879 \u8f6c\u6362\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u5931\u8d25\uff0c\u5931\u8d25\u540e\u91cd\u8bd5\u5373\u53ef\u3002\u5efa\u8bae\u5728\u7f51\u7edc\u7a33\u5b9a\u7684\u73af\u5883\u4e0b\u8fdb\u884c\u64cd\u4f5c\u3002","title":"\u6b65\u9aa4 3\uff1a\u6267\u884c\u58f0\u97f3\u8f6c\u6362"},{"location":"rvc/#_5","text":"","title":"\ud83d\udccb \u64cd\u4f5c\u8981\u70b9\u603b\u7ed3"},{"location":"rvc/index-en/","text":"\ud83c\udfa4 RVC Voice Cloning Technology Guide Professional Voice Synthesis and Conversion Solution Based on Deep Learning \ud83d\udd2c Technical Overview **RVC Voice Cloning Technology** (Retrieval-based-Voice-Conversion-WebUI) is a voice synthesis technology based on deep learning. Its core principle lies in training deep learning models to learn and match the input voice samples with the target speaker's voice characteristics. Subsequently, this model is used to synthesize speech for new text, making the synthesized voice sound like the target speaker. \ud83c\udfaf Workflow Voice Sample Training \u2192 Feature Learning & Matching \u2192 Model Generation \u2192 New Audio Inference \u2192 Voice Cloning Complete \ud83d\ude80 Quick Start ### \ud83d\udccd Service Access After completing model deployment, you can see the model usage instructions on the service instance overview page. The public network address opens the corresponding Web page. \ud83d\udca1 Usage Process RVC requires training with prepared voice samples first. After training to obtain the corresponding model, inference is performed on the audio to be processed, which can convert the target audio to the voice used in training, achieving voice conversion effects. \ud83c\udf93 Training Tutorial Step 1: Enter Training Page Click the public network address in the service instance details to enter the RVC Web page, first go to the training page. Step 2: Configure Training Parameters Configure training-related settings, mainly setting the experiment name and training folder. Note that the folder here refers to the corresponding directory in the container Pod. \u26a0\ufe0f Important Note The folder path must point to the actual directory within the container Pod, ensure path accuracy Step 3: Upload Voice Samples Upload the voice samples to be trained to the set training folder: #### 3.1 Connect to Container Pod In the service instance, click \"Resources\" \u2192 \"Container Pod Resources\", find the rvc corresponding Pod, click \"Remote Connection\". #### 3.2 Create Training Directory Create a train directory under /workspace/rvc-git directory inside the Pod as the training folder. #### 3.3 Upload Voice Files Upload prepared voice samples to the train directory through the file tree interface. Step 4: Data Processing After uploading voice samples, click \"Process Data\" to perform data processing. The output information will show processing progress. Step 5: Feature Extraction Click \"Feature Extraction\" to perform feature extraction. The output information will show feature extraction progress. Step 6: Model Training Click \"Train Model\" to perform model training. \u26a0\ufe0f Important Notes An Error message may appear here, but it's actually a false alarm - training is still proceeding normally. Training progress can be monitored by executing tail -f /var/logs/app.log command in the Pod. Step 7: Build Feature Index After training completion, click \"Train Feature Index\". When you see successful index construction, the training is complete. \u2705 Training Complete Successful index construction indicates model training is complete and inference operations can be performed \ud83c\udfaf Inference Tutorial After training completion, we can perform inference on the voice we want to convert: Step 1: Load Trained Model Return to the model inference page on the RVC web interface, click \"Refresh Voice List and Index Path\" to load the just-trained model. Step 2: Configure Inference Parameters Select our just-trained model and set the path for the audio file to be processed. \ud83d\udca1 Path Setting Instructions \u2022 Single Inference : Path should point to specific filename \u2022 Batch Inference : Set to directory \u2022 Files need to be uploaded to Pod container first (refer to training step 3) Step 3: Execute Voice Conversion Click convert to start voice conversion on the target audio. After conversion completion, the output audio can be played directly or downloaded. \u26a0\ufe0f Important Notes The conversion process may fail, simply retry if it fails. It's recommended to operate in a stable network environment. \ud83d\udccb Key Points Summary \u2705 Success Factors High-quality voice samples : Clear, noise-free Sufficient training data : Recommended 10-30 minutes of audio Correct path configuration : Ensure file path accuracy Patient training wait : Long training time is normal \u274c Common Issues Incorrect file paths causing file not found Poor voice sample quality affecting results Insufficient training time preventing model convergence Unstable network causing operation failures \ud83c\udfa4 RVC Voice Cloning Technology | Making Every Voice Perfectly Replicable","title":"Index en"},{"location":"rvc/index-en/#technical-overview","text":"**RVC Voice Cloning Technology** (Retrieval-based-Voice-Conversion-WebUI) is a voice synthesis technology based on deep learning. Its core principle lies in training deep learning models to learn and match the input voice samples with the target speaker's voice characteristics. Subsequently, this model is used to synthesize speech for new text, making the synthesized voice sound like the target speaker. \ud83c\udfaf Workflow Voice Sample Training \u2192 Feature Learning & Matching \u2192 Model Generation \u2192 New Audio Inference \u2192 Voice Cloning Complete","title":"\ud83d\udd2c Technical Overview"},{"location":"rvc/index-en/#quick-start","text":"### \ud83d\udccd Service Access After completing model deployment, you can see the model usage instructions on the service instance overview page. The public network address opens the corresponding Web page. \ud83d\udca1 Usage Process RVC requires training with prepared voice samples first. After training to obtain the corresponding model, inference is performed on the audio to be processed, which can convert the target audio to the voice used in training, achieving voice conversion effects.","title":"\ud83d\ude80 Quick Start"},{"location":"rvc/index-en/#training-tutorial","text":"","title":"\ud83c\udf93 Training Tutorial"},{"location":"rvc/index-en/#step-1-enter-training-page","text":"Click the public network address in the service instance details to enter the RVC Web page, first go to the training page.","title":"Step 1: Enter Training Page"},{"location":"rvc/index-en/#step-2-configure-training-parameters","text":"Configure training-related settings, mainly setting the experiment name and training folder. Note that the folder here refers to the corresponding directory in the container Pod. \u26a0\ufe0f Important Note The folder path must point to the actual directory within the container Pod, ensure path accuracy","title":"Step 2: Configure Training Parameters"},{"location":"rvc/index-en/#step-3-upload-voice-samples","text":"Upload the voice samples to be trained to the set training folder: #### 3.1 Connect to Container Pod In the service instance, click \"Resources\" \u2192 \"Container Pod Resources\", find the rvc corresponding Pod, click \"Remote Connection\". #### 3.2 Create Training Directory Create a train directory under /workspace/rvc-git directory inside the Pod as the training folder. #### 3.3 Upload Voice Files Upload prepared voice samples to the train directory through the file tree interface.","title":"Step 3: Upload Voice Samples"},{"location":"rvc/index-en/#step-4-data-processing","text":"After uploading voice samples, click \"Process Data\" to perform data processing. The output information will show processing progress.","title":"Step 4: Data Processing"},{"location":"rvc/index-en/#step-5-feature-extraction","text":"Click \"Feature Extraction\" to perform feature extraction. The output information will show feature extraction progress.","title":"Step 5: Feature Extraction"},{"location":"rvc/index-en/#step-6-model-training","text":"Click \"Train Model\" to perform model training. \u26a0\ufe0f Important Notes An Error message may appear here, but it's actually a false alarm - training is still proceeding normally. Training progress can be monitored by executing tail -f /var/logs/app.log command in the Pod.","title":"Step 6: Model Training"},{"location":"rvc/index-en/#step-7-build-feature-index","text":"After training completion, click \"Train Feature Index\". When you see successful index construction, the training is complete. \u2705 Training Complete Successful index construction indicates model training is complete and inference operations can be performed","title":"Step 7: Build Feature Index"},{"location":"rvc/index-en/#inference-tutorial","text":"After training completion, we can perform inference on the voice we want to convert:","title":"\ud83c\udfaf Inference Tutorial"},{"location":"rvc/index-en/#step-1-load-trained-model","text":"Return to the model inference page on the RVC web interface, click \"Refresh Voice List and Index Path\" to load the just-trained model.","title":"Step 1: Load Trained Model"},{"location":"rvc/index-en/#step-2-configure-inference-parameters","text":"Select our just-trained model and set the path for the audio file to be processed. \ud83d\udca1 Path Setting Instructions \u2022 Single Inference : Path should point to specific filename \u2022 Batch Inference : Set to directory \u2022 Files need to be uploaded to Pod container first (refer to training step 3)","title":"Step 2: Configure Inference Parameters"},{"location":"rvc/index-en/#step-3-execute-voice-conversion","text":"Click convert to start voice conversion on the target audio. After conversion completion, the output audio can be played directly or downloaded. \u26a0\ufe0f Important Notes The conversion process may fail, simply retry if it fails. It's recommended to operate in a stable network environment.","title":"Step 3: Execute Voice Conversion"},{"location":"rvc/index-en/#key-points-summary","text":"","title":"\ud83d\udccb Key Points Summary"},{"location":"rvc/original-index/","text":"\u7b80\u4ecb RVC\u58f0\u97f3\u514b\u9686\u6280\u672f\uff08Retrieval-based-Voice-Conversion-WebUI\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u58f0\u97f3\u5408\u6210\u6280\u672f\u3002 \u5176\u6838\u5fc3\u539f\u7406\u5728\u4e8e\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c \u5c06\u8f93\u5165\u7684\u8bed\u97f3\u6837\u672c\u4e0e\u76ee\u6807\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\u7279\u5f81\u8fdb\u884c\u5b66\u4e60\u548c\u5339\u914d\u3002 \u968f\u540e\uff0c\u5229\u7528\u8fd9\u4e2a\u6a21\u578b\u5bf9\u65b0\u7684\u6587\u672c\u8fdb\u884c\u8bed\u97f3\u5408\u6210\uff0c\u4f7f\u5f97\u5408\u6210\u7684\u8bed\u97f3\u542c\u8d77\u6765\u5c31\u50cf\u76ee\u6807\u8bf4\u8bdd\u8005\u4e00\u6837\u3002 \u4f7f\u7528\u8bf4\u660e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7684\u516c\u7f51\u5730\u5740\u6253\u5f00\u5c31\u662f\u5bf9\u5e94\u7684Web\u9875\u9762\u3002 RVC\u7684\u4f7f\u7528\u8981\u5148\u7528\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u83b7\u53d6\u5bf9\u5e94\u7684\u6a21\u578b\u540e\uff0c\u518d\u5bf9\u5f85\u5904\u7406\u97f3\u9891\u8fdb\u884c\u63a8\u7406\uff0c\u5c31\u53ef\u4ee5\u5c06\u5f85\u5904\u7406\u97f3\u9891\u8f6c\u6362\u4e3a\u8bad\u7ec3\u6240\u7528\u7684\u8bed\u58f0\uff0c\u8fbe\u5230\u53d8\u58f0\u7684\u6548\u679c\u3002 \u8bad\u7ec3\u6559\u7a0b \u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u4e2d\u7684\u516c\u7f51\u5730\u5740\uff0c\u5373\u53ef\u8fdb\u5165\u5230RVC Web\u9875\u9762\uff0c\u9996\u5148\u8fdb\u5230\u8bad\u7ec3\u9875\u9762\u3002 \u8fd9\u91cc\u5148\u8981\u8fdb\u884c\u8bad\u7ec3\u76f8\u5173\u7684\u914d\u7f6e\uff0c\u4e3b\u8981\u8981\u8bbe\u7f6e\u5b9e\u9a8c\u540d\u79f0\uff0c\u8bad\u7ec3\u6587\u4ef6\u5939\uff0c\u6ce8\u610f\u8fd9\u91cc\u7684\u6587\u4ef6\u5939\u4e3a\u5bb9\u5668Pod\u91cc\u5bf9\u5e94\u7684\u76ee\u5f55\uff0c\u5177\u4f53\u5982\u4e0b\u6240\u793a\u3002 \u5c06\u8981\u8bad\u7ec3\u7684\u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5230\u6211\u4eec\u4e0a\u9762\u8bbe\u7f6e\u7684\u8bad\u7ec3\u6587\u4ef6\u5939\u4e2d\uff0c\u4e0b\u9762\u7ed9\u51fa\u5177\u4f53\u8fc7\u7a0b\u3002 \u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u4e2d\uff0c\u70b9\u51fb\u300a\u8d44\u6e90\u300b\uff0c\u7136\u540e\u70b9\u51fb\u300a\u5bb9\u5668Pod\u8d44\u6e90\u300b\uff0c\u53ef\u4ee5\u627e\u5230rvc\u5bf9\u5e94\u7684Pod\uff0c\u70b9\u51fb\u300a\u8fdc\u7a0b\u8fde\u63a5\u300b\u3002 - \u70b9\u51fb\u300a\u8fdc\u7a0b\u8fde\u63a5\u300b\u540e\uff0c\u53ef\u4ee5\u8fdb\u5230Pod\u5185\u90e8\uff0c\u5728Pod\u5185\u90e8/workspace/rvc-git\u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2atrain\u76ee\u5f55\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6587\u4ef6\u5939\u3002 - \u5c06\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5230train\u76ee\u5f55\u4e2d\uff0c\u8fd9\u91cc\u5148\u70b9\u51fb\u63a7\u5236\u53f0\u4e0a\u7684\u6587\u4ef6\u6253\u5f00\u6587\u4ef6\u6811\uff0c\u7136\u540e\u627e\u5230train\u76ee\u5f55\uff0c\u5c31\u53ef\u4ee5\u628a\u4e8b\u5148\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u4e0a\u4f20\u4e86\u3002 4. \u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u8fdb\u884c\u8bad\u7ec3\u4e86\uff0c\u9996\u5148\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u70b9\u51fb\u300a\u5904\u7406\u6570\u636e\u300b\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u5904\u7406\u8fdb\u5ea6\u3002 \u70b9\u51fb\u300a\u7279\u5f81\u63d0\u53d6\u300b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u7279\u5f81\u63d0\u53d6\u8fdb\u5ea6\u3002 \u70b9\u51fb\u300a\u8bad\u7ec3\u6a21\u578b\u300b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u8fd9\u91cc\u4f1a\u63d0\u793aError\uff0c\u4f46\u5b9e\u9645\u4e0a\u662f\u8bef\u62a5\uff0c\u8bad\u7ec3\u8fd8\u662f\u5728\u6b63\u5e38\u8fdb\u884c\uff0c \u8bad\u7ec3\u8fdb\u5ea6\u53ef\u4ee5\u5728\u6b65\u9aa43\u4e2d\u7684Pod\u4e2d\u6267\u884c tail -f /var/logs/app.log \u547d\u4ee4\u67e5\u770b\u8bad\u7ec3\u8fdb\u5ea6\u3002 \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u300a\u8bad\u7ec3\u7279\u5f81\u7d22\u5f15\u300b\uff0c\u770b\u5230\u6210\u529f\u6784\u5efa\u7d22\u5f15\uff0c\u5c31\u662f\u8bad\u7ec3\u6210\u529f\u4e86\u3002 \u63a8\u7406\u6559\u7a0b \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5bf9\u6211\u4eec\u60f3\u8981\u53d8\u58f0\u7684\u8bed\u97f3\u8fdb\u884c\u63a8\u7406\u4e86\uff0c\u5177\u4f53\u6b65\u9aa4\u5982\u4e0b\uff1a 1. RVC web\u9875\u9762\u56de\u5230\u6a21\u578b\u63a8\u7406\u9875\u9762\uff0c\u70b9\u51fb\u300a\u5237\u65b0\u97f3\u8272\u5217\u8868\u548c\u7d22\u5f15\u8def\u5f84\u300b\uff0c\u53bb\u52a0\u8f7d\u521a\u624d\u8bad\u7ec3\u5b8c\u6210\u7684\u6a21\u578b\u3002 \u9009\u62e9\u6211\u4eec\u521a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u8bbe\u7f6e\u5f85\u5904\u7406\u97f3\u9891\u6587\u4ef6\u8def\u5f84\uff0c\u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5355\u6b21\u63a8\u7406\u5bf9\u5e94\u7684\u8def\u5f84\u8981\u5230\u6587\u4ef6\u540d\u79f0\uff0c \u591a\u6b21\u63a8\u7406\u8bbe\u7f6e\u5230\u76ee\u5f55\u5373\u53ef\uff0c\u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u5f85\u5904\u7406\u97f3\u9891\u7684\u8def\u5f84\uff0c\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e00\u6837\uff0c\u6587\u4ef6\u8981\u5148\u4e0a\u4f20\u5230Pod\u5bb9\u5668\u4e2d\uff0c\u5177\u4f53\u89c1 \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6b65\u9aa43\u3002 \u70b9\u51fb\u8f6c\u6362\uff0c\u5c31\u4f1a\u5f00\u59cb\u5c06\u5f85\u5904\u7406\u7684\u97f3\u9891\u8fdb\u884c\u53d8\u58f0\uff0c\u53d8\u58f0\u5b8c\u6210\u540e\uff0c\u8f93\u51fa\u97f3\u9891\u4f1a\u6709\u5bf9\u5e94\u7684\u97f3\u9891\uff0c\u53ef\u4ee5\u76f4\u63a5\u64ad\u653e\uff0c\u4e5f\u53ef\u4ee5\u8fdb\u884c\u4e0b\u8f7d\u3002 \u8fd9\u4e2a\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u5931\u8d25\uff0c\u5931\u8d25\u540e\u91cd\u8bd5\u5373\u53ef\u3002","title":"Original index"},{"location":"rvc/original-index/#_1","text":"RVC\u58f0\u97f3\u514b\u9686\u6280\u672f\uff08Retrieval-based-Voice-Conversion-WebUI\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u58f0\u97f3\u5408\u6210\u6280\u672f\u3002 \u5176\u6838\u5fc3\u539f\u7406\u5728\u4e8e\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c \u5c06\u8f93\u5165\u7684\u8bed\u97f3\u6837\u672c\u4e0e\u76ee\u6807\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\u7279\u5f81\u8fdb\u884c\u5b66\u4e60\u548c\u5339\u914d\u3002 \u968f\u540e\uff0c\u5229\u7528\u8fd9\u4e2a\u6a21\u578b\u5bf9\u65b0\u7684\u6587\u672c\u8fdb\u884c\u8bed\u97f3\u5408\u6210\uff0c\u4f7f\u5f97\u5408\u6210\u7684\u8bed\u97f3\u542c\u8d77\u6765\u5c31\u50cf\u76ee\u6807\u8bf4\u8bdd\u8005\u4e00\u6837\u3002","title":"\u7b80\u4ecb"},{"location":"rvc/original-index/#_2","text":"\u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7684\u516c\u7f51\u5730\u5740\u6253\u5f00\u5c31\u662f\u5bf9\u5e94\u7684Web\u9875\u9762\u3002 RVC\u7684\u4f7f\u7528\u8981\u5148\u7528\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u83b7\u53d6\u5bf9\u5e94\u7684\u6a21\u578b\u540e\uff0c\u518d\u5bf9\u5f85\u5904\u7406\u97f3\u9891\u8fdb\u884c\u63a8\u7406\uff0c\u5c31\u53ef\u4ee5\u5c06\u5f85\u5904\u7406\u97f3\u9891\u8f6c\u6362\u4e3a\u8bad\u7ec3\u6240\u7528\u7684\u8bed\u58f0\uff0c\u8fbe\u5230\u53d8\u58f0\u7684\u6548\u679c\u3002","title":"\u4f7f\u7528\u8bf4\u660e"},{"location":"rvc/original-index/#_3","text":"\u70b9\u51fb\u670d\u52a1\u5b9e\u4f8b\u8be6\u60c5\u4e2d\u7684\u516c\u7f51\u5730\u5740\uff0c\u5373\u53ef\u8fdb\u5165\u5230RVC Web\u9875\u9762\uff0c\u9996\u5148\u8fdb\u5230\u8bad\u7ec3\u9875\u9762\u3002 \u8fd9\u91cc\u5148\u8981\u8fdb\u884c\u8bad\u7ec3\u76f8\u5173\u7684\u914d\u7f6e\uff0c\u4e3b\u8981\u8981\u8bbe\u7f6e\u5b9e\u9a8c\u540d\u79f0\uff0c\u8bad\u7ec3\u6587\u4ef6\u5939\uff0c\u6ce8\u610f\u8fd9\u91cc\u7684\u6587\u4ef6\u5939\u4e3a\u5bb9\u5668Pod\u91cc\u5bf9\u5e94\u7684\u76ee\u5f55\uff0c\u5177\u4f53\u5982\u4e0b\u6240\u793a\u3002 \u5c06\u8981\u8bad\u7ec3\u7684\u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5230\u6211\u4eec\u4e0a\u9762\u8bbe\u7f6e\u7684\u8bad\u7ec3\u6587\u4ef6\u5939\u4e2d\uff0c\u4e0b\u9762\u7ed9\u51fa\u5177\u4f53\u8fc7\u7a0b\u3002 \u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u4e2d\uff0c\u70b9\u51fb\u300a\u8d44\u6e90\u300b\uff0c\u7136\u540e\u70b9\u51fb\u300a\u5bb9\u5668Pod\u8d44\u6e90\u300b\uff0c\u53ef\u4ee5\u627e\u5230rvc\u5bf9\u5e94\u7684Pod\uff0c\u70b9\u51fb\u300a\u8fdc\u7a0b\u8fde\u63a5\u300b\u3002 - \u70b9\u51fb\u300a\u8fdc\u7a0b\u8fde\u63a5\u300b\u540e\uff0c\u53ef\u4ee5\u8fdb\u5230Pod\u5185\u90e8\uff0c\u5728Pod\u5185\u90e8/workspace/rvc-git\u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2atrain\u76ee\u5f55\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6587\u4ef6\u5939\u3002 - \u5c06\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5230train\u76ee\u5f55\u4e2d\uff0c\u8fd9\u91cc\u5148\u70b9\u51fb\u63a7\u5236\u53f0\u4e0a\u7684\u6587\u4ef6\u6253\u5f00\u6587\u4ef6\u6811\uff0c\u7136\u540e\u627e\u5230train\u76ee\u5f55\uff0c\u5c31\u53ef\u4ee5\u628a\u4e8b\u5148\u51c6\u5907\u597d\u7684\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u4e0a\u4f20\u4e86\u3002 4. \u8bed\u97f3\u6837\u672c\u4e0a\u4f20\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u5f00\u59cb\u8fdb\u884c\u8bad\u7ec3\u4e86\uff0c\u9996\u5148\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u70b9\u51fb\u300a\u5904\u7406\u6570\u636e\u300b\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u5904\u7406\u8fdb\u5ea6\u3002 \u70b9\u51fb\u300a\u7279\u5f81\u63d0\u53d6\u300b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u4fe1\u606f\u4f1a\u63d0\u793a\u7279\u5f81\u63d0\u53d6\u8fdb\u5ea6\u3002 \u70b9\u51fb\u300a\u8bad\u7ec3\u6a21\u578b\u300b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u8fd9\u91cc\u4f1a\u63d0\u793aError\uff0c\u4f46\u5b9e\u9645\u4e0a\u662f\u8bef\u62a5\uff0c\u8bad\u7ec3\u8fd8\u662f\u5728\u6b63\u5e38\u8fdb\u884c\uff0c \u8bad\u7ec3\u8fdb\u5ea6\u53ef\u4ee5\u5728\u6b65\u9aa43\u4e2d\u7684Pod\u4e2d\u6267\u884c tail -f /var/logs/app.log \u547d\u4ee4\u67e5\u770b\u8bad\u7ec3\u8fdb\u5ea6\u3002 \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u300a\u8bad\u7ec3\u7279\u5f81\u7d22\u5f15\u300b\uff0c\u770b\u5230\u6210\u529f\u6784\u5efa\u7d22\u5f15\uff0c\u5c31\u662f\u8bad\u7ec3\u6210\u529f\u4e86\u3002","title":"\u8bad\u7ec3\u6559\u7a0b"},{"location":"rvc/original-index/#_4","text":"\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5bf9\u6211\u4eec\u60f3\u8981\u53d8\u58f0\u7684\u8bed\u97f3\u8fdb\u884c\u63a8\u7406\u4e86\uff0c\u5177\u4f53\u6b65\u9aa4\u5982\u4e0b\uff1a 1. RVC web\u9875\u9762\u56de\u5230\u6a21\u578b\u63a8\u7406\u9875\u9762\uff0c\u70b9\u51fb\u300a\u5237\u65b0\u97f3\u8272\u5217\u8868\u548c\u7d22\u5f15\u8def\u5f84\u300b\uff0c\u53bb\u52a0\u8f7d\u521a\u624d\u8bad\u7ec3\u5b8c\u6210\u7684\u6a21\u578b\u3002 \u9009\u62e9\u6211\u4eec\u521a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u8bbe\u7f6e\u5f85\u5904\u7406\u97f3\u9891\u6587\u4ef6\u8def\u5f84\uff0c\u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5355\u6b21\u63a8\u7406\u5bf9\u5e94\u7684\u8def\u5f84\u8981\u5230\u6587\u4ef6\u540d\u79f0\uff0c \u591a\u6b21\u63a8\u7406\u8bbe\u7f6e\u5230\u76ee\u5f55\u5373\u53ef\uff0c\u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u5f85\u5904\u7406\u97f3\u9891\u7684\u8def\u5f84\uff0c\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e00\u6837\uff0c\u6587\u4ef6\u8981\u5148\u4e0a\u4f20\u5230Pod\u5bb9\u5668\u4e2d\uff0c\u5177\u4f53\u89c1 \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6b65\u9aa43\u3002 \u70b9\u51fb\u8f6c\u6362\uff0c\u5c31\u4f1a\u5f00\u59cb\u5c06\u5f85\u5904\u7406\u7684\u97f3\u9891\u8fdb\u884c\u53d8\u58f0\uff0c\u53d8\u58f0\u5b8c\u6210\u540e\uff0c\u8f93\u51fa\u97f3\u9891\u4f1a\u6709\u5bf9\u5e94\u7684\u97f3\u9891\uff0c\u53ef\u4ee5\u76f4\u63a5\u64ad\u653e\uff0c\u4e5f\u53ef\u4ee5\u8fdb\u884c\u4e0b\u8f7d\u3002 \u8fd9\u4e2a\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u5931\u8d25\uff0c\u5931\u8d25\u540e\u91cd\u8bd5\u5373\u53ef\u3002","title":"\u63a8\u7406\u6559\u7a0b"},{"location":"template/llm-instructions-en/","text":"\ud83d\udcd6 User Guide \ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services.","title":"Llm instructions en"},{"location":"template/llm-instructions-en/#user-guide","text":"\ud83d\udca1 Quick Start After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses, and ApiKey.","title":"\ud83d\udcd6 User Guide"},{"location":"template/llm-instructions-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"template/llm-instructions-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"Write a letter to my daughter from the future year 2035, telling her to study technology well, become the master of technology, and promote technological and economic development; she is currently in 3rd grade\" } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"template/llm-instructions-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code: from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, please introduce yourself in as much detail as possible.\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK Call"},{"location":"template/llm-instructions-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"template/llm-instructions/","text":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\u3002","title":"Llm instructions"},{"location":"template/llm-instructions/#_1","text":"\ud83d\udca1 \u5feb\u901f\u5f00\u59cb \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\u3001Web\u5e94\u7528\u5730\u5740\u548c ApiKey\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"template/llm-instructions/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"template/llm-instructions/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"user\", \"content\": \"\u7ed9\u95fa\u5973\u5199\u4e00\u4efd\u6765\u81ea\u672a\u67652035\u7684\u4fe1\uff0c\u540c\u65f6\u544a\u8bc9\u5979\u8981\u597d\u597d\u5b66\u4e60\u79d1\u6280\uff0c\u505a\u79d1\u6280\u7684\u4e3b\u4eba\uff0c\u63a8\u52a8\u79d1\u6280\uff0c\u7ecf\u6d4e\u53d1\u5c55\uff1b\u5979\u73b0\u5728\u662f3\u5e74\u7ea7\" } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"template/llm-instructions/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff1a from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id print(model) def main(): stream = True chat_completion = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f60\u597d\uff0c\u4ecb\u7ecd\u4e00\u4e0b\u4f60\u81ea\u5df1\uff0c\u8d8a\u8be6\u7ec6\u8d8a\u597d\u3002\", } ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion.choices[0].message.content print(result) if __name__ == \"__main__\": main()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"template/llm-instructions/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"},{"location":"template/mllm-instructions-en/","text":"\ud83d\udcd6 User Guide \ud83d\udca1 After Deployment After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses (available after enabling public network access), and Api_Key. \ud83d\udd0c API Call Methods \ud83d\udda5\ufe0f Curl Command Call \ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name \ud83d\uddbc\ufe0f Image Format Support The image_url parameter supports two formats: \u2022 HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 Encoding : data:image/jpeg;base64,<base64 encoded image format> Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image format>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }' \ud83d\udc0d Python SDK Call \u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code that supports image and video processing: import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Answer in Chinese, what number should be in the box in the image?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Please describe the video content\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video() \ud83c\udf10 Web Application Access #### \ud83d\udcf1 Access Steps \ud83d\udd17 Step 1: Get Access Link On the service instance overview page, click the link corresponding to the Web application to directly access the model service Web interface. \ud83d\udcac Step 2: Start Conversation Enter your question in the input box on the model service Web page to start conversing with the large language model. #### \ud83d\uddbc\ufe0f Interface Display \ud83d\udca1 Access Tips Find the link corresponding to the Web application on the service instance overview page and click it to directly access the model service Web interface. \u2705 Usage Instructions Enter your questions or requirements in the input box, and the system will respond in real-time and provide corresponding model services.","title":"Mllm instructions en"},{"location":"template/mllm-instructions-en/#user-guide","text":"\ud83d\udca1 After Deployment After completing the model deployment, you can view the model usage instructions on the Computing Nest service instance overview page, which provides API call examples, internal network access addresses, public network access addresses (available after enabling public network access), and Api_Key.","title":"\ud83d\udcd6 User Guide"},{"location":"template/mllm-instructions-en/#api-call-methods","text":"","title":"\ud83d\udd0c API Call Methods"},{"location":"template/mllm-instructions-en/#curl-command-call","text":"\ud83d\udccb Parameter Description \u2022 ${ServerIP} : IP address from internal or public network address \u2022 ${ApiKey} : ApiKey provided on the page \u2022 ${ModelName} : Model name \ud83d\uddbc\ufe0f Image Format Support The image_url parameter supports two formats: \u2022 HTTP URL : e.g., https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 Encoding : data:image/jpeg;base64,<base64 encoded image format> Curl command calls can directly use the API call examples from the service instance overview page. The specific structure for calling the model API is as follows: curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<base64 encoded image format>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }'","title":"\ud83d\udda5\ufe0f Curl Command Call"},{"location":"template/mllm-instructions-en/#python-sdk-call","text":"\u2699\ufe0f Configuration Instructions \u2022 ${ApiKey} : Fill in the ApiKey from the page \u2022 ${ServerUrl} : Fill in the public or internal network address from the page, must include /v1 The following is Python example code that supports image and video processing: import base64 import requests from openai import OpenAI ##### API Configuration ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Answer in Chinese, what number should be in the box in the image?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Please describe the video content\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video()","title":"\ud83d\udc0d Python SDK Call"},{"location":"template/mllm-instructions-en/#web-application-access","text":"#### \ud83d\udcf1 Access Steps","title":"\ud83c\udf10 Web Application Access"},{"location":"template/mllm-instructions/","text":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e \ud83d\udca1 \u90e8\u7f72\u5b8c\u6210\u540e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\uff08\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u540e\u4f1a\u6709\uff09\u548c Api_Key\u3002 \ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f \ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528 \ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \ud83d\uddbc\ufe0f \u56fe\u7247\u683c\u5f0f\u652f\u6301 image_url \u53c2\u6570\u652f\u6301\u4e24\u79cd\u683c\u5f0f\uff1a \u2022 HTTP URL \uff1a\u5982 https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f> Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }' \ud83d\udc0d Python SDK \u8c03\u7528 \u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff1a import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f7f\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u56fe\u4e2d\u65b9\u6846\u5904\u5e94\u8be5\u662f\u6570\u5b57\u591a\u5c11?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"\u8bf7\u63cf\u8ff0\u4e0b\u89c6\u9891\u5185\u5bb9\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video() \ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee #### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4 \ud83d\udd17 \u6b65\u9aa4\u4e00\uff1a\u83b7\u53d6\u8bbf\u95ee\u94fe\u63a5 \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\uff0c\u70b9\u51fb Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u5373\u53ef\u76f4\u63a5\u8fdb\u884c\u6a21\u578b\u670d\u52a1 Web \u8bbf\u95ee\u3002 \ud83d\udcac \u6b65\u9aa4\u4e8c\uff1a\u5f00\u59cb\u5bf9\u8bdd \u5728\u6a21\u578b\u670d\u52a1 Web \u9875\u9762\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u95ee\u9898\uff0c\u5c31\u53ef\u4ee5\u548c\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u8bdd\u4e86\u3002 #### \ud83d\uddbc\ufe0f \u754c\u9762\u5c55\u793a \ud83d\udca1 \u8bbf\u95ee\u63d0\u793a \u5728\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u627e\u5230 Web \u5e94\u7528\u5bf9\u5e94\u7684\u94fe\u63a5\uff0c\u70b9\u51fb\u5373\u53ef\u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u670d\u52a1\u7684 Web \u754c\u9762\u3002 \u2705 \u4f7f\u7528\u8bf4\u660e \u5728\u8f93\u5165\u6846\u4e2d\u8f93\u5165\u60a8\u7684\u95ee\u9898\u6216\u9700\u6c42\uff0c\u7cfb\u7edf\u5c06\u5b9e\u65f6\u54cd\u5e94\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u6a21\u578b\u670d\u52a1\uff0c\u5bf9\u4e8e\u591a\u6a21\u6001\u9700\u8981\u8f93\u5165\u56fe\u7247\u7684\u573a\u666f\uff0c\u53ef\u4ee5\u5728\u8f93\u5165\u6846\u53f3\u4e0b\u89d2\u9009\u62e9\u56fe\u7247\u8fdb\u884c\u4e0a\u4f20\u3002","title":"Mllm instructions"},{"location":"template/mllm-instructions/#_1","text":"\ud83d\udca1 \u90e8\u7f72\u5b8c\u6210\u540e \u5728\u5b8c\u6210\u6a21\u578b\u90e8\u7f72\u540e\uff0c\u53ef\u4ee5\u5728\u8ba1\u7b97\u5de2\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u770b\u5230\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u91cc\u9762\u63d0\u4f9b\u4e86 API \u8c03\u7528\u793a\u4f8b\u3001\u5185\u7f51\u8bbf\u95ee\u5730\u5740\u3001\u516c\u7f51\u8bbf\u95ee\u5730\u5740\uff08\u5f00\u542f\u516c\u7f51\u8bbf\u95ee\u540e\u4f1a\u6709\uff09\u548c Api_Key\u3002","title":"\ud83d\udcd6 \u4f7f\u7528\u8bf4\u660e"},{"location":"template/mllm-instructions/#api","text":"","title":"\ud83d\udd0c API \u8c03\u7528\u65b9\u5f0f"},{"location":"template/mllm-instructions/#curl","text":"\ud83d\udccb \u53c2\u6570\u8bf4\u660e \u2022 ${ServerIP} \uff1a\u5185\u7f51\u5730\u5740\u6216\u516c\u7f51\u5730\u5740\u4e2d\u7684 IP \u5730\u5740 \u2022 ${ApiKey} \uff1a\u9875\u9762\u63d0\u4f9b\u7684 ApiKey \u2022 ${ModelName} \uff1a\u6a21\u578b\u540d\u79f0 \ud83d\uddbc\ufe0f \u56fe\u7247\u683c\u5f0f\u652f\u6301 image_url \u53c2\u6570\u652f\u6301\u4e24\u79cd\u683c\u5f0f\uff1a \u2022 HTTP URL \uff1a\u5982 https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png \u2022 Base64 \u7f16\u7801 \uff1a data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f> Curl \u547d\u4ee4\u8c03\u7528\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u670d\u52a1\u5b9e\u4f8b\u6982\u89c8\u9875\u9762\u4e2d\u7684 API \u8c03\u7528\u793a\u4f8b\uff0c\u8c03\u7528\u6a21\u578b API \u7684\u5177\u4f53\u7ed3\u6784\u5982\u4e0b\uff1a curl -X Post http://${ServerIP}:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: ${ApiKey}\" \\ -d '{ \"model\": \"${ModelName}\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/jpeg;base64,<\u56fe\u7247\u7684 base64 \u7f16\u7801\u683c\u5f0f>\" } }, { \"type\": \"text\", \"text\": \"How many sheep are there in the picture?\" } ] } ] }'","title":"\ud83d\udda5\ufe0f Curl \u547d\u4ee4\u8c03\u7528"},{"location":"template/mllm-instructions/#python-sdk","text":"\u2699\ufe0f \u914d\u7f6e\u8bf4\u660e \u2022 ${ApiKey} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684 ApiKey \u2022 ${ServerUrl} \uff1a\u586b\u5199\u9875\u9762\u4e0a\u7684\u516c\u7f51\u5730\u5740\u6216\u5185\u7f51\u5730\u5740\uff0c\u9700\u8981\u5e26\u4e0a /v1 \u4ee5\u4e0b\u4e3a Python \u793a\u4f8b\u4ee3\u7801\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff1a import base64 import requests from openai import OpenAI ##### API \u914d\u7f6e ##### openai_api_key = \"${ApiKey}\" openai_api_base = \"${ServerUrl}\" client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) models = client.models.list() model = models.data[0].id def encode_base64_content_from_url(content_url: str) -> str: \"\"\"Encode a content retrieved from a remote url to base64 format.\"\"\" with requests.get(content_url) as response: response.raise_for_status() result = base64.b64encode(response.content).decode(\"utf-8\") return result def infer_image(): image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\" stream = True image_base64 = encode_base64_content_from_url(image_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"\u4f7f\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u56fe\u4e2d\u65b9\u6846\u5904\u5e94\u8be5\u662f\u6570\u5b57\u591a\u5c11?\", }, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, }, ], } ], model=model, max_completion_tokens=1024, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) def infer_video(): video_url = \"https://pai-quickstart-predeploy-hangzhou.oss-cn-hangzhou.aliyuncs.com/modelscope/algorithms/ms-swift/video_demo.mp4\" stream = True video_base64 = encode_base64_content_from_url(video_url) chat_completion_from_base64 = client.chat.completions.create( messages=[ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"\u8bf7\u63cf\u8ff0\u4e0b\u89c6\u9891\u5185\u5bb9\"}, { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, }, ], } ], model=model, max_completion_tokens=512, stream=stream, ) if stream: for chunk in chat_completion_from_base64: print(chunk.choices[0].delta.content, end=\"\") else: result = chat_completion_from_base64.choices[0].message.content print(result) if __name__ == \"__main__\": infer_image() infer_video()","title":"\ud83d\udc0d Python SDK \u8c03\u7528"},{"location":"template/mllm-instructions/#web","text":"#### \ud83d\udcf1 \u8bbf\u95ee\u6b65\u9aa4","title":"\ud83c\udf10 Web \u5e94\u7528\u8bbf\u95ee"}]}