## 简介
DeepSeek-R1 是一款由杭州深度求索公司研发的大型语言模型（LLM），专门针对数学、代码和逻辑推理等任务进行了优化。它采用了混合专家模型（MoE）和多头潜注意力（MLA）等先进技术，拥有6710亿参数，并支持长达128,000 token的输入上下文. DeepSeek-R1 的目标是达到或超越OpenAI 的o1 模型在推理任务上的性能。

- 推理能力强劲：DeepSeek-R1 在数学、代码生成、自然语言推理等任务上表现突出，甚至超越了同类型模型。

- MoE架构：采用了混合专家模型，通过在每层中使用大量的专家来处理不同的输入，从而提高推理能力。

- 支持长上下文：可以处理更长的输入上下文（128,000 token），这对于复杂的推理任务至关重要。

- 完全开源：深度求索公司将DeepSeek-R1 的训练技术和模型权重完全开源，使开发者能够进行进一步的探索和研究。

- 开源蒸馏模型：DeepSeek R1 通过蒸馏技术，生成了6个小模型（如Qwen2.5和Llama3.1）供社区使用。


{% include './docs/template/llm-base.md' %}
