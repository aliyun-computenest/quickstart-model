## 简介
DeepSeek-V3，是一个强大的专家混合（MoE）语言模型，总参数为 6710 亿，其中每个 token 激活 370 亿个参数。为了实现高效的推理和经济的训练成本，DeepSeek-V3 采用了多头潜在注意力（MLA）和 DeepSeekMoE 架构，这些架构在 DeepSeek-V2 中已经经过彻底验证。此外，DeepSeek-V3 率先采用了一种无辅助损失的负载平衡策略，并设定了多 token 预测的训练目标以提升性能。DeepSeek 在 14.8 万亿种多样且高质量的 token 上对 DeepSeek-V3 进行预训练，随后进行监督微调和强化学习阶段，以充分发挥其能力。全面评估表明，DeepSeek-V3 优于其他开源模型，并实现了与领先的闭源模型相当的性能。并且 DeepSeek-V3 仅需 2.788 百万 H800 GPU 小时即可完成全部训练。此外，其训练过程非常稳定。在整个训练过程中，没有遇到任何不可恢复的损失峰值，也未进行任何回滚操作。


{% include './docs/template/llm-instructions.md' %}

