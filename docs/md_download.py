#!/usr/bin/env python3
"""
Markdownå›¾ç‰‡ä¸‹è½½å™¨ - ç®€åŒ–ç‰ˆ
è‡ªåŠ¨ä»å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰MDæ–‡ä»¶ä¸­è¯†åˆ«å›¾ç‰‡é“¾æ¥ï¼Œä¸‹è½½åˆ°ä¸MDæ–‡ä»¶ç›¸åŒçš„ç›®å½•ä¸‹å¹¶æ›¿æ¢å¼•ç”¨
ä¿æŒåŸå§‹æ–‡ä»¶å†…å®¹ä¸å˜
æ”¯æŒPython 3.10+
"""

import os
import re
import requests
import hashlib
import time
from pathlib import Path
from urllib.parse import urlparse, unquote
from typing import List, Tuple, Set, Dict
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('md_image_downloader.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MarkdownImageDownloader:
    def __init__(self,
                 images_subdir: str = "images",
                 timeout: int = 30,
                 max_retries: int = 3,
                 delay_between_downloads: float = 1.0):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            images_subdir: å›¾ç‰‡å­ç›®å½•åç§°ï¼ˆåœ¨æ¯ä¸ªMDæ–‡ä»¶ç›®å½•ä¸‹åˆ›å»ºï¼‰
            timeout: ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay_between_downloads: ä¸‹è½½é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.images_subdir = images_subdir
        self.timeout = timeout
        self.max_retries = max_retries
        self.delay_between_downloads = delay_between_downloads
        self.downloaded_urls: Dict[str, str] = {}  # url -> local_path æ˜ å°„

        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        self.image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg', '.ico', '.tiff', '.tif'}

        # åˆ›å»ºä¼šè¯å¹¶é…ç½®é‡è¯•ç­–ç•¥
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'image',
            'Sec-Fetch-Mode': 'no-cors',
            'Sec-Fetch-Site': 'cross-site',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })

    def find_markdown_files(self, directory: str = ".") -> List[Path]:
        """æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰çš„Markdownæ–‡ä»¶"""
        md_files = []
        for file_path in Path(directory).rglob("*.md"):
            if file_path.is_file():
                md_files.append(file_path)

        logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")
        return md_files

    def extract_image_urls(self, content: str) -> List[Tuple[str, str, str]]:
        """
        ä»Markdownå†…å®¹ä¸­æå–å›¾ç‰‡URL

        Returns:
            List of (original_match, alt_text, url)
        """
        image_urls = []

        # Markdownæ ¼å¼: ![alt](url)
        markdown_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        for match in re.finditer(markdown_pattern, content):
            original_match = match.group(0)
            alt_text = match.group(1)
            url = match.group(2).strip()

            if self.is_remote_image_url(url):
                image_urls.append((original_match, alt_text, url))

        # HTML imgæ ‡ç­¾æ ¼å¼
        html_patterns = [
            r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>',
            r'<img[^>]+src=([^\s>]+)[^>]*>'
        ]

        for pattern in html_patterns:
            for match in re.finditer(pattern, content, re.IGNORECASE):
                original_match = match.group(0)
                url = match.group(1).strip().strip('"\'')

                if self.is_remote_image_url(url):
                    # å°è¯•ä»imgæ ‡ç­¾ä¸­æå–altå±æ€§
                    alt_match = re.search(r'alt=["\']([^"\']*)["\']', original_match, re.IGNORECASE)
                    alt_text = alt_match.group(1) if alt_match else ""

                    image_urls.append((original_match, alt_text, url))

        return image_urls

    def is_remote_image_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¿œç¨‹å›¾ç‰‡URL"""
        if not url:
            return False

        # è·³è¿‡æœ¬åœ°è·¯å¾„
        if not url.startswith(('http://', 'https://')):
            return False

        # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡URLï¼ˆé€šè¿‡æ‰©å±•åæˆ–å¸¸è§å›¾ç‰‡æœåŠ¡ï¼‰
        parsed_url = urlparse(url.lower())
        path = parsed_url.path

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if any(path.endswith(ext) for ext in self.image_extensions):
            return True

        # æ£€æŸ¥å¸¸è§å›¾ç‰‡æœåŠ¡åŸŸå
        image_domains = [
            'imgur.com', 'i.imgur.com',
            'github.com', 'raw.githubusercontent.com',
            'cdn.', 'img.', 'image.',
            'unsplash.com', 'pixabay.com',
            'pexels.com', 'freepik.com',
            'githubusercontent.com'
        ]

        if any(domain in parsed_url.netloc for domain in image_domains):
            return True

        return False

    def get_file_extension_from_url(self, url: str, content_type: str = None) -> str:
        """ä»URLæˆ–Content-Typeè·å–æ–‡ä»¶æ‰©å±•å"""
        # é¦–å…ˆå°è¯•ä»URLè·å–æ‰©å±•å
        parsed_url = urlparse(url)
        path = unquote(parsed_url.path)
        _, ext = os.path.splitext(path)

        if ext.lower() in self.image_extensions:
            return ext.lower()

        # å¦‚æœURLæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•ä»Content-Typeæ¨æ–­
        if content_type:
            content_type = content_type.lower().split(';')[0].strip()
            ext_map = {
                'image/jpeg': '.jpg',
                'image/jpg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'image/bmp': '.bmp',
                'image/webp': '.webp',
                'image/svg+xml': '.svg',
                'image/x-icon': '.ico',
                'image/tiff': '.tiff',
                'image/tif': '.tif',
                'text/xml': '.svg',
                'application/xml': '.svg'
            }
            return ext_map.get(content_type, '.jpg')

        return '.jpg'  # é»˜è®¤æ‰©å±•å

    def generate_filename(self, url: str, alt_text: str = "", content_type: str = None) -> str:
        """ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å"""
        # è·å–æ‰©å±•å
        ext = self.get_file_extension_from_url(url, content_type)

        # ç”ŸæˆåŸºäºURLçš„å“ˆå¸Œå€¼ä½œä¸ºæ–‡ä»¶å
        url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()[:12]

        # å¦‚æœæœ‰altæ–‡æœ¬ï¼Œå°è¯•ä½¿ç”¨å®ƒä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
        if alt_text:
            # æ¸…ç†altæ–‡æœ¬ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œä¸­æ–‡
            clean_alt = re.sub(r'[^\w\u4e00-\u9fff]', '_', alt_text)[:30]
            clean_alt = re.sub(r'_+', '_', clean_alt).strip('_')
            if clean_alt:
                filename = f"{clean_alt}_{url_hash}{ext}"
            else:
                filename = f"image_{url_hash}{ext}"
        else:
            filename = f"image_{url_hash}{ext}"

        return filename

    def get_image_dir_for_md(self, md_file_path: Path) -> Path:
        """è·å–MDæ–‡ä»¶å¯¹åº”çš„å›¾ç‰‡ç›®å½•"""
        return md_file_path.parent / self.images_subdir

    def download_image(self, url: str, target_path: Path) -> bool:
        """ä¸‹è½½å›¾ç‰‡åˆ°æŒ‡å®šè·¯å¾„ - ä¿æŒåŸå§‹å†…å®¹ä¸å˜"""
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡è¿™ä¸ªURLåˆ°ç›¸åŒä½ç½®
        if url in self.downloaded_urls and Path(self.downloaded_urls[url]).exists():
            logger.info(f"å›¾ç‰‡å·²ä¸‹è½½è¿‡ï¼Œè·³è¿‡: {url}")
            return True

        # åˆ›å»ºç›®æ ‡ç›®å½•
        target_path.parent.mkdir(parents=True, exist_ok=True)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
        if target_path.exists():
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {target_path}")
            self.downloaded_urls[url] = str(target_path)
            return True

        for attempt in range(self.max_retries):
            try:
                logger.info(f"æ­£åœ¨ä¸‹è½½ ({attempt + 1}/{self.max_retries}): {url}")
                logger.info(f"ä¿å­˜åˆ°: {target_path}")

                # ä½¿ç”¨ä¼šè¯å‘é€è¯·æ±‚
                response = self.session.get(
                    url,
                    timeout=self.timeout,
                    allow_redirects=True
                )
                response.raise_for_status()

                # è·å–å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '').lower().split(';')[0].strip()
                logger.debug(f"Content-Type: {content_type}")

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    logger.error(f"HTTPçŠ¶æ€ç é”™è¯¯: {response.status_code}")
                    continue

                # æ£€æŸ¥å†…å®¹é•¿åº¦
                content_length = len(response.content)
                if content_length == 0:
                    logger.error(f"ä¸‹è½½å†…å®¹ä¸ºç©º: {url}")
                    continue

                # æ ¹æ®å®é™…å†…å®¹ç±»å‹è°ƒæ•´æ–‡ä»¶æ‰©å±•å
                correct_ext = self.get_file_extension_from_url(url, content_type)
                if not target_path.name.endswith(correct_ext):
                    # æ›´æ–°ç›®æ ‡è·¯å¾„çš„æ‰©å±•å
                    new_target_path = target_path.with_suffix(correct_ext)
                    if new_target_path != target_path:
                        logger.info(f"æ ¹æ®Content-Typeè°ƒæ•´æ–‡ä»¶æ‰©å±•å: {target_path.name} -> {new_target_path.name}")
                        target_path = new_target_path

                # ç›´æ¥ä¿å­˜åŸå§‹å“åº”å†…å®¹ï¼Œä¸åšä»»ä½•å¤„ç†
                with open(target_path, 'wb') as f:
                    f.write(response.content)

                # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
                if target_path.exists() and target_path.stat().st_size > 0:
                    file_size = target_path.stat().st_size
                    logger.info(f"ä¸‹è½½æˆåŠŸ: {target_path.name} ({file_size} bytes)")

                    # ç®€å•éªŒè¯ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆç†
                    if file_size < 50:  # å°äº50å­—èŠ‚å¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                        logger.warning(f"æ–‡ä»¶å¤§å°å¼‚å¸¸å°ï¼Œå¯èƒ½ä¸‹è½½å¤±è´¥: {file_size} bytes")
                        # è¯»å–æ–‡ä»¶å†…å®¹æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯ä¿¡æ¯
                        try:
                            with open(target_path, 'r', encoding='utf-8', errors='ignore') as f:
                                content_preview = f.read(200)
                            if any(error_text in content_preview.lower() for error_text in
                                   ['404', 'not found', 'error', 'forbidden', 'access denied']):
                                logger.error(f"ä¸‹è½½çš„æ˜¯é”™è¯¯é¡µé¢ï¼Œåˆ é™¤æ–‡ä»¶: {target_path}")
                                target_path.unlink()
                                continue
                        except:
                            pass

                    self.downloaded_urls[url] = str(target_path)

                    # ä¸‹è½½é—´éš”
                    if self.delay_between_downloads > 0:
                        time.sleep(self.delay_between_downloads)

                    return True
                else:
                    logger.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {target_path}")
                    continue

            except requests.exceptions.RequestException as e:
                logger.warning(f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

            except Exception as e:
                logger.error(f"ä¸‹è½½æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {url} - {str(e)}")
                break

        return False

    def process_markdown_file(self, file_path: Path) -> bool:
        """å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶"""
        logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")

        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–å›¾ç‰‡URL
            image_urls = self.extract_image_urls(content)

            if not image_urls:
                logger.info(f"æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°è¿œç¨‹å›¾ç‰‡é“¾æ¥: {file_path}")
                return True

            logger.info(f"æ‰¾åˆ° {len(image_urls)} ä¸ªå›¾ç‰‡é“¾æ¥")

            # è·å–è¯¥MDæ–‡ä»¶å¯¹åº”çš„å›¾ç‰‡ç›®å½•
            images_dir = self.get_image_dir_for_md(file_path)

            # è®°å½•æ›¿æ¢ä¿¡æ¯
            replacements = []
            modified_content = content

            # ä¸‹è½½å›¾ç‰‡å¹¶è®°å½•æ›¿æ¢ä¿¡æ¯
            for original_match, alt_text, url in image_urls:
                # å…ˆå°è¯•è·å–å†…å®¹ç±»å‹æ¥ç”Ÿæˆæ­£ç¡®çš„æ–‡ä»¶å
                try:
                    head_response = self.session.head(url, timeout=10, allow_redirects=True)
                    content_type = head_response.headers.get('content-type', '')
                except:
                    content_type = None

                filename = self.generate_filename(url, alt_text, content_type)
                target_path = images_dir / filename

                if self.download_image(url, target_path):
                    # ä½¿ç”¨å®é™…ä¿å­˜çš„æ–‡ä»¶åï¼ˆå¯èƒ½å› ä¸ºæ‰©å±•åè°ƒæ•´è€Œæ”¹å˜ï¼‰
                    actual_filename = target_path.name
                    relative_path = f"{self.images_subdir}/{actual_filename}"

                    # æ ¹æ®åŸå§‹æ ¼å¼ç”Ÿæˆæ–°çš„å¼•ç”¨
                    if original_match.startswith('!['):
                        # Markdownæ ¼å¼
                        new_reference = f"![{alt_text}]({relative_path})"
                    else:
                        # HTMLæ ¼å¼ï¼Œä¿æŒåŸæœ‰å±æ€§ä½†æ›¿æ¢src
                        new_reference = re.sub(
                            r'src=["\']?[^"\'>\s]+["\']?',
                            f'src="{relative_path}"',
                            original_match,
                            flags=re.IGNORECASE
                        )

                    replacements.append((original_match, new_reference, url))
                else:
                    logger.error(f"ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡æ›¿æ¢: {url}")

            # æ‰§è¡Œæ›¿æ¢
            if replacements:
                for original, new_ref, url in replacements:
                    modified_content = modified_content.replace(original, new_ref)
                    logger.info(f"æ›¿æ¢: {url} -> {new_ref}")

                # å¤‡ä»½åŸæ–‡ä»¶
                backup_path = file_path.with_suffix(f"{file_path.suffix}.backup")
                if not backup_path.exists():
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    logger.info(f"åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_path}")

                # å†™å…¥ä¿®æ”¹åçš„å†…å®¹
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(modified_content)

                logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path} (æ›¿æ¢äº† {len(replacements)} ä¸ªå›¾ç‰‡é“¾æ¥)")
                logger.info(f"å›¾ç‰‡ä¿å­˜åœ¨: {images_dir}")

            return True

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ {file_path}: {str(e)}")
            return False

    def run(self, directory: str = ".") -> None:
        """è¿è¡Œä¸»ç¨‹åº"""
        logger.info("å¼€å§‹å¤„ç†Markdownæ–‡ä»¶ä¸­çš„å›¾ç‰‡é“¾æ¥...")

        # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
        md_files = self.find_markdown_files(directory)

        if not md_files:
            logger.info("æ²¡æœ‰æ‰¾åˆ°Markdownæ–‡ä»¶")
            return

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        success_count = 0

        for md_file in md_files:
            if self.process_markdown_file(md_file):
                success_count += 1

        logger.info(f"å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç† {success_count}/{len(md_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"æ€»å…±ä¸‹è½½äº† {len(self.downloaded_urls)} ä¸ªå›¾ç‰‡")

        # æ˜¾ç¤ºæ¯ä¸ªç›®å½•çš„å›¾ç‰‡ç»Ÿè®¡
        dir_stats = {}
        for url, local_path in self.downloaded_urls.items():
            dir_name = Path(local_path).parent
            if dir_name not in dir_stats:
                dir_stats[dir_name] = 0
            dir_stats[dir_name] += 1

        logger.info("å„ç›®å½•å›¾ç‰‡ç»Ÿè®¡:")
        for dir_path, count in dir_stats.items():
            logger.info(f"  {dir_path}: {count} ä¸ªå›¾ç‰‡")

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    config = {
        'images_subdir': 'images',            # å›¾ç‰‡å­ç›®å½•åç§°
        'timeout': 30,                        # ä¸‹è½½è¶…æ—¶æ—¶é—´
        'max_retries': 3,                     # æœ€å¤§é‡è¯•æ¬¡æ•°
        'delay_between_downloads': 1.0,       # ä¸‹è½½é—´éš”æ—¶é—´
    }

    print("ğŸ¨ Markdownå›¾ç‰‡ä¸‹è½½å™¨ - ç®€åŒ–ç‰ˆ")
    print("=" * 50)
    print(f"ğŸ“ å›¾ç‰‡å°†ä¿å­˜åˆ°æ¯ä¸ªMDæ–‡ä»¶ç›®å½•ä¸‹çš„ '{config['images_subdir']}' å­ç›®å½•ä¸­")
    print(f"â±ï¸  ä¸‹è½½è¶…æ—¶: {config['timeout']}ç§’")
    print(f"ğŸ”„ æœ€å¤§é‡è¯•: {config['max_retries']}æ¬¡")
    print(f"â³ ä¸‹è½½é—´éš”: {config['delay_between_downloads']}ç§’")
    print("ğŸ”§ ç®€åŒ–ç­–ç•¥:")
    print("   - ä¿æŒåŸå§‹æ–‡ä»¶å†…å®¹ä¸å˜")
    print("   - ä¸è¿›è¡Œä»»ä½•å†…å®¹å¤„ç†æˆ–éªŒè¯")
    print("   - ç›´æ¥ä¿å­˜å“åº”å†…å®¹")
    print("=" * 50)

    # åˆ›å»ºä¸‹è½½å™¨å®ä¾‹
    downloader = MarkdownImageDownloader(**config)

    # è¿è¡Œå¤„ç†
    try:
        downloader.run()
        print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        print("\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {str(e)}")


if __name__ == "__main__":
    main()
